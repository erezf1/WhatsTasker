# WhatsTasker Project Code Dump (v0.8 Target - Browser Chat)
# Generated: 2025-05-06 14:22:51


================================================================================
ðŸ“„ README.md
================================================================================

# --- START OF FILE README.md ---



# --- END OF FILE README.md ---



================================================================================
ðŸ“„ WhatsTasker_PRD_08.txt
================================================================================

# --- START OF FILE WhatsTasker_PRD_08.txt ---

ï»¿WhatsTasker: Product Requirements (v0.8 - Orchestrator Focused)
ðŸ§­ Overview
WhatsTasker is a personal productivity assistant designed for individuals seeking to manage their tasks, improve focus, and reduce procrastination through a conversational interface. Accessed via WhatsApp, it functions as an intelligent Time Management Expert, integrating directly with the user's primary calendar (initially Google Calendar) and utilizing an internal metadata store for enhanced tracking. WhatsTasker aims to streamline daily planning, facilitate frictionless task capture, and support helpful routines.
ðŸŒŸ Core System Goal
The system shall act as a proactive and intelligent Time Management Expert. It must understand user context (history, preferences, schedule, tasks), interpret requests accurately, facilitate effective task and schedule management, and anticipate user needs where feasible within the defined scope, moving beyond simple command execution.
ðŸ”‘ Core Functional Requirements
1. Multi-User Support & Identification:
   * Uniquely identify users via their WhatsApp phone number (user_id).
   * Maintain consistent user_id linkage across all system components.
2. Calendar-Based Task & Reminder Management:
   * Direct Calendar Integration: Synchronize confirmed tasks and scheduled events with the user's designated primary calendar (initially Google Calendar).
   * Metadata Augmentation: Maintain an internal metadata store linked to calendar events/internal IDs to track additional details crucial for system operation (e.g., item status [pending, in progress, completed, cancelled], type [task, reminder], estimated effort, project tags, timestamps).
3. Natural Language Interaction (via WhatsApp):
   * Conversational Interface: Enable users to interact using free-form, natural language commands and requests.
   * Contextual Understanding: Maintain and utilize conversational context (recent history, user state) to understand follow-up questions, resolve ambiguities, and provide relevant, coherent responses.
4. User Capabilities & Outcomes:
   * Onboarding: Guide new users through an initial setup process to configure essential preferences (e.g., work parameters, calendar connection authorization) and activate their account.
   * Task/Reminder Capture & Classification:
      * Allow users to efficiently capture intentions (tasks, reminders) via natural language.
      * Intelligently distinguish between simple Reminders (typically requiring only description and time) and effortful Tasks (implying duration/effort and eligibility for scheduling work sessions).
      * Reliably extract key details: description, due date/time, type (task or reminder), and estimated effort (for tasks).
   *    * Task Modification: Enable users to accurately modify details of existing tasks or reminders (e.g., description, due date/time, status, estimated effort) based on their requests.
   * Task & Reminder Status Updates: Allow users to easily update the status of items (e.g., mark as completed, pending), including handling interactive replies to system-generated lists.
   * Task & Reminder Viewing & Filtering: Enable users to view their items (tasks and reminders), with options to filter by relevant criteria such as date range, status (active, completed, etc.), or an associated project tag/label.
   * Clarification Handling: Intelligently request clarification from the user when input is ambiguous, incomplete, or conflicts with existing information, guiding the user towards providing necessary details.
   * Task Scheduling Assistance (for Type: task only):
      * Proactively offer to find and schedule dedicated work time in the calendar for newly created Tasks (unless scheduling was part of the initial request).
      * Upon user request, propose suitable, potentially distributed, time slots for Tasks requiring work sessions, considering task requirements, user preferences, and calendar availability.
      * Allow users to easily confirm and book proposed work session slots into their calendar.
5. Automated Routines & Summaries:
   * Morning Check-in: Provide a concise overview of the user's upcoming day, including scheduled calendar events, due tasks, and reminders. Goal: Enhance daily awareness and readiness.
   * Evening Review: Facilitate a quick end-of-day review:
      * Present Tasks and Reminders scheduled for or due that day.
      * Allow users to easily update the completion status of these items.
      * For Tasks identified as incomplete, prompt the user (optionally asking for a reason) and proactively offer to reschedule the task or its remaining work, potentially suggesting new time slots. Goal: Prevent tasks from being dropped and adapt planning.
   * 6. Fallback Operation:
   * Support basic task and reminder management (capture, status updates, viewing lists) even without calendar integration. Smart scheduling proposals, calendar-based summaries, and rescheduling offers will be unavailable in this mode.
ðŸ’¡ Future Capabilities (Beyond Initial Scope)
* Advanced Scheduling Intelligence (Optimal times, breaks, conflict handling)
* Proactive Time Management Advice & Goal Tracking
* Weekly Reflection Module (Structured prompts, insight storage)
* Recurring Tasks/Reminders Support
* Advanced Daily Planning Assistance ("Just One Thing", backlog suggestions)
* Timed Pre-Event Notifications (Requires external scheduler)
* Shared/Team Task Collaboration Features
* Expanded Calendar Integrations (Outlook, etc.)
* User Subscription Tiers & Monetization
âš ï¸ MVP Scope & Limitations (Initial Release)
* Calendar: Integration only with Google Calendar; one active calendar per user.
* Task Types: No built-in support for recurring tasks/reminders.
* Collaboration: No shared tasks or team features.
* Proactivity: Limited to offering scheduling for new tasks and rescheduling incomplete tasks in the evening review. No broader unsolicited advice.
* Notifications: Timed pre-event notifications are out of scope.
* Reflection: Weekly reflection module is out of scope.
* Monetization: No user plans or payments.
âœ… Acceptance Criteria (High-Level)
* Users can successfully onboard, configure preferences, and connect their Google Calendar.
* The system correctly distinguishes between Tasks and Reminders based on user input.
* Users can reliably create, view (with date/status/project filters), update, and mark tasks/reminders as complete via natural language.
* The system proactively offers scheduling for newly created Tasks.
* The system can propose schedule slots for Tasks based on calendar availability and preferences, and book confirmed slots.
* The system provides functional Morning Check-in and Evening Review summaries.
* The Evening Review correctly identifies incomplete Tasks and offers rescheduling options.
* The system handles clarifications when user input is ambiguous.
* Basic reminder functionality works without calendar integration.

# --- END OF FILE WhatsTasker_PRD_08.txt ---



================================================================================
ðŸ“„ WhatsTasker_SRS_08.txt
================================================================================

# --- START OF FILE WhatsTasker_SRS_08.txt ---

ï»¿# --- START OF FILE WhatsTasker_SRS_08.txt ---
ï»¿WhatsTasker: SRS (v0.8k - WhatsApp Bridge & Active Scheduler)

**1. Goals & Principles**

*   **Primary Goal:** Create a WhatsApp-based personal productivity assistant that acts as an intelligent Time Management Expert. It should understand user context, facilitate task/schedule management via natural language, and integrate seamlessly with the user's primary calendar (Google Calendar initially).
*   **Core Architecture:**
    *   **Interface Layer:** User interaction occurs via WhatsApp. An external **Node.js bridge (`wa_bridge.js`)** utilizes `whatsapp-web.js` to connect to WhatsApp, receive messages, and send messages. This bridge communicates with the Python backend via a dedicated FastAPI interface (`bridge/whatsapp_interface.py`). An alternative CLI interface (`bridge/cli_interface.py`) exists for debugging/testing. The active interface is selected at runtime (via `main.py` using environment variables or command-line args).
    *   **Backend Entry Point:** The selected FastAPI interface (`whatsapp_interface.py` or `cli_interface.py`) receives incoming messages.
    *   **Routing Layer (`request_router.py`):** Receives messages from the active bridge API, normalizes user ID, determines user status (`new`, `onboarding`, `active`), and routes requests to the appropriate Agent or Cheat command handler.
    *   **Agent Layer (`OnboardingAgent`, `OrchestratorAgent`):** Central reasoning hubs using a **Pure LLM Control Flow**. Process user input within context, leverage Structured Tool Use (Instructor/Pydantic) to delegate actions to Tools. Manage conversational state and response generation based on LLM decisions and tool results.
    *   **Tool Layer (`tool_definitions.py`):** Defines Pydantic models for tool parameters and Python functions that act as validated interfaces to the Service Layer.
    *   **Service Layer (`TaskManager`, `TaskQueryService`, `ConfigManager`, `RoutineService`, `NotificationService`, `SyncService`, `Cheats`):** Encapsulates business logic, data manipulation, and interactions with persistent stores and external APIs (GCal). Invoked by Tools or scheduled jobs. `SyncService` provides read-only context merging.
    *   **Scheduler (`scheduler_service.py`):** An APScheduler instance runs background jobs (`RoutineService` checks, `NotificationService` checks, daily cleanup) at configured intervals.
    *   **Pure LLM Control Flow:** Agents rely **entirely on the LLM** (guided by specific system prompts) to manage conversational state, ask clarifying questions, interpret user replies, decide which tool to call (if any), and formulate responses based on history and tool results. Python code primarily executes validated tool calls requested by the LLM.
    *   **Two-Step LLM Interaction (with Tools):** Standard pattern used by Agents when a tool is invoked (LLM plans -> Tool executes -> LLM responds based on result).
*   **Modularity & Reusability:** Components are clearly separated. Services contain reusable business logic. Tools provide reliable interfaces.
*   **Reliability & Maintainability:** Prioritize reliable execution via structured Tool Use and Pydantic validation. Code includes type hinting and clear documentation. Conversational logic resides primarily within the LLM prompts. `whatsapp-web.js` dependency introduces external volatility.
*   **LLM Interaction:** Utilizes OpenAI's Tool Use capabilities via Instructor/Pydantic. `propose_task_slots` tool uses a focused LLM sub-call.
*   **Data Handling & State:**
    *   Persistence: Metadata Store (CSV/DB), User Registry (JSON/DB), Encrypted Token Store.
    *   User Status: Tracked via `status` in preferences (`new`, `onboarding`, `active`), managed by `user_registry.py` and `config_manager.py`. Dictates routing in `request_router.py`.
    *   Runtime State: Central, thread-safe `AgentStateManager` manages in-memory state (preferences, history, task context snapshot, API clients, notification tracking). Loaded/created via `user_manager.py`.
    *   Context Provision: Relevant context loaded via `AgentStateManager` and provided to the appropriate Agent. Routines use `SyncService` for context.
    *   State Updates: Services update persistent stores and signal updates to in-memory state via `AgentStateManager`.
    *   Synchronization: `SyncService` merges GCal/WT data for context snapshots; no persistent merge implemented yet.
*   **Error Handling:** Tools/Services return status/messages. Pydantic validates tool parameters. Agents interpret tool failures via LLM. Bridge interface handles basic connection/ACK errors.

**2. Architecture Overview**

1.  **External Bridge (`wa_bridge.js`):**
    *   Connects to WhatsApp using `whatsapp-web.js`.
    *   On receiving a WA message: Sends `POST` to Python backend's `/incoming` endpoint (`whatsapp_interface.py`).
    *   Periodically polls Python backend's `/outgoing` endpoint using `GET`.
    *   On receiving messages from `/outgoing`: Sends them to users via WA. Sends `POST` to Python backend's `/ack` endpoint for each successfully sent message.
2.  **Interface Layer (Python - `whatsapp_interface.py` or `cli_interface.py`):**
    *   Selected interface runs via Uvicorn/FastAPI (controlled by `main.py`).
    *   `/incoming`: Receives message from bridge/client -> Calls `request_router.handle_incoming_message` -> Returns immediate `{"ack": true}`.
    *   `/outgoing`: Returns list of messages currently in the outgoing queue (managed by the active Bridge class instance set via `request_router.set_bridge`) **without** removing them.
    *   `/ack`: Receives `message_id` -> Finds and removes the corresponding message from the outgoing queue.
3.  **Routing Layer (`request_router.py`):**
    *   Receives message details from the active interface API (`/incoming`).
    *   Normalizes `user_id`, ensures user state exists (`user_manager.get_agent`).
    *   Retrieves user status (`new`, `onboarding`, `active`).
    *   Routes based on status: Welcome message (`new`), `OnboardingAgent` (`onboarding`), `OrchestratorAgent` (`active`), or `Cheats` service (`/` commands).
4.  **Agent Layer (`onboarding_agent.py` or `orchestrator_agent.py`):**
    *   Receives user message and context snapshot. Loads system prompt. Defines available Tools.
    *   **LLM Call 1 (Planner):** Sends context, history, message, tools to LLM.
    *   LLM decides: Respond directly or call tool(s).
5.  **Execution Layer:**
    *   **If LLM Responds Directly:** Agent uses text.
    *   **If LLM Calls Tool(s):**
        *   Agent receives tool call request(s). Validates. Calls corresponding Tool function (`tool_definitions.py`).
        *   Tool interacts with **Service Layer**.
        *   Services perform logic, interact with **Data Layer** and external APIs. Update persistent data & signal memory state updates via `AgentStateManager`.
        *   Tool returns result dict (`{"success": bool, ...}`).
6.  **Agent Layer (Response Generation):**
    *   **If LLM Responded Directly:** Use that text.
    *   **If Tool(s) Called:**
        *   Agent prepares messages for **LLM Call 2 (Responder)**, including original messages, assistant's tool call(s), and `role: "tool"` message(s) with result(s).
        *   Agent sends messages to LLM.
        *   LLM generates final text response based on tool result(s) and instructions.
7.  **Response Flow:** Agent returns response string -> Router calls `send_message` (from the active Bridge class instance) -> Bridge class adds message with ID to its outgoing queue -> User (via external bridge polling `/outgoing` and sending).
8.  **Scheduled Tasks (`scheduler_service.py`):**
    *   Runs independently via APScheduler.
    *   Triggers jobs in `NotificationService` (check events) and `RoutineService` (check triggers, daily cleanup).
    *   Triggered routines (`check_routine_triggers`) get context via `SyncService`, generate messages, and return them to the scheduler wrapper.
    *   Scheduler wrapper calls `request_router.send_message` to queue generated routine messages for sending via the active Bridge.

*Diagram:* (The high-level diagram concept remains similar, but the Interface Layer now explicitly involves the external `wa_bridge.js` communicating with `whatsapp_interface.py` via polling/ACK, which then feeds into the `request_router`.)

**3. Module & Function Breakdown**

*   **External Components:**
    *   `wa_bridge.js`: Node.js script using `whatsapp-web.js` to interface with WhatsApp and communicate with the Python backend API.
*   **Core Infrastructure:**
    *   `main.py`: **Updated** - Selects bridge interface (`cli` or `whatsapp`) based on ENV/args, starts Uvicorn with the correct app path, initializes agents, starts scheduler.
    *   `bridge/whatsapp_interface.py`: **NEW** - FastAPI app providing `/incoming`, `/outgoing`, `/ack` endpoints for `wa_bridge.js`. Uses `WhatsAppBridge` class for queuing. Includes `calendar_router` for OAuth.
    *   `bridge/cli_interface.py`: **Updated** - Alternative FastAPI app for CLI mock testing. Uses `CLIBridge` class.
    *   `bridge/request_router.py`: **Updated** - Receives messages from the active bridge API. Handles routing based on user status (`new`, `onboarding`, `active`), cheat codes. Uses `set_bridge` to configure the active message queuing class (`WhatsAppBridge` or `CLIBridge`).
    *   `tools/logger.py`, `tools/encryption.py`, `tools/token_store.py`, `tools/metadata_store.py`, `tools/google_calendar_api.py`, `tools/calendar_tool.py`: Core utilities and specific API/data interactions.
    *   `users/user_registry.py`, `users/user_manager.py`: User profile and in-memory state management.
    *   `services/agent_state_manager.py`: Central management of the in-memory `_AGENT_STATE_STORE`.
*   **Service Layer:**
    *   `services/task_manager.py`: Core task/reminder/session logic.
    *   `services/config_manager.py`: User configuration logic.
    *   `services/task_query_service.py`: Data retrieval and formatting logic.
    *   `services/cheats.py`: Handles cheat code commands.
    *   `services/llm_interface.py`: Initializes Instructor-patched OpenAI client.
    *   `services/sync_service.py`: **Active** - Provides merged context snapshots (WT meta + GCal events) for read-only use.
    *   `services/scheduler_service.py`: **Active** - Manages APScheduler, schedules jobs.
    *   `services/notification_service.py`: **Active** - Logic for checking and formatting event notifications. Triggered by scheduler.
    *   `services/routine_service.py`: **Active** - Logic for checking routine triggers (morning/evening) and generating summaries. Includes daily cleanup. Triggered by scheduler.
*   **Agent Layer:**
    *   `agents/orchestrator_agent.py`: Central reasoning hub for **active** users (Pure LLM flow).
    *   `agents/onboarding_agent.py`: Central reasoning hub for **onboarding** users (Pure LLM flow).
    *   `agents/tool_definitions.py`: Pydantic models and Python functions for the toolset, interfacing with services.
*   **(Obsolete Files):** `agents/intention_agent.py`, `agents/task_agent.py`, `agents/config_agent.py`, `agents/scheduler_agent.py`, `agents/scheduling_logic.py`, `agents/list_reply_logic.py`, `langchain_chains/*`.

**4. Configuration Files**

*   `config/prompts.yaml`: Contains system prompts for Orchestrator, Onboarding, and Session Scheduler sub-LLM. Critical for system behavior.
*   `config/messages.yaml`: Standard user-facing messages.
*   `.env`: Secrets (API keys, client IDs/secrets, encryption key), `BRIDGE_TYPE` (optional).

**5. Key Considerations / Future Work**

*   **Prompt Engineering:** Success heavily relies on prompt quality for the Pure LLM flow. Continuous refinement needed.
*   **LLM Reliability:** Dependence on LLM for state/flow management might be less robust than explicit state machines for complex interactions. Requires thorough testing.
*   **WhatsApp Bridge Stability:** `whatsapp-web.js` is unofficial and can break with WhatsApp updates. Requires monitoring and potential library updates or alternative solutions (official API).
*   **Deployment Complexity:** Requires running both the Python backend (FastAPI/Uvicorn) and the Node.js bridge process. Containerization (e.g., Docker Compose) recommended for managing this.
*   **Error Handling:** Ensure robust error handling between the Node.js bridge and Python backend (connection issues, timeouts, ACK failures).
*   **Security:** Secure the communication between the bridge and backend if deployed separately. Ensure API keys and tokens are handled securely.
*   **Scalability:** Polling mechanism in the bridge might not be the most scalable approach for very high message volumes (WebSockets could be an alternative if needed later).
*   **Synchronization (`SyncService`):** Current implementation is read-only merge. Full two-way sync is deferred but important long-term.
*   **Testing:** Requires end-to-end testing involving the actual WhatsApp interface, testing LLM prompt adherence, tool execution, scheduled jobs, and the bridge communication.

# --- END OF FILE WhatsTasker_SRS_08.txt ---

# --- END OF FILE WhatsTasker_SRS_08.txt ---



================================================================================
ðŸ“„ requirements.txt
================================================================================

# --- START OF FILE requirements.txt ---

# --- START OF FILE requirements.txt ---
# Web Framework & Server
fastapi>=0.110.0,<0.112.0
uvicorn[standard]>=0.29.0,<0.30.0

# Langchain Core & OpenAI Integration
langchain>=0.1.16,<0.2.0
langchain-core>=0.1.40,<0.2.0
langchain-openai>=0.1.3,<0.2.0

# Google API Libraries
google-api-python-client>=2.120.0,<3.0.0
google-auth-oauthlib>=1.2.0,<2.0.0
google-auth>=2.29.0,<3.0.0

# Configuration & Environment
python-dotenv>=1.0.1,<2.0.0
PyYAML>=6.0.1,<7.0.0

# Utilities
requests>=2.31.0,<3.0.0
pytz>=2024.1
cryptography>=42.0.0,<43.0.0
PyJWT>=2.8.0,<3.0.0

# Pydantic (Core dependency for FastAPI & Langchain)
pydantic>=2.7.0,<3.0.0

# --- ADDED FOR SCHEDULING ---
APScheduler>=3.10.0,<4.0.0
# --------------------------
instructor>=0.5.2,<1.0.0  # Or similar version specifier
openai>=1.0.0,<2.0.0     # Instructor depends on OpenAI v1+
# Optional: If using pandas checks (e.g., pd.isna) - uncomment if needed
# pandas>=2.0.0,<3.0.0

Flask>=2.0.0,<4.0.0
# --- END OF FILE requirements.txt ---

# --- END OF FILE requirements.txt ---



================================================================================
ðŸ“„ package.json
================================================================================

# --- START OF FILE package.json ---

{
  "dependencies": {
    "axios": "^1.8.4",
    "qrcode-terminal": "^0.12.0",
    "whatsapp-web.js": "^1.27.0"
  }
}


# --- END OF FILE package.json ---



================================================================================
ðŸ“„ .gitignore
================================================================================

# --- START OF FILE .gitignore ---

# --- Python ---
# Virtual Environments
venv/
.venv/
env/
ENV/
# Compiled Python files
*.pyc
__pycache__/
# Distribution / packaging
.Python
build/
develop-eggs/
dist/
downloads/
eggs/
.eggs/
lib/
lib64/
parts/
sdist/
var/
wheels/
share/python-wheels/
*.egg-info/
.installed.cfg
*.egg
MANIFEST
# PyInstaller
*.manifest
*.spec
# Installer logs
pip-log.txt
pip-delete-this-directory.txt
# Unit test / coverage reports
htmlcov/
.tox/
.nox/
.coverage
.coverage.*
.cache
nosetests.xml
coverage.xml
*.cover
*.py,cover
.hypothesis/
.pytest_cache/
cover/
# Jupyter Notebook
.ipynb_checkpoints

# --- Node.js ---
# Logs
logs
*.log
npm-debug.log*
yarn-debug.log*
yarn-error.log*
pnpm-debug.log*
lerna-debug.log*
# Diagnostic reports (https://nodejs.org/api/report.html)
report.[0-9]*.[0-9]*.[0-9]*.[0-9]*.json
# Dependency directories
node_modules/
jspm_packages/
# Snowpack dependency directory (https://snowpack.dev/)
web_modules/
# Build folder
build/
dist/
# TypeScript cache
*.tsbuildinfo
# Optional eslint cache
.eslintcache
# Optional stylelint cache
.stylelintcache
# Output of 'npm pack'
*.tgz

# --- WhatsApp Web JS ---
.wwebjs_auth/

# --- Project Specific ---
# Environment variables (NEVER commit secrets)
.env
# Data files (User registry, tokens, database - contains user data/secrets)
data/
# Log files generated by the application
logs/
# Output files from tests/scripts
startup_error.log
mock_output.txt
*.dump*.txt
# Simple viewer SQLite DB if it creates one (unlikely)
viewer_messages.db
# Temporary files
*.tmp

# --- IDE/Editor ---
# VS Code
.vscode/*
!.vscode/settings.json
!.vscode/tasks.json
!.vscode/launch.json
!.vscode/extensions.json
.history/
# PyCharm
.idea/
*.iml

# --- OS Specific ---
# macOS
.DS_Store
# Windows
Thumbs.db
ehthumbs.db
Desktop.ini

# --- END OF FILE .gitignore ---



================================================================================
ðŸ“„ wa_bridge.js
================================================================================

# --- START OF FILE wa_bridge.js ---

// whatsapp_bridge.js

const { Client, LocalAuth } = require('whatsapp-web.js');
const qrcode = require('qrcode-terminal');
const axios = require('axios');

// --- Configuration ---
const FASTAPI_BASE_URL = 'http://localhost:8000'; // Make base URL configurable
const POLLING_INTERVAL_MS = 1000; // Poll every 1 second
const RETRY_INTERVAL_MS = 5000; // Retry connection errors every 5 seconds
// --- End Configuration ---

let isClientReady = false; // Flag to track client readiness

// Initialize the WhatsApp client
const client = new Client({
    authStrategy: new LocalAuth(),
    puppeteer: {
        headless: true, // Recommended for server
        args: ['--no-sandbox', '--disable-setuid-sandbox'] // Often needed in server/container environments
     }
});

// --- Event Handlers ---
client.on('qr', (qr) => {
    console.log('Scan the QR code below:');
    qrcode.generate(qr, { small: true });
});

client.on('ready', () => {
    console.log('WhatsApp client is ready!');
    isClientReady = true;
    // --- Start polling ONLY AFTER client is ready ---
    // Check immediately in case messages queued while starting
    pollForOutgoingMessages();
    // -------------------------------------------------
});

client.on('auth_failure', msg => {
    console.error('AUTHENTICATION FAILURE:', msg);
    isClientReady = false; // Mark as not ready
    // Consider exiting or attempting re-auth
});

client.on('disconnected', (reason) => {
    console.log('Client was logged out:', reason);
    isClientReady = false; // Mark as not ready
    // Exit or attempt re-initialization
    process.exit(1); // Exit on disconnect
});

// Listen for incoming messages
client.on('message', async (message) => {
    // Ignore messages if client isn't ready (might happen during init)
    if (!isClientReady) {
        console.log(`Ignoring message from ${message.from} (client not ready)`);
        return;
    }
    try {
        console.log(`Received message from ${message.from}: "${message.body.substring(0, 50)}..."`);
        // Send the incoming message to FastAPI and await only an acknowledgment.
        await axios.post(`${FASTAPI_BASE_URL}/incoming`, {
            user_id: message.from, // e.g., number@c.us
            message: message.body
        });
        // console.log(`Ack received for message from ${message.from}`); // Can be verbose
    } catch (error) {
        console.error(`Error sending incoming message from ${message.from} to FastAPI:`, error.message || error);
    }
});
// --- End Event Handlers ---


// --- Polling Function ---
async function pollForOutgoingMessages() {
    // Only proceed if client is ready
    if (!isClientReady) {
        console.log('Polling paused: WhatsApp client not ready.');
        // Schedule next check later, gives client time to reconnect/ready
        setTimeout(pollForOutgoingMessages, RETRY_INTERVAL_MS);
        return;
    }

    let nextPollDelay = POLLING_INTERVAL_MS; // Default delay

    try {
        // GET messages waiting to be sent.
        const response = await axios.get(`${FASTAPI_BASE_URL}/outgoing`);
        const messages = response.data.messages; // Expected array

        if (messages && messages.length > 0) {
             console.log(`Polling: Found ${messages.length} message(s) to send.`);
            for (const msg of messages) {
                if (!msg.user_id || msg.message === undefined || !msg.message_id) {
                    console.warn('Polling: Skipping invalid message structure from backend:', msg);
                    continue;
                }
                try {
                    // Send the outgoing message to the correct user.
                    // msg.user_id should now be correctly formatted (e.g., number@c.us) by Python backend
                    console.log(`Sending to ${msg.user_id} (ID: ${msg.message_id}): "${msg.message.substring(0, 50)}..."`);
                    await client.sendMessage(msg.user_id, msg.message);

                    // Notify FastAPI that the message was sent.
                    await axios.post(`${FASTAPI_BASE_URL}/ack`, {
                        user_id: msg.user_id, // Include user_id in ACK for logging on backend
                        message_id: msg.message_id
                    });
                    console.log(`ACK sent for message ID: ${msg.message_id}`);
                } catch (sendError) {
                    // Handle errors during send/ack for a specific message
                    console.error(`Error sending message ID ${msg.message_id} to ${msg.user_id}:`, sendError.message || sendError);
                    // Decide if you want to retry this specific message later or just skip it
                    // For now, we log the error and the loop continues to the next message / next poll cycle.
                    // If the error is 'invalid wid', the user_id format is still wrong from the backend.
                    // If it's another error, it might be a temporary WA issue.
                }
            }
        }
        // else { console.log('Polling: No messages found.'); } // Can be verbose

    } catch (error) {
        // Handle errors during the polling GET request itself
        if (error.code === 'ECONNREFUSED' || error.code === 'ECONNRESET') {
            console.error(`Polling Error: Connection to FastAPI (${FASTAPI_BASE_URL}) refused/reset. Is the backend running?`);
            nextPollDelay = RETRY_INTERVAL_MS; // Wait longer before retrying connection
        } else if (axios.isAxiosError(error)) {
             console.error(`Polling Error: Axios error polling FastAPI - ${error.message} (Status: ${error.response?.status})`);
             if (error.response?.status >= 500) {
                  nextPollDelay = RETRY_INTERVAL_MS; // Wait longer on server errors
             }
        }
         else {
            console.error('Polling Error: Unexpected error during polling:', error);
            nextPollDelay = RETRY_INTERVAL_MS; // Wait longer on unexpected errors
        }
    } finally {
        // Schedule the next poll using the determined delay
        setTimeout(pollForOutgoingMessages, nextPollDelay);
    }
}
// --- End Polling Function ---

// --- Initialization ---
console.log('Initializing WhatsApp client...');
client.initialize().catch(err => {
    console.error('Client initialization failed:', err);
    process.exit(1); // Exit if initialization fails critically
});

console.log('WhatsApp Bridge script started. Waiting for client ready event to start polling...');
// NOTE: Polling now starts inside the 'ready' event handler.
// --- End Initialization ---

# --- END OF FILE wa_bridge.js ---



================================================================================
ðŸ“„ monitor_whatstasker.sh
================================================================================

# --- START OF FILE monitor_whatstasker.sh ---

#!/bin/bash

# === Configuration ===
PROJECT_DIR="/home/whatstasker/WhatsTasker" # <-- !!! ADJUST: Absolute path to your project
PYTHON_EXEC="$PROJECT_DIR/venv/bin/python3" # <-- !!! ADJUST: Path to python in venv
NODE_EXEC=$(which node) # Should find node if installed correctly
MAIN_PY_SCRIPT="$PROJECT_DIR/main.py"
NODE_JS_SCRIPT="$PROJECT_DIR/wa_bridge.js"

LOG_DIR="$PROJECT_DIR/logs" # Log directory within the project
PYTHON_LOG="$LOG_DIR/backend_app.log"
NODE_LOG="$LOG_DIR/whatsapp_bridge.log"
MONITOR_LOG="$LOG_DIR/monitor.log"

CHECK_INTERVAL_SECONDS=10 # Check every hour (3600 seconds) - Set back from 10 for production
# CHECK_INTERVAL_SECONDS=10 # Use 10 for quick testing

MONITOR_PID=$$ # Get the PID of this monitor script itself

# === Ensure Log Directory Exists ===
mkdir -p "$LOG_DIR"

# === Logging Function for Monitor Script ===
log_message() {
    # Include Monitor PID in logs for clarity if multiple were accidentally run
    echo "$(date '+%Y-%m-%d %H:%M:%S') [Monitor PID: $MONITOR_PID] - $1" >> "$MONITOR_LOG"
}

# === Function to Start Python Backend ===
start_python() {
    log_message "Attempting to start Python backend..."
    # Activate venv, change to project dir, run main.py, redirect output, run in background
    # Use exec to replace the subshell process with the python process? Maybe not needed here.
    ( source "$PROJECT_DIR/venv/bin/activate" && cd "$PROJECT_DIR" && "$PYTHON_EXEC" "$MAIN_PY_SCRIPT" >> "$PYTHON_LOG" 2>&1 ) &
    PYTHON_BG_PID=$! # Capture the PID of the background process *started by this function*
    log_message "Python backend start command issued (Attempted PID: $PYTHON_BG_PID)."
    # Note: This PID might not be the final Python process if it forks, pgrep is more reliable for checking.
}

# === Function to Start Node.js Bridge ===
start_node() {
    log_message "Attempting to start Node.js bridge..."
    # Change to project dir, run node script, redirect output, run in background
    ( cd "$PROJECT_DIR" && "$NODE_EXEC" "$NODE_JS_SCRIPT" >> "$NODE_LOG" 2>&1 ) &
    NODE_BG_PID=$! # Capture the PID
    log_message "Node.js bridge start command issued (Attempted PID: $NODE_BG_PID)."
}

# === Function to Check if Process is Running ===
# Returns 0 if running, 1 if not running. Also stores PID in global CHECK_PID variable if found.
CHECK_PID="" # Global variable to store found PID
check_process() {
    local script_name="$1"
    # Use pgrep -f to find the PID. Use -o to get the oldest matching process if multiple exist.
    # Using -x might be too strict if the command line has extra args later.
    CHECK_PID=$(pgrep -f -o "$script_name")
    if [[ -n "$CHECK_PID" ]]; then
        # Optionally double-check if the found PID actually contains the script name in its command line
        # cmdline=$(ps -p $CHECK_PID -o cmd=) # This can be less reliable across systems
        # if [[ "$cmdline" == *"$script_name"* ]]; then
            return 0 # Process is running
        # fi
    fi
    CHECK_PID="" # Clear if not found or check fails
    return 1 # Process is NOT running
}

# === Cleanup Function (Triggered by TRAP) ===
cleanup() {
    log_message "Termination signal received. Cleaning up managed processes..."

    # Find and kill Python backend
    if check_process "$MAIN_PY_SCRIPT" && [[ -n "$CHECK_PID" ]]; then
        log_message "Stopping Python backend (PID: $CHECK_PID)..."
        kill "$CHECK_PID" # Send TERM signal
    else
        log_message "Python backend not found running during cleanup."
    fi

    # Find and kill Node.js bridge
    if check_process "$NODE_JS_SCRIPT" && [[ -n "$CHECK_PID" ]]; then
        log_message "Stopping Node.js bridge (PID: $CHECK_PID)..."
        kill "$CHECK_PID" # Send TERM signal
    else
        log_message "Node.js bridge not found running during cleanup."
    fi

    log_message "Cleanup actions complete. Monitor script exiting."
    exit 0 # Exit the script cleanly after cleanup
}

# === Trap Signals ===
# Call the 'cleanup' function when the script receives TERM, INT, QUIT, or EXIT signals
trap cleanup TERM INT QUIT EXIT
log_message "Signal traps set for TERM, INT, QUIT, EXIT."

# === Initial Startup ===
log_message "Monitor script starting (PID: $MONITOR_PID). Performing initial process check/start."
if ! check_process "$MAIN_PY_SCRIPT"; then
    log_message "Python backend not running. Starting..."
    start_python
else
    log_message "Python backend already running (PID: $CHECK_PID)."
fi
sleep 2 # Small delay between starts

if ! check_process "$NODE_JS_SCRIPT"; then
    log_message "Node.js bridge not running. Starting..."
    start_node
else
    log_message "Node.js bridge already running (PID: $CHECK_PID)."
fi

log_message "Initial checks complete. Starting monitoring loop (Interval: $CHECK_INTERVAL_SECONDS seconds)."

# === Monitoring Loop ===
while true; do
    # Check Python Backend
    if check_process "$MAIN_PY_SCRIPT"; then
        log_message "CHECK: Python backend is running (PID: $CHECK_PID)."
    else
        log_message "ALERT: Python backend stopped. Restarting..."
        start_python
        sleep 5 # Give it a moment after restart
    fi

    # Check Node Bridge
    if check_process "$NODE_JS_SCRIPT"; then
        log_message "CHECK: Node.js bridge is running (PID: $CHECK_PID)."
    else
        log_message "ALERT: Node.js bridge stopped. Restarting..."
        start_node
        sleep 5 # Give it a moment after restart
    fi

    # Wait for the next check interval
    sleep "$CHECK_INTERVAL_SECONDS"
done

# --- END OF FILE monitor_whatstasker.sh ---



================================================================================
ðŸ“„ config/prompts.yaml
================================================================================

# --- START OF FILE config/prompts.yaml ---

# --- START OF FILE config/prompts.yaml ---

# =====================================================
# Prompt(s) for OrchestratorAgent (v0.8 Flow Option 1: Find Time First)
# =====================================================
# =====================================================
# Prompt(s) for OrchestratorAgent (v0.8 Flow Option 1: Find Time First)
# =====================================================
orchestrator_agent_system_prompt: |
  You are an expert Time Management Assistant for WhatsTasker. Your goal is to help users manage tasks (ToDo Tasks and Working Tasks) and reminders efficiently via WhatsApp. You also manage user preferences. Be concise and clear.

  **CORE TASK:** Understand the user's request based on their LATEST message AND the conversation HISTORY. Decide the single next step: ask a clarifying question, call ONE tool, or respond directly.

  **CONTEXT IS KEY:** Always review the recent `Conversation History`, including previous user messages, your own responses, and TOOL RESULTS (especially `propose_task_slots` results containing `proposed_slots` and `search_context`) before deciding your next action. Avoid re-asking for information already provided or taking actions inconsistent with the current conversational state.

  **TASK TYPES:**
  *   **ToDo Task:** An item the user needs to remember to do, but doesn't require scheduled calendar time via this system. Created using `create_task`.
  *   **Working Task:** An item requiring effort that the user wants assistance scheduling dedicated time for in their calendar. Uses the Task Scheduling Flow (`propose_task_slots` and `finalize_task_and_book_sessions`).
  *   **Reminder:** A simple alert for a specific date/time. Created using `create_reminder`.

  **GENERAL FLOWS:**

  *   **Reminders:** Get description & date/time -> Call `create_reminder`. Ask for missing info once.
  *   **Task Creation:** Determine if user wants a ToDo Task or a Working Task -> Follow **Task Creation Flow**.
  *   **Manage Existing:** Identify item (ToDo, Working Task, or Reminder using history/list mapping) -> Call `update_item_details`, `update_item_status`, or `cancel_task_sessions`.
  *   **List Items:** User asks to see items -> Call `get_formatted_task_list`.
  *   **Preferences:** User asks to see/change settings -> Follow **Preference Management Flow**.
  *   **Calendar Connect:** User asks to connect -> Call `initiate_calendar_connection`.

  **Flow: Task Creation (ToDo vs. Working Task)**
    1. Intent: User wants to add a task (e.g., "add task", "need to do", "work on X").
    2. Extract Initial Details: From the user's message, extract:
       - `description` (Required)
       - `estimated_duration` (Optional, e.g., "2 hours", "90m")
       - `timeframe` or `due_date` (Optional, e.g., "next week", "by Friday")
    3. **Determine Task Type & Gather Info:**
       - **Case A: Duration AND Timeframe/Due Date Provided?**
         - If YES: Assume it's a **Working Task**. Proceed *directly* to **Working Task Scheduling Flow** (Step 5). Do NOT ask if they want to schedule.
       - **Case B: Only Duration Provided?**
         - If YES: Assume it's likely a **Working Task**. Ask *only* for the timeframe/due date (e.g., "Okay, task '[description]' for [duration]. When should this be done by (or timeframe)?"). Once provided, proceed to **Working Task Scheduling Flow** (Step 5).
       - **Case C: Only Timeframe/Due Date Provided?**
         - If YES: Assume it's likely a **Working Task**. Ask *only* for the duration (e.g., "Okay, task '[description]' for [timeframe]. How long do you estimate it will take?"). Once provided, proceed to **Working Task Scheduling Flow** (Step 5).
       - **Case D: NEITHER Duration NOR Timeframe/Due Date Provided?**
         - If YES: Ask the clarifying question: "Okay, I can add '[description]' to your list. Do you also want my help finding and scheduling time in your calendar to work on it (as a Working Task)? Or just add it as a ToDo Task?"
         - Based on reply:
           - If YES (wants scheduling help): Treat as a **Working Task**. Ask for BOTH estimated duration and timeframe/due date. Once provided, proceed to **Working Task Scheduling Flow** (Step 5).
           - If NO (just add to list): Treat as a **ToDo Task**. Call the `create_task` tool with *only* the `description`. Confirm "OK, ToDo Task '[description]' added to your list." -> END.

  **Flow: Working Task Scheduling Flow (Starts at Step 5)**
    5. Prepare for Slot Proposal:
       - Ensure `description`, `duration`, and `timeframe`/`due_date` are known for the Working Task.
    6. Extract Scheduling Constraints:
       - Analyze the user's request and conversation history for specific constraints or preferences (`scheduling_hints`). Examples: "in the afternoon", "not on Monday", "needs to be one continuous block", "split into sessions if possible". Determine conceptual need for splitting.
    7. Tool Call: Call `propose_task_slots` with description, duration, timeframe, and hints. Remember the `search_context` returned.
    8. Present Slots: Show numbered `proposed_slots`. Ask user to confirm.
    9. Handle Confirmation/Rejection (IMPORTANT STATE CHANGE):
      *   **User Confirms:**
          *   Identify chosen slot number. Retrieve slot details (`date`, `time`, `end_time`) and `search_context` from HISTORY.
          *   **Your ONLY next action is to call `finalize_task_and_book_sessions`.** Pass correct `search_context` and `approved_slots` list (formatted correctly).
      *   **User Rejects/Corrects:**
          *   Extract new timeframe/hints. Identify original task ID if rescheduling (look in previous `search_context` or tool results).
          *   Re-call `propose_task_slots` with updated info. If rescheduling, ensure original ID is passed in `search_context` as `rescheduled_item_id`.
          *   Loop back to Step 8.
      *   **User Rejects Entirely:** Ask if they want to save as a ToDo task instead. Handle YES/NO appropriately.
    10. Finalize Tool Call (`finalize_task_and_book_sessions`): Pass full `search_context` (potentially with `rescheduled_item_id`) and correctly formatted `approved_slots`.
    11. Response: Confirm "Working Task '[description]' created/updated and scheduled."

  **Flow: Updating Items**
    1. Detect user wants to modify an item.
    2. Determine if it's: Updating definition (description, date, time, duration, project) or Changing status (pending, completed, cancelled).
    3. If updating definition: Call `update_item_details` with `item_id` and `updates` dictionary.
    4. If updating status: Call `update_item_status` with `item_id` and `new_status`.

  # --- >>>>> NEW FLOW: PREFERENCE MANAGEMENT <<<<< ---
  **Flow: Preference Management**
    1. Intent: User asks to view or change settings/preferences (e.g., "show my settings", "change work start time", "set timezone to X").
    2. **Identify Goal:** Is the user asking to VIEW or UPDATE preferences?
    3. **If VIEW:**
       - Respond directly by summarizing the relevant preferences from the `User Preferences` context provided. Format clearly (e.g., "Your current work hours are [Start] to [End] on [Days]. Your timezone is [TZ]."). Do NOT call a tool.
    4. **If UPDATE:**
       - Extract the specific preference(s) to change and the desired new value(s) from the user message.
       - **Validate Format:** Ensure the new value is in the correct format (e.g., HH:MM for time, Olson name for TimeZone, duration string like '60m' for session length). If format is unclear or invalid, ask for clarification, reminding the user of the required format.
       - **Tool Call:** Once value is validated, call `update_user_preferences` with an `updates` dictionary containing the {key: value} pairs. Example: `{"updates": {"Work_Start_Time": "09:30"}}`.
       - **Response:** Based on tool result: Confirm success ("OK, I've updated your [Preference Name].") or relay failure ("Sorry, I couldn't update that preference.").
  # --- >>>>> END NEW FLOW <<<<< ---

  **CRITICAL RULES:**
  *   **Use History:** Base decisions on the LATEST user message + HISTORY + TOOL RESULTS.
  *   **Minimal Questions:** Only ask for essential missing info.
  *   **One Tool:** Max ONE tool per turn.
  *   **Post-Proposal State:** After `propose_task_slots`, user confirmation leads ONLY to `finalize_task_and_book_sessions`.
  *   **`search_context`:** Pass the full dictionary from `propose_task_slots` result to `finalize_task_and_book_sessions`.
  *   **`approved_slots` Format:** Ensure `[{"date": "YYYY-MM-DD", "time": "HH:MM", "end_time": "HH:MM"}]` format.
  *   **Rescheduling:** Preserve original `item_id` in `search_context` (as `rescheduled_item_id`) when re-proposing slots for an existing task, so `finalize_task_and_book_sessions` updates correctly.

  **CONTEXT PROVIDED:** Current Time/Date, User Preferences (JSON object), Conversation History (User, Assistant, Tool Results), Active Items (DB), Calendar Events (GCal).

  # --- >>>>> REVISED AVAILABLE TOOLS SECTION <<<<< ---
  **Available Tools & Parameters:**
    *   `create_reminder`: {description: str, date: str (YYYY-MM-DD), time?: str (HH:MM), project?: str}
        - Creates a simple reminder, potentially in GCal if time is specified.
    *   `create_task`: {description: str, date?: str (YYYY-MM-DD), estimated_duration?: str (e.g., "2h", "90m"), project?: str}
        - Creates a ToDo Task (metadata only, no GCal booking). Date/duration are optional metadata.
    *   `propose_task_slots`: {description?: str, duration: str (e.g., "2h", "90m"), timeframe: str (e.g., "next week", "on YYYY-MM-DD"), scheduling_hints?: str}
        - Finds potential calendar slots for a Working Task based on duration, timeframe, and hints. Returns proposed slots and search context. Return format: `{success, proposed_slots: List[Dict], message, search_context: Dict}`
    *   `finalize_task_and_book_sessions`: {search_context: Dict, approved_slots: List[Dict], project?: str}
        - Creates/updates task metadata based on `search_context` (checks for `rescheduled_item_id`) and books approved sessions into GCal. `approved_slots` format: `[{"date": "YYYY-MM-DD", "time": "HH:MM", "end_time": "HH:MM"}]`.
    *   `update_item_details`: {item_id: str, updates: Dict}
        - Updates non-status details (description, date, time, estimated_duration, project) of an existing item. `updates` dict keys must be valid fields.
    *   `update_item_status`: {item_id: str, new_status: str}
        - Updates the status of an item ("pending", "in_progress", "completed", "cancelled"). Use "cancelled" for deletion.
    *   `update_user_preferences`: {updates: Dict}
        - Updates user settings. `updates` dict keys must be valid preference names (e.g., "Work_Start_Time", "TimeZone"). Values must be correctly formatted.
    *   `initiate_calendar_connection`: {}
        - Starts the Google Calendar authentication flow. Returns auth URL if needed.
    *   `cancel_task_sessions`: {task_id: str, session_ids_to_cancel: List[str]}
        - Removes specific scheduled GCal sessions linked to a task and updates task metadata.
    *   `interpret_list_reply`: {user_reply: str, list_mapping: Dict[str, str]}
        - Interprets user replies (e.g., "1", "done 2") referring to a previously displayed numbered list. Returns identified item IDs.
    *   `get_formatted_task_list`: {date_range?: List[str], status_filter?: str, project_filter?: str}
        - Retrieves and formats a list of tasks/reminders based on filters. Status filter defaults to 'active'. Date range is [start_date, end_date]. Returns formatted string and item mapping.
  # --- >>>>> END REVISED AVAILABLE TOOLS SECTION <<<<< ---

# =====================================================
# Prompt(s) for Session Scheduling LLM (Used BY propose_task_slots TOOL)
# =====================================================
session_scheduler_system_prompt: |
  You are an expert Scheduler assistant used by the propose_task_slots tool for WhatsTasker.
  Your goal is to propose a schedule of work sessions for a specific task, distributing them reasonably over the available time, based on user preferences, task details, and existing calendar events.

  **Core Task:** Given the task details, user preferences, existing calendar events, the number of slots requested (`num_slots_requested`), the desired duration for *each* slot (`user_session_length`), and **scheduling hints**, generate a list of proposed work session slots **strictly within the calculated search window**.

  **Input Variables Provided:**
  - Task Description: {task_description}
  - Task Due Date: {task_due_date}
  - Task Estimated Duration: {task_estimated_duration}
  - User Working Days: {user_working_days}
  - User Work Start Time: {user_work_start_time}
  - User Work End Time: {user_work_end_time}
  - User Session Length: {user_session_length} - Duration of EACH slot to find.
  - Existing Calendar Events (JSON list): {existing_events_json}
  - Current Date: {current_date}
  - Number of Slots to Propose: {num_slots_requested} - Find exactly this many slots.
  - Search Start Date: {search_start_date} - **CRITICAL: Only propose slots ON or AFTER this date.**
  - Search End Date: {search_end_date} - **CRITICAL: Only propose slots ON or BEFORE this date.**
  - Scheduling Hints: {scheduling_hints} (Natural language constraints like 'afternoon preferred', 'not Monday', 'continuous block needed', 'can be split')

  **Processing Logic:**
  1.  **Calculate Slot Duration:** Use the provided `user_session_length`.
  2.  **Identify Available Time Slots:**
      - Consider dates **STRICTLY BETWEEN** `search_start_date` and `search_end_date` (inclusive). **DO NOT propose slots outside this range.**
      - Filter based on `user_working_days`.
      - Consider time window between `user_work_start_time` and `user_work_end_time`.
      - Check `existing_events_json` for conflicts.
      - Find time slots within working hours, free, and matching the **required duration**.
  3.  **Select & Distribute Sessions:** From available slots within the search window, select exactly `num_slots_requested` sessions.
      - **CRITICAL: Apply `scheduling_hints`:** Consider afternoon/morning preference, day exclusions, etc.
      - Attempt reasonable distribution.
  4.  **Calculate End Times:** For each selected session start time (`date`, `time`), calculate `end_time`. Ensure `end_time` <= `user_work_end_time`.
  5.  **Format Output:** Create the JSON output.

  **Output Format Requirements:**
  Respond ONLY with a single, valid JSON object containing exactly two keys:
  1.  `"proposed_sessions"`: A JSON list of proposed sessions. **Each element in the list MUST be a dictionary** with the following keys **EXACTLY**:
      - `"slot_ref"`: (Integer, starting from 1)
      - `"date"`: (String, "YYYY-MM-DD" format)
      - **`"time"`**: (String, "HH:MM" format - **USE THIS KEY NAME FOR START TIME**)
      - `"end_time"`: (String, "HH:MM" format)
      Return an empty list `[]` if no suitable slots found within the specified search dates that meet all constraints.
  2.  `"response_message"`: A user-facing message summarizing the proposal or explaining failure. Avoid claiming sessions were scheduled if the `proposed_sessions` list is empty.

session_scheduler_human_prompt: |
  **Task Details:**
  - Description: {task_description}
  - Due Date: {task_due_date}
  - Estimated Duration: {task_estimated_duration}

  **User Preferences & Slot Request:**
  - Working Days: {user_working_days}
  - Work Start Time: {user_work_start_time}
  - Work End Time: {user_work_end_time}
  - **Duration of EACH slot to find**: {user_session_length}
  - **Number of Slots to Find**: {num_slots_requested}

  **Calendar Context & Search Window:**
  - Today's Date: {current_date}
  - Existing Events (JSON): {existing_events_json}
  - **Search Start Date (Inclusive)**: {search_start_date}
  - **Search End Date (Inclusive)**: {search_end_date}
  - **Scheduling Hints (Apply these!)**: {scheduling_hints}

  **Your Task:** Propose exactly {num_slots_requested} schedule slots, each of duration {user_session_length}. Proposals MUST fall strictly between {search_start_date} and {search_end_date}. Consider the **Scheduling Hints**. Respond ONLY in the specified JSON format. **Crucially, ensure each dictionary within the `proposed_sessions` list uses the exact keys: `slot_ref`, `date`, `time`, `end_time`.** Ensure JSON validity.

# =====================================================
# Prompt(s) for OnboardingAgent (Keep as is)
# =====================================================
onboarding_agent_system_prompt: |
  You are the Onboarding Assistant for WhatsTasker. Your goal is to guide a new user through the initial setup process conversationally by collecting essential preferences and ensuring data is in the correct format BEFORE calling any tools.

  **Core Task & Rules:**
  1. Examine the `Current User Preferences` provided in the context.
  2. Identify the *first* essential preference that is missing (`null`). The required preferences and their **STRICT required formats** are:
     *   `TimeZone`: Must be a valid Olson Timezone Name (e.g., `America/New_York`, `Europe/London`, `Asia/Jerusalem`).
     *   `Work_Start_Time`: Must be in **HH:MM format (24-hour clock)** (e.g., `09:00`, `17:30`).
     *   `Work_End_Time`: Must be in **HH:MM format (24-hour clock)** (e.g., `18:00`).
     *   `Preferred_Session_Length`: Must be a string indicating duration (e.g., `60m`, `90m`, `1.5h`, `2h`).
  3. Ask the user a clear, friendly question for **only that specific missing preference**. Clearly state the required format in your question.
  4. **Interpret the user's reply.** Try to understand common variations:
     *   For **Time**: Convert inputs like `6pm` to `18:00`, `9am` to `09:00`, `noon` to `12:00`, `midnight` to `00:00`. Handle `0900` as `09:00`, `1730` as `17:30`.
     *   For **TimeZone**: If the user gives a city like `london` or `new york` or `tel aviv`, try to infer the correct Olson name (`Europe/London`, `America/New_York`, `Asia/Jerusalem`). If they give an abbreviation like `EST`, try to map it to a common Olson name (but be cautious, ask if unsure, e.g., "Do you mean America/New_York for EST?").
     *   For **Duration**: Convert `1 hour` to `60m`, `1.5 hours` to `90m`, `2 hours` to `2h`.
  5. **If you can confidently convert the user's reply to the EXACT required format:** Call the `update_user_preferences` tool. The parameters MUST be `{{"updates": {{KEY: "FORMATTED_VALUE"}}}}`. Example: `{{"updates": {{"Work_End_Time": "18:00"}}}}`.
  6. **If you CANNOT confidently interpret or convert the user's reply to the required format:** DO NOT call the tool. Instead, **ask the user for clarification**, reminding them of the specific format needed. Example: "Sorry, I need the time in HH:MM format like 09:00 or 17:30. Could you please provide your work start time again?"
  7. After the tool runs successfully (you'll see `success: true` in the tool result), repeat from step 1: check the preferences for the *next* missing item and ask for it.
  8. Once all four required preferences (`TimeZone`, `Work_Start_Time`, `Work_End_Time`, `Preferred_Session_Length`) have been collected and successfully saved via the tool:
     - Ask the user if they want to connect Google Calendar: "I've got your basic preferences setup. Would you like to connect your Google Calendar now...? (yes/no)"
     - If 'yes': Call `initiate_calendar_connection`. Relay the message/URL from the tool result. End interaction for now.
     - If 'no': Acknowledge their choice. Proceed to the final step.
  9. **Final Step (After prefs collected AND calendar handled):** Call `update_user_preferences` with `{{"updates": {{"status": "active"}}}}`.
  10. After the final status update succeeds, respond with a concluding message like: "Great, setup is complete! ..."

  **Important:** Be precise. Ask for one thing. Interpret carefully. Format correctly *before* calling the tool. Ask again if unsure or if the format is wrong.

  **Context Provided:**
  - Current User Preferences (JSON Object): Check for `null` values for required keys.
  - Conversation History: See what you last asked for and how the user replied.

  **Tools Available During Onboarding:**
  - `update_user_preferences`: `{{"updates": {{KEY: "VALUE"}}}}` (VALUE must be correctly formatted by you).
  - `initiate_calendar_connection`: No parameters.

onboarding_agent_human_prompt: |
  Current Preferences:
  ```json
  {current_preferences_json}
    History:
    {conversation_history}
    User message: {message}
    Your Task: Based on the system instructions, determine the next step: Ask for the next missing REQUIRED preference (TimeZone, Work_Start_Time, Work_End_Time, Preferred_Session_Length), ask about calendar connection, call a tool (update_user_preferences or initiate_calendar_connection), or finalize onboarding (by calling update_user_preferences with status: active). Formulate your response or tool call.
# --- END OF FILE config/prompts.yaml ---

# --- END OF FILE config/prompts.yaml ---



================================================================================
ðŸ“„ config/messages.yaml
================================================================================

# --- START OF FILE config/messages.yaml ---

# config/messages.yaml

# --- Add these ---
welcome_confirmation_message: |
  Hello! ðŸ‘‹ Ready to take control of your time and focus? Welcome to WhatsTasker!
  Imagine having a personal Time Management Expert right here in WhatsApp, helping you:
  ðŸ§  **Capture Everything Instantly:** Just text me your tasks and reminders like you normally would â€“ "remind me about the team sync at 10", "add task finish the report by Friday, takes about 3 hours". No more switching apps!
  ðŸ“… **Stay Aligned with Your Calendar:** I connect directly to your Google Calendar, ensuring your tasks and scheduled work time fit seamlessly into your day.
  â±ï¸ **Beat Procrastination:** Need focused time for important tasks? I can find empty slots in your calendar and block out dedicated work sessions for you.
  â˜€ï¸ **Start Strong, End Clear:** Get ahead with helpful morning check-ins and wrap up your day effectively with quick evening reviews to ensure nothing falls through the cracks (coming soon!).
  Think of me as your partner in productivity, helping you plan better, focus deeply, and get more done with less stress.
  Ready to unlock WhatsTasker? 

setup_starting_message: "Great! Let's set things up. I'll ask a few questions to configure your preferences."
setup_declined_message: "Okay, no problem. Just message me again whenever you're ready to set things up!"
ask_confirmation_again_message: "Sorry, I didn't quite understand that. Are you ready to start the setup process? (yes/no)"
user_registered_already_message: "Welcome back! How can I help you today?" # Optional: For returning users found by get_agent

# --- Keep existing messages ---
generic_error_message: "Sorry, an unexpected error occurred. Please try again."
intent_parse_error_message: "Sorry, I had trouble understanding the structure of that request."
intent_unknown_message: "Sorry, I'm not sure how to help with that. You can ask me to add tasks, list tasks, or change settings."
intent_clarify_message: "Sorry, I didn't quite understand that. Could you please rephrase?"
# ... add any other messages you have ...

# --- END OF FILE config/messages.yaml ---



================================================================================
ðŸ“„ config/settings.yaml
================================================================================

# --- START OF FILE config/settings.yaml ---



# --- END OF FILE config/settings.yaml ---



================================================================================
ðŸ“„ main.py
================================================================================

# --- START OF FILE main.py ---

# --- START OF FULL main.py ---

import os
import sys
import asyncio
import signal
import argparse
from dotenv import load_dotenv
load_dotenv()

# --- Determine Bridge Type ---
DEFAULT_BRIDGE = "whatsapp"
ALLOWED_BRIDGES = ["cli", "whatsapp"]
bridge_type_env = os.getenv("BRIDGE_TYPE", "").lower()
parser = argparse.ArgumentParser(description="Run WhatsTasker Backend")
parser.add_argument("--bridge", type=str, choices=ALLOWED_BRIDGES, help=f"Specify the bridge interface ({', '.join(ALLOWED_BRIDGES)})")
args = parser.parse_args()
bridge_type_arg = args.bridge.lower() if args.bridge else None
bridge_type = DEFAULT_BRIDGE
if bridge_type_arg: bridge_type = bridge_type_arg
if bridge_type_env in ALLOWED_BRIDGES: bridge_type = bridge_type_env
if bridge_type not in ALLOWED_BRIDGES: print(f"ERROR: Invalid bridge type '{bridge_type}'."); sys.exit(1)

# --- Logger Import (must happen early) ---
# Use a try-except block for robust logger initialization
try:
    from tools.logger import log_info, log_error, log_warning
    # Test log after import attempt
    log_info("main", "init", "Logger imported successfully.")
except ImportError as log_import_err:
    # Fallback if logger import fails catastrophically
    print(f"FATAL ERROR: Failed to import logger: {log_import_err}")
    sys.exit(1)
except Exception as log_init_e:
    print(f"FATAL ERROR during initial logging setup: {log_init_e}")
    sys.exit(1)


# --- Dynamic Bridge and App Import ---
uvicorn_app_path = None
bridge_module_name = None
bridge_instance = None

try:
    if bridge_type == "cli":
        from bridge.cli_interface import app as fastapi_app, CLIBridge, outgoing_cli_messages, cli_queue_lock
        uvicorn_app_path = "bridge.cli_interface:app"
        bridge_module_name = "CLI Bridge"
        bridge_instance = CLIBridge(outgoing_cli_messages, cli_queue_lock)
    elif bridge_type == "whatsapp":
        from bridge.whatsapp_interface import app as fastapi_app, WhatsAppBridge, outgoing_whatsapp_messages, whatsapp_queue_lock
        uvicorn_app_path = "bridge.whatsapp_interface:app"
        bridge_module_name = "WhatsApp Bridge"
        bridge_instance = WhatsAppBridge(outgoing_whatsapp_messages, whatsapp_queue_lock)
    else:
        # This should not happen due to initial checks, but keeps linters happy
        raise ValueError(f"Internal logic error determining bridge type: {bridge_type}")

    # --- Set the Bridge in the Router ---
    from bridge.request_router import set_bridge
    set_bridge(bridge_instance) # <-- EXPLICITLY SET THE BRIDGE HERE
    # ------------------------------------

    log_info("main", "init", f"WhatsTasker v0.8 starting...")
    log_info("main", "init", f"Using Bridge Interface: {bridge_module_name} (Selected: '{bridge_type}', Path: '{uvicorn_app_path}')")

except ImportError as import_err:
    log_error("main", "init", f"Failed to import bridge module for type '{bridge_type}': {import_err}", import_err)
    sys.exit(1)
except Exception as bridge_setup_err:
    log_error("main", "init", f"Failed during bridge setup for type '{bridge_type}': {bridge_setup_err}", bridge_setup_err)
    sys.exit(1)


# --- Other Imports ---
import uvicorn
from users.user_manager import init_all_agents
import traceback # Keep traceback import

# --- Scheduler Import ---
try:
    from services.scheduler_service import start_scheduler, shutdown_scheduler
    SCHEDULER_IMPORTED = True
    log_info("main", "import", "Scheduler service imported successfully.")
except ImportError as sched_import_err:
    log_error("main", "import", f"Scheduler service not found or failed import: {sched_import_err}. Background tasks disabled.", sched_import_err)
    SCHEDULER_IMPORTED = False
    # Define dummy functions to prevent crashes
    def start_scheduler(): return False
    def shutdown_scheduler(): pass
# ----------------------------


async def handle_shutdown_signal(sig, loop):
    """Async signal handler helper."""
    log_warning("main", "handle_shutdown_signal", f"Received signal {sig.name}. Initiating shutdown...")
    # Signal the main tasks to stop (implementation depends on how server/tasks are managed)
    # For Uvicorn, we can tell the server instance to exit
    if server: # Check if server object exists
         server.should_exit = True
    # Additionally, cancel other background tasks if necessary
    # Example: Cancel a long-running task
    # if some_background_task and not some_background_task.done():
    #    some_background_task.cancel()

    # Give tasks a moment to finish cleanup
    await asyncio.sleep(1)

    # Optionally force stop loop if tasks don't exit gracefully
    # loop.stop()


server: uvicorn.Server | None = None # Define server variable in outer scope

async def main_async():
    global server # Allow modification of the global server variable
    # Agent state init
    log_info("main", "main_async", "Initializing agent states...")
    try:
        init_all_agents()
        log_info("main", "main_async", "Agent state initialization complete.")
    except Exception as init_e:
        log_error("main", "main_async", "CRITICAL error during init_all_agents.", init_e)
        sys.exit(1) # Exit if agent init fails

    # Scheduler start
    if SCHEDULER_IMPORTED:
        try:
            log_info("main", "main_async", "Starting scheduler service...")
            if start_scheduler():
                log_info("main", "main_async", "Scheduler service started successfully.")
            else:
                # Error should be logged by start_scheduler if it returns False
                log_error("main", "main_async", "Scheduler service FAILED to start.")
        except Exception as sched_e:
            log_error("main", "main_async", "CRITICAL error starting scheduler.", sched_e)
            # Decide if this is fatal - potentially continue without scheduler?
            # For now, let's log and continue

    # Uvicorn config
    reload_enabled = os.getenv("APP_ENV", "production").lower() == "development"
    log_level = "debug" if reload_enabled else "info"
    # --- Define Port ---
    server_port = int(os.getenv("PORT", "8000")) # Read from env or default to 8000
    # -------------------
    log_info("main", "main_async", f"Starting FastAPI server via Uvicorn...")
    log_info("main", "main_async", f"Target App: '{uvicorn_app_path}'")
    log_info("main", "main_async", f"Host: 0.0.0.0, Port: {server_port}") # Log the port
    log_info("main", "main_async", f"Reload: {reload_enabled}, Log Level: {log_level}")

    config = uvicorn.Config(
        uvicorn_app_path,
        host="0.0.0.0",
        port=server_port, # Use the variable
        reload=reload_enabled,
        access_log=False, # Keep access log off unless needed for debugging
        log_level=log_level,
        lifespan="on" # Recommended for modern FastAPI startup/shutdown events
    )
    server = uvicorn.Server(config)

    # --- Graceful shutdown setup (Modern asyncio) ---
    loop = asyncio.get_running_loop()
    for sig in (signal.SIGINT, signal.SIGTERM):
        try:
            # Use loop.create_task for the handler to run it in the loop
            loop.add_signal_handler(sig, lambda s=sig: asyncio.create_task(handle_shutdown_signal(s, loop)))
        except NotImplementedError:
            # Fallback for systems like Windows that might not support add_signal_handler
            signal.signal(sig, lambda s, f: asyncio.create_task(handle_shutdown_signal(signal.Signals(s), loop)))
    # --- End shutdown setup ---

    try:
        await server.serve()
    finally:
        log_info("main", "main_async", "Server stopped. Performing final cleanup...")
        if SCHEDULER_IMPORTED:
            try:
                log_info("main", "main_async", "Shutting down scheduler...")
                shutdown_scheduler()
                log_info("main", "main_async", "Scheduler shut down.")
            except Exception as e:
                log_error("main", "main_async", "Error shutting down scheduler.", e)
        log_info("main", "main_async", "Main async process finished.")


if __name__ == "__main__":
    try:
        # Check logger exists before trying to use it
        _ = log_info
    except NameError:
        print("FATAL: Logger not defined or failed import.")
        sys.exit(1)

    try:
        asyncio.run(main_async())
    except SystemExit as se: # Catch SystemExit from Uvicorn startup failure
         log_error("main", "__main__", f"Server exited with code: {se.code}. Check previous errors (e.g., port conflict).")
         sys.exit(se.code) # Propagate the exit code
    except KeyboardInterrupt:
        log_warning("main", "__main__", "KeyboardInterrupt received. Exiting.")
        # Perform minimal cleanup if needed
        if SCHEDULER_IMPORTED:
             try: shutdown_scheduler()
             except Exception: pass
        sys.exit(0)
    except Exception as e:
        log_error("main", "__main__", f"Unhandled error during server execution/shutdown.", e)
        log_error("main", "__main__", f"Traceback:\n{traceback.format_exc()}")
        # Final attempt to shutdown scheduler
        if SCHEDULER_IMPORTED:
            try: shutdown_scheduler()
            except Exception as final_e: log_error("main","main", f"Error in final shutdown attempt: {final_e}")
        sys.exit(1)
    finally:
        log_info("main", "__main__", "Application exiting.")

# --- END OF FULL main.py ---

# --- END OF FILE main.py ---



================================================================================
ðŸ“„ bridge/request_router.py
================================================================================

# --- START OF FILE bridge/request_router.py ---

# --- START OF FULL bridge/request_router.py ---

import re
import os
import yaml
import traceback
import json
from typing import Optional, Tuple, List # Keep Optional if used elsewhere internally
from tools.logger import log_info, log_error, log_warning # Keep logger for operational logs

# --- Database Logging Import ---
try:
    import tools.activity_db as activity_db
    ACTIVITY_DB_IMPORTED = True
except ImportError:
    log_error("request_router", "import", "Failed to import activity_db. Message DB logging disabled.", None)
    ACTIVITY_DB_IMPORTED = False
    # Define a dummy function if import fails
    class activity_db:
        @staticmethod
        def log_message_db(*args, **kwargs): pass
# --- End DB Import ---

# State manager imports
from services.agent_state_manager import get_agent_state, add_message_to_user_history

# User/Agent Management
from users.user_manager import get_agent

# Orchestrator Import
try:
    from agents.orchestrator_agent import handle_user_request as route_to_orchestrator
    ORCHESTRATOR_IMPORTED = True
    log_info("request_router", "import", "Successfully imported OrchestratorAgent handler.")
except ImportError as e:
    ORCHESTRATOR_IMPORTED = False; log_error("request_router", "import", f"OrchestratorAgent import failed: {e}", e); route_to_orchestrator = None

# Onboarding Agent Import
try:
    from agents.onboarding_agent import handle_onboarding_request
    ONBOARDING_AGENT_IMPORTED = True
    log_info("request_router", "import", "Successfully imported OnboardingAgent handler.")
except ImportError as e:
     ONBOARDING_AGENT_IMPORTED = False; log_error("request_router", "import", f"OnboardingAgent import failed: {e}", e); handle_onboarding_request = None

# Task Query Service Import
try:
    from services.task_query_service import get_context_snapshot
    QUERY_SERVICE_IMPORTED = True
except ImportError as e:
     QUERY_SERVICE_IMPORTED = False; log_error("request_router", "import", f"TaskQueryService import failed: {e}", e); get_context_snapshot = None

# Cheat Service Import
try:
    from services.cheats import handle_cheat_command
    CHEATS_IMPORTED = True
    log_info("request_router", "import", "Successfully imported Cheats service.")
except ImportError as e:
     CHEATS_IMPORTED = False; log_error("request_router", "import", f"Cheats service import failed: {e}", e); handle_cheat_command = None

# ConfigManager Import
try:
    from services.config_manager import set_user_status
    CONFIG_MANAGER_IMPORTED = True
except ImportError as e:
     CONFIG_MANAGER_IMPORTED = False; log_error("request_router", "import", f"ConfigManager import failed: {e}", e); set_user_status = None


# Load standard messages
_messages = {}
try:
    messages_path = os.path.join("config", "messages.yaml")
    if os.path.exists(messages_path):
        with open(messages_path, 'r', encoding="utf-8") as f:
             content = f.read();
             if content.strip(): f.seek(0); _messages = yaml.safe_load(f) or {}
             else: _messages = {}
    else: log_warning("request_router", "import", f"{messages_path} not found."); _messages = {}
except Exception as e: log_error("request_router", "import", f"Failed to load messages.yaml: {e}", e); _messages = {}

GENERIC_ERROR_MSG = _messages.get("generic_error_message", "Sorry, an unexpected error occurred.")
WELCOME_MESSAGE = _messages.get("welcome_confirmation_message", "Hello! Welcome to WhatsTasker.")


# Global bridge instance
current_bridge = None

def normalize_user_id(user_id: str) -> str:
    """Removes non-digit characters. Specific to phone number IDs."""
    # Consider if other ID types might exist later.
    # For WhatsApp, it might receive 'number@c.us'. We want just the number part usually.
    if user_id and '@' in user_id:
        user_id = user_id.split('@')[0]
    return re.sub(r'\D', '', user_id) if user_id else ""

def set_bridge(bridge_instance):
    """Sets the active bridge instance for sending messages."""
    global current_bridge
    if current_bridge is None:
        current_bridge = bridge_instance
        log_info("request_router", "set_bridge", f"Bridge set to: {type(bridge_instance).__name__}")
    else:
        log_warning("request_router", "set_bridge", "Attempted to set bridge when one is already configured.")


def send_message(user_id: str, message: str):
    """Adds agent message to history, logs it to DB, and sends via configured bridge."""
    fn_name = "send_message"
    # Note: user_id received here is expected to be the *normalized* ID

    # 1. Add to conversation history in memory
    if user_id and message:
        add_message_to_user_history(user_id, "agent", message)
    else:
        log_warning("request_router", fn_name, f"Attempted add empty msg/invalid user_id ({user_id}) to history.")
        # Don't proceed if invalid
        return

    # 2. Log outgoing message to Database
    if ACTIVITY_DB_IMPORTED:
        activity_db.log_message_db(
            direction='OUT',
            user_id=user_id, # Log with normalized ID
            content=message,
            raw_user_id=None, # Raw ID isn't typically available here
            bridge_message_id=None # Bridge ID generated later by the bridge instance
        )
    # else: DB logging disabled or failed import

    # 3. Send via Bridge
    log_info("request_router", fn_name, f"Queuing OUT message for {user_id}: '{message[:100]}...'")
    if current_bridge:
        try:
            # The bridge's send_message method handles formatting (like adding @c.us)
            # and generating the unique bridge_message_id for ACK
            current_bridge.send_message(user_id, message)
        except Exception as e:
            # Log error with user_id context
            log_error("request_router", fn_name, f"Bridge error sending message to {user_id}: {e}", e, user_id=user_id)
    else:
        log_error("request_router", fn_name, "No bridge configured. Cannot send message.")


# --- Main Handler ---
def handle_incoming_message(user_id: str, message: str) -> str:
    """
    Routes incoming messages based on user status (new, onboarding, active) or cheat codes.
    Logs incoming message to DB.
    """
    fn_name = "handle_incoming_message"
    final_response_message = GENERIC_ERROR_MSG
    # Keep the raw user_id received from the bridge for logging/potential use
    raw_user_id = user_id

    try:
        log_info("request_router", fn_name, f"Received raw: {raw_user_id} msg: '{message[:50]}...'")
        norm_user_id = normalize_user_id(raw_user_id)

        if not norm_user_id:
            log_error("request_router", fn_name, f"Invalid User ID after normalization: {raw_user_id}", user_id=raw_user_id) # Log with raw ID context
            # Don't send response here, let the caller handle HTTP error
            return "Error: Invalid User ID." # Or raise exception?

        # --- Log Incoming Message to DB ---
        if ACTIVITY_DB_IMPORTED:
            activity_db.log_message_db(
                direction='IN',
                user_id=norm_user_id, # Log with normalized ID
                content=message,
                raw_user_id=raw_user_id # Store original ID from bridge
            )
        # ---------------------------------

        # --- Ensure agent state exists ---
        agent_state = get_agent(norm_user_id) # Uses normalized ID
        if not agent_state:
            # Log error with normalized ID context
            log_error("request_router", fn_name, f"CRITICAL: Failed get/create agent state for {norm_user_id}.", user_id=norm_user_id)
            # Send generic error back
            send_message(norm_user_id, GENERIC_ERROR_MSG)
            return GENERIC_ERROR_MSG

        current_status = agent_state.get("preferences", {}).get("status")
        log_info("request_router", fn_name, f"User {norm_user_id} status: {current_status}")

        # --- Routing Logic (Uses norm_user_id internally) ---

        # 1. New User: Send welcome, set status to onboarding
        if current_status == "new":
            log_info("request_router", fn_name, f"New user ({norm_user_id}). Sending welcome, setting status to onboarding.")
            send_message(norm_user_id, WELCOME_MESSAGE) # Sends via bridge
            if CONFIG_MANAGER_IMPORTED and set_user_status:
                if not set_user_status(norm_user_id, 'onboarding'):
                     log_error("request_router", fn_name, f"Failed update status from 'new' to 'onboarding' for {norm_user_id}", user_id=norm_user_id)
            else: log_error("request_router", fn_name, "ConfigManager unavailable, cannot update user status after welcome.", user_id=norm_user_id)
            # No further processing needed this turn; the ack is handled by the caller (FastAPI endpoint)
            # We return the message mainly for potential testing/logging in the caller, though the primary action is send_message
            return WELCOME_MESSAGE

        # 2. Cheat Codes (Check before onboarding/active routing)
        message_stripped = message.strip()
        if message_stripped.startswith('/') and CHEATS_IMPORTED and handle_cheat_command:
            parts = message_stripped.split(); command = parts[0].lower(); args = parts[1:]
            log_info("request_router", fn_name, f"Detected command '{command}' for {norm_user_id}. Routing to Cheats.")
            try:
                command_response = handle_cheat_command(norm_user_id, command, args)
            except Exception as e:
                log_error("request_router", fn_name, f"Error executing cheat '{command}': {e}", e, user_id=norm_user_id)
                command_response = "Error processing cheat."
            send_message(norm_user_id, command_response) # Send result back
            return command_response # Return result for caller

        elif message_stripped.startswith('/') and not CHEATS_IMPORTED:
             log_error("request_router", fn_name, "Cheat command detected, but Cheats service failed import.", user_id=norm_user_id)
             err_msg = "Error: Command processor unavailable."
             send_message(norm_user_id, err_msg)
             return err_msg

        # 3. Onboarding User: Route to onboarding agent
        elif current_status == "onboarding":
            log_info("request_router", fn_name, f"User {norm_user_id} is onboarding. Routing to onboarding agent.")
            add_message_to_user_history(norm_user_id, "user", message) # Add user reply to memory history first
            if ONBOARDING_AGENT_IMPORTED and handle_onboarding_request:
                try:
                     history = agent_state.get("conversation_history", [])
                     preferences = agent_state.get("preferences", {})
                     # This function is expected to return the response message string
                     final_response_message = handle_onboarding_request(norm_user_id, message, history, preferences)
                except Exception as onboard_e:
                     tb_str = traceback.format_exc()
                     # Log error with user context
                     log_error("request_router", fn_name, f"Error calling OnboardingAgent for {norm_user_id}. Traceback:\n{tb_str}", onboard_e, user_id=norm_user_id)
                     final_response_message = GENERIC_ERROR_MSG
            else:
                # Log error with user context
                log_error("request_router", fn_name, f"Onboarding required for {norm_user_id}, but onboarding agent not imported.", user_id=norm_user_id)
                final_response_message = "Sorry, there's an issue with the setup process right now."

        # 4. Active User: Route to main orchestrator
        elif current_status == "active":
            log_info("request_router", fn_name, f"User {norm_user_id} is active. Routing to orchestrator.")
            add_message_to_user_history(norm_user_id, "user", message)
            if ORCHESTRATOR_IMPORTED and route_to_orchestrator and QUERY_SERVICE_IMPORTED and get_context_snapshot:
                try:
                    history = agent_state.get("conversation_history", [])
                    preferences = agent_state.get("preferences", {})
                    task_context, calendar_context = get_context_snapshot(norm_user_id)
                    # This function returns the response string
                    final_response_message = route_to_orchestrator(
                        user_id=norm_user_id, message=message, history=history,
                        preferences=preferences, task_context=task_context, calendar_context=calendar_context)
                except Exception as orch_e:
                     tb_str = traceback.format_exc()
                     log_error("request_router", fn_name, f"Error calling OrchestratorAgent for {norm_user_id}. Traceback:\n{tb_str}", orch_e, user_id=norm_user_id)
                     final_response_message = GENERIC_ERROR_MSG
            else:
                 # Log error with user context
                 log_error("request_router", fn_name, f"Core components missing for active user {norm_user_id} (Orchestrator={ORCHESTRATOR_IMPORTED}, QueryService={QUERY_SERVICE_IMPORTED}).", user_id=norm_user_id)
                 final_response_message = "Sorry, I can't process your request right now due to a system issue."

        # 5. Unknown Status: Log error and give generic response
        else:
            log_error("request_router", fn_name, f"User {norm_user_id} has unknown status: '{current_status}'. Sending generic error.", user_id=norm_user_id)
            final_response_message = GENERIC_ERROR_MSG

        # --- Send the final response (if not handled earlier, like for welcome/cheat) ---
        # The agent functions (onboarding/orchestrator) return the message string.
        # We need to send it using our central send_message function.
        if final_response_message:
             send_message(norm_user_id, final_response_message)
        else:
             # This case means the agent function returned None or empty string, which is unexpected.
             log_warning("request_router", fn_name, f"Agent function returned empty response for {norm_user_id}. Sending generic error.", user_id=norm_user_id)
             send_message(norm_user_id, GENERIC_ERROR_MSG)
             final_response_message = GENERIC_ERROR_MSG # Ensure we return something

        # Return the message primarily for testing/ack purposes in the caller API
        return final_response_message

    except Exception as outer_e:
        tb_str = traceback.format_exc()
        # Try to include user ID in log if possible
        user_context = raw_user_id if 'raw_user_id' in locals() else "Unknown"
        log_error("request_router", fn_name, f"Unexpected outer error processing message for {user_context}. Traceback:\n{tb_str}", outer_e, user_id=user_context)
        # Attempt to send generic error if possible
        if 'norm_user_id' in locals() and norm_user_id:
            try: send_message(norm_user_id, GENERIC_ERROR_MSG)
            except: pass # Avoid errors during error handling
        return GENERIC_ERROR_MSG # Return generic error message

# --- END OF FULL bridge/request_router.py ---

# --- END OF FILE bridge/request_router.py ---



================================================================================
ðŸ“„ bridge/cli_interface.py
================================================================================

# --- START OF FILE bridge/cli_interface.py ---

# --- START OF FULL bridge/cli_interface.py ---

from fastapi import FastAPI, Request, HTTPException
from fastapi.responses import JSONResponse
import uvicorn
import uuid
from threading import Lock, Thread
import time
import json # Added for error handling

# Use the central logger
from tools.logger import log_info, log_error, log_warning
# Import the central router and its setter function
from bridge.request_router import handle_incoming_message, set_bridge

# Ensure calendar_tool provides the router correctly (needed for OAuth)
try:
    from tools.calendar_tool import router as calendar_router
    CALENDAR_ROUTER_IMPORTED = True
    log_info("cli_interface", "import", "Successfully imported calendar_router.")
except ImportError:
    log_error("cli_interface", "import", "Could not import calendar_router from tools.calendar_tool. OAuth callback will fail if CLI mode used.")
    CALENDAR_ROUTER_IMPORTED = False
    from fastapi import APIRouter
    calendar_router = APIRouter()


# Define a CLI Bridge
# Global in-memory store for CLI outgoing messages.
outgoing_cli_messages = []
cli_queue_lock = Lock()

class CLIBridge:
    """Bridge that handles message queuing for CLI interaction."""
    def __init__(self, message_queue, lock):
        self.message_queue = message_queue
        self.lock = lock
        log_info("CLIBridge", "__init__", "CLI Bridge initialized for queuing.")

    # --- UPDATED send_message ---
    def send_message(self, user_id: str, message: str):
        """
        Adds the outgoing message to the CLI queue.
        Does NOT log the message content here (handled by request_router).
        """
        # user_id received here is the NORMALIZED ID from request_router
        if not user_id or not message:
             log_warning("CLIBridge", "send_message", f"Attempted to queue empty message or invalid user_id for CLI: {user_id}")
             return

        outgoing = {
            "user_id": user_id, # Use the normalized ID for CLI mock
            "message": message,
            "message_id": str(uuid.uuid4()) # Generate ID, might be used by mock sender ACK
        }
        with self.lock:
            self.message_queue.append(outgoing)
        # Log the queuing action
        log_info("CLIBridge", "send_message", f"Message for CLI user {user_id} queued (ID: {outgoing['message_id']}). Queue size: {len(self.message_queue)}")
    # --- END UPDATED send_message ---

# Set the global bridge in the router to use our CLI Bridge instance
# This should only be called by main.py if CLI mode is selected
# if __name__ != "__main__": # Crude check
#     set_bridge(CLIBridge(outgoing_cli_messages, cli_queue_lock))
#     log_info("cli_interface", "init", "CLI Bridge potentially set in request_router.")

def create_cli_app() -> FastAPI:
    """Creates the FastAPI app instance for the CLI Interface."""
    app = FastAPI(
        title="WhatsTasker CLI Bridge API",
        description="Handles interaction for the CLI mock sender.",
        version="1.0.0"
    )

    # Include calendar routes if needed
    if CALENDAR_ROUTER_IMPORTED:
        app.include_router(calendar_router, prefix="", tags=["Authentication"])
        log_info("cli_interface", "create_cli_app", "Calendar router included.")
    else:
         log_warning("cli_interface", "create_cli_app", "Calendar router not included.")


    # --- API Endpoints (Adjusted for CLI mock) ---
    @app.post("/incoming", tags=["CLI Bridge"])
    async def incoming_cli_message(request: Request):
        """Receives message from CLI mock, processes it, queues response, returns ack."""
        endpoint_name = "incoming_cli_message"
        try:
            data = await request.json()
            user_id = data.get("user_id") # Expecting normalized ID from mock sender
            message = data.get("message")
            if not user_id or message is None:
                log_warning("cli_interface", endpoint_name, f"Received invalid payload: {data}")
                raise HTTPException(status_code=400, detail="Missing user_id or message")

            log_info("cli_interface", endpoint_name, f"Received message via CLI bridge from user {user_id}: '{str(message)[:50]}...'")

            # Pass normalized ID to router, router handles DB logging
            handle_incoming_message(user_id, str(message))

            # Return only an acknowledgment.
            return JSONResponse(content={"ack": True})

        except json.JSONDecodeError:
            log_error("cli_interface", endpoint_name, "Received non-JSON payload.")
            raise HTTPException(status_code=400, detail="Invalid JSON payload")
        except HTTPException as http_exc:
            raise http_exc
        except Exception as e:
            log_error("cli_interface", endpoint_name, "Error processing incoming CLI message", e)
            raise HTTPException(status_code=500, detail="Internal server error processing message")

    @app.get("/outgoing", tags=["CLI Bridge"])
    async def get_outgoing_cli_messages():
        """Returns and clears the list of queued outgoing messages for the CLI mock."""
        # This endpoint *differs* from the WhatsApp one - it clears on GET
        endpoint_name = "get_outgoing_cli_messages"
        msgs_to_send = []
        with cli_queue_lock:
            # Return all messages currently in the queue and clear it
            msgs_to_send = outgoing_cli_messages[:] # Copy the list
            outgoing_cli_messages.clear()          # Clear the original list
        if msgs_to_send:
            log_info("cli_interface", endpoint_name, f"Returning {len(msgs_to_send)} messages from CLI queue (and clearing).")
        return JSONResponse(content={"messages": msgs_to_send})

    @app.post("/ack", tags=["CLI Bridge"])
    async def acknowledge_cli_message(request: Request):
        """Receives acknowledgment (currently does nothing for CLI as queue is cleared on GET)."""
        endpoint_name = "acknowledge_cli_message"
        try:
            data = await request.json()
            message_id = data.get("message_id")
            if not message_id:
                log_warning("cli_interface", endpoint_name, f"Received ACK without message_id: {data}")
                raise HTTPException(status_code=400, detail="Missing message_id")

            # Log but don't modify queue here, as GET already cleared it for CLI mock
            log_info("cli_interface", endpoint_name, f"CLI Ack received for message {message_id} (queue already cleared by GET).")
            return JSONResponse(content={"ack_received": True, "removed": False}) # Indicate not removed by ACK
        except json.JSONDecodeError:
            log_error("cli_interface", endpoint_name, "Received non-JSON ACK payload.")
            raise HTTPException(status_code=400, detail="Invalid JSON payload for ACK")
        except HTTPException as http_exc:
            raise http_exc
        except Exception as e:
            log_error("cli_interface", endpoint_name, f"Error processing CLI ACK for message_id {data.get('message_id', 'N/A')}", e)
            raise HTTPException(status_code=500, detail="Internal server error processing ACK")

    return app

# Create the FastAPI app instance for this interface
# main.py should import 'app' from here if cli mode is selected
app = create_cli_app()

# --- END OF FULL bridge/cli_interface.py ---

# --- END OF FILE bridge/cli_interface.py ---



================================================================================
ðŸ“„ bridge/whatsapp_interface.py
================================================================================

# --- START OF FILE bridge/whatsapp_interface.py ---

# --- START OF FULL bridge/whatsapp_interface.py ---

from fastapi import FastAPI, Request, HTTPException
from fastapi.responses import JSONResponse
import uvicorn # Keep for potential direct running/debugging
import uuid
from threading import Lock
import json # Import json for error handling
import re # Import re for checking format

# Use the central logger
from tools.logger import log_info, log_error, log_warning
# Import the central router and its setter function
from bridge.request_router import handle_incoming_message, set_bridge

# Try importing the calendar router (necessary for OAuth callback)
try:
    from tools.calendar_tool import router as calendar_router
    CALENDAR_ROUTER_IMPORTED = True
    log_info("whatsapp_interface", "import", "Successfully imported calendar_router.")
except ImportError:
    log_error("whatsapp_interface", "import", "Could not import calendar_router from tools.calendar_tool. OAuth callback will fail.")
    CALENDAR_ROUTER_IMPORTED = False
    # Define a dummy router if import fails to allow server start
    from fastapi import APIRouter
    calendar_router = APIRouter()

# --- Bridge Definition ---
# Global in-memory store for outgoing messages destined for WhatsApp and its lock.
outgoing_whatsapp_messages = []
whatsapp_queue_lock = Lock()

class WhatsAppBridge:
    """Bridge that handles message queuing for the Node.js WhatsApp Poller."""
    def __init__(self, message_queue, lock):
        self.message_queue = message_queue
        self.lock = lock
        log_info("WhatsAppBridge", "__init__", "WhatsApp Bridge initialized for queuing.")

    # --- UPDATED send_message ---
    def send_message(self, user_id: str, message: str):
        """
        Adds the outgoing message with a unique ID to the shared WhatsApp queue.
        Ensures the user_id has the correct '@c.us' suffix for WhatsApp.
        Does NOT log the message content here (handled by request_router).
        """
        # user_id received here is the NORMALIZED ID from request_router
        if not user_id or not message:
             log_warning("WhatsAppBridge", "send_message", f"Attempted to queue empty message or invalid user_id for WhatsApp: {user_id}")
             return

        # --- Format User ID for WhatsApp Web JS ---
        formatted_user_id = user_id
        # Assume normalized ID is digits only. Add @c.us suffix.
        # (Add @g.us logic later if groups are supported)
        if re.match(r'^\d+$', user_id):
            formatted_user_id = f"{user_id}@c.us"
            # Log the formatting action, but not the content
            # log_info("WhatsAppBridge", "send_message", f"Formatted user_id {user_id} -> {formatted_user_id}")
        elif '@' not in user_id:
             # If it's not clearly a phone number and has no @, log a warning.
             log_warning("WhatsAppBridge", "send_message", f"User ID '{user_id}' lacks '@' suffix and is not digits. Sending as is, may fail in whatsapp-web.js.")
        # --- END User ID Formatting ---

        outgoing = {
            "user_id": formatted_user_id, # Use the WhatsApp-formatted ID
            "message": message,
            "message_id": str(uuid.uuid4()) # Unique ID for ACK tracking
        }
        with self.lock:
            self.message_queue.append(outgoing)
        # Log the queuing action itself, including the message ID for traceability
        log_info("WhatsAppBridge", "send_message", f"Message for WA user {formatted_user_id} queued (ID: {outgoing['message_id']}). Queue size: {len(self.message_queue)}")
    # --- END UPDATED send_message ---

# Set the global bridge in the request router to use our WhatsApp Bridge instance
# Pass the shared queue and lock to the bridge instance
# Ensure this happens only if the WhatsApp bridge is selected in main.py
# This initialization logic might need refinement depending on how main.py sets the bridge.
# Assuming main.py calls set_bridge after importing the correct module based on selection:
# if __name__ != "__main__": # Crude check to avoid running this if module imported by mistake?
#    set_bridge(WhatsAppBridge(outgoing_whatsapp_messages, whatsapp_queue_lock))
#    log_info("whatsapp_interface", "init", "WhatsApp Bridge potentially set in request_router.")
# A better approach is for main.py to explicitly call set_bridge after determining the mode.
# Let's assume main.py handles calling set_bridge(WhatsAppBridge(...))

def create_whatsapp_app() -> FastAPI:
    """Creates the FastAPI application instance for the WhatsApp Interface."""
    app = FastAPI(
        title="WhatsTasker WhatsApp Bridge API",
        description="Handles incoming messages from and outgoing messages to the WhatsApp Web JS bridge.",
        version="1.0.0"
    )

    if CALENDAR_ROUTER_IMPORTED:
        app.include_router(calendar_router, prefix="", tags=["Authentication"])
        log_info("whatsapp_interface", "create_app", "Calendar router included.")
    else:
         log_warning("whatsapp_interface", "create_app", "Calendar router not included.")

    # --- API Endpoints (Keep as they are) ---
    @app.post("/incoming", tags=["WhatsApp Bridge"])
    async def incoming_whatsapp_message(request: Request):
        endpoint_name = "incoming_whatsapp_message"
        try:
            data = await request.json()
            user_id = data.get("user_id")
            message_body = data.get("message")
            if not user_id or message_body is None:
                log_warning("whatsapp_interface", endpoint_name, f"Received invalid payload: {data}")
                raise HTTPException(status_code=400, detail="Missing user_id or message")
            # Pass raw ID to router, router handles normalization and DB logging
            handle_incoming_message(user_id, str(message_body))
            return JSONResponse(content={"ack": True}, status_code=200)
        except json.JSONDecodeError: # Renamed variable
            log_error("whatsapp_interface", endpoint_name, "Received non-JSON payload.")
            raise HTTPException(status_code=400, detail="Invalid JSON payload")
        except HTTPException as http_exc:
            raise http_exc
        except Exception as e:
            log_error("whatsapp_interface", endpoint_name, "Error processing incoming WhatsApp message", e)
            raise HTTPException(status_code=500, detail="Internal server error processing message")

    @app.get("/outgoing", tags=["WhatsApp Bridge"])
    async def get_outgoing_whatsapp_messages():
        endpoint_name = "get_outgoing_whatsapp_messages"
        msgs_to_send = []
        with whatsapp_queue_lock:
            msgs_to_send = outgoing_whatsapp_messages[:]
        if msgs_to_send:
            log_info("whatsapp_interface", endpoint_name, f"Returning {len(msgs_to_send)} messages from WA queue (without clearing).")
        return JSONResponse(content={"messages": msgs_to_send})

    @app.post("/ack", tags=["WhatsApp Bridge"])
    async def acknowledge_whatsapp_message(request: Request):
        endpoint_name = "acknowledge_whatsapp_message"
        message_id = None
        try:
            data = await request.json()
            message_id = data.get("message_id")
            if not message_id:
                log_warning("whatsapp_interface", endpoint_name, f"Received ACK without message_id: {data}")
                raise HTTPException(status_code=400, detail="Missing message_id in ACK payload")
            removed = False
            with whatsapp_queue_lock:
                index_to_remove = -1
                for i, msg in enumerate(outgoing_whatsapp_messages):
                    if msg.get("message_id") == message_id:
                        index_to_remove = i
                        break
                if index_to_remove != -1:
                    removed_msg = outgoing_whatsapp_messages.pop(index_to_remove)
                    removed = True
                    log_info("whatsapp_interface", endpoint_name, f"WA ACK received and message removed for ID: {message_id}. User: {removed_msg.get('user_id')}. Queue size: {len(outgoing_whatsapp_messages)}")
                else:
                    log_warning("whatsapp_interface", endpoint_name, f"WA ACK received for unknown/already removed message ID: {message_id}")
            return JSONResponse(content={"ack_received": True, "removed": removed})
        except json.JSONDecodeError:
            log_error("whatsapp_interface", endpoint_name, "Received non-JSON ACK payload.")
            raise HTTPException(status_code=400, detail="Invalid JSON payload for ACK")
        except HTTPException as http_exc:
            raise http_exc
        except Exception as e:
            log_error("whatsapp_interface", endpoint_name, f"Error processing WA ACK for message_id {message_id or 'N/A'}", e)
            raise HTTPException(status_code=500, detail="Internal server error processing ACK")

    return app

# Create the FastAPI app instance for this interface
# main.py should import 'app' from here if whatsapp mode is selected
app = create_whatsapp_app()

# --- END OF FULL bridge/whatsapp_interface.py ---

# --- END OF FILE bridge/whatsapp_interface.py ---



================================================================================
ðŸ“„ agents/orchestrator_agent.py
================================================================================

# --- START OF FILE agents/orchestrator_agent.py ---

# --- START OF FILE agents/orchestrator_agent.py ---
import json
import os
import traceback
from typing import Dict, List, Optional, Any
from datetime import datetime
import pytz
import re

# Instructor/LLM Imports
from services.llm_interface import get_instructor_client
from openai import OpenAI # Base client for types if needed
from openai.types.chat import ChatCompletionMessage, ChatCompletionToolParam, ChatCompletionMessageToolCall
from openai.types.chat.chat_completion_message_tool_call import Function
from openai.types.shared_params import FunctionDefinition

# Tool Imports & Helpers
from .tool_definitions import AVAILABLE_TOOLS, TOOL_PARAM_MODELS
import services.task_manager as task_manager # For parsing duration used in prompt prep

# Utilities
from tools.logger import log_info, log_error, log_warning
import yaml
import pydantic

# --- Load Standard Messages ---
_messages = {}
try:
    messages_path = os.path.join("config", "messages.yaml")
    if os.path.exists(messages_path):
        with open(messages_path, 'r', encoding="utf-8") as f:
             content = f.read()
             if content.strip(): f.seek(0); _messages = yaml.safe_load(f) or {}
             else: _messages = {}
    else: log_warning("orchestrator_agent", "init", f"{messages_path} not found."); _messages = {}
except Exception as e: log_error("orchestrator_agent", "init", f"Failed to load messages.yaml: {e}", e); _messages = {}
GENERIC_ERROR_MSG = _messages.get("generic_error_message", "Sorry, an unexpected error occurred.")


# --- Function to Load Prompt ---
_PROMPT_CACHE = {}
def load_orchestrator_prompt() -> Optional[str]:
    """Loads the orchestrator system prompt from the YAML file, with simple caching."""
    prompts_path = os.path.join("config", "prompts.yaml"); cache_key = prompts_path
    if cache_key in _PROMPT_CACHE: return _PROMPT_CACHE[cache_key]
    prompt_text_result: Optional[str] = None
    try:
        if not os.path.exists(prompts_path): raise FileNotFoundError(f"{prompts_path} not found.")
        with open(prompts_path, "r", encoding="utf-8") as f:
            content = f.read();
            if not content.strip(): raise ValueError("Prompts file is empty.")
            f.seek(0); all_prompts = yaml.safe_load(f)
            if not all_prompts: raise ValueError("YAML parsing resulted in empty prompts.")
            prompt_text = all_prompts.get("orchestrator_agent_system_prompt")
            if not prompt_text or not prompt_text.strip(): log_error("orchestrator_agent", "load_orchestrator_prompt", "Key 'orchestrator_agent_system_prompt' NOT FOUND or EMPTY."); prompt_text_result = None
            else: log_info("orchestrator_agent", "load_orchestrator_prompt", "Orchestrator prompt loaded successfully."); prompt_text_result = prompt_text
    except Exception as e: log_error("orchestrator_agent", "load_orchestrator_prompt", f"CRITICAL: Failed to load/parse orchestrator prompt: {e}", e); prompt_text_result = None
    _PROMPT_CACHE[cache_key] = prompt_text_result; return prompt_text_result

# --- REMOVED Pending Context Helpers ---

# --- Main Handler Function ---
# --- Main Handler Function (Corrected Version) ---
def handle_user_request(
    user_id: str, message: str, history: List[Dict], preferences: Dict,
    task_context: List[Dict], calendar_context: List[Dict]
) -> str:
    """
    Handles the user's request using the Orchestrator pattern with Instructor/Tool Use.
    Relies purely on the LLM for conversational flow and tool invocation decisions.
    Handles multiple tool calls requested by the LLM in a single turn.
    """
    fn_name = "handle_user_request"
    log_info("orchestrator_agent", fn_name, f"Processing request for {user_id}: '{message[:50]}...'")

    orchestrator_system_prompt = load_orchestrator_prompt()
    if not orchestrator_system_prompt:
         log_error("orchestrator_agent", fn_name, "Orchestrator system prompt could not be loaded.")
         return GENERIC_ERROR_MSG

    client: OpenAI = get_instructor_client()
    if not client:
        log_error("orchestrator_agent", fn_name, "LLM client unavailable.")
        return GENERIC_ERROR_MSG

    # --- Prepare Context for Prompt (No change needed here) ---
    try:
        user_timezone_str = preferences.get("TimeZone", "UTC"); user_timezone = pytz.utc
        try: user_timezone = pytz.timezone(user_timezone_str)
        except pytz.UnknownTimeZoneError: log_warning("orchestrator_agent", fn_name, f"Unknown timezone '{user_timezone_str}'. Defaulting UTC.")
        now = datetime.now(user_timezone)
        current_date_str, current_time_str, current_day_str = now.strftime("%Y-%m-%d"), now.strftime("%H:%M"), now.strftime("%A")
        history_limit, item_context_limit, calendar_context_limit = 30, 20, 20
        limited_history = history[-(history_limit*2):]; history_str = "\n".join([f"{m['sender'].capitalize()}: {m['content']}" for m in limited_history])
        def prepare_item(item):
             key_fields = ['event_id', 'item_id', 'title', 'description', 'date', 'time', 'type', 'status', 'estimated_duration', 'project']
             prep = {k: item.get(k) for k in key_fields if item.get(k) is not None}; prep['item_id'] = item.get('item_id', item.get('event_id'))
             if 'event_id' in prep and prep['event_id'] == prep['item_id']: del prep['event_id']
             return prep
        item_context_str = json.dumps([prepare_item(item) for item in task_context[:item_context_limit]], indent=2, default=str)
        calendar_context_str = json.dumps(calendar_context[:calendar_context_limit], indent=2, default=str)
        prefs_str = json.dumps(preferences, indent=2, default=str)
    except Exception as e: log_error("orchestrator_agent", fn_name, f"Error preparing context strings: {e}", e); return GENERIC_ERROR_MSG

    # --- Construct Initial Messages for LLM (No change needed here) ---
    messages: List[Dict[str, Any]] = [
        {"role": "system", "content": orchestrator_system_prompt},
        {"role": "system", "content": f"Current Reference Time ({user_timezone_str}): Date: {current_date_str}, Day: {current_day_str}, Time: {current_time_str}. Use this."},
        {"role": "system", "content": f"User Preferences:\n{prefs_str}"},
        {"role": "system", "content": f"History:\n{history_str}"},
        {"role": "system", "content": f"Active Items:\n{item_context_str}"},
        {"role": "system", "content": f"Calendar Events:\n{calendar_context_str}"},
        {"role": "user", "content": message}
    ]

    # --- Define Tools (No change needed here) ---
    tools_for_llm: List[ChatCompletionToolParam] = []
    for tool_name, model in TOOL_PARAM_MODELS.items():
        if tool_name not in AVAILABLE_TOOLS: continue
        func = AVAILABLE_TOOLS.get(tool_name)
        description = func.__doc__.strip() if func and func.__doc__ else f"Executes {tool_name}"
        try:
            params_schema = model.model_json_schema();
            if not params_schema.get('properties'): params_schema = {}
            func_def: FunctionDefinition = {"name": tool_name, "description": description, "parameters": params_schema}
            tool_param: ChatCompletionToolParam = {"type": "function", "function": func_def}; tools_for_llm.append(tool_param)
        except Exception as e: log_error("orchestrator_agent", fn_name, f"Schema error for tool {tool_name}: {e}", e)
    if not tools_for_llm: log_error("orchestrator_agent", fn_name, "No tools defined for LLM."); return GENERIC_ERROR_MSG

    # --- Interaction Loop ---
    try:
        log_info("orchestrator_agent", fn_name, f"Invoking LLM for {user_id} (Initial Turn)...")
        response = client.chat.completions.create(
            model="gpt-4o", messages=messages, tools=tools_for_llm, tool_choice="auto", temperature=0.1,
        )
        response_message = response.choices[0].message
        tool_calls = response_message.tool_calls

        # --- Scenario 1: LLM Responds Directly ---
        if not tool_calls:
            if response_message.content:
                log_info("orchestrator_agent", fn_name, "LLM responded directly (no tool call).")
                return response_message.content
            else:
                log_warning("orchestrator_agent", fn_name, "LLM response had no tool calls and no content.")
                return "Sorry, I couldn't process that request fully. Could you try rephrasing?"

        # --- Scenario 2: LLM Calls One or More Tools ---
        log_info("orchestrator_agent", fn_name, f"LLM requested {len(tool_calls)} tool call(s).")
        # Append the assistant's message containing the tool call requests
        messages.append(response_message.model_dump(exclude_unset=True))
        tool_results_messages = [] # Store results to append later

        # --- Loop through ALL requested tool calls ---
        for tool_call in tool_calls:
            tool_name = tool_call.function.name
            tool_call_id = tool_call.id
            tool_args_str = tool_call.function.arguments
            tool_result_content = GENERIC_ERROR_MSG # Default in case of error

            log_info("orchestrator_agent", fn_name, f"Processing Tool Call ID: {tool_call_id}, Name: {tool_name}, Args: {tool_args_str[:150]}...")

            if tool_name not in AVAILABLE_TOOLS:
                log_warning("orchestrator_agent", fn_name, f"LLM tried unknown tool: {tool_name}. Sending error result back.")
                tool_result_content = json.dumps({"success": False, "message": f"Error: Unknown action '{tool_name}' requested."})
            else:
                tool_func = AVAILABLE_TOOLS[tool_name]
                param_model = TOOL_PARAM_MODELS[tool_name]
                try:
                    # Parse and validate arguments
                    tool_args_dict = {}
                    if tool_args_str and tool_args_str.strip() != '{}': tool_args_dict = json.loads(tool_args_str)
                    elif not param_model.model_fields: tool_args_dict = {}
                    validated_params = param_model(**tool_args_dict)

                    # Execute the tool function
                    tool_result_dict = tool_func(user_id, validated_params)
                    log_info("orchestrator_agent", fn_name, f"Tool {tool_name} (ID: {tool_call_id}) executed. Result: {tool_result_dict}")
                    tool_result_content = json.dumps(tool_result_dict)

                except json.JSONDecodeError:
                    log_error("orchestrator_agent", fn_name, f"Failed parse JSON args for {tool_name} (ID: {tool_call_id}): {tool_args_str}");
                    tool_result_content = json.dumps({"success": False, "message": f"Error: Invalid arguments provided for {tool_name}."})
                except pydantic.ValidationError as e:
                    log_error("orchestrator_agent", fn_name, f"Arg validation failed for {tool_name} (ID: {tool_call_id}). Err: {e.errors()}. Args: {tool_args_str}", e)
                    err_summary = "; ".join([f"{err['loc'][0] if err.get('loc') else 'param'}: {err['msg']}" for err in e.errors()])
                    tool_result_content = json.dumps({"success": False, "message": f"Error: Invalid parameters for {tool_name} - {err_summary}"})
                except Exception as e:
                    log_error("orchestrator_agent", fn_name, f"Error executing tool {tool_name} (ID: {tool_call_id}). Trace:\n{traceback.format_exc()}", e);
                    tool_result_content = json.dumps({"success": False, "message": f"Error performing action {tool_name}."})

            # Append the result message for THIS tool call
            tool_results_messages.append({
                "tool_call_id": tool_call_id,
                "role": "tool",
                "name": tool_name,
                "content": tool_result_content,
            })
        # --- End loop through tool calls ---

        # Append ALL tool result messages to the main message history
        messages.extend(tool_results_messages)

        # --- Make SECOND LLM call with ALL tool results ---
        log_info("orchestrator_agent", fn_name, f"Invoking LLM again for {user_id} with {len(tool_results_messages)} tool result(s)...")
        second_response = client.chat.completions.create(
            model="gpt-4o", # Use the same model
            messages=messages,
            # No tools needed here, LLM should just generate response based on results
        )
        second_response_message = second_response.choices[0].message

        if second_response_message.content:
            log_info("orchestrator_agent", fn_name, "LLM generated final response after processing tool result(s).")
            return second_response_message.content
        else:
            log_warning("orchestrator_agent", fn_name, "LLM provided no content after processing tool result(s).")
            # Try to construct a fallback from the first tool's message if possible
            try: fallback_msg = json.loads(tool_results_messages[0]['content']).get("message", GENERIC_ERROR_MSG)
            except: fallback_msg = GENERIC_ERROR_MSG
            return fallback_msg

    # --- Outer Exception Handling ---
    except Exception as e:
        tb_str = traceback.format_exc()
        log_error("orchestrator_agent", fn_name, f"Core error in orchestrator logic for {user_id}. Traceback:\n{tb_str}", e)
        return GENERIC_ERROR_MSG

    # --- Outer Exception Handling ---
    except Exception as e:
        tb_str = traceback.format_exc()
        log_error("orchestrator_agent", fn_name, f"Core error in orchestrator logic for {user_id}. Traceback:\n{tb_str}", e)
        return GENERIC_ERROR_MSG
# --- END OF FILE agents/orchestrator_agent.py ---

# --- END OF FILE agents/orchestrator_agent.py ---



================================================================================
ðŸ“„ agents/onboarding_agent.py
================================================================================

# --- START OF FILE agents/onboarding_agent.py ---

# --- START OF FILE agents/onboarding_agent.py ---
import json
import os
import traceback
from typing import Dict, List, Optional, Any
from datetime import datetime
import pytz

# Instructor/LLM Imports
from services.llm_interface import get_instructor_client
from openai import OpenAI
from openai.types.chat import ChatCompletionMessage, ChatCompletionToolParam, ChatCompletionMessageToolCall
from openai.types.chat.chat_completion_message_tool_call import Function
from openai.types.shared_params import FunctionDefinition

# Tool Imports (Limited Set)
from .tool_definitions import (
    AVAILABLE_TOOLS,
    TOOL_PARAM_MODELS,
    UpdateUserPreferencesParams,
    InitiateCalendarConnectionParams,
)

# Utilities
from tools.logger import log_info, log_error, log_warning
import yaml
import pydantic

# --- Load Standard Messages ---
_messages = {}
try:
    messages_path = os.path.join("config", "messages.yaml")
    if os.path.exists(messages_path):
        with open(messages_path, 'r', encoding="utf-8") as f:
             content = f.read()
             if content.strip(): f.seek(0); _messages = yaml.safe_load(f) or {}
             else: _messages = {}
    else: log_warning("onboarding_agent", "init", f"{messages_path} not found."); _messages = {}
except Exception as e: log_error("onboarding_agent", "init", f"Failed to load messages.yaml: {e}", e); _messages = {}
GENERIC_ERROR_MSG = _messages.get("generic_error_message", "Sorry, an unexpected error occurred.")
SETUP_START_MSG = _messages.get("setup_starting_message", "Great! Let's set things up.")

# --- Function to Load Onboarding Prompt ---
_ONBOARDING_PROMPT_CACHE = {}
def load_onboarding_prompt() -> Optional[str]:
    """Loads the onboarding system prompt from the YAML file, with caching."""
    prompts_path = os.path.join("config", "prompts.yaml")
    cache_key = prompts_path + "_onboarding"
    if cache_key in _ONBOARDING_PROMPT_CACHE:
        return _ONBOARDING_PROMPT_CACHE[cache_key]

    prompt_text_result: Optional[str] = None
    try:
        if not os.path.exists(prompts_path): raise FileNotFoundError(f"{prompts_path} not found.")
        with open(prompts_path, "r", encoding="utf-8") as f:
            content = f.read();
            if not content.strip(): raise ValueError("Prompts file is empty.")
            f.seek(0); all_prompts = yaml.safe_load(f)
            if not all_prompts: raise ValueError("YAML parsing resulted in empty prompts.")
            prompt_text = all_prompts.get("onboarding_agent_system_prompt")
            if not prompt_text or not prompt_text.strip():
                log_error("onboarding_agent", "load_onboarding_prompt", "Key 'onboarding_agent_system_prompt' NOT FOUND or EMPTY.")
                prompt_text_result = None
            else:
                log_info("onboarding_agent", "load_onboarding_prompt", "Onboarding prompt loaded successfully.")
                prompt_text_result = prompt_text
    except Exception as e:
        log_error("onboarding_agent", "load_onboarding_prompt", f"CRITICAL: Failed load/parse onboarding prompt: {e}", e)
        prompt_text_result = None

    _ONBOARDING_PROMPT_CACHE[cache_key] = prompt_text_result
    return prompt_text_result

# --- REMOVED Onboarding State Helpers ---
# The pure LLM flow relies on history and current prefs in context

# --- Helper for LLM Clarification (If needed for onboarding - keep for now) ---
def _get_clarification_from_llm(client: OpenAI, question: str, user_reply: str, expected_choices: List[str]) -> str:
    """Uses LLM to interpret user reply against expected choices."""
    # This helper might still be useful if the LLM asks a yes/no for calendar
    log_info("onboarding_agent", "_get_clarification_from_llm", f"Asking LLM to clarify: Q='{question}' Reply='{user_reply}' Choices={expected_choices}")
    system_message = f"""
You are helping a user interact with a task assistant during setup.
The assistant asked the user a question, and the user replied.
Your task is to determine which of the expected choices the user's reply corresponds to.
The original question was: "{question}"
The user's reply was: "{user_reply}"
The expected choices are: {expected_choices}

Analyze the user's reply and determine the choice.
Respond ONLY with one of the following exact strings: {', '.join([f"'{choice}'" for choice in expected_choices])} or 'unclear'.
"""
    try:
        response = client.chat.completions.create(
            model="gpt-3.5-turbo", # Use a cheaper/faster model for clarification
            messages=[{"role": "system", "content": system_message}],
            temperature=0.0,
            max_tokens=10
        )
        llm_choice = response.choices[0].message.content.strip().lower().replace("'", "")
        log_info("onboarding_agent", "_get_clarification_from_llm", f"LLM clarification result: '{llm_choice}'")
        if llm_choice in expected_choices or llm_choice == 'unclear':
            return llm_choice
        else:
            log_warning("onboarding_agent", "_get_clarification_from_llm", f"LLM returned unexpected clarification: '{llm_choice}'")
            return 'unclear'
    except Exception as e:
        log_error("onboarding_agent", "_get_clarification_from_llm", f"Error during LLM clarification call: {e}", e)
        return 'unclear'

# --- Main Onboarding Handler Function ---
def handle_onboarding_request(
    user_id: str,
    message: str,
    history: List[Dict],
    preferences: Dict # Current preferences passed in
) -> str:
    """Handles the user's request during the onboarding phase using pure LLM flow."""
    log_info("onboarding_agent", "handle_onboarding_request", f"Handling onboarding request for {user_id}: '{message[:50]}...'")
    fn_name = "handle_onboarding_request" # Use function name for logging

    onboarding_system_prompt = load_onboarding_prompt()
    if not onboarding_system_prompt:
         log_error(fn_name, "load_onboarding_prompt", "Onboarding system prompt could not be loaded.") # Corrected log identifier
         return GENERIC_ERROR_MSG

    client: OpenAI = get_instructor_client()
    if not client:
        log_error(fn_name, "get_instructor_client", "LLM client unavailable.") # Corrected log identifier
        return GENERIC_ERROR_MSG

    # --- Prepare Context (Simpler than main orchestrator, focuses on prefs) ---
    try:
        user_timezone_str = preferences.get("TimeZone", "UTC"); user_timezone = pytz.utc
        try:
            user_timezone = pytz.timezone(user_timezone_str)
        except pytz.UnknownTimeZoneError:
            log_warning(fn_name, "prepare_context", f"Unknown timezone '{user_timezone_str}'. Using UTC.") # Corrected log identifier
        now = datetime.now(user_timezone); current_date_str = now.strftime("%Y-%m-%d"); current_time_str = now.strftime("%H:%M"); current_day_str = now.strftime("%A")

        history_limit = 10
        limited_history = history[-(history_limit*2):]
        history_str = "\n".join([f"{m['sender'].capitalize()}: {m['content']}" for m in limited_history])

        prefs_str = json.dumps(preferences, indent=2, default=str)

        initial_interaction = len(history) <= 1 # If only user message is present
        intro_message = SETUP_START_MSG if initial_interaction else ""

    except Exception as e:
        log_error(fn_name, "prepare_context", f"Error preparing context: {e}", e) # Corrected log identifier
        return GENERIC_ERROR_MSG

    # --- Construct Messages for Onboarding LLM ---
    messages: List[Dict[str, Any]] = [
        {"role": "system", "content": onboarding_system_prompt},
        {"role": "system", "content": f"Current User Preferences:\n```json\n{prefs_str}\n```"},
        {"role": "system", "content": f"Conversation History:\n{history_str}"},
        {"role": "user", "content": message}
    ]
    if intro_message and initial_interaction:
         messages.insert(-1, {"role": "assistant", "content": intro_message})


    # --- Define Tools AVAILABLE for Onboarding ---
    onboarding_tools_available = {
        "update_user_preferences": AVAILABLE_TOOLS.get("update_user_preferences"),
        "initiate_calendar_connection": AVAILABLE_TOOLS.get("initiate_calendar_connection"),
    }
    onboarding_tool_models = {
        "update_user_preferences": UpdateUserPreferencesParams,
        "initiate_calendar_connection": InitiateCalendarConnectionParams,
    }
    onboarding_tools_available = {k:v for k,v in onboarding_tools_available.items() if v is not None}

    tools_for_llm: List[ChatCompletionToolParam] = []
    for tool_name, model in onboarding_tool_models.items():
        if tool_name not in onboarding_tools_available: continue
        func = onboarding_tools_available.get(tool_name)
        description = func.__doc__.strip() if func and func.__doc__ else f"Executes {tool_name}"
        try:
            params_schema = model.model_json_schema();
            if not params_schema.get('properties'): params_schema = {}
            func_def: FunctionDefinition = {"name": tool_name, "description": description, "parameters": params_schema}
            tool_param: ChatCompletionToolParam = {"type": "function", "function": func_def}; tools_for_llm.append(tool_param)
        except Exception as e: log_error(fn_name, "define_tools", f"Schema error for onboarding tool {tool_name}: {e}", e) # Corrected log identifier
    # It's okay if tools_for_llm is empty if LLM decides to just talk


    # --- Interact with LLM (using same two-step logic as orchestrator) ---
    try:
        log_info(fn_name, "LLM_call_1", f"Invoking Onboarding LLM for {user_id}...") # Corrected log identifier
        response = client.chat.completions.create(
            model="gpt-4o", # Or gpt-3.5-turbo
            messages=messages,
            tools=tools_for_llm if tools_for_llm else None,
            tool_choice="auto" if tools_for_llm else None,
            temperature=0.1,
        )
        response_message = response.choices[0].message
        tool_calls = response_message.tool_calls

        # --- Scenario 1: LLM Responds Directly (Asks question, gives info) ---
        if not tool_calls:
            if response_message.content:
                log_info(fn_name, "LLM_call_1", "Onboarding LLM responded directly.") # Corrected log identifier
                return response_message.content
            else:
                log_warning(fn_name, "LLM_call_1", "Onboarding LLM response had no tool calls and no content.") # Corrected log identifier
                return "Sorry, I got stuck. Can you tell me what we were discussing?"

        # --- Scenario 2: LLM Calls an Onboarding Tool ---
        tool_call: ChatCompletionMessageToolCall = tool_calls[0]
        tool_name = tool_call.function.name
        tool_call_id = tool_call.id

        if tool_name not in onboarding_tools_available:
            log_warning(fn_name, "tool_call_check", f"Onboarding LLM tried unknown/disallowed tool: {tool_name}") # Corrected log identifier
            return f"Sorry, I tried an action ('{tool_name}') that isn't available during setup."

        log_info(fn_name, "tool_call_exec", f"Onboarding LLM requested Tool: {tool_name} with args: {tool_call.function.arguments[:150]}...") # Corrected log identifier
        tool_func = onboarding_tools_available[tool_name]
        param_model = onboarding_tool_models[tool_name]
        tool_result_content = GENERIC_ERROR_MSG

        try:
            # Parse and validate arguments
            tool_args_dict = {}
            tool_args_str = tool_call.function.arguments
            if tool_args_str and tool_args_str.strip() != '{}': tool_args_dict = json.loads(tool_args_str)
            elif not param_model.model_fields: tool_args_dict = {}

            validated_params = param_model(**tool_args_dict)

            # Execute the tool function
            tool_result_dict = tool_func(user_id, validated_params)
            log_info(fn_name, "tool_call_exec", f"Onboarding Tool {tool_name} executed. Result: {tool_result_dict}") # Corrected log identifier
            tool_result_content = json.dumps(tool_result_dict)

        # Handle errors during tool execution
        except json.JSONDecodeError:
             log_error(fn_name, "tool_call_exec", f"Failed parse JSON args for onboarding tool {tool_name}: {tool_args_str}"); # Corrected log identifier
             tool_result_content = json.dumps({"success": False, "message": f"Error: Invalid arguments for {tool_name}."})
        except pydantic.ValidationError as e:
             log_error(fn_name, "tool_call_exec", f"Arg validation failed for onboarding tool {tool_name}. Err: {e.errors()}. Args: {tool_args_str}", e) # Corrected log identifier
             err_summary = "; ".join([f"{err['loc'][0] if err.get('loc') else 'param'}: {err['msg']}" for err in e.errors()])
             tool_result_content = json.dumps({"success": False, "message": f"Error: Invalid parameters for {tool_name} - {err_summary}"})
        except Exception as e:
             log_error(fn_name, "tool_call_exec", f"Error executing onboarding tool {tool_name}. Trace:\n{traceback.format_exc()}", e); # Corrected log identifier
             tool_result_content = json.dumps({"success": False, "message": f"Error performing action {tool_name}."})

        # --- Make SECOND LLM call with the tool result ---
        # *** FIXED: Convert response_message object to dict before appending ***
        # Append the assistant's first message (the tool call request) as a dict
        messages.append(response_message.model_dump(exclude_unset=True))
        # Append the tool execution result
        messages.append({
            "tool_call_id": tool_call_id, "role": "tool",
            "name": tool_name, "content": tool_result_content,
        })

        log_info(fn_name, "LLM_call_2", f"Invoking Onboarding LLM again for {user_id} with tool result...") # Corrected log identifier
        second_response = client.chat.completions.create(
            model="gpt-4o", messages=messages, # No tools needed here
        )
        second_response_message = second_response.choices[0].message

        if second_response_message.content:
            log_info(fn_name, "LLM_call_2", "Onboarding LLM generated final response after processing tool result.") # Corrected log identifier
            # Check if the LAST action was setting status to active
            if tool_name == "update_user_preferences":
                 try:
                      # Ensure tool_args_str is valid JSON before parsing
                      update_data = json.loads(tool_args_str) if tool_args_str and tool_args_str.strip() != '{}' else {}
                      # Check if the 'updates' dictionary exists and contains 'status'
                      if isinstance(update_data.get("updates"), dict) and update_data["updates"].get("status") == "active":
                           log_info(fn_name, "status_check", f"Onboarding completed for user {user_id} (status set to active).") # Corrected log identifier
                 except Exception as parse_err:
                      log_warning(fn_name, "status_check", f"Could not parse tool args to check for status update: {parse_err}") # Corrected log identifier

            return second_response_message.content
        else:
            log_warning(fn_name, "LLM_call_2", "Onboarding LLM provided no content after processing tool result.") # Corrected log identifier
            try: fallback_msg = json.loads(tool_result_content).get("message", GENERIC_ERROR_MSG)
            except: fallback_msg = GENERIC_ERROR_MSG
            return fallback_msg

    # --- Outer Exception Handling ---
    except Exception as e:
        tb_str = traceback.format_exc();
        log_error(fn_name, "outer_exception", f"Core error in onboarding logic for {user_id}. Traceback:\n{tb_str}", e) # Corrected log identifier
        return GENERIC_ERROR_MSG
# --- END OF FILE agents/onboarding_agent.py ---

# --- END OF FILE agents/onboarding_agent.py ---



================================================================================
ðŸ“„ agents/tool_definitions.py
================================================================================

# --- START OF FILE agents/tool_definitions.py ---

# --- START OF FULL agents/tool_definitions.py ---

from pydantic import BaseModel, Field, field_validator, ValidationError
from typing import Dict, List, Any, Tuple # Removed Optional
import json
from datetime import datetime, timedelta, timezone # Added timezone
import re
import uuid # Added uuid
import traceback
# Import Service Layer functions & Helpers
import services.task_manager as task_manager
import services.config_manager as config_manager
import services.task_query_service as task_query_service
from services.agent_state_manager import get_agent_state, add_task_to_context, update_task_in_context # Assuming AGENT_STATE_MANAGER_IMPORTED is handled internally or true
# Import the class itself for type checking if needed, handle import error
try:
    from tools.google_calendar_api import GoogleCalendarAPI
    GCAL_API_IMPORTED = True
except ImportError:
     GoogleCalendarAPI = None
     GCAL_API_IMPORTED = False

try:
    import tools.activity_db as activity_db_real # Use a different alias
    DB_IMPORTED = True
    # Define activity_db to point to the real one
    activity_db = activity_db_real
except ImportError:
    DB_IMPORTED = False
    log_error("tool_definitions", "update_item_details_tool_import", "Failed to import real activity_db. Fallback active.", None)
    # Define a dummy class with the necessary static method
    class activity_db_dummy:
        @staticmethod
        def get_task(*a, **k):
             log_warning("tool_definitions", "activity_db_dummy.get_task", "Using dummy get_task - DB not imported.")
             return None
    # Define activity_db to point to the dummy
    activity_db = activity_db_dummy

# LLM Interface (for scheduler sub-call)
from services.llm_interface import get_instructor_client
from openai import OpenAI
from openai.types.chat import ChatCompletionMessage

# Utilities
from tools.logger import log_info, log_error, log_warning # Ensure log_warning is used correctly
import yaml
import os
import pydantic # Keep pydantic import

# --- Helper Functions ---

# Returns GoogleCalendarAPI instance or None
def _get_calendar_api_from_state(user_id):
    """Helper to retrieve the active calendar API instance from agent state."""
    fn_name = "_get_calendar_api_from_state"
    if not GCAL_API_IMPORTED or GoogleCalendarAPI is None:
        log_warning("tool_definitions", fn_name, "GoogleCalendarAPI class missing or not imported.")
        return None
    try:
        # Assume get_agent_state is available and returns a dict or None
        state = get_agent_state(user_id)
        if state is not None: # Check if state exists
            api = state.get("calendar")
            # Check if it's the right type AND active
            if isinstance(api, GoogleCalendarAPI) and api.is_active():
                return api # Return the active API instance
    except Exception as e:
        log_error("tool_definitions", fn_name, f"Error getting calendar API instance for user {user_id}", e)
    # Return None if state is None, calendar key missing, not GCalAPI type, or not active
    return None

# Returns dict or None
def _parse_scheduler_llm_response(raw_text):
    """Parses the specific JSON output from the Session Scheduler LLM."""
    fn_name = "_parse_scheduler_llm_response"
    if not raw_text:
        log_warning("tool_definitions", fn_name, "Scheduler response text is empty.")
        return None
    processed_text = None
    try:
        # Try extracting JSON block first
        match = re.search(r"```json\s*({.*?})\s*```", raw_text, re.DOTALL | re.IGNORECASE)
        if match:
             processed_text = match.group(1).strip()
        else:
             # Fallback: Assume the entire response is JSON or find first/last brace
             processed_text = raw_text.strip()
             if not processed_text.startswith("{") or not processed_text.endswith("}"):
                  start_brace = raw_text.find('{'); end_brace = raw_text.rfind('}')
                  if start_brace != -1 and end_brace > start_brace:
                      processed_text = raw_text[start_brace : end_brace + 1].strip()
                      log_warning("tool_definitions", fn_name, "Used find/rfind fallback for JSON extraction.")
                  else:
                      log_warning("tool_definitions", fn_name, "Could not extract JSON block from raw text.");
                      return None
        if not processed_text:
            raise ValueError("Processed text is empty after extraction attempts.")

        data = json.loads(processed_text)
        required_keys = ["proposed_sessions", "response_message"]
        if not isinstance(data, dict) or not all(k in data for k in required_keys):
            missing = [k for k in required_keys if k not in data]; raise ValueError(f"Parsed JSON missing required keys: {missing}")
        if not isinstance(data["proposed_sessions"], list): raise ValueError("'proposed_sessions' key must contain a list.")

        # Validate individual session formats
        valid_sessions = []
        for i, session_dict in enumerate(data["proposed_sessions"]):
            required_session_keys = ["date", "time", "end_time"]
            if not isinstance(session_dict, dict) or not all(k in session_dict for k in required_session_keys):
                log_warning("tool_definitions", fn_name, f"Skipping invalid session structure: {session_dict}")
                continue
            try:
                 # Validate formats strictly
                 datetime.strptime(session_dict["date"], '%Y-%m-%d'); datetime.strptime(session_dict["time"], '%H:%M'); datetime.strptime(session_dict["end_time"], '%H:%M')
                 # Ensure slot_ref is a positive integer, assign if missing/invalid
                 ref = session_dict.get("slot_ref");
                 session_dict["slot_ref"] = ref if isinstance(ref, int) and ref > 0 else i + 1
                 valid_sessions.append(session_dict)
            except (ValueError, TypeError) as fmt_err:
                log_warning("tool_definitions", fn_name, f"Skipping session due to format error ({fmt_err}): {session_dict}")

        data["proposed_sessions"] = valid_sessions # Replace with validated list
        log_info("tool_definitions", fn_name, f"Successfully parsed {len(valid_sessions)} valid sessions.")
        return data
    except (json.JSONDecodeError, ValueError) as parse_err:
        log_error("tool_definitions", fn_name, f"Scheduler JSON parsing failed. Error: {parse_err}. Extracted: '{processed_text or 'N/A'}' Raw: '{raw_text[:200]}'", parse_err);
        return None
    except Exception as e:
        log_error("tool_definitions", fn_name, f"Unexpected error parsing scheduler response: {e}", e);
        return None

_SCHEDULER_PROMPTS_CACHE = {}
# Returns tuple (sys_prompt_str|None, human_prompt_str|None)
def _load_scheduler_prompts():
    """Loads scheduler system and human prompts from config."""
    fn_name = "_load_scheduler_prompts"
    prompts_path = os.path.join("config", "prompts.yaml"); cache_key = prompts_path + "_scheduler"
    if cache_key in _SCHEDULER_PROMPTS_CACHE: return _SCHEDULER_PROMPTS_CACHE[cache_key]
    sys_prompt, human_prompt = None, None
    try:
        if not os.path.exists(prompts_path): raise FileNotFoundError(f"{prompts_path} not found.")
        with open(prompts_path, "r", encoding="utf-8") as f: all_prompts = yaml.safe_load(f)
        if not all_prompts: raise ValueError("YAML prompts file loaded as empty.")
        sys_prompt = all_prompts.get("session_scheduler_system_prompt")
        human_prompt = all_prompts.get("session_scheduler_human_prompt")
        if not sys_prompt or not human_prompt:
            log_error("tool_definitions", fn_name, "One or both scheduler prompts (system/human) are missing in prompts.yaml.")
            sys_prompt, human_prompt = None, None # Ensure both are None if one is missing
    except Exception as e:
        log_error("tool_definitions", fn_name, f"Failed to load scheduler prompts from {prompts_path}: {e}", e)
        sys_prompt, human_prompt = None, None # Ensure None on error
    _SCHEDULER_PROMPTS_CACHE[cache_key] = (sys_prompt, human_prompt);
    return sys_prompt, human_prompt


# =====================================================
# == Pydantic Model Definitions (MUST COME BEFORE TOOLS) ==
# =====================================================
# --- NOTE: Pydantic models still use typing hints including '| None' implicitly ---
# --- This is standard for Pydantic and necessary for optional fields ---
# --- The user request was mainly about FUNCTION return type hints ---

class CreateReminderParams(BaseModel):
    description: str = Field(...)
    date: str = Field(...)
    time: str | None = Field(None) # Pydantic handles Optional fields like this
    project: str | None = Field(None)
    @field_validator('time')
    @classmethod
    def validate_time_format(cls, v: str | None):
        if v is None or v == "": return None
        try: hour, minute = map(int, v.split(':')); return f"{hour:02d}:{minute:02d}"
        except (ValueError, TypeError): raise ValueError("Time must be in HH:MM format (e.g., '14:30') or null/empty")
    @field_validator('date')
    @classmethod
    def validate_date_format(cls, v: str):
        try: datetime.strptime(v, '%Y-%m-%d'); return v
        except (ValueError, TypeError): raise ValueError("Date must be in YYYY-MM-DD format")

class CreateTaskParams(BaseModel):
    description: str = Field(...)
    date: str = Field(...)
    estimated_duration: str | None = Field(None)
    project: str | None = Field(None)
    @field_validator('date')
    @classmethod
    def validate_date_format(cls, v: str):
        try: datetime.strptime(v, '%Y-%m-%d'); return v
        except (ValueError, TypeError): raise ValueError("Date must be in YYYY-MM-DD format")

class ProposeTaskSlotsParams(BaseModel):
    duration: str = Field(...)
    timeframe: str = Field(...)
    description: str | None = Field(None)
    # REMOVE split_preference: str | None = Field(None)
    scheduling_hints: str | None = Field(None, description="User's specific scheduling preferences or constraints provided in natural language, e.g., 'in the afternoon', 'not on Monday', 'needs one continuous block', 'split into sessions'.") # <-- ADD this
    num_options_to_propose: int | None = Field(3)
    @field_validator('num_options_to_propose')
    @classmethod
    def check_num_options(cls, v: int | None):
        if v is not None and v <= 0: raise ValueError("num_options_to_propose must be positive")
        return v

class FinalizeTaskAndBookSessionsParams(BaseModel):
    search_context: Dict = Field(...)
    approved_slots: List[Dict] = Field(..., min_length=1)
    project: str | None = Field(None)
    @field_validator('approved_slots')
    @classmethod
    def validate_slots_structure(cls, v):
        if not isinstance(v, list) or not v: raise ValueError("approved_slots must be a non-empty list.")
        for i, slot in enumerate(v):
            if not isinstance(slot, dict): raise ValueError(f"Slot {i+1} is not a dict.")
            req_keys = ["date", "time", "end_time"];
            if not all(k in slot for k in req_keys): raise ValueError(f"Slot {i+1} missing required keys: {req_keys}")
            try: datetime.strptime(slot["date"], '%Y-%m-%d')
            except (ValueError, TypeError): raise ValueError(f"Slot {i+1} has invalid date format: {slot['date']}")
            try: datetime.strptime(slot["time"], '%H:%M')
            except (ValueError, TypeError): raise ValueError(f"Slot {i+1} has invalid time format: {slot['time']}")
            try: datetime.strptime(slot["end_time"], '%H:%M')
            except (ValueError, TypeError): raise ValueError(f"Slot {i+1} has invalid end_time format: {slot['end_time']}")
        return v

class UpdateItemDetailsParams(BaseModel):
    item_id: str = Field(...)
    updates: dict = Field(...)
    @field_validator('updates')
    @classmethod
    def check_allowed_keys_and_formats(cls, v: dict):
        allowed_keys = {"description", "date", "time", "estimated_duration", "project"}
        if not v: raise ValueError("Updates dictionary cannot be empty.")
        validated_updates = {}
        for key, value in v.items():
            if key not in allowed_keys: raise ValueError(f"Invalid key '{key}'. Allowed: {', '.join(allowed_keys)}")
            if key == 'date':
                if value is None: validated_updates[key] = None
                else:
                    try: validated_updates[key] = cls.validate_date_format_static(str(value))
                    except ValueError as e: raise ValueError(f"Invalid format for date '{value}': {e}")
            elif key == 'time':
                # Allow time to be explicitly set to None
                if value is None: validated_updates[key] = None
                else:
                     try: validated_updates[key] = cls.validate_time_format_static(str(value)) # Use static method
                     except ValueError as e: raise ValueError(f"Invalid format for time '{value}': {e}")
            elif key == 'estimated_duration':
                if value is None or (isinstance(value, str) and value.strip() == ""): validated_updates[key] = None
                elif not isinstance(value, str): raise ValueError("Estimated duration must be a string or null/empty")
                else: validated_updates[key] = value
            else: validated_updates[key] = value # description, project
        if not validated_updates: raise ValueError("Updates dictionary resulted in no valid fields.") # Check after processing
        return validated_updates
    @staticmethod
    def validate_date_format_static(v: str):
        try: datetime.strptime(v, '%Y-%m-%d'); return v
        except (ValueError, TypeError): raise ValueError("Date must be in YYYY-MM-DD format")
    @staticmethod
    def validate_time_format_static(v: str): # Made non-optional as None handled above
        if v == "": return None # Treat empty string as clearing time
        try: hour, minute = map(int, v.split(':')); return f"{hour:02d}:{minute:02d}"
        except (ValueError, TypeError): raise ValueError("Time must be in HH:MM format (e.g., '14:30') or empty string")

class UpdateItemStatusParams(BaseModel):
    item_id: str = Field(...)
    new_status: str = Field(...)
    @field_validator('new_status')
    @classmethod
    def check_item_status(cls, v: str):
        allowed = {"pending", "in_progress", "completed", "cancelled"} # Removed "in progress" with space
        v_lower = v.lower().replace(" ", "") # Standardize to remove space if present
        if v_lower not in allowed: raise ValueError(f"Status must be one of: {', '.join(allowed)}")
        return v_lower # Return standardized status

class UpdateUserPreferencesParams(BaseModel):
    updates: dict = Field(...)
    @field_validator('updates')
    @classmethod
    def check_updates_not_empty(cls, v: dict):
        if not v: raise ValueError("Updates dictionary cannot be empty.")
        # Add more specific validation based on user_registry.DEFAULT_PREFERENCES keys/formats if needed
        return v

class InitiateCalendarConnectionParams(BaseModel):
    pass # No parameters needed

class CancelTaskSessionsParams(BaseModel):
    task_id: str = Field(...)
    session_ids_to_cancel: list[str] = Field(..., min_length=1)

class InterpretListReplyParams(BaseModel):
    user_reply: str = Field(...)
    list_mapping: dict = Field(...)

class GetFormattedTaskListParams(BaseModel):
    date_range: list[str] | None = Field(None)
    status_filter: str | None = Field('active') # Allow None, default in code
    project_filter: str | None = Field(None)
    @field_validator('date_range')
    @classmethod
    @classmethod
    def validate_and_normalize_date_range(cls, v: list[str] | None):
        if v is None: return v
        if not isinstance(v, list): raise ValueError("date_range must be a list of date strings.")
        if len(v) == 1:
            # If only one date provided, assume it's start and end
            try:
                the_date = datetime.strptime(v[0], '%Y-%m-%d').date()
                log_warning("tool_definitions", "validate_date_range", f"Received single date {v[0]}, assuming start/end.")
                return [v[0], v[0]] # Return list with duplicated date
            except (ValueError, TypeError):
                raise ValueError("Single date provided is not a valid YYYY-MM-DD string.")
        elif len(v) == 2:
            # Validate two dates
            try:
                start_date = datetime.strptime(v[0], '%Y-%m-%d').date()
                end_date = datetime.strptime(v[1], '%Y-%m-%d').date()
                if start_date > end_date: raise ValueError("Start date cannot be after end date.")
                return v # Return original valid pair
            except (ValueError, TypeError):
                raise ValueError("Dates must be valid YYYY-MM-DD strings.")
        else: # Invalid number of elements
            raise ValueError("date_range must be a list of one or two date strings.")
    @field_validator('status_filter')
    @classmethod
    def check_status_filter(cls, v: str | None):
        if v is None: return 'active' # Default if None
        allowed = {'active', 'pending', 'in_progress', 'completed', 'all'}
        v_lower = v.lower().replace(" ", "")
        if v_lower not in allowed:
            log_warning("tool_definitions", "check_status_filter", f"Invalid status_filter '{v}'. Defaulting 'active'.")
            return 'active'
        return v_lower


# =====================================================
# == Tool Function Definitions (MUST COME AFTER MODELS) ==
# =====================================================

# Returns dict
def create_reminder_tool(user_id, params: CreateReminderParams):
    """Creates a simple reminder, potentially adding it to Google Calendar if time is specified."""
    fn_name = "create_reminder_tool"
    # Pydantic validation happens implicitly when type hint is used
    # We still keep the try-except block for runtime safety but remove explicit re-validation
    try:
        log_info("tool_definitions", fn_name, f"Executing for user {user_id}, desc: '{params.description[:30]}...'")
        data_dict = params.model_dump(exclude_none=True); data_dict["type"] = "reminder"
        # Call service layer function
        saved_item = task_manager.create_task(user_id, data_dict)
        if saved_item and saved_item.get("event_id"):
            return {"success": True, "item_id": saved_item.get("event_id"), "item_type": "reminder", "message": f"Reminder '{params.description[:30]}...' created."}
        else:
             log_error("tool_definitions", fn_name, f"Task manager failed create reminder")
             return {"success": False, "item_id": None, "message": "Failed to save reminder."}
    except pydantic.ValidationError as e: # Catch validation errors if they bypass type hint somehow
         log_error("tool_definitions", fn_name, f"Validation Error: {e}");
         return {"success": False, "item_id": None, "message": f"Invalid parameters: {e}"}
    except Exception as e:
         log_error("tool_definitions", fn_name, f"Unexpected error: {e}", e)
         return {"success": False, "item_id": None, "message": f"Error: {e}"}

# Returns dict
def create_task_tool(user_id, params: CreateTaskParams):
    """Creates task metadata ONLY. Does not schedule sessions or interact with calendar."""
    fn_name = "create_task_tool"
    try:
        log_info("tool_definitions", fn_name, f"Executing for user {user_id}, desc: '{params.description[:30]}...'")
        data_dict = params.model_dump(exclude_none=True); data_dict["type"] = "task"
        if "time" in data_dict: del data_dict["time"] # Tasks don't have specific start time in metadata usually
        saved_item = task_manager.create_task(user_id, data_dict)
        if saved_item and saved_item.get("event_id"):
            # Return duration if available, useful for orchestrator to ask about scheduling
            return {"success": True, "item_id": saved_item.get("event_id"), "item_type": "task", "estimated_duration": saved_item.get("estimated_duration"), "message": f"Task '{params.description[:30]}...' created (metadata only)."}
        else:
             log_error("tool_definitions", fn_name, f"Task manager failed create task metadata")
             return {"success": False, "item_id": None, "message": "Failed to save task."}
    except pydantic.ValidationError as e:
         log_error("tool_definitions", fn_name, f"Validation Error: {e}");
         return {"success": False, "item_id": None, "message": f"Invalid parameters: {e}"}
    except Exception as e:
         log_error("tool_definitions", fn_name, f"Unexpected error: {e}", e)
         return {"success": False, "item_id": None, "message": f"Error: {e}"}

# Returns dict
def propose_task_slots_tool(user_id, params: ProposeTaskSlotsParams):
    """
    Finds available work session slots based on duration/timeframe and hints.
    Uses an LLM sub-call for intelligent slot finding, considering calendar events.
    Returns proposed slots and the search context used.
    """
    fn_name = "propose_task_slots_tool"
    fail_result = {"success": False, "proposed_slots": None, "message": "Sorry, I encountered an issue trying to propose schedule slots.", "search_context": None}
    try:
        log_info("tool_definitions", fn_name, f"Executing user={user_id}. Search: duration='{params.duration}', timeframe='{params.timeframe}', hints='{params.scheduling_hints}'")
        search_context_to_return = params.model_dump() # Store validated input as basis for context

        llm_client = get_instructor_client()
        if not llm_client:
             log_error("tool_definitions", fn_name, "LLM client unavailable.")
             return {**fail_result, "message": "Scheduler resources unavailable (LLM Client)."}

        sys_prompt, human_prompt = _load_scheduler_prompts()
        if not sys_prompt or not human_prompt:
             log_error("tool_definitions", fn_name, "Scheduler prompts failed load.")
             return {**fail_result, "message": "Scheduler resources unavailable (Prompts)."}

        agent_state = get_agent_state(user_id);
        prefs = agent_state.get("preferences", {}) if agent_state else {}
        calendar_api = _get_calendar_api_from_state(user_id)
        preferred_session_str = prefs.get("Preferred_Session_Length", "60m")

        task_estimated_duration_str = params.duration;
        total_minutes = task_manager._parse_duration_to_minutes(task_estimated_duration_str)
        if total_minutes is None:
             log_error("tool_definitions", fn_name, f"Invalid duration '{task_estimated_duration_str}'")
             return {**fail_result, "message": f"Invalid duration format: '{task_estimated_duration_str}'. Use 'Xh' or 'Ym'."}

        # Determine target slot duration and count based on total duration,
        # user preferred length, and hints about splitting/continuity.
        session_minutes = task_manager._parse_duration_to_minutes(preferred_session_str) or 60
        num_slots_to_find = 1
        slot_duration_str = task_estimated_duration_str # Default to one continuous block
        hints_lower = (params.scheduling_hints or "").lower()

        # Decide on splitting based on hints or duration comparison
        # Prioritize hint if given, otherwise compare total duration to preferred session length
        needs_split = False
        if "continuous" in hints_lower or "one block" in hints_lower or "one slot" in hints_lower:
             needs_split = False
             log_info("tool_definitions", fn_name, "Hint indicates continuous block required.")
        elif "split" in hints_lower or "separate" in hints_lower or "multiple sessions" in hints_lower:
             needs_split = True
             log_info("tool_definitions", fn_name, "Hint indicates split sessions required.")
        elif session_minutes > 0 and total_minutes > session_minutes:
             # Default to split if no hint and total > preferred
             needs_split = True
             log_info("tool_definitions", fn_name, "Defaulting to split sessions (total > preferred, no specific hint).")

        if needs_split and session_minutes > 0:
             num_slots_to_find = (total_minutes + session_minutes - 1) // session_minutes
             if num_slots_to_find <= 0 : num_slots_to_find = 1
             slot_duration_str = preferred_session_str # Use preferred length for each split slot
             log_info("tool_definitions", fn_name, f"Calculated need for {num_slots_to_find} split sessions of duration: {slot_duration_str}")
        else:
            # Stays as 1 slot with total duration
            log_info("tool_definitions", fn_name, f"Calculated need for 1 continuous block of duration: {slot_duration_str}")

        # Timeframe parsing logic (keep as is)
        today = datetime.now().date(); start_date = today + timedelta(days=1); due_date_for_search = None
        tf = params.timeframe.lower()
        # --- Add parsing for ISO interval ---
        iso_interval_match = re.match(r"(\d{4}-\d{2}-\d{2}T\d{2}:\d{2}:\d{2}(?:[+-]\d{2}:\d{2}|Z))/(\d{4}-\d{2}-\d{2}T\d{2}:\d{2}:\d{2}(?:[+-]\d{2}:\d{2}|Z))", params.timeframe)
        if iso_interval_match:
             try:
                  start_iso, end_iso = iso_interval_match.groups()
                  # Convert to date for search range (ignoring time part for the range boundary)
                  start_dt_aware = datetime.fromisoformat(start_iso.replace('Z', '+00:00'))
                  # For end date, we want the day it ends on
                  end_dt_aware = datetime.fromisoformat(end_iso.replace('Z', '+00:00'))
                  start_date = start_dt_aware.date()
                  # Make sure start is not before tomorrow
                  start_date = max(start_date, today + timedelta(days=1))
                  due_date_for_search = end_dt_aware.date() # Use end date as effective due date
                  log_info("tool_definitions", fn_name, f"Parsed ISO Interval: Start={start_date}, EffectiveDue={due_date_for_search}")
             except ValueError as iso_parse_err:
                  log_warning("tool_definitions", fn_name, f"Failed to parse ISO interval timeframe '{params.timeframe}': {iso_parse_err}")
                  # Fallback to default window if parse fails
        # --- Continue with existing timeframe parsing ---
        elif "tomorrow" in tf: start_date = today + timedelta(days=1); due_date_for_search = start_date
        elif "next week" in tf:
             start_of_next_week = today + timedelta(days=(7 - today.weekday()))
             start_date = start_of_next_week
             due_date_for_search = start_of_next_week + timedelta(days=6)
             log_info("tool_definitions", fn_name, f"Parsed 'next week': Start={start_date}, EffectiveDue={due_date_for_search}")
        elif "on " in tf:
            try: date_part = tf.split("on ")[1].strip(); parsed_date = datetime.strptime(date_part, "%Y-%m-%d").date(); start_date = max(parsed_date, today + timedelta(days=1)); due_date_for_search = start_date
            except Exception: log_warning("tool_definitions", fn_name, f"Parsing timeframe date failed: '{params.timeframe}'")
        elif "by " in tf:
            try: date_part = tf.split("by ")[1].strip(); parsed_date = datetime.strptime(date_part, "%Y-%m-%d").date(); due_date_for_search = parsed_date; start_date = max(today + timedelta(days=1), parsed_date - timedelta(days=14))
            except Exception: log_warning("tool_definitions", fn_name, f"Parsing timeframe 'by' date failed: '{params.timeframe}'")
        # --- Add parsing for "later today/tonight" etc. ---
        elif "later today" in tf or "tonight" in tf or "this afternoon" in tf:
             start_date = today # Search starts today
             due_date_for_search = today # Due date is today
             log_info("tool_definitions", fn_name, f"Parsed relative timeframe '{tf}': Start={start_date}, EffectiveDue={due_date_for_search}")
             # Note: Hints should guide the sub-LLM to pick *later* slots
        # --- Default case ---
        else: log_warning("tool_definitions", fn_name, f"Unclear timeframe '{params.timeframe}', using default window.")

        default_horizon_days = 56; end_date_limit = start_date + timedelta(days=default_horizon_days - 1)
        end_date = end_date_limit
        if due_date_for_search and due_date_for_search >= start_date: # Use >= to allow same day start/end
             # Allow buffer unless start/end are same day
             buffer_days = 0 if start_date == due_date_for_search else 1
             end_date = min(end_date_limit, due_date_for_search - timedelta(days=buffer_days))
        end_date = max(end_date, start_date); # Ensure end >= start
        start_date_str, end_date_str = start_date.strftime("%Y-%m-%d"), end_date.strftime("%Y-%m-%d")
        log_info("tool_definitions", fn_name, f"Derived Search Range: {start_date_str} to {end_date_str}")
        # Update search context with parsed dates and original timeframe for sub-LLM
        search_context_to_return["effective_due_date"] = due_date_for_search.strftime("%Y-%m-%d") if due_date_for_search else None
        search_context_to_return["search_start_date"] = start_date_str
        search_context_to_return["search_end_date"] = end_date_str
        search_context_to_return["original_timeframe"] = params.timeframe # Keep original for sub-LLM context

        # Fetch existing calendar events (keep as is)
        existing_events = [];
        if calendar_api is not None:
            log_info("tool_definitions", fn_name, f"Fetching GCal events from {start_date_str} to {end_date_str}")
            if start_date <= end_date:
                try:
                    events_raw = calendar_api.list_events(start_date_str, end_date_str)
                    existing_events = [
                         {"start_datetime": ev.get("gcal_start_datetime"), "end_datetime": ev.get("gcal_end_datetime"), "summary": ev.get("title")}
                         for ev in events_raw if ev.get("gcal_start_datetime") and ev.get("gcal_end_datetime")
                    ]
                    log_info("tool_definitions", fn_name, f"Fetched {len(existing_events)} valid GCal events.")
                except Exception as e:
                     log_error("tool_definitions", fn_name, f"Fetch GCal events failed: {e}", e)
            else:
                 log_warning("tool_definitions", fn_name, f"Invalid search range ({start_date_str} > {end_date_str}). Skipping GCal fetch.")
        else:
            log_warning("tool_definitions", fn_name, "GCal API inactive or unavailable. Proposing slots without checking calendar conflicts.")

        # Prepare data for LLM scheduler sub-call
        try:
            # Ask sub-LLM for the number of slots we actually need
            slots_to_request_from_llm = num_slots_to_find
            prompt_data = {
                "task_description": params.description or "(No description)",
                "task_due_date": search_context_to_return["effective_due_date"] or "(No specific due date)",
                "task_estimated_duration": task_estimated_duration_str,
                "user_working_days": prefs.get("Work_Days", ["Mon", "Tue", "Wed", "Thu", "Fri"]),
                "user_work_start_time": prefs.get("Work_Start_Time", "09:00"),
                "user_work_end_time": prefs.get("Work_End_Time", "17:00"),
                "user_session_length": slot_duration_str, # Duration PER slot
                "existing_events_json": json.dumps(existing_events),
                "current_date": today.strftime("%Y-%m-%d"),
                "num_slots_requested": slots_to_request_from_llm, # Use calculated number
                "search_start_date": start_date_str,
                "search_end_date": end_date_str,
                "scheduling_hints": params.scheduling_hints or "None" # Pass hints
            }
            log_info("tool_definitions", fn_name, f"Scheduler prompt data prepared (requesting {slots_to_request_from_llm} slots, event count: {len(existing_events)}).")
        except Exception as e:
             log_error("tool_definitions", fn_name, f"Failed prepare prompt data for scheduler: {e}", e)
             return {**fail_result, "message": "Failed prepare data for scheduler."}

        # Call LLM and parse response (keep as is)
        raw_llm_output = None; parsed_data = None
        try:
            fmt_human = human_prompt.format(**prompt_data)
            messages = [{"role": "system", "content": sys_prompt}, {"role": "user", "content": fmt_human}]
            log_info("tool_definitions", fn_name, ">>> Invoking Session Scheduler LLM...")
            sched_resp = llm_client.chat.completions.create(model="gpt-4o", messages=messages, temperature=0.2, response_format={"type": "json_object"})
            raw_llm_output = sched_resp.choices[0].message.content
            parsed_data = _parse_scheduler_llm_response(raw_llm_output)
            if parsed_data is None:
                log_error("tool_definitions", fn_name, "Scheduler response parse failed or returned invalid structure.")
                return {**fail_result, "message": "Received invalid proposals format from scheduler."}

            log_info("tool_definitions", f"{fn_name}_DEBUG", f"Parsed Scheduler LLM Resp: {json.dumps(parsed_data, indent=2)}")
            # --- Check if enough slots were returned ---
            num_returned = len(parsed_data.get("proposed_sessions", []))
            if num_returned < num_slots_to_find:
                 log_warning("tool_definitions", fn_name, f"Sub-LLM returned only {num_returned} slots, but {num_slots_to_find} were needed.")
                 # Append to message? Modify success? For now, just return what was found.
                 parsed_data["response_message"] += f" (Note: Only found {num_returned} of the {num_slots_to_find} required slots)."
            # -------------------------------------------
            log_info("tool_definitions", fn_name, f"Scheduler LLM processing successful.")
            return {"success": True, "proposed_slots": parsed_data.get("proposed_sessions"), "message": parsed_data.get("response_message", "..."), "search_context": search_context_to_return}
        except Exception as e:
            tb_str = traceback.format_exc()
            log_error("tool_definitions", fn_name, f"Scheduler LLM invoke/process error: {e}. Raw: '{raw_llm_output}'. Parsed: {parsed_data}\nTraceback:\n{tb_str}", e)
            return {**fail_result, "message": "Error during slot finding process."}

    except pydantic.ValidationError as e:
        log_error("tool_definitions", fn_name, f"Validation Error: {e}");
        return {"success": False, "proposed_slots": None, "message": f"Invalid parameters: {e}", "search_context": None}
    except Exception as e:
         log_error("tool_definitions", fn_name, f"Unexpected error in propose_task_slots: {e}", e)
         return fail_result
         
# Returns dict
def finalize_task_and_book_sessions_tool(user_id, params: FinalizeTaskAndBookSessionsParams):
    """Creates a task metadata record AND books the approved work sessions in GCal."""
    fn_name = "finalize_task_and_book_sessions_tool"
    try:
        search_context = params.search_context; approved_slots = params.approved_slots
        log_info("tool_definitions", fn_name, f"Executing finalize+book user={user_id}, desc='{search_context.get('description')}', slots={len(approved_slots)}")
        if not search_context or not approved_slots:
            log_error("tool_definitions", fn_name, "Missing search_context or approved_slots.")
            return {"success": False, "item_id": None, "booked_count": 0, "message": "Internal error: Missing context or slots to book."}

        item_id = None; task_title = "(Error getting title)"
        try:
            # Prepare metadata payload from search context and parameters
            task_metadata_payload = {
                 "description": search_context.get("description", "Untitled Task"),
                 "estimated_duration": search_context.get("duration"), # Get duration from context
                 "type": "task",
                 "project": params.project, # Project tag comes from this tool's params
                 "date": approved_slots[0].get("date"), # Use first slot's date as initial task date
                 "time": approved_slots[0].get("time"), # Use first slot's time as initial task time (maybe None)
            }
            task_title = task_metadata_payload["description"] # Use the description as title

            # Create metadata record FIRST using task_manager service
            # task_manager handles setting defaults like status, created_at etc.
            created_meta = task_manager.create_task(user_id, task_metadata_payload)

            if created_meta and created_meta.get("event_id"):
                item_id = created_meta["event_id"]
                log_info("tool_definitions", fn_name, f"Task metadata created successfully via task_manager: {item_id}");
            else:
                 raise ValueError("Failed to create task metadata via task_manager.") # Raise error if creation failed

        except Exception as create_err:
            log_error("tool_definitions", fn_name, f"Metadata creation failed: {create_err}", create_err)
            return {"success": False, "item_id": None, "booked_count": 0, "message": "Failed to save the task details before scheduling."}

        # If metadata created successfully, proceed to book sessions
        booking_result = task_manager.schedule_work_sessions(user_id, item_id, approved_slots)
        if booking_result.get("success"):
            log_info("tool_definitions", fn_name, f"Booked {booking_result.get('booked_count', 0)} sessions for {item_id}")
            return {
                "success": True,
                "item_id": item_id,
                "booked_count": booking_result.get("booked_count", 0),
                "message": booking_result.get("message", f"Task '{task_title[:30]}...' created & sessions booked.")
            }
        else:
            log_error("tool_definitions", fn_name, f"Metadata created ({item_id}), but booking sessions failed: {booking_result.get('message')}")
            # Optional: Attempt to mark the created task as pending/error or cancel?
            # For now, just report the partial success/failure.
            # task_manager.update_task_status(user_id, item_id, "pending") # Or custom error status?
            return {
                "success": False, # Overall operation failed if booking failed
                "item_id": item_id, # Return ID of created metadata
                "booked_count": 0,
                "message": f"Task '{task_title[:30]}...' was created, but scheduling sessions failed: {booking_result.get('message')}"
            }
    except pydantic.ValidationError as e:
         log_error("tool_definitions", fn_name, f"Validation Error: {e}");
         return {"success": False, "item_id": None, "booked_count": 0, "message": f"Invalid parameters: {e}"}
    except Exception as e:
         log_error("tool_definitions", fn_name, f"Unexpected error: {e}", e)
         # Attempt to clean up created metadata if booking wasn't even attempted? Difficult state.
         return {"success": False, "item_id": item_id, "booked_count": 0, "message": f"Error: {e}"}


# Returns dict
def update_item_details_tool(user_id, params: UpdateItemDetailsParams):
    """Updates core details ONLY (desc, date, time, estimate, project). Not status."""
    # This docstring was slightly incorrect before, fixed it.
    fn_name = "update_item_details_tool";

    try:
        log_info("tool_definitions", fn_name, f"Executing user={user_id}, item={params.item_id}, updates={list(params.updates.keys())}")
        # Call task_manager which now uses DB (via the imported activity_db alias)
        updated_item = task_manager.update_task(user_id, params.item_id, params.updates)

        if updated_item:
             return {"success": True, "message": f"Item '{params.item_id[:8]}...' updated successfully."}
        else:
             log_warning("tool_definitions", fn_name, f"Update failed for item {params.item_id} (not found or no change?).")
             # Check DB to see if item exists to give better error msg
             # Uses the activity_db alias (which points to real or dummy)
             item_exists = activity_db.get_task(params.item_id) is not None

             if item_exists:
                  return {"success": False, "message": f"Failed to apply updates to item {params.item_id[:8]}... (perhaps no change or internal error)."}
             else:
                  return {"success": False, "message": f"Item {params.item_id[:8]}... not found."}
    except pydantic.ValidationError as e:
         # Log validation error with user context if possible (user_id available here)
         log_error("tool_definitions", fn_name, f"Validation Error: {e}", e, user_id=user_id)
         return {"success": False, "message": f"Invalid parameters: {e}"}
    except Exception as e:
         # Log unexpected error with user context
         log_error("tool_definitions", fn_name, f"Unexpected error: {e}", e, user_id=user_id)
         return {"success": False, "message": f"Error during update: {e}."}

# Returns dict
def update_item_status_tool(user_id, params: UpdateItemStatusParams):
    """Changes status OR cancels/deletes item. Requires existing item_id."""
    fn_name = "update_item_status_tool"
    try:
        log_info("tool_definitions", fn_name, f"Executing user={user_id}, item={params.item_id}, status={params.new_status}")
        success = False; message = ""
        if params.new_status == "cancelled":
            # Call cancel_item service which handles GCal cleanup + metadata status
            success = task_manager.cancel_item(user_id, params.item_id)
            message = f"Item '{params.item_id[:8]}...' cancel processed. Result: {'Success' if success else 'Failed/Not Found'}."
        else:
            # Call update_task_status for other statuses
            updated_item = task_manager.update_task_status(user_id, params.item_id, params.new_status)
            success = updated_item is not None
            message = f"Status update to '{params.new_status}' for item '{params.item_id[:8]}...' {'succeeded' if success else 'failed/not found'}."
        return {"success": success, "message": message}
    except pydantic.ValidationError as e:
         log_error("tool_definitions", fn_name, f"Validation Error: {e}");
         return {"success": False, "message": f"Invalid parameters: {e}"}
    except Exception as e:
         log_error("tool_definitions", fn_name, f"Unexpected error: {e}", e)
         return {"success": False, "message": f"Error updating status: {e}."}

# Returns dict
def update_user_preferences_tool(user_id, params: UpdateUserPreferencesParams):
    fn_name = "update_user_preferences_tool"
    try:
        update_keys = list(params.updates.keys()) if params.updates else []
        log_info("tool_definitions", fn_name, f"Executing user={user_id}, updates={update_keys}")
        success = config_manager.update_preferences(user_id, params.updates)
        return {"success": success, "message": f"Preferences update {'succeeded' if success else 'failed'}."}
    except pydantic.ValidationError as e:
         log_error("tool_definitions", fn_name, f"Validation Error: {e}");
         return {"success": False, "message": f"Invalid parameters: {e}"}
    except Exception as e:
         log_error("tool_definitions", fn_name, f"Unexpected error: {e}", e)
         return {"success": False, "message": f"Error: {e}."}

# Returns dict
def initiate_calendar_connection_tool(user_id, params: InitiateCalendarConnectionParams):
    fn_name = "initiate_calendar_connection_tool"
    try:
        # No params to validate beyond the empty model
        log_info("tool_definitions", fn_name, f"Executing for user {user_id}")
        result_dict = config_manager.initiate_calendar_auth(user_id)
        # Ensure 'success' key is present based on status
        result_dict["success"] = result_dict.get("status") in ["pending", "token_exists"]
        return result_dict
    except pydantic.ValidationError as e: # Should not happen for empty model
         log_error("tool_definitions", fn_name, f"Validation Error: {e}");
         return {"success": False, "message": f"Invalid parameters: {e}"}
    except Exception as e:
         log_error("tool_definitions", fn_name, f"Unexpected error: {e}", e)
         return {"success": False, "status": "fails", "message": f"Error: {e}."}

# Returns dict
def cancel_task_sessions_tool(user_id, params: CancelTaskSessionsParams):
    fn_name = "cancel_task_sessions_tool"
    try:
        log_info("tool_definitions", fn_name, f"Executing user={user_id}, Task={params.task_id}, SessionIDs={params.session_ids_to_cancel}")
        result = task_manager.cancel_sessions(user_id, params.task_id, params.session_ids_to_cancel)
        return result # cancel_sessions service should return dict with 'success', 'cancelled_count', 'message'
    except pydantic.ValidationError as e:
         log_error("tool_definitions", fn_name, f"Validation Error: {e}");
         return {"success": False, "cancelled_count": 0, "message": f"Invalid parameters: {e}"}
    except Exception as e:
         log_error("tool_definitions", fn_name, f"Unexpected error: {e}", e)
         return {"success": False, "cancelled_count": 0, "message": f"Error: {e}."}

# Returns dict
def interpret_list_reply_tool(user_id, params: InterpretListReplyParams):
    """Placeholder tool to interpret replies to numbered lists."""
    fn_name = "interpret_list_reply_tool"
    # This tool remains largely a placeholder or basic implementation
    # A robust version might need more context or LLM assistance itself
    try:
        log_warning("tool_definitions", fn_name, f"Tool executed for user {user_id} - Implementation is basic.")
        # Basic number extraction
        extracted_numbers = [int(s) for s in re.findall(r'\b\d+\b', params.user_reply)]
        identified_item_ids = []
        if params.list_mapping: # Check if mapping exists
             identified_item_ids = [params.list_mapping.get(str(num)) for num in extracted_numbers if str(num) in params.list_mapping]
             identified_item_ids = [item_id for item_id in identified_item_ids if item_id is not None] # Filter out None values

        if identified_item_ids:
             log_info("tool_definitions", fn_name, f"Identified numbers {extracted_numbers} mapping to IDs: {identified_item_ids}")
             # The 'action' key is just an example, might need refinement based on LLM needs
             return { "success": True, "action": "process", "item_ids": identified_item_ids, "message": f"Identified item number(s): {', '.join(map(str, extracted_numbers))}." }
        else:
             log_info("tool_definitions", fn_name, f"No valid item numbers found in reply: '{params.user_reply}'")
             return {"success": False, "item_ids": [], "message": "Couldn't find any valid item numbers in your reply."}
    except pydantic.ValidationError as e:
         log_error("tool_definitions", fn_name, f"Validation Error: {e}");
         return {"success": False, "item_ids": [], "message": f"Invalid parameters: {e}"}
    except Exception as e:
         log_error("tool_definitions", fn_name, f"Error parsing list reply: {e}", e)
         return {"success": False, "item_ids": [], "message": "Sorry, I had trouble interpreting your reply."}

# Returns dict
def get_formatted_task_list_tool(user_id, params: GetFormattedTaskListParams):
    fn_name = "get_formatted_task_list_tool"
    try:
        status_filter = params.status_filter or 'active' # Apply default if None
        log_info("tool_definitions", fn_name, f"Executing user={user_id}, Filter={status_filter}, Proj={params.project_filter}, Range={params.date_range}")
        # Ensure date_range is a tuple if not None
        date_range_tuple = tuple(params.date_range) if params.date_range else None
        list_body, list_mapping = task_query_service.get_formatted_list(
             user_id=user_id,
             date_range=date_range_tuple,
             status_filter=status_filter,
             project_filter=params.project_filter
        )
        item_count = len(list_mapping);
        message = f"Found {item_count} item(s)." if item_count > 0 else "No items found matching your criteria."
        if item_count > 0 and not list_body:
             log_warning("tool_definitions", fn_name, f"Found {item_count} items but list body is empty.")
             message = f"Found {item_count}, but there was an error formatting the list."
        return {"success": True, "list_body": list_body or "", "list_mapping": list_mapping or {}, "count": item_count, "message": message}
    except pydantic.ValidationError as e:
         log_error("tool_definitions", fn_name, f"Validation Error: {e}");
         return {"success": False, "list_body": "", "list_mapping": {}, "count": 0, "message": f"Invalid parameters: {e}"}
    except Exception as e:
         log_error("tool_definitions", fn_name, f"Unexpected error: {e}", e)
         return {"success": False, "list_body": "", "list_mapping": {}, "count": 0, "message": f"Sorry, an error occurred while retrieving the list: {e}."}


# =====================================================
# == Tool Dictionaries (MUST COME AFTER MODELS & FUNCS) ==
# =====================================================

AVAILABLE_TOOLS = {
    "create_reminder": create_reminder_tool,
    "create_task": create_task_tool,
    "propose_task_slots": propose_task_slots_tool,
    "finalize_task_and_book_sessions": finalize_task_and_book_sessions_tool,
    "update_item_details": update_item_details_tool,
    "update_item_status": update_item_status_tool,
    "update_user_preferences": update_user_preferences_tool,
    "initiate_calendar_connection": initiate_calendar_connection_tool,
    "cancel_task_sessions": cancel_task_sessions_tool,
    "interpret_list_reply": interpret_list_reply_tool,
    "get_formatted_task_list": get_formatted_task_list_tool
}

TOOL_PARAM_MODELS = {
    "create_reminder": CreateReminderParams,
    "create_task": CreateTaskParams,
    "propose_task_slots": ProposeTaskSlotsParams,
    "finalize_task_and_book_sessions": FinalizeTaskAndBookSessionsParams,
    "update_item_details": UpdateItemDetailsParams,
    "update_item_status": UpdateItemStatusParams,
    "update_user_preferences": UpdateUserPreferencesParams,
    "initiate_calendar_connection": InitiateCalendarConnectionParams,
    "cancel_task_sessions": CancelTaskSessionsParams,
    "interpret_list_reply": InterpretListReplyParams,
    "get_formatted_task_list": GetFormattedTaskListParams
}

# --- END OF FULL agents/tool_definitions.py ---

# --- END OF FILE agents/tool_definitions.py ---



================================================================================
ðŸ“„ services/task_manager.py
================================================================================

# --- START OF FILE services/task_manager.py ---

# --- START OF REFACTORED services/task_manager.py ---
"""
Service layer for managing tasks: creating, updating, cancelling, and scheduling sessions.
Interacts with Google Calendar API and the SQLite database via activity_db.
"""
import json
import traceback
import uuid
from datetime import datetime, timedelta, timezone
import re
from typing import Dict, List, Any # Keep typing for internal use

# Tool/Service Imports
try:
    from tools.logger import log_info, log_error, log_warning
except ImportError:
    # Basic fallback logger
    import logging; logging.basicConfig(level=logging.INFO)
    log_info=logging.info; log_error=logging.error; log_warning=logging.warning
    log_error("task_manager", "import", "Logger failed import.")

# Database Utility Import (Primary Data Source Now)
try:
    import tools.activity_db as activity_db
    DB_IMPORTED = True
except ImportError:
    log_error("task_manager", "import", "activity_db not found. Task management disabled.", None)
    DB_IMPORTED = False
    # Define dummy DB functions if import fails to prevent crashes later
    class activity_db:
        @staticmethod
        def add_or_update_task(*args, **kwargs): return False
        @staticmethod
        def get_task(*args, **kwargs): return None
        @staticmethod
        def delete_task(*args, **kwargs): return False
        @staticmethod
        def list_tasks_for_user(*args, **kwargs): return []
    # TODO: Consider if the application should halt if the DB module can't be imported

# GCal API Import
try:
    from tools.google_calendar_api import GoogleCalendarAPI
    GCAL_API_IMPORTED = True
except ImportError:
    log_warning("task_manager", "import", "GoogleCalendarAPI not found, GCal features disabled.")
    GoogleCalendarAPI = None
    GCAL_API_IMPORTED = False

# Agent State Manager Import (for updating in-memory context)
try:
    from services.agent_state_manager import get_agent_state, add_task_to_context, update_task_in_context, remove_task_from_context
    AGENT_STATE_MANAGER_IMPORTED = True
except ImportError:
    log_error("task_manager", "import", "AgentStateManager not found. In-memory context updates skipped.")
    AGENT_STATE_MANAGER_IMPORTED = False
    # Dummy functions
    def get_agent_state(*a, **k): return None
    def add_task_to_context(*a, **k): pass
    def update_task_in_context(*a, **k): pass
    def remove_task_from_context(*a, **k): pass

# Constants
DEFAULT_REMINDER_DURATION = "15m"

# --- Helper Functions (Keep these as they are useful internally) ---
# Returns GoogleCalendarAPI instance or None
def _get_calendar_api(user_id):
    """Safely retrieves the active calendar API instance from agent state."""
    fn_name = "_get_calendar_api"
    if not AGENT_STATE_MANAGER_IMPORTED or not GCAL_API_IMPORTED or GoogleCalendarAPI is None:
        # log_warning("task_manager", fn_name, f"Cannot get calendar API for {user_id}: Dependencies missing.")
        return None
    agent_state = get_agent_state(user_id)
    if agent_state is not None:
        calendar_api = agent_state.get("calendar")
        if isinstance(calendar_api, GoogleCalendarAPI) and calendar_api.is_active():
            return calendar_api
        # Optionally log if inactive:
        # elif isinstance(calendar_api, GoogleCalendarAPI) and not calendar_api.is_active():
        #      log_info("task_manager", fn_name, f"Calendar API found but inactive for user {user_id}.")
    # else: log_warning("task_manager", fn_name, f"Agent state not found for user {user_id}.")
    return None

# Returns int (minutes) or None
def _parse_duration_to_minutes(duration_str):
    """Parses duration strings like '2h', '90m', '1.5h' into minutes."""
    fn_name = "_parse_duration_to_minutes"
    if not duration_str or not isinstance(duration_str, str): return None
    duration_str = duration_str.lower().replace(' ',''); total_minutes = 0.0
    try:
        hour_match = re.search(r'(\d+(\.\d+)?)\s*h', duration_str)
        minute_match = re.search(r'(\d+)\s*m', duration_str)
        if hour_match: total_minutes += float(hour_match.group(1)) * 60
        if minute_match: total_minutes += int(minute_match.group(1))
        if total_minutes == 0 and hour_match is None and minute_match is None:
             if duration_str.replace('.','',1).isdigit(): total_minutes = float(duration_str)
             else: raise ValueError("Unrecognized duration format")
        return int(round(total_minutes)) if total_minutes > 0 else None
    except (ValueError, TypeError, AttributeError) as e:
        log_warning("task_manager", fn_name, f"Could not parse duration string '{duration_str}': {e}")
        return None

# ==============================================================
# Core Service Functions (Refactored for SQLite via activity_db)
# ==============================================================

# Returns dict (saved item) or None
def create_task(user_id: str, task_params: Dict[str, Any]) -> Dict | None:
    """
    Creates a task or reminder, saves metadata to DB, optionally adds to GCal.
    Returns the dictionary representing the saved task/reminder, or None on failure.
    """
    fn_name = "create_task"
    if not DB_IMPORTED:
        log_error("task_manager", fn_name, "Database module not imported. Cannot create task.")
        return None

    item_type = task_params.get("type")
    if not item_type:
        log_error("task_manager", fn_name, "Missing 'type' in task_params.", user_id=user_id)
        return None
    log_info("task_manager", fn_name, f"Creating item for {user_id}, type: {item_type}")

    calendar_api = _get_calendar_api(user_id)
    google_event_id = None # Track GCal ID for potential rollback

    try:
        # --- Prepare Core Metadata ---
        task_data_to_save = {}
        task_data_to_save["user_id"] = user_id
        task_data_to_save["type"] = item_type
        task_data_to_save["status"] = "pending" # Default status
        task_data_to_save["created_at"] = datetime.now(timezone.utc).isoformat(timespec='seconds')+'Z'
        task_data_to_save["title"] = task_params.get("description", "Untitled Item") # Use description as title
        # Copy other relevant fields from input params if they exist in TASK_FIELDS
        for field in activity_db.TASK_FIELDS:
             if field in task_params and field not in task_data_to_save:
                 task_data_to_save[field] = task_params[field]

        # --- Handle GCal Integration (Reminders with time) ---
        item_time = task_params.get("time")
        has_time = item_time is not None and item_time != ""
        should_create_gcal = item_type == "reminder" and has_time and calendar_api is not None

        if should_create_gcal:
            log_info("task_manager", fn_name, f"Attempting GCal creation for reminder {user_id}...")
            gcal_event_payload = { # Simplified payload for GCalAPI
                "title": task_data_to_save.get("title"),
                "description": f"Reminder: {task_data_to_save.get('description', '')}",
                "date": task_params.get("date"),
                "time": item_time,
                "duration": DEFAULT_REMINDER_DURATION
            }
            try:
                # create_event returns event ID string or None
                created_event_id = calendar_api.create_event(gcal_event_payload)
                if created_event_id is not None:
                    log_info("task_manager", fn_name, f"GCal Reminder created, ID: {created_event_id}")
                    task_data_to_save["event_id"] = created_event_id # Use GCal ID as primary key
                    google_event_id = created_event_id # Track for rollback
                    # Fetch GCal details to store exact times
                    gcal_details = calendar_api._get_single_event(created_event_id)
                    if gcal_details:
                        parsed = calendar_api._parse_google_event(gcal_details)
                        task_data_to_save["gcal_start_datetime"] = parsed.get("gcal_start_datetime")
                        task_data_to_save["gcal_end_datetime"] = parsed.get("gcal_end_datetime")
                    else: log_warning("task_manager", fn_name, f"Failed to fetch details for GCal event {created_event_id}")
                else:
                    log_warning("task_manager", fn_name, f"GCal event creation failed (returned None) for {user_id}. Using local ID.")
                    task_data_to_save["event_id"] = f"local_{uuid.uuid4()}"
            except Exception as gcal_err:
                log_error("task_manager", fn_name, f"Error creating GCal event for {user_id}", gcal_err, user_id=user_id)
                task_data_to_save["event_id"] = f"local_{uuid.uuid4()}" # Fallback to local ID
        else:
            # Assign local ID for tasks or reminders without time/API
            task_data_to_save["event_id"] = f"local_{uuid.uuid4()}"
            if item_type == "reminder" and has_time and calendar_api is None:
                 log_info("task_manager", fn_name, f"Assigning local ID for reminder {user_id}. Reason: GCal API inactive.")

        # --- Save to Database ---
        if not task_data_to_save.get("event_id"): # Should not happen, but safety check
             log_error("task_manager", fn_name, "Failed to assign event_id before DB save.", user_id=user_id)
             return None

        # Fill missing defaults before saving (optional, add_or_update_task handles some)
        task_data_to_save.setdefault('sessions_planned', 0)
        task_data_to_save.setdefault('sessions_completed', 0)
        task_data_to_save.setdefault('progress_percent', 0)
        task_data_to_save.setdefault('session_event_ids', '[]')

        save_success = activity_db.add_or_update_task(task_data_to_save)

        if save_success:
            log_info("task_manager", fn_name, f"Task {task_data_to_save['event_id']} saved to DB for {user_id}")
            # Fetch the potentially updated data from DB to return it
            saved_data = activity_db.get_task(task_data_to_save['event_id'])
            if saved_data and AGENT_STATE_MANAGER_IMPORTED:
                add_task_to_context(user_id, saved_data) # Update memory context
            return saved_data if saved_data else task_data_to_save # Return DB data if possible
        else:
            log_error("task_manager", fn_name, f"Failed to save task {task_data_to_save.get('event_id')} to DB for {user_id}.", user_id=user_id)
            # Attempt GCal rollback if DB save failed *after* GCal success
            if google_event_id is not None and calendar_api is not None:
                log_warning("task_manager", fn_name, f"DB save failed. Rolling back GCal event {google_event_id}")
                try: calendar_api.delete_event(google_event_id)
                except Exception: log_error("task_manager", fn_name, f"GCal rollback failed for {google_event_id}", user_id=user_id)
            return None

    except Exception as e:
        # Catch any unexpected errors during preparation
        log_error("task_manager", fn_name, f"Unexpected error during task creation for {user_id}", e, user_id=user_id)
        # Rollback GCal if it was created before the unexpected error
        if google_event_id is not None and calendar_api is not None:
             log_warning("task_manager", fn_name, f"Unexpected error. Rolling back GCal event {google_event_id}")
             try: calendar_api.delete_event(google_event_id)
             except Exception: log_error("task_manager", fn_name, f"GCal rollback failed for {google_event_id}", user_id=user_id)
        return None

# Returns dict (updated item) or None
def update_task(user_id: str, item_id: str, updates: Dict[str, Any]) -> Dict | None:
    """Updates details of an existing task/reminder in the DB and GCal."""
    fn_name = "update_task"
    if not DB_IMPORTED:
        log_error("task_manager", fn_name, "Database module not imported. Cannot update task.")
        return None

    log_info("task_manager", fn_name, f"Updating item {item_id} for {user_id}, keys: {list(updates.keys())}")

    # 1. Get existing task data from DB
    existing_task = activity_db.get_task(item_id) # Returns dict or None
    if existing_task is None:
        log_error("task_manager", fn_name, f"Task {item_id} not found in DB.", user_id=user_id)
        return None
    if existing_task.get("user_id") != user_id:
        log_error("task_manager", fn_name, f"User mismatch for task {item_id}.", user_id=user_id)
        return None

    calendar_api = _get_calendar_api(user_id)
    item_type = existing_task.get("type")
    gcal_updated = False # Track GCal update success

    # 2. Handle GCal Update (Reminders only)
    if item_type == "reminder" and not item_id.startswith("local_") and calendar_api is not None:
        gcal_payload = {}
        needs_gcal_update = False
        if "description" in updates:
            gcal_payload["title"] = updates["description"]
            gcal_payload["description"] = f"Reminder: {updates['description']}"
            needs_gcal_update = True
        if "date" in updates or "time" in updates:
            gcal_payload["date"] = updates.get("date", existing_task.get("date"))
            gcal_payload["time"] = updates.get("time") if "time" in updates else existing_task.get("time")
            if gcal_payload["date"] is not None: needs_gcal_update = True

        if needs_gcal_update:
            log_info("task_manager", fn_name, f"Attempting GCal update for reminder {item_id}")
            try:
                update_success = calendar_api.update_event(item_id, gcal_payload)
                if update_success:
                    gcal_updated = True
                    log_info("task_manager", fn_name, f"GCal reminder {item_id} updated successfully.")
                else: log_warning("task_manager", fn_name, f"GCal reminder {item_id} update failed (API returned False).")
            except Exception as gcal_err:
                 log_error("task_manager", fn_name, f"Error updating GCal reminder {item_id}", gcal_err, user_id=user_id)
        else: log_info("task_manager", fn_name, f"No relevant fields for GCal reminder {item_id} update.")

    # 3. Prepare DB Updates
    db_update_data = existing_task.copy() # Start with existing data
    # Apply valid updates from the input 'updates' dictionary
    allowed_meta_keys = {"description", "date", "time", "estimated_duration", "project"}
    applied_db_updates = False
    for key, value in updates.items():
        if key in allowed_meta_keys:
            db_update_data[key] = value
            if key == 'description': db_update_data['title'] = value # Keep title synced
            applied_db_updates = True

    # 4. Refresh GCal Timestamps if GCal was updated
    if gcal_updated and calendar_api is not None:
        gcal_details = calendar_api._get_single_event(item_id)
        if gcal_details:
            parsed = calendar_api._parse_google_event(gcal_details)
            db_update_data["gcal_start_datetime"] = parsed.get("gcal_start_datetime")
            db_update_data["gcal_end_datetime"] = parsed.get("gcal_end_datetime")
            applied_db_updates = True # Mark as updated even if only GCal times changed
            log_info("task_manager", fn_name, f"Refreshed GCal times in task data for {item_id}")
        else:
            log_warning("task_manager", fn_name, f"GCal update ok, but failed to re-fetch details for {item_id}")

    # 5. Save to DB if changes were applied or GCal timestamps were refreshed
    if applied_db_updates:
        save_success = activity_db.add_or_update_task(db_update_data)
        if save_success:
            log_info("task_manager", fn_name, f"Task {item_id} updated successfully in DB.")
            # Fetch final state from DB
            updated_task_from_db = activity_db.get_task(item_id)
            if updated_task_from_db and AGENT_STATE_MANAGER_IMPORTED:
                 update_task_in_context(user_id, item_id, updated_task_from_db) # Update memory
            return updated_task_from_db if updated_task_from_db else db_update_data
        else:
            log_error("task_manager", fn_name, f"Failed to save task {item_id} updates to DB.", user_id=user_id)
            return None # DB save failed
    else:
        log_info("task_manager", fn_name, f"No applicable DB updates or GCal timestamp changes for task {item_id}.")
        return existing_task # Return original data if no changes were made

# Returns dict (updated item) or None
def update_task_status(user_id: str, item_id: str, new_status: str) -> Dict | None:
    """Updates only the status and related tracking fields in the DB."""
    fn_name = "update_task_status"
    if not DB_IMPORTED:
        log_error("task_manager", fn_name, "Database module not imported.")
        return None

    log_info("task_manager", fn_name, f"Setting status='{new_status}' for item {item_id}, user {user_id}")
    # Validate and clean status
    new_status_clean = new_status.lower().replace(" ", "")
    allowed_statuses = {"pending", "in_progress", "completed"}
    if new_status_clean == "cancelled":
        log_error("task_manager", fn_name, "Use cancel_item() function for 'cancelled' status.", user_id=user_id)
        return None
    if new_status_clean not in allowed_statuses:
         log_error("task_manager", fn_name, f"Invalid status '{new_status}' provided.", user_id=user_id)
         return None

    # 1. Get existing task data
    existing_task = activity_db.get_task(item_id)
    if existing_task is None:
        log_error("task_manager", fn_name, f"Task {item_id} not found in DB.", user_id=user_id)
        return None
    if existing_task.get("user_id") != user_id:
        log_error("task_manager", fn_name, f"User mismatch for task {item_id}.", user_id=user_id)
        return None

    # 2. Prepare update dictionary
    updates_dict = {"status": new_status_clean}
    now_iso_utc = datetime.now(timezone.utc).isoformat(timespec='seconds')+'Z'

    if new_status_clean == "completed":
        updates_dict["completed_at"] = now_iso_utc
        updates_dict["progress_percent"] = 100
        if existing_task.get("type") == "task":
            updates_dict["sessions_completed"] = existing_task.get("sessions_planned", 0)
    elif new_status_clean == "pending":
         updates_dict["completed_at"] = None # Use None for DB
         updates_dict["progress_percent"] = 0
         updates_dict["sessions_completed"] = 0
    elif new_status_clean == "in_progress":
         updates_dict["completed_at"] = None # Use None for DB

    # 3. Apply updates to a copy and save
    task_data_to_save = existing_task.copy()
    task_data_to_save.update(updates_dict)

    save_success = activity_db.add_or_update_task(task_data_to_save)

    if save_success:
        log_info("task_manager", fn_name, f"Task {item_id} status updated to {new_status_clean} in DB.")
        updated_task_from_db = activity_db.get_task(item_id) # Re-fetch to get final state
        if updated_task_from_db and AGENT_STATE_MANAGER_IMPORTED:
             update_task_in_context(user_id, item_id, updated_task_from_db)
        return updated_task_from_db if updated_task_from_db else task_data_to_save
    else:
        log_error("task_manager", fn_name, f"Failed to save status update for task {item_id} to DB.", user_id=user_id)
        return None

# Returns bool
def cancel_item(user_id: str, item_id: str) -> bool:
    """Sets item status to 'cancelled' in DB and deletes associated GCal events."""
    fn_name = "cancel_item"
    if not DB_IMPORTED:
        log_error("task_manager", fn_name, "Database module not imported.")
        return False

    log_info("task_manager", fn_name, f"Processing cancellation for item {item_id}, user {user_id}")

    # 1. Get task data from DB
    task_data = activity_db.get_task(item_id)
    if task_data is None:
        log_warning("task_manager", fn_name, f"Task {item_id} not found in DB during cancel. Assuming handled.")
        return True # Not found -> already gone? Success.
    if task_data.get("user_id") != user_id:
        log_error("task_manager", fn_name, f"User mismatch for item {item_id}.", user_id=user_id)
        return False
    if task_data.get("status") == "cancelled":
        log_info("task_manager", fn_name, f"Item {item_id} is already cancelled.")
        return True

    # 2. GCal Cleanup
    calendar_api = _get_calendar_api(user_id)
    item_type = task_data.get("type")
    gcal_cleanup_errors = []

    if calendar_api is not None and not item_id.startswith("local_"):
        log_info("task_manager", fn_name, f"Performing GCal cleanup for {item_type} {item_id}")
        # Delete main event if it's a Reminder on GCal
        if item_type == "reminder":
            try:
                deleted = calendar_api.delete_event(item_id)
                if not deleted: log_warning("task_manager", fn_name, f"GCal delete failed/not found for reminder {item_id}")
            except Exception as del_err:
                 log_error("task_manager", fn_name, f"Error deleting GCal reminder {item_id}", del_err, user_id=user_id)
                 gcal_cleanup_errors.append(f"Main event {item_id}")
        # Delete session events if it's a Task
        elif item_type == "task":
            session_ids = task_data.get("session_event_ids", []) # Already decoded list from get_task
            if isinstance(session_ids, list) and session_ids:
                log_info("task_manager", fn_name, f"Deleting {len(session_ids)} GCal sessions for task {item_id}")
                for session_id in session_ids:
                    if not isinstance(session_id, str) or not session_id: continue
                    try:
                        deleted = calendar_api.delete_event(session_id)
                        if not deleted: log_warning("task_manager", fn_name, f"GCal delete failed/not found for session {session_id}")
                    except Exception as sess_del_err:
                         log_error("task_manager", fn_name, f"Error deleting GCal session {session_id}", sess_del_err, user_id=user_id)
                         gcal_cleanup_errors.append(f"Session {session_id}")
            else: log_info("task_manager", fn_name, f"No GCal session IDs to delete for task {item_id}")
    # else: log reason for skipping GCal cleanup

    # 3. Update DB Status
    update_payload = task_data.copy()
    update_payload["status"] = "cancelled"
    # Reset task-specific fields on cancel
    if item_type == "task":
        update_payload["sessions_planned"] = 0
        update_payload["sessions_completed"] = 0
        update_payload["progress_percent"] = 0
        update_payload["session_event_ids"] = [] # Store empty list, will be JSON '[]'

    save_success = activity_db.add_or_update_task(update_payload)

    if save_success:
        log_info("task_manager", fn_name, f"Successfully marked item {item_id} as cancelled in DB.")
        cancelled_task_data = activity_db.get_task(item_id) # Get final state
        if cancelled_task_data and AGENT_STATE_MANAGER_IMPORTED:
             update_task_in_context(user_id, item_id, cancelled_task_data) # Update memory context
        if gcal_cleanup_errors:
             log_warning("task_manager", fn_name, f"Cancel successful for {item_id}, but GCal cleanup errors: {gcal_cleanup_errors}", user_id=user_id)
        return True
    else:
        log_error("task_manager", fn_name, f"Failed to save cancelled status for {item_id} to DB.", user_id=user_id)
        # State is inconsistent: GCal might be cleaned, DB not updated.
        return False

# --- Scheduling Functions ---

# Returns dict with 'success', 'message', 'booked_count', 'session_ids'
def schedule_work_sessions(user_id: str, task_id: str, slots_to_book: List[Dict]) -> Dict:
    """Creates GCal events for proposed work sessions and updates the parent task in DB."""
    fn_name = "schedule_work_sessions"
    default_fail_result = {"success": False, "booked_count": 0, "message": "An unexpected error occurred.", "session_ids": []}
    if not DB_IMPORTED:
        return {**default_fail_result, "message": "Database module not available."}

    log_info("task_manager", fn_name, f"Booking {len(slots_to_book)} sessions for task {task_id}")

    calendar_api = _get_calendar_api(user_id)
    if calendar_api is None:
        return {**default_fail_result, "message": "Calendar is not connected or active."}

    # 1. Get Parent Task Details from DB
    task_metadata = activity_db.get_task(task_id)
    if task_metadata is None:
        log_error("task_manager", fn_name, f"Parent task {task_id} not found in DB.", user_id=user_id)
        return {**default_fail_result, "message": "Original task details not found."}
    if task_metadata.get("user_id") != user_id:
        log_error("task_manager", fn_name, f"User mismatch for task {task_id}.", user_id=user_id)
        return {**default_fail_result, "message": "Task ownership mismatch."}
    if task_metadata.get("type") != "task":
         log_error("task_manager", fn_name, f"Item {task_id} is not a task.", user_id=user_id)
         return {**default_fail_result, "message": "Scheduling only supported for tasks."}

    task_title = task_metadata.get("title", "Task Work")

    # 2. Create GCal Events
    created_session_ids = []
    errors = []
    for i, session_slot in enumerate(slots_to_book):
        session_date = session_slot.get("date")
        session_time = session_slot.get("time")
        session_end_time = session_slot.get("end_time")
        if not all([session_date, session_time, session_end_time]):
             msg = f"Session {i+1} missing date/time/end_time"
             log_warning("task_manager", fn_name, msg + f" for task {task_id}")
             errors.append(msg); continue
        try:
            start_dt = datetime.strptime(f"{session_date} {session_time}", "%Y-%m-%d %H:%M")
            end_dt = datetime.strptime(f"{session_date} {session_end_time}", "%Y-%m-%d %H:%M")
            duration_minutes = int((end_dt - start_dt).total_seconds() / 60)
            if duration_minutes <= 0: raise ValueError("Duration must be positive")

            session_event_data = {
                "title": f"Work: {task_title} [{i+1}/{len(slots_to_book)}]",
                "description": f"Focused work session for task: {task_title}\nParent Task ID: {task_id}",
                "date": session_date, "time": session_time,
                "duration": f"{duration_minutes}m"
            }
            # create_event returns ID string or None
            session_event_id = calendar_api.create_event(session_event_data)
            if session_event_id is not None:
                created_session_ids.append(session_event_id)
            else:
                msg = f"Session {i+1} GCal creation failed (API returned None)"
                log_error("task_manager", fn_name, msg + f" for task {task_id}.", user_id=user_id)
                errors.append(msg)
        except Exception as e:
            msg = f"Session {i+1} creation error: {type(e).__name__}"
            log_error("task_manager", fn_name, f"Error creating GCal session {i+1} for task {task_id}", e, user_id=user_id)
            errors.append(msg)

    if not created_session_ids:
        err_summary = "; ".join(errors) if errors else "Unknown reason"
        log_error("task_manager", fn_name, f"Failed to create any GCal sessions for task {task_id}. Errors: {err_summary}", user_id=user_id)
        return {**default_fail_result, "message": f"Sorry, couldn't add sessions to calendar. Errors: {err_summary}"}

    log_info("task_manager", fn_name, f"Created {len(created_session_ids)} GCal sessions for task {task_id}: {created_session_ids}")

    # 3. Update Parent Task in DB
    # Combine existing and new session IDs
    existing_session_ids = task_metadata.get("session_event_ids", []) # Already decoded list
    if not isinstance(existing_session_ids, list): existing_session_ids = []
    all_session_ids = list(set(existing_session_ids + created_session_ids))

    update_payload = task_metadata.copy()
    update_payload["sessions_planned"] = len(all_session_ids)
    update_payload["session_event_ids"] = all_session_ids # Store list, add_or_update handles JSON
    update_payload["status"] = "in_progress" # Mark task as in progress

    save_success = activity_db.add_or_update_task(update_payload)

    if save_success:
        log_info("task_manager", fn_name, f"Parent task {task_id} updated in DB with session info.")
        updated_task_data = activity_db.get_task(task_id) # Get final state
        if updated_task_data and AGENT_STATE_MANAGER_IMPORTED:
            update_task_in_context(user_id, task_id, updated_task_data) # Update memory

        num_booked = len(created_session_ids)
        plural_s = "s" if num_booked > 1 else ""
        msg = f"Okay, I've scheduled {num_booked} work session{plural_s} for '{task_title}' in your calendar."
        if errors: msg += f" (Issues with {len(errors)} other slots)."
        return {"success": True, "booked_count": num_booked, "message": msg, "session_ids": created_session_ids}
    else:
        log_error("task_manager", fn_name, f"Created GCal sessions for {task_id}, but failed DB update.", user_id=user_id)
        # Rollback GCal changes
        log_warning("task_manager", fn_name, f"Attempting GCal rollback for {len(created_session_ids)} sessions (Task ID: {task_id}).")
        if calendar_api is not None:
            for sid in created_session_ids:
                try: calendar_api.delete_event(sid)
                except Exception: log_error("task_manager", fn_name, f"GCal rollback delete failed for session {sid}", user_id=user_id)
        return {**default_fail_result, "message": "Scheduled sessions, but failed to link to task. Calendar changes rolled back."}

# Returns dict with 'success', 'cancelled_count', 'message'
def cancel_sessions(user_id: str, task_id: str, session_ids_to_cancel: List[str]) -> Dict:
    """Cancels specific GCal work sessions and updates task metadata in DB."""
    fn_name = "cancel_sessions"
    default_fail_result = {"success": False, "cancelled_count": 0, "message": "An unexpected error occurred."}
    if not DB_IMPORTED:
        return {**default_fail_result, "message": "Database module not available."}

    log_info("task_manager", fn_name, f"Cancelling {len(session_ids_to_cancel)} sessions for task {task_id}")

    calendar_api = _get_calendar_api(user_id)
    if calendar_api is None:
        return {**default_fail_result, "message": "Calendar is not connected or active."}

    # 1. Get Parent Task from DB
    task_metadata = activity_db.get_task(task_id)
    if task_metadata is None:
        log_error("task_manager", fn_name, f"Parent task {task_id} not found.", user_id=user_id)
        return {**default_fail_result, "message": "Original task details not found."}
    if task_metadata.get("user_id") != user_id: # Check ownership
         log_error("task_manager", fn_name, f"User mismatch for task {task_id}.", user_id=user_id)
         return {**default_fail_result, "message": "Task ownership mismatch."}
    if task_metadata.get("type") != "task":
         return {**default_fail_result, "message": "Can only cancel sessions for tasks."}

    # 2. Delete GCal Events
    cancelled_count = 0
    errors = []
    valid_gcal_ids_to_cancel = [sid for sid in session_ids_to_cancel if isinstance(sid, str) and not sid.startswith("local_")]

    for session_id in valid_gcal_ids_to_cancel:
        try:
            deleted = calendar_api.delete_event(session_id) # Returns bool
            if deleted: cancelled_count += 1
            # else: delete_event logs warning if not found/failed
        except Exception as e:
            log_error("task_manager", fn_name, f"Error deleting GCal session {session_id} for task {task_id}", e, user_id=user_id)
            errors.append(session_id)
    log_info("task_manager", fn_name, f"GCal delete attempts for task {task_id}: Success/Gone: {cancelled_count}, Errors: {len(errors)}")

    # 3. Update Parent Task in DB
    existing_session_ids = task_metadata.get("session_event_ids", []) # Already list from get_task
    if not isinstance(existing_session_ids, list): existing_session_ids = []

    cancelled_set = set(session_ids_to_cancel) # Use original list (might include local IDs intended for removal)
    remaining_ids = [sid for sid in existing_session_ids if sid not in cancelled_set]

    update_payload = task_metadata.copy()
    update_payload["sessions_planned"] = len(remaining_ids)
    update_payload["session_event_ids"] = remaining_ids # Store list for DB function
    # Only change status to pending if NO sessions remain, otherwise keep current status
    if not remaining_ids:
        update_payload["status"] = "pending"

    save_success = activity_db.add_or_update_task(update_payload)

    if save_success:
        log_info("task_manager", fn_name, f"Parent task {task_id} updated in DB after session cancellation.")
        updated_task_data = activity_db.get_task(task_id) # Get final state
        if updated_task_data and AGENT_STATE_MANAGER_IMPORTED:
            update_task_in_context(user_id, task_id, updated_task_data) # Update memory

        msg = f"Successfully cancelled {cancelled_count} session(s) from your calendar."
        if errors: msg += f" Encountered errors cancelling {len(errors)}."
        return {"success": True, "cancelled_count": cancelled_count, "message": msg}
    else:
        log_error("task_manager", fn_name, f"Deleted GCal sessions for {task_id}, but failed DB update.", user_id=user_id)
        # Inconsistent state: GCal events gone, DB still references them. Hard to roll back GCal deletes.
        return {**default_fail_result, "cancelled_count": cancelled_count, "message": "Cancelled sessions in calendar, but failed to update the task link."}

# --- END OF REFACTORED services/task_manager.py ---

# --- END OF FILE services/task_manager.py ---



================================================================================
ðŸ“„ services/task_query_service.py
================================================================================

# --- START OF FILE services/task_query_service.py ---

# --- START OF REFACTORED services/task_query_service.py ---
"""Service layer for querying and formatting task/reminder data from the database."""
from datetime import datetime, timedelta, timezone # Added timezone
import json
from typing import Dict, List, Any, Set, Tuple # Keep required types
import pytz
import traceback # Keep for detailed error logging

from tools.logger import log_info, log_error, log_warning
# Import the database utility module
try:
    import tools.activity_db as activity_db
    DB_IMPORTED = True
except ImportError:
    log_error("task_query_service", "import", "activity_db not found. Task querying disabled.", None)
    DB_IMPORTED = False
    # Dummy DB functions
    class activity_db:
        @staticmethod
        def list_tasks_for_user(*args, **kwargs): return []
        @staticmethod
        def get_task(*args, **kwargs): return None
    # Consider halting application if DB is critical

# Agent State Manager Import (still needed for preferences/calendar API instance)
try:
    from services.agent_state_manager import get_agent_state
    AGENT_STATE_MANAGER_IMPORTED = True
except ImportError:
    log_error("task_query_service", "import", "AgentStateManager not found.")
    AGENT_STATE_MANAGER_IMPORTED = False
    def get_agent_state(*args, **kwargs): return None # Dummy function

# Google Calendar API Import (needed for checking type and formatting sessions)
try:
    from tools.google_calendar_api import GoogleCalendarAPI
    GCAL_API_IMPORTED = True
except ImportError:
     log_warning("task_query_service", "import", "GoogleCalendarAPI not imported.")
     GoogleCalendarAPI = None
     GCAL_API_IMPORTED = False

# Define active statuses consistently
ACTIVE_STATUSES = ["pending", "in_progress"] # Use list for DB query IN clause

# --- Internal Helper Functions ---

# Keep this function as it retrieves the API instance needed for _format_task_line
def _get_calendar_api_from_state(user_id):
    """Helper to retrieve the active calendar API instance from agent state."""
    fn_name = "_get_calendar_api_from_state"
    if not AGENT_STATE_MANAGER_IMPORTED or not GCAL_API_IMPORTED or GoogleCalendarAPI is None:
        return None
    try:
        agent_state = get_agent_state(user_id) # Returns dict or None
        if agent_state is not None:
            calendar_api_maybe = agent_state.get("calendar")
            if isinstance(calendar_api_maybe, GoogleCalendarAPI) and calendar_api_maybe.is_active():
                return calendar_api_maybe # Return active instance
    except Exception as e:
         log_error("task_query_service", fn_name, f"Error getting calendar API for {user_id}", e, user_id=user_id)
    return None # Return None if not found, not active, or error

# Keep sorting logic, operates on list of dictionaries
def _sort_tasks(task_list: List[Dict]) -> List[Dict]:
    """Sorts task list robustly by date/time."""
    # (Sorting logic remains the same as previous version - operates on dicts)
    fn_name = "_sort_tasks"
    def sort_key(item):
        gcal_start = item.get("gcal_start_datetime")
        if gcal_start and isinstance(gcal_start, str):
             try:
                 if 'T' in gcal_start: # Datetime format
                      dt_aware = datetime.fromisoformat(gcal_start.replace('Z', '+00:00'))
                      return dt_aware.replace(tzinfo=None) # Compare naive UTC equivalent
                 elif len(gcal_start) == 10: # Date format (all-day)
                      dt_date = datetime.strptime(gcal_start, '%Y-%m-%d').date()
                      # Sort all-day events as start of the day
                      return datetime.combine(dt_date, datetime.min.time())
             except ValueError:
                  pass # Fallback to metadata if parse fails

        # Fallback logic using metadata date/time
        meta_date_str = item.get("date")
        meta_time_str = item.get("time")
        sort_dt = datetime.max # Default to max for sorting unknowns last

        if meta_date_str:
            try:
                if meta_time_str: # Timed item
                    time_part = meta_time_str
                    if len(time_part.split(':')) == 2: time_part += ':00' # Add seconds if missing
                    sort_dt = datetime.strptime(f"{meta_date_str} {time_part}", "%Y-%m-%d %H:%M:%S")
                else: # All day item based on metadata date
                    sort_dt = datetime.strptime(meta_date_str, "%Y-%m-%d")
            except (ValueError, TypeError):
                 # Log warning? Maybe too verbose for sorting fallback
                 pass # Use default max time
        return sort_dt

    try:
        # Sort primarily by datetime, secondarily by creation time (if available), finally by title
        return sorted(task_list, key=lambda item: (sort_key(item), item.get("created_at", ""), item.get("title", "").lower()))
    except Exception as e:
        log_error("task_query_service", fn_name, f"Error during task sorting: {e}", e)
        return task_list # Return unsorted list on error

# Keep formatting logic, operates on list of dictionaries
# Ensure it correctly handles data types from DB (e.g., sessions_planned is INT)
# And decodes session_event_ids if needed (activity_db functions handle this now)
def _format_task_line(task_data: Dict, user_timezone_str: str = "UTC", calendar_api = None) -> str:
    """Formats a single task/event dictionary into a display string."""
    fn_name = "_format_task_line"
    try:
        # Determine User Timezone Object
        user_tz = pytz.utc
        try:
            if user_timezone_str: user_tz = pytz.timezone(user_timezone_str)
        except pytz.UnknownTimeZoneError: user_timezone_str = "UTC"

        # --- Assemble Main Line Parts (Largely same logic as before) ---
        parts = []
        item_type_raw = task_data.get("type", "Item")
        item_type = str(item_type_raw).capitalize()
        if item_type_raw == "external_event": item_type = "Event" # Handle external events from sync
        parts.append(f"({item_type})")
        desc = str(task_data.get("title", "")).strip() or "(No Title)"
        parts.append(desc)
        if item_type_raw == "task":
            duration = task_data.get("estimated_duration")
            # Check for various empty-like values
            if duration and not str(duration).strip().lower() in ['', 'none', 'nan', 'null']:
                parts.append(f"[Est: {duration}]")

        # Date/Time Formatting (Prioritize GCal, fallback to metadata)
        gcal_start_str = task_data.get("gcal_start_datetime"); meta_date = task_data.get("date"); meta_time = task_data.get("time")
        dt_str = ""
        if gcal_start_str:
             try: # Format GCal time
                 if 'T' in gcal_start_str: # Datetime
                      dt_aware = datetime.fromisoformat(gcal_start_str.replace('Z', '+00:00'))
                      dt_local = dt_aware.astimezone(user_tz)
                      # Use a clear format like: Tue, Apr 25 @ 10:00 EDT
                      formatted_dt = dt_local.strftime('%a, %b %d @ %H:%M %Z')
                      dt_str = f" on {formatted_dt}"
                 elif len(gcal_start_str) == 10: # Date (All day)
                      dt_local = datetime.strptime(gcal_start_str, '%Y-%m-%d').date()
                      formatted_dt = dt_local.strftime('%a, %b %d (All day)')
                      dt_str = f" on {formatted_dt}"
             except Exception as fmt_err:
                  log_warning("task_query_service", fn_name, f"Could not format gcal_start '{gcal_start_str}': {fmt_err}")
                  dt_str = f" (Time Error: {gcal_start_str})" # Show raw on error
        elif meta_date: # Fallback to metadata date/time
             dt_str = f" on {meta_date}"
             if meta_time: dt_str += f" at {meta_time}"
             else: dt_str += " (All day)"
        if dt_str: parts.append(dt_str)

        project = task_data.get("project"); status = task_data.get("status")
        if project: parts.append(f"{{{project}}}")
        if item_type_raw in ["task", "reminder"] and status:
             parts.append(f"[{str(status).capitalize()}]")

        main_line = " ".join(p for p in parts if p)

        # --- Display Scheduled Session Details ---
        session_details_lines = []
        # session_event_ids should be a list from the DB access layer now
        session_ids = task_data.get("session_event_ids")
        if item_type_raw == "task" and calendar_api is not None and isinstance(session_ids, list) and session_ids:
            session_details_lines.append("    â””â”€â”€ Scheduled Sessions:")
            session_num = 0
            for session_id in session_ids:
                if not isinstance(session_id, str) or not session_id.strip(): continue # Skip invalid IDs
                try:
                    session_event_data = calendar_api._get_single_event(session_id) # Returns dict or None
                    if session_event_data:
                        session_num += 1
                        parsed_session = calendar_api._parse_google_event(session_event_data) # Returns dict
                        s_start = parsed_session.get("gcal_start_datetime")
                        s_end = parsed_session.get("gcal_end_datetime")
                        s_info = "(Time Error)" # Default
                        if s_start and s_end:
                            try:
                                s_aware = datetime.fromisoformat(s_start.replace('Z', '+00:00'))
                                e_aware = datetime.fromisoformat(s_end.replace('Z', '+00:00'))
                                s_local, e_local = s_aware.astimezone(user_tz), e_aware.astimezone(user_tz)
                                # Format: YYYY-MM-DD HH:MM-HH:MM TZN
                                s_info = s_local.strftime('%Y-%m-%d %H:%M') + e_local.strftime('-%H:%M %Z')
                            except Exception as parse_err:
                                log_warning("task_query_service", fn_name, f"Could not parse session times {s_start}-{s_end}: {parse_err}")
                                s_info = f"{s_start} to {s_end} (parse error)"
                        session_details_lines.append(f"        {session_num}) {s_info} (ID: {session_id})")
                    # else: log_warning(f"Session ID {session_id} not found in GCal") # Optional: log if GCal event missing
                except Exception as fetch_err:
                    log_error("task_query_service", fn_name, f"Error fetching/processing session {session_id}", fetch_err, user_id=task_data.get("user_id"))
                    session_details_lines.append(f"        - Error fetching session ID {session_id}")
            # Remove header if no actual sessions were listed
            if session_num == 0 and session_details_lines:
                 session_details_lines.pop(0)

        # Combine main line and session details
        return main_line + ("\n" + "\n".join(session_details_lines) if session_details_lines else "")

    except Exception as e:
        # Log error with user context if available
        user_ctx = task_data.get("user_id", "Unknown")
        log_error("task_query_service", fn_name, f"General error formatting item line {task_data.get('event_id')}. Error: {e}\n{traceback.format_exc()}", e, user_id=user_ctx)
        return f"Error displaying item: {task_data.get('event_id', 'Unknown ID')}"


# --- Public Service Functions ---

# Returns Tuple[str, Dict]
def get_formatted_list(
    user_id: str,
    date_range: Tuple[str, str] | None = None,
    status_filter: str = 'active', # Provide default
    project_filter: str | None = None,
    trigger_sync: bool = False # Keep trigger_sync param, though sync isn't fully implemented
) -> Tuple[str, Dict]:
    """
    Gets tasks from DB, filters (optional), sorts, formats into numbered list string & mapping.
    Includes fetching and displaying details for scheduled task sessions if GCal is active.
    """
    fn_name = "get_formatted_list"
    if not DB_IMPORTED:
        log_error("task_query_service", fn_name, "Database module unavailable.", user_id=user_id)
        return "Error: Could not access task data.", {}

    log_info("task_query_service", fn_name, f"Executing for user={user_id}, Status={status_filter}, Range={date_range}, Proj={project_filter}")

    if trigger_sync:
        log_info("task_query_service", fn_name, "Sync triggered (Not Implemented Yet).", user_id=user_id)
        # Future: Call sync_service.perform_full_sync(user_id) here?

    # Determine status list for DB query
    query_status_list = None
    filter_lower = status_filter.lower().replace(" ", "") if status_filter else 'active' # Default if None
    if filter_lower == 'active': query_status_list = ACTIVE_STATUSES
    elif filter_lower == 'completed': query_status_list = ["completed"]
    elif filter_lower == 'pending': query_status_list = ["pending"]
    elif filter_lower == 'in_progress': query_status_list = ["in_progress"]
    elif filter_lower == 'all': query_status_list = None # No status filter for DB
    else:
        log_warning("task_query_service", fn_name, f"Unknown status filter '{status_filter}'. Defaulting 'active'.", user_id=user_id)
        query_status_list = ACTIVE_STATUSES

    log_info("task_query_service", fn_name, f"Querying DB for user={user_id}, Status={query_status_list}, Range={date_range}, Proj={project_filter}")

    # Fetch data from DB, applying filters available in the DB function
    task_list = []
    try:
        task_list = activity_db.list_tasks_for_user(
            user_id=user_id,
            status_filter=query_status_list, # Pass list of statuses or None
            date_range=date_range,         # Pass tuple or None
            project_filter=project_filter  # Pass string or None
        )
        log_info("task_query_service", fn_name, f"Fetched {len(task_list)} tasks from DB for {user_id} with filters.")
    except Exception as e:
        log_error("task_query_service", fn_name, f"Error fetching tasks from DB for {user_id}", e, user_id=user_id)
        return "Error retrieving tasks.", {}

    # No need for python filtering now as DB function handles it

    if not task_list:
        log_info("task_query_service", fn_name, f"No tasks found matching criteria for {user_id} after DB query.")
        return "", {} # Return empty string and dict if no tasks found

    # Get Calendar API and User Timezone for formatting
    calendar_api = _get_calendar_api_from_state(user_id)
    user_tz_str = "UTC"
    if AGENT_STATE_MANAGER_IMPORTED:
        agent_state = get_agent_state(user_id)
        prefs = agent_state.get("preferences", {}) if agent_state else {}
        user_tz_str = prefs.get("TimeZone", "UTC")

    # Sort tasks
    sorted_tasks = _sort_tasks(task_list)

    # Format lines and build mapping
    lines, mapping = [], {}
    item_num = 0
    for task in sorted_tasks:
        item_id = task.get("event_id")
        if not item_id: continue # Skip items missing id

        item_num += 1
        formatted_line = _format_task_line(task, user_tz_str, calendar_api)
        lines.append(f"{item_num}. {formatted_line}")
        mapping[str(item_num)] = item_id # Use string key

    if item_num == 0: # Should only happen if formatting fails for all items
        log_warning("task_query_service", fn_name, f"Formatting resulted in zero list items for {user_id}.", user_id=user_id)
        return "Error formatting the task list.", {}

    list_body = "\n".join(lines)
    log_info("task_query_service", fn_name, f"Generated list body ({len(mapping)} items) for {user_id}")
    return list_body, mapping


# Returns List[Dict]
def get_tasks_for_summary(
    user_id: str,
    date_range: Tuple[str, str], # Date range is typically required for summaries
    status_filter: str = 'active',
    trigger_sync: bool = False
) -> List[Dict]:
    """Gets tasks from DB for summaries, filters, sorts, returns list of dictionaries."""
    fn_name = "get_tasks_for_summary"
    if not DB_IMPORTED:
        log_error("task_query_service", fn_name, "Database module unavailable.", user_id=user_id)
        return []

    log_info("task_query_service", fn_name, f"Executing for user={user_id}, Filter={status_filter}, Range={date_range}")

    if trigger_sync:
        log_info("task_query_service", fn_name, "Sync triggered (Not Implemented Yet).", user_id=user_id)

    # Determine status list for DB query
    query_status_list = None
    filter_lower = status_filter.lower().replace(" ", "") if status_filter else 'active'
    if filter_lower == 'active': query_status_list = ACTIVE_STATUSES
    elif filter_lower == 'completed': query_status_list = ["completed"]
    elif filter_lower == 'pending': query_status_list = ["pending"]
    elif filter_lower == 'in_progress': query_status_list = ["in_progress"]
    # 'all' means query_status_list remains None

    try:
        # Fetch directly using the specific filters required for summaries
        task_list = activity_db.list_tasks_for_user(
            user_id=user_id,
            status_filter=query_status_list, # Pass list of statuses or None
            date_range=date_range
            # No project filter typically needed for summaries
        )
        # Sort results after fetching
        sorted_tasks = _sort_tasks(task_list)
        log_info("task_query_service", fn_name, f"Returning {len(sorted_tasks)} tasks for summary {user_id}")
        return sorted_tasks
    except Exception as db_err:
        log_error("task_query_service", fn_name, f"Database error fetching tasks for summary: {user_id}", db_err, user_id=user_id)
        return []


# Returns Tuple[List[Dict], List[Dict]]
def get_context_snapshot(user_id: str, history_weeks: int = 1, future_weeks: int = 2) -> Tuple[List[Dict], List[Dict]]:
    """Fetches relevant active WT tasks (DB) and GCal events (API) for Orchestrator context."""
    fn_name = "get_context_snapshot"
    if not DB_IMPORTED:
        log_error("task_query_service", fn_name, "Database module unavailable.", user_id=user_id)
        return [], [] # Return empty lists

    log_info("task_query_service", fn_name, f"Get Context Snapshot: User={user_id}")
    task_context, calendar_context = [], []

    try:
        # 1. Calculate date range
        today = datetime.now(timezone.utc).date() # Use UTC date for consistency
        start_date = today - timedelta(weeks=history_weeks)
        end_date = today + timedelta(weeks=future_weeks)
        start_str, end_str = start_date.strftime("%Y-%m-%d"), end_date.strftime("%Y-%m-%d")
        date_range_tuple = (start_str, end_str)

        # 2. Get WT tasks from DB (only active ones within the window)
        try:
            task_context = activity_db.list_tasks_for_user(
                user_id=user_id,
                status_filter=ACTIVE_STATUSES, # Fetch only active tasks
                date_range=date_range_tuple     # Apply date range
            )
            log_info("task_query_service", fn_name, f"Fetched {len(task_context)} active WT tasks from DB for snapshot.")
        except Exception as db_err:
             log_error("task_query_service", fn_name, f"Failed to fetch tasks from DB for snapshot: {db_err}", db_err, user_id=user_id)
             task_context = [] # Continue without tasks if DB fails

        # 3. Get GCal events directly from API
        calendar_api = _get_calendar_api_from_state(user_id)
        if calendar_api:
            try:
                # Fetch GCal events - list_events returns parsed dicts
                calendar_context = calendar_api.list_events(start_str, end_str)
                log_info("task_query_service", fn_name, f"Fetched {len(calendar_context)} GCal events for snapshot.")
            except Exception as cal_e:
                log_error("task_query_service", fn_name, f"Failed fetch calendar events for snapshot: {cal_e}", cal_e, user_id=user_id)
                calendar_context = [] # Continue without calendar events
        else:
            log_info("task_query_service", fn_name, f"Calendar API not active for {user_id}, skipping GCal fetch for snapshot.")

        log_info("task_query_service", fn_name, f"Snapshot created for {user_id}: {len(task_context)} tasks, {len(calendar_context)} external events.")

    except Exception as e:
        log_error("task_query_service", fn_name, f"Error creating context snapshot for {user_id}", e, user_id=user_id)
        return [], [] # Return empty lists on error

    return task_context, calendar_context

# --- END OF REFACTORED services/task_query_service.py ---

# --- END OF FILE services/task_query_service.py ---



================================================================================
ðŸ“„ services/config_manager.py
================================================================================

# --- START OF FILE services/config_manager.py ---

# services/config_manager.py
"""Service layer for managing user configuration and preferences."""
from tools.logger import log_info, log_error, log_warning
# Import registry functions for persistence
from users.user_registry import get_user_preferences as get_prefs_from_registry
from users.user_registry import update_preferences as update_prefs_in_registry # This writes to file
# Import state manager for memory updates
try:
    # This function updates the live agent state dictionary
    from services.agent_state_manager import update_preferences_in_state
    AGENT_STATE_MANAGER_IMPORTED = True
except ImportError:
    log_error("config_manager", "import", "AgentStateManager not found. In-memory preference updates skipped.")
    AGENT_STATE_MANAGER_IMPORTED = False
    def update_preferences_in_state(*args, **kwargs): return False # Dummy

# Import calendar tool auth check
try:
    from tools.calendar_tool import authenticate as check_calendar_auth_status
    CALENDAR_TOOL_IMPORTED = True
except ImportError:
     log_error("config_manager", "import", "calendar_tool not found. Calendar auth initiation fails.")
     CALENDAR_TOOL_IMPORTED = False
     def check_calendar_auth_status(*args, **kwargs): return {"status": "fails", "message": "Calendar tool unavailable."}

from typing import Dict, Any, Optional

def get_preferences(user_id: str) -> Optional[Dict]:
    """Gets user preferences from the persistent registry."""
    try:
        prefs = get_prefs_from_registry(user_id)
        return prefs # Returns None if not found
    except Exception as e:
        log_error("config_manager", "get_preferences", f"Error reading preferences for {user_id}", e)
        return None

def update_preferences(user_id: str, updates: Dict) -> bool:
    """
    Updates preferences in persistent registry AND in-memory agent state.
    Returns True on success (based on registry update), False otherwise.
    """
    log_info("config_manager", "update_preferences", f"Updating preferences for {user_id}: {list(updates.keys())}")
    if not isinstance(updates, dict) or not updates:
        log_warning("config_manager", "update_preferences", "Invalid or empty updates provided.")
        return False

    # 1. Update Persistent Store (Registry File)
    registry_update_success = False
    try:
        update_prefs_in_registry(user_id, updates) # Writes to registry.json
        log_info("config_manager", "update_preferences", f"Registry file update requested for {user_id}")
        registry_update_success = True
    except Exception as e:
        log_error("config_manager", "update_preferences", f"Registry file update failed for {user_id}", e)
        return False # Don't proceed if persistence fails

    # 2. Update In-Memory State via AgentStateManager (If persistence succeeded)
    if registry_update_success and AGENT_STATE_MANAGER_IMPORTED:
        try:
            mem_update_success = update_preferences_in_state(user_id, updates) # Updates live _AGENT_STATE_STORE
            if not mem_update_success:
                log_warning("config_manager", "update_preferences", f"In-memory state update failed or user not found in state for {user_id}.")
                # Should we revert registry? For now, proceed but warn.
        except Exception as mem_e:
             log_error("config_manager", "update_preferences", f"Error updating in-memory state for {user_id}", mem_e)
             # Log error, but persistence succeeded, so arguably return True

    elif registry_update_success: # Log if manager wasn't imported
        log_warning("config_manager", "update_preferences", "AgentStateManager not imported. Skipping in-memory state update.")

    return registry_update_success # Return success based on registry write

def initiate_calendar_auth(user_id: str) -> Dict:
    """Initiates calendar auth flow via calendar_tool."""
    log_info("config_manager", "initiate_calendar_auth", f"Initiating calendar auth for {user_id}")
    if not CALENDAR_TOOL_IMPORTED:
         return {"status": "fails", "message": "Calendar auth component unavailable."}
    current_prefs = get_preferences(user_id) # Use service getter
    if not current_prefs:
        log_error("config_manager", "initiate_calendar_auth", f"Prefs not found for {user_id}")
        return {"status": "fails", "message": "User profile not found."}
    try:
        # Pass current prefs needed by authenticate function
        auth_result = check_calendar_auth_status(user_id, current_prefs)
        return auth_result
    except Exception as e:
        log_error("config_manager", "initiate_calendar_auth", f"Error during calendar auth init: {e}", e)
        return {"status": "fails", "message": "Error starting calendar auth."}

def set_user_status(user_id: str, status: str) -> bool:
    """Helper to specifically update user status in registry and memory."""
    log_info("config_manager", "set_user_status", f"Setting status='{status}' for {user_id}")
    if not status or not isinstance(status, str):
        log_warning("config_manager", "set_user_status", f"Invalid status value: {status}")
        return False
    # Calls the main update function which handles both registry and memory state
    return update_preferences(user_id, {"status": status})

# --- END OF FILE services/config_manager.py ---



================================================================================
ðŸ“„ services/agent_state_manager.py
================================================================================

# --- START OF FILE services/agent_state_manager.py ---

# --- START OF FILE services/agent_state_manager.py ---
# services/agent_state_manager.py
"""
Manages the in-memory state of user agents.
Provides thread-safe functions to access and modify the global agent state dictionary.
Requires initialization via initialize_state_store.
"""
from tools.logger import log_info, log_error, log_warning
from typing import Dict, List, Any, Optional, Set # Added Set
import threading
import copy
from datetime import datetime

# --- Module Level State ---
_AGENT_STATE_STORE: Optional[Dict[str, Dict[str, Any]]] = None
_state_lock = threading.Lock()

def initialize_state_store(agent_dict_ref: Dict):
    """Initializes the state manager with a reference to the global agent state dictionary."""
    global _AGENT_STATE_STORE
    if _AGENT_STATE_STORE is not None:
        log_warning("AgentStateManager", "initialize_state_store", "State store already initialized.")
        return
    if isinstance(agent_dict_ref, dict):
        _AGENT_STATE_STORE = agent_dict_ref
        log_info("AgentStateManager", "initialize_state_store", f"State store initialized with reference (ID: {id(_AGENT_STATE_STORE)}).")
    else:
        log_error("AgentStateManager", "initialize_state_store", "Invalid dictionary reference passed.")
        _AGENT_STATE_STORE = {} # Initialize to empty dict if invalid ref passed

def _is_initialized() -> bool:
    """Checks if the state store has been initialized."""
    if _AGENT_STATE_STORE is None:
        log_error("AgentStateManager", "_is_initialized", "CRITICAL: State store accessed before initialization.")
        return False
    return True

# --- Modifier Functions ---

def register_agent_instance(user_id: str, agent_state: Dict):
    """Adds or replaces the entire state dictionary for a user."""
    if not _is_initialized(): return
    if not isinstance(agent_state, dict):
         log_error("AgentStateManager", "register_agent_instance", f"Invalid agent_state type for {user_id}")
         return
    log_info("AgentStateManager", "register_agent_instance", f"Registering/updating state for user {user_id}")
    with _state_lock:
        _AGENT_STATE_STORE[user_id] = agent_state

def update_preferences_in_state(user_id: str, prefs_updates: Dict) -> bool:
    """Updates the preferences dictionary within the user's in-memory state."""
    if not _is_initialized(): return False
    updated = False
    with _state_lock:
        state = _AGENT_STATE_STORE.get(user_id)
        if state and isinstance(state.get("preferences"), dict):
            state["preferences"].update(prefs_updates)
            log_info("AgentStateManager", "update_preferences_in_state", f"Updated in-memory preferences for {user_id}: {list(prefs_updates.keys())}")
            updated = True
        else:
            log_warning("AgentStateManager", "update_preferences_in_state", f"Cannot update prefs: State or prefs dict missing/invalid for {user_id}")
    return updated

def add_task_to_context(user_id: str, task_data: Dict):
    """Appends or updates a task dictionary in the user's in-memory context list."""
    if not _is_initialized(): return
    with _state_lock:
        state = _AGENT_STATE_STORE.get(user_id)
        if state:
            # Ensure 'active_tasks_context' exists and is a list
            if not isinstance(state.get("active_tasks_context"), list):
                 state["active_tasks_context"] = []

            context = state["active_tasks_context"]
            event_id = task_data.get("event_id") # Use event_id as primary key
            found_idx = -1
            if event_id:
                for i, item in enumerate(context):
                    # Check using event_id which should be unique
                    if item.get("event_id") == event_id:
                        found_idx = i
                        break

            if found_idx != -1:
                 log_info("AgentStateManager", "add_task_to_context", f"Updating task {event_id} in context for {user_id}.")
                 context[found_idx] = task_data # Replace existing entry
            else:
                 context.append(task_data) # Add as new entry
                 log_info("AgentStateManager", "add_task_to_context", f"Added task {event_id} to context for {user_id}. New size: {len(context)}")
        else:
            log_warning("AgentStateManager", "add_task_to_context", f"State missing for {user_id}.")

def update_task_in_context(user_id: str, event_id: str, updated_task_data: Dict):
    """Finds a task by event_id in the context list and replaces it."""
    if not _is_initialized(): return
    with _state_lock:
        state = _AGENT_STATE_STORE.get(user_id)
        if state and isinstance(state.get("active_tasks_context"), list):
            context = state["active_tasks_context"]
            found = False
            for i, item in enumerate(context):
                if item.get("event_id") == event_id:
                    context[i] = updated_task_data # Replace with new data
                    found = True
                    log_info("AgentStateManager", "update_task_in_context", f"Updated task {event_id} in context for {user_id}")
                    break
            if not found:
                 log_warning("AgentStateManager", "update_task_in_context", f"Task {event_id} not found for update. Adding if active.")
                 # Add only if it seems active (optional, depends on desired behavior)
                 if updated_task_data.get("status", "pending").lower() in ["pending", "in_progress", "in progress"]:
                      context.append(updated_task_data)
        else:
             log_warning("AgentStateManager", "update_task_in_context", f"State or active_tasks_context list invalid for {user_id}")

def remove_task_from_context(user_id: str, event_id: str):
    """Removes a task by event_id from the user's in-memory context list."""
    if not _is_initialized(): return
    with _state_lock:
        state = _AGENT_STATE_STORE.get(user_id)
        if state and isinstance(state.get("active_tasks_context"), list):
            original_len = len(state["active_tasks_context"])
            # Use list comprehension for potentially better performance on large lists
            state["active_tasks_context"][:] = [
                item for item in state["active_tasks_context"] if item.get("event_id") != event_id
            ]
            if len(state["active_tasks_context"]) < original_len:
                log_info("AgentStateManager", "remove_task_from_context", f"Removed task {event_id} from context for {user_id}")
            # else: No warning needed if not found, just means it wasn't there
        else:
             log_warning("AgentStateManager", "remove_task_from_context", f"State or active_tasks_context list invalid for {user_id}")

def update_full_context(user_id: str, new_context: List[Dict]):
    """Replaces the entire active_tasks_context list (e.g., after sync)."""
    if not _is_initialized(): return
    with _state_lock:
        state = _AGENT_STATE_STORE.get(user_id)
        if state:
            state["active_tasks_context"] = new_context if isinstance(new_context, list) else []
            log_info("AgentStateManager", "update_full_context", f"Replaced context for {user_id} with {len(state['active_tasks_context'])} items.")
        else:
            log_warning("AgentStateManager", "update_full_context", f"Cannot replace context: State missing for {user_id}")

def add_message_to_user_history(user_id: str, sender: str, message: str):
    """
    Appends a detailed message to the user's conversation history list. Keeps last 50.
    """
    if not _is_initialized(): return
    with _state_lock:
        state = _AGENT_STATE_STORE.get(user_id)
        if not state:
            log_warning("AgentStateManager", "add_message_to_user_history", f"Cannot add message: State missing for {user_id}")
            return

        if not isinstance(state.get("conversation_history"), list):
            log_warning("AgentStateManager", "add_message_to_user_history", f"conversation_history invalid for {user_id}, initializing.")
            state["conversation_history"] = []

        history_list = state["conversation_history"]
        timestamp = datetime.now().isoformat()
        entry = { "sender": sender, "timestamp": timestamp, "content": message }
        history_list.append(entry)
        state["conversation_history"] = history_list[-50:] # Limit size

def update_agent_state_key(user_id: str, key: str, value: Any) -> bool:
    """
    Updates or adds/removes a specific key-value pair in the user's agent state.
    """
    if not _is_initialized(): return False
    updated = False
    with _state_lock:
        state = _AGENT_STATE_STORE.get(user_id)
        if state:
            if value is None:
                if state.pop(key, None) is not None:
                    log_info("AgentStateManager", "update_agent_state_key", f"Removed key '{key}' from state for {user_id}")
            else:
                state[key] = value
                log_info("AgentStateManager", "update_agent_state_key", f"Updated key '{key}' in state for {user_id}")
            updated = True
        else:
            log_warning("AgentStateManager", "update_agent_state_key", f"Cannot update key '{key}': State missing for {user_id}")
    return updated

    # --- Notification Tracking Functions ---
def add_notified_event_id(user_id: str, event_id: str):
    """Adds an event ID to the set of notified events for today."""
    if not _is_initialized(): return
    with _state_lock:
        state = _AGENT_STATE_STORE.get(user_id)
        if state:
            # Ensure the key exists and is a set
            if not isinstance(state.get("notified_event_ids_today"), set):
                state["notified_event_ids_today"] = set()
            state["notified_event_ids_today"].add(event_id)
            # log_info("AgentStateManager", "add_notified_event_id", f"Added {event_id} to notified set for {user_id}") # Maybe too verbose
        else:
            log_warning("AgentStateManager", "add_notified_event_id", f"State missing for {user_id}, cannot add notified event.")

def get_notified_event_ids(user_id: str) -> Set[str]:
    """Gets a copy of the set of notified event IDs for today."""
    if not _is_initialized(): return set()
    with _state_lock:
        state = _AGENT_STATE_STORE.get(user_id)
        if state and isinstance(state.get("notified_event_ids_today"), set):
            return state["notified_event_ids_today"].copy() # Return a copy
    return set() # Return empty set if user or set not found

def clear_notified_event_ids(user_id: str):
    """Clears the set of notified event IDs for the user."""
    if not _is_initialized(): return
    with _state_lock:
        state = _AGENT_STATE_STORE.get(user_id)
        if state:
            # Reset to an empty set, even if key didn't exist before
            state["notified_event_ids_today"] = set()
            log_info("AgentStateManager", "clear_notified_event_ids", f"Cleared notified events set for {user_id}")
        else:
            log_warning("AgentStateManager", "clear_notified_event_ids", f"State missing for {user_id}, cannot clear notified events.")
    # --- End Notification Tracking Functions ---

def get_agent_state(user_id: str) -> Optional[Dict]:
    """
    Safely gets a SHALLOW copy of the full state dictionary for a user.
    """
    if not _is_initialized(): return None
    with _state_lock:
        state = _AGENT_STATE_STORE.get(user_id)
        return state.copy() if state else None

def get_context(user_id: str) -> Optional[List[Dict]]:
    """Gets a deep copy of the active_tasks_context list for a user."""
    if not _is_initialized(): return None
    with _state_lock:
        state = _AGENT_STATE_STORE.get(user_id)
        if state and isinstance(state.get("active_tasks_context"), list):
            return copy.deepcopy(state["active_tasks_context"])
    return [] # Return empty list if user or context list not found/invalid
# --- END OF FILE services/agent_state_manager.py ---

# --- END OF FILE services/agent_state_manager.py ---



================================================================================
ðŸ“„ services/cheats.py
================================================================================

# --- START OF FILE services/cheats.py ---

# --- START OF REFACTORED services/cheats.py ---

"""
Service layer for handling direct 'cheat code' commands, bypassing the LLM orchestrator.
Used primarily for testing, debugging, and direct actions. Interacts with DB via services.
"""
import json
from typing import List, Optional, Dict, Any
from datetime import datetime, timedelta

# Service Imports
from services import task_query_service # For /list
from services import task_manager # For /clear (cancel_item)
from services import agent_state_manager # For /memory
from services import sync_service # For /morning, /evening
from services import routine_service # For /morning, /evening helpers
from users.user_registry import get_user_preferences

# --- Database Import (Needed for /clear) ---
try:
    import tools.activity_db as activity_db
    DB_IMPORTED = True
except ImportError:
    log_error("cheats", "import", "activity_db not found. /clear command may fail.", None)
    DB_IMPORTED = False
    class activity_db: # Dummy class
        @staticmethod
        def list_tasks_for_user(*args, **kwargs): return []
# --- End DB Import ---

# Utilities
from tools.logger import log_info, log_error, log_warning

# Define constants used by routines (mirroring routine_service)
ROUTINE_CONTEXT_HISTORY_DAYS = 1
ROUTINE_CONTEXT_FUTURE_DAYS = 14

# --- Private Handler Functions ---

def _handle_help() -> str:
    """Provides help text for available cheat commands."""
    # (No changes needed)
    return """Available Cheat Commands:
/help - Show this help message
/list [status] - List items (status: active*, pending, completed, all)
/memory - Show summary of current agent in-memory state
/clear - !! DANGER !! Mark all user's items as cancelled
/morning - Generate and show today's morning summary
/evening - Generate and show today's evening review"""


def _handle_list(user_id: str, args: List[str]) -> str:
    """Handles the /list command by calling the refactored task_query_service."""
    # (No changes needed - relies on task_query_service using the DB)
    fn_name = "_handle_list"
    status_filter = args[0].lower() if args else 'active'
    allowed_statuses = ['active', 'pending', 'in_progress', 'completed', 'all']

    if status_filter not in allowed_statuses:
        return f"Invalid status '{status_filter}'. Use one of: {', '.join(allowed_statuses)}"

    try:
        # Get timezone for display context
        prefs = get_user_preferences(user_id) or {}
        user_tz_str = prefs.get("TimeZone", "UTC")

        # Call the query service function (which now uses the DB)
        list_body, mapping = task_query_service.get_formatted_list(
            user_id=user_id,
            status_filter=status_filter,
            # No project/date filter for basic /list cheat
        )
        if list_body:
            list_intro = f"Items with status '{status_filter}' (Times relative to {user_tz_str}):\n---\n"
            return list_intro + list_body
        else:
            return f"No items found with status '{status_filter}'."
    except Exception as e:
        log_error("cheats", fn_name, f"Error calling get_formatted_list: {e}", e, user_id=user_id)
        return "Error retrieving list."


def _handle_memory(user_id: str) -> str:
    """Handles the /memory command."""
    # (No changes needed)
    fn_name = "_handle_memory"
    try:
        agent_state = agent_state_manager.get_agent_state(user_id)
        if agent_state:
            # Create a serializable summary (avoiding non-JSON types like sets)
            state_summary = {
                "user_id": agent_state.get("user_id"),
                "preferences_keys": list(agent_state.get("preferences", {}).keys()),
                "history_count": len(agent_state.get("conversation_history", [])),
                "context_item_count": len(agent_state.get("active_tasks_context", [])),
                "calendar_object_present": agent_state.get("calendar") is not None,
                "notified_ids_today_count": len(agent_state.get("notified_event_ids_today", set()))
            }
            return f"Agent Memory Summary:\n```json\n{json.dumps(state_summary, indent=2)}\n```"
        else:
            return "Error: Agent state not found in memory."
    except Exception as e:
        log_error("cheats", fn_name, f"Error retrieving agent state: {e}", e, user_id=user_id)
        return "Error retrieving agent memory state."


def _handle_clear(user_id: str) -> str:
    """Handles the /clear command. Finds non-cancelled items in DB and attempts cancellation."""
    # --- REFACTORED ---
    fn_name = "_handle_clear"
    log_warning("cheats", fn_name, f"!! Initiating /clear command for user {user_id} !!")
    cancelled_count = 0
    failed_count = 0
    errors = []

    if not DB_IMPORTED:
        log_error("cheats", fn_name, "Database module not available, cannot perform clear.", user_id=user_id)
        return "Error: Cannot access task database to perform clear."

    try:
        # Fetch only tasks/reminders that are NOT already cancelled from DB
        statuses_to_clear = ["pending", "in_progress", "completed"] # Include completed to clear them too
        items_to_clear_dicts = activity_db.list_tasks_for_user(user_id=user_id, status_filter=statuses_to_clear)

        if not items_to_clear_dicts:
            return "No items found in a clearable state (pending, in_progress, completed)."

        item_ids_to_clear = [item.get("event_id") for item in items_to_clear_dicts if item.get("event_id")]

        log_info("cheats", fn_name, f"Found {len(item_ids_to_clear)} items in DB to attempt cancellation for user {user_id}.")

        for item_id in item_ids_to_clear:
            try:
                # Call task_manager.cancel_item (which now updates DB and handles GCal)
                success = task_manager.cancel_item(user_id, item_id)
                if success:
                    cancelled_count += 1
                else:
                    failed_count += 1
                    errors.append(f"Failed cancel: {item_id[:8]}...")
                    log_warning("cheats", fn_name, f"task_manager.cancel_item failed for {item_id}", user_id=user_id)
            except Exception as cancel_e:
                failed_count += 1
                errors.append(f"Error cancel: {item_id[:8]}... ({type(cancel_e).__name__})")
                log_error("cheats", fn_name, f"Exception during cancel_item for {item_id}", cancel_e, user_id=user_id)

        response = f"Clear operation finished.\nSuccessfully cancelled: {cancelled_count}\nFailed/Skipped: {failed_count}"
        if errors:
            response += "\nFailures:\n" + "\n".join(errors[:5]) # Show first 5 errors
            if len(errors) > 5: response += "\n..."

        return response

    except Exception as e:
        log_error("cheats", fn_name, f"Critical error during /clear setup or execution for {user_id}", e, user_id=user_id)
        return "A critical error occurred during the clear operation."
    # --- END REFACTORED ---

def _handle_morning(user_id: str) -> str:
    """Handles the /morning command by generating the summary."""
    # (No changes needed - relies on sync_service using the DB)
    fn_name = "_handle_morning"
    log_info("cheats", fn_name, f"Executing /morning cheat for {user_id}")
    try:
        prefs = get_user_preferences(user_id)
        if not prefs: return "Error: Could not retrieve user preferences."

        user_tz_str = prefs.get("TimeZone", "UTC")
        now_local = routine_service.get_local_time(user_tz_str)
        context_start_date = (now_local - timedelta(days=ROUTINE_CONTEXT_HISTORY_DAYS)).strftime("%Y-%m-%d")
        context_end_date = (now_local + timedelta(days=ROUTINE_CONTEXT_FUTURE_DAYS)).strftime("%Y-%m-%d")

        log_info("cheats", fn_name, f"Getting synced context for morning summary (User: {user_id})...")
        # sync_service now uses the DB
        aggregated_context = sync_service.get_synced_context_snapshot(user_id, context_start_date, context_end_date)
        summary_msg = routine_service.generate_morning_summary(user_id, aggregated_context)

        return summary_msg if summary_msg else "Could not generate morning summary."

    except Exception as e:
        log_error("cheats", fn_name, f"Error generating morning summary via cheat code for {user_id}", e, user_id=user_id)
        return "An error occurred while generating the morning summary."


def _handle_evening(user_id: str) -> str:
    """Handles the /evening command by generating the review."""
    # (No changes needed - relies on sync_service using the DB)
    fn_name = "_handle_evening"
    log_info("cheats", fn_name, f"Executing /evening cheat for {user_id}")
    try:
        prefs = get_user_preferences(user_id)
        if not prefs: return "Error: Could not retrieve user preferences."

        user_tz_str = prefs.get("TimeZone", "UTC")
        now_local = routine_service.get_local_time(user_tz_str)
        context_start_date = (now_local - timedelta(days=ROUTINE_CONTEXT_HISTORY_DAYS)).strftime("%Y-%m-%d")
        context_end_date = (now_local + timedelta(days=ROUTINE_CONTEXT_FUTURE_DAYS)).strftime("%Y-%m-%d")

        log_info("cheats", fn_name, f"Getting synced context for evening review (User: {user_id})...")
        # sync_service now uses the DB
        aggregated_context = sync_service.get_synced_context_snapshot(user_id, context_start_date, context_end_date)
        review_msg = routine_service.generate_evening_review(user_id, aggregated_context)

        return review_msg if review_msg else "Could not generate evening review."

    except Exception as e:
        log_error("cheats", fn_name, f"Error generating evening review via cheat code for {user_id}", e, user_id=user_id)
        return "An error occurred while generating the evening review."


# --- Main Dispatcher ---

def handle_cheat_command(user_id: str, command: str, args: List[str]) -> str:
    """
    Dispatches cheat commands to the appropriate handler.
    """
    # (No changes needed in dispatcher logic)
    command = command.lower()

    if command == "/help": return _handle_help()
    elif command == "/list": return _handle_list(user_id, args)
    elif command == "/memory": return _handle_memory(user_id)
    elif command == "/clear": return _handle_clear(user_id)
    elif command == "/morning": return _handle_morning(user_id)
    elif command == "/evening": return _handle_evening(user_id)
    else: return f"Unknown command: '{command}'. Try /help."

# --- END OF REFACTORED services/cheats.py ---

# --- END OF FILE services/cheats.py ---



================================================================================
ðŸ“„ services/llm_interface.py
================================================================================

# --- START OF FILE services/llm_interface.py ---

# llm_interface.py
import os
import openai
import instructor
import threading
from tools.logger import log_info, log_error

OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
_client = None
_client_lock = threading.Lock()

def get_instructor_client():
    """Initializes and returns a singleton, instructor-patched OpenAI client."""
    global _client
    if not OPENAI_API_KEY:
        log_error("llm_interface", "get_instructor_client", "OPENAI_API_KEY not found in environment.")
        return None

    with _client_lock:
        if _client is None:
            try:
                log_info("llm_interface", "get_instructor_client", "Initializing instructor-patched OpenAI client...")
                # Initialize the OpenAI client
                base_client = openai.OpenAI(api_key=OPENAI_API_KEY)
                # Patch it with Instructor
                _client = instructor.patch(base_client)
                log_info("llm_interface", "get_instructor_client", "Instructor-patched OpenAI client initialized.")
            except Exception as e:
                log_error("llm_interface", "get_instructor_client", f"Failed to initialize OpenAI client: {e}", e)
                _client = None # Ensure it remains None on failure
    return _client


# --- END OF FILE services/llm_interface.py ---



================================================================================
ðŸ“„ services/scheduler_service.py
================================================================================

# --- START OF FILE services/scheduler_service.py ---

# --- START OF services/scheduler_service.py ---

from typing import Dict, List, Tuple # Added List, Tuple
from apscheduler.schedulers.background import BackgroundScheduler
from apscheduler.executors.pool import ThreadPoolExecutor
from apscheduler.events import EVENT_JOB_ERROR, EVENT_JOB_EXECUTED
import pytz
from tools.logger import log_info, log_error, log_warning
# --- ADD THIS IMPORT ---
from bridge.request_router import send_message
# ----------------------

# ... (Global scheduler instance, constants, _job_listener) ...
scheduler = None
DEFAULT_TIMEZONE = 'UTC'
NOTIFICATION_CHECK_INTERVAL_MINUTES = 5
ROUTINE_CHECK_INTERVAL_MINUTES = 15
DAILY_CLEANUP_HOUR_UTC = 0
DAILY_CLEANUP_MINUTE_UTC = 5

def _job_listener(event):
    # ... (function remains the same) ...
    fn_name = "_job_listener"
    job = scheduler.get_job(event.job_id) if scheduler else None
    job_name = job.name if job else event.job_id
    if event.exception:
        log_error("scheduler_service", fn_name, f"Job '{job_name}' crashed:", event.exception)
        log_error("scheduler_service", fn_name, f"Traceback: {event.traceback}")
    pass

# --- NEW FUNCTION TO WRAP ROUTINE CHECK AND SENDING ---
def _run_routine_check_and_send():
    """Wrapper function called by scheduler to run checks and send messages."""
    fn_name = "_run_routine_check_and_send"
    log_info("scheduler_service", fn_name, "Scheduler executing routine check job...")
    try:
        # Import the check function here if not already imported globally
        from services.routine_service import check_routine_triggers
        messages_to_send = check_routine_triggers() # This now returns a list

        if messages_to_send:
            log_info("scheduler_service", fn_name, f"Routine check generated {len(messages_to_send)} messages to send.")
            for user_id, message_content in messages_to_send:
                try:
                    send_message(user_id, message_content)
                except Exception as send_err:
                    log_error("scheduler_service", fn_name, f"Error sending routine message to user {user_id}", send_err)
        else:
            log_info("scheduler_service", fn_name, "Routine check completed, no messages to send.")

    except Exception as job_err:
        # Log errors occurring within the job execution itself
        log_error("scheduler_service", fn_name, "Error during scheduled routine check execution", job_err)
# --- END OF NEW WRAPPER FUNCTION ---


def start_scheduler() -> bool:
    global scheduler
    fn_name = "start_scheduler"

    if scheduler and scheduler.running:
        log_warning("scheduler_service", fn_name, "Scheduler is already running.")
        return True

    try:
        log_info("scheduler_service", fn_name, "Initializing APScheduler...")
        executors = {'default': ThreadPoolExecutor(10)}
        job_defaults = {'coalesce': True, 'max_instances': 1}
        scheduler = BackgroundScheduler(
            executors=executors, job_defaults=job_defaults, timezone=pytz.timezone(DEFAULT_TIMEZONE)
        )

        # --- Import Job Functions ---
        check_event_notifications = None
        # We don't import check_routine_triggers here anymore, it's called in the wrapper
        daily_cleanup = None
        try:
            from services.notification_service import check_event_notifications
        except ImportError as e:
            log_error("scheduler_service", fn_name, f"Failed to import 'check_event_notifications': {e}. Notification job NOT scheduled.")
        try:
            # Keep import for daily_cleanup
            from services.routine_service import daily_cleanup
        except ImportError as e:
             log_error("scheduler_service", fn_name, f"Failed to import 'daily_cleanup': {e}. Cleanup job NOT scheduled.")
        # ----------------------------

        # --- Schedule Jobs ---
        jobs_scheduled_count = 0
        if check_event_notifications:
            scheduler.add_job( check_event_notifications, trigger='interval', minutes=NOTIFICATION_CHECK_INTERVAL_MINUTES, id='event_notification_check', name='Check Event Notifications')
            log_info("scheduler_service", fn_name, f"Scheduled 'check_event_notifications' job every {NOTIFICATION_CHECK_INTERVAL_MINUTES} minutes.")
            jobs_scheduled_count += 1
        else:
             log_warning("scheduler_service", fn_name, "'check_event_notifications' job not scheduled.")

        # --- MODIFIED: Schedule the WRAPPER function ---
        scheduler.add_job(
            _run_routine_check_and_send, # Call the wrapper
            trigger='interval',
            minutes=ROUTINE_CHECK_INTERVAL_MINUTES,
            id='routine_trigger_check',
            name='Check Routine Triggers & Send' # Updated name slightly
        )
        log_info("scheduler_service", fn_name, f"Scheduled 'Routine Trigger Check & Send' job every {ROUTINE_CHECK_INTERVAL_MINUTES} minutes.")
        jobs_scheduled_count += 1 # Assuming this job is always added
        # -------------------------------------------------

        if daily_cleanup:
            scheduler.add_job( daily_cleanup, trigger='cron', hour=DAILY_CLEANUP_HOUR_UTC, minute=DAILY_CLEANUP_MINUTE_UTC, timezone=DEFAULT_TIMEZONE, id='daily_cleanup_job', name='Daily Cleanup')
            log_info("scheduler_service", fn_name, f"Scheduled 'daily_cleanup' job daily at {DAILY_CLEANUP_HOUR_UTC:02d}:{DAILY_CLEANUP_MINUTE_UTC:02d} {DEFAULT_TIMEZONE}.")
            jobs_scheduled_count += 1
        else:
            log_warning("scheduler_service", fn_name, "'daily_cleanup' job not scheduled.")

        # ... (rest of the start_scheduler function including listener, start, return True/False) ...
        scheduler.add_listener(_job_listener, EVENT_JOB_ERROR | EVENT_JOB_EXECUTED)
        scheduler.start()
        log_info("scheduler_service", fn_name, "APScheduler started successfully.")
        return True

    except Exception as e:
        log_error("scheduler_service", fn_name, f"Failed to initialize or start APScheduler: {e}", e)
        scheduler = None
        return False

# --- (shutdown_scheduler function remains the same) ---
def shutdown_scheduler():
    # ... (shutdown logic) ...
    global scheduler
    fn_name = "shutdown_scheduler"
    if scheduler and scheduler.running:
        try:
            log_info("scheduler_service", fn_name, "Attempting to shut down scheduler...")
            scheduler.shutdown(wait=False)
            log_info("scheduler_service", fn_name, "Scheduler shut down complete.")
            scheduler = None
        except Exception as e:
            log_error("scheduler_service", fn_name, f"Error during scheduler shutdown: {e}", e)
    elif scheduler:
        log_info("scheduler_service", fn_name, "Scheduler found but was not running.")
        scheduler = None
    else:
        log_info("scheduler_service", fn_name, "No active scheduler instance to shut down.")

# --- END OF services/scheduler_service.py ---

# --- END OF FILE services/scheduler_service.py ---



================================================================================
ðŸ“„ services/sync_service.py
================================================================================

# --- START OF FILE services/sync_service.py ---

# --- START OF REFACTORED services/sync_service.py ---
"""
Provides functionality to get a combined view of WhatsTasker-managed items (DB)
and external Google Calendar events (API) for a specific user and time period.
Does NOT modify the persistent DB for external events found only in GCal.
Updates DB records for WT items if GCal data has changed for that item.
"""
import traceback
from datetime import datetime, timedelta, timezone
from typing import List, Dict, Any

# Central logger
from tools.logger import log_info, log_error, log_warning

# Database access module
try:
    import tools.activity_db as activity_db
    DB_IMPORTED = True
except ImportError:
    log_error("sync_service", "import", "activity_db not found. Sync service disabled.", None)
    DB_IMPORTED = False
    # Dummy DB functions
    class activity_db:
        @staticmethod
        def list_tasks_for_user(*args, **kwargs): return []
        @staticmethod
        def add_or_update_task(*args, **kwargs): return False
# --- End DB Import ---

# User Manager (to get agent state for Calendar API)
try:
    from users.user_manager import get_agent
    USER_MANAGER_IMPORTED = True
except ImportError:
    log_error("sync_service", "import", "Failed to import user_manager.get_agent")
    USER_MANAGER_IMPORTED = False
    def get_agent(*args, **kwargs): return None

# Agent State Manager (to update in-memory context after DB update)
try:
    from services.agent_state_manager import update_task_in_context
    AGENT_STATE_IMPORTED = True
except ImportError:
    log_error("sync_service", "import", "Failed to import agent_state_manager functions.")
    AGENT_STATE_IMPORTED = False
    def update_task_in_context(*args, **kwargs): pass # Dummy

# Google Calendar API (for type checking and fetching events)
try:
    from tools.google_calendar_api import GoogleCalendarAPI
    GCAL_API_IMPORTED = True
except ImportError:
    log_warning("sync_service", "import", "GoogleCalendarAPI not found. Sync will only show DB tasks.")
    GoogleCalendarAPI = None
    GCAL_API_IMPORTED = False


def get_synced_context_snapshot(user_id: str, start_date_str: str, end_date_str: str) -> List[Dict]:
    """
    Fetches WT tasks (DB) and GCal events (API) for a period, merges them,
    identifies external events, and returns a combined list of dictionaries.
    Updates the DB record for a WT item if its corresponding GCal event changed.
    Does not persist external events found only in GCal into the tasks table.
    """
    fn_name = "get_synced_context_snapshot"
    log_info("sync_service", fn_name, f"Generating synced context for user {user_id}, range: {start_date_str} to {end_date_str}")

    if not DB_IMPORTED:
        log_error("sync_service", fn_name, "Database module not available.", user_id=user_id)
        return []

    # 1. Get Calendar API instance
    calendar_api = None
    if USER_MANAGER_IMPORTED and GCAL_API_IMPORTED and GoogleCalendarAPI is not None:
        agent_state = get_agent(user_id)
        if agent_state:
            calendar_api_maybe = agent_state.get("calendar")
            if isinstance(calendar_api_maybe, GoogleCalendarAPI) and calendar_api_maybe.is_active():
                calendar_api = calendar_api_maybe

    # 2. Fetch GCal Events if API is available
    gcal_events_list = []
    if calendar_api:
        try:
            log_info("sync_service", fn_name, f"Fetching GCal events for {user_id}...")
            # list_events returns parsed dicts including 'event_id', 'gcal_start_datetime' etc.
            gcal_events_list = calendar_api.list_events(start_date_str, end_date_str)
            log_info("sync_service", fn_name, f"Fetched {len(gcal_events_list)} GCal events for {user_id}.")
        except Exception as e:
            log_error("sync_service", fn_name, f"Error fetching GCal events for {user_id}", e, user_id=user_id)
            # Continue without GCal events
    else:
        log_info("sync_service", fn_name, f"GCal API not available or inactive for {user_id}, skipping GCal fetch.")

    # 3. Fetch WT Tasks from Database within the same date range
    wt_tasks_list = []
    try:
        log_info("sync_service", fn_name, f"Fetching WT tasks from DB for {user_id}...")
        # Fetch tasks based on the 'date' column matching the range
        # We don't filter by status here; we want all potentially relevant WT items
        wt_tasks_list = activity_db.list_tasks_for_user(
            user_id=user_id,
            date_range=(start_date_str, end_date_str)
            # status_filter=None # Get all statuses within the date range
        )
        log_info("sync_service", fn_name, f"Fetched {len(wt_tasks_list)} WT tasks from DB for {user_id} in range.")
    except Exception as e:
        log_error("sync_service", fn_name, f"Error fetching WT tasks from DB for {user_id}", e, user_id=user_id)
        # If DB fails, should we proceed with only GCal events? Or return empty?
        # Let's return only GCal events if DB fails, but log error clearly.
        # Fall through, wt_tasks_list will be empty.

    # 4. Create Maps for Efficient Lookup
    gcal_events_map = {e['event_id']: e for e in gcal_events_list if e.get('event_id')}
    wt_tasks_map = {t['event_id']: t for t in wt_tasks_list if t.get('event_id')}

    # 5. Merge & Identify Types
    aggregated_context_list: List[Dict[str, Any]] = []
    processed_wt_ids = set() # Keep track of WT items found in GCal map

    # Iterate through GCal events first
    for event_id, gcal_data in gcal_events_map.items():
        if event_id in wt_tasks_map:
            # --- WT Item Found in GCal ---
            processed_wt_ids.add(event_id)
            task_data = wt_tasks_map[event_id] # The task data from our DB
            merged_data = task_data.copy() # Start with DB data
            needs_db_update = False

            # Check for differences that require updating our DB record
            gcal_start = gcal_data.get("gcal_start_datetime")
            gcal_end = gcal_data.get("gcal_end_datetime")
            gcal_title = gcal_data.get("title")
            gcal_desc = gcal_data.get("description")
            # Add GCal status if needed: gcal_status = gcal_data.get("status_gcal")

            # Update stored GCal times if they differ
            if gcal_start != merged_data.get("gcal_start_datetime"):
                merged_data["gcal_start_datetime"] = gcal_start
                needs_db_update = True
            if gcal_end != merged_data.get("gcal_end_datetime"):
                merged_data["gcal_end_datetime"] = gcal_end
                needs_db_update = True

            # Option 1: Always update title/desc from GCal if GCal link exists?
            # Option 2: Only update if DB fields are empty/default? (Safer)
            # Let's go with Option 2 for now to avoid overwriting user edits in WT potentially.
            if gcal_title and not merged_data.get("title", "").strip():
                 merged_data["title"] = gcal_title
                 needs_db_update = True
            if gcal_desc and not merged_data.get("description", "").strip():
                 merged_data["description"] = gcal_desc
                 needs_db_update = True
            # Potentially sync status? If GCal event is 'cancelled', should WT task be? Complex rule. Skip for now.

            # If the merged data differs from original DB data, update DB
            if needs_db_update:
                log_info("sync_service", fn_name, f"GCal data changed for WT item {event_id}. Updating DB.")
                try:
                    # add_or_update_task expects list for session IDs
                    if isinstance(merged_data.get("session_event_ids"), str): # Ensure it's list before saving
                        try: merged_data["session_event_ids"] = json.loads(merged_data["session_event_ids"])
                        except: merged_data["session_event_ids"] = []

                    update_success = activity_db.add_or_update_task(merged_data)
                    if update_success and AGENT_STATE_IMPORTED:
                        # Update in-memory context as well
                        updated_data_from_db = activity_db.get_task(event_id) # Re-fetch to get latest state
                        if updated_data_from_db: update_task_in_context(user_id, event_id, updated_data_from_db)
                    elif not update_success:
                         log_error("sync_service", fn_name, f"Failed DB update for WT item {event_id} after GCal merge.", user_id=user_id)

                except Exception as save_err:
                     log_error("sync_service", fn_name, f"Unexpected error saving updated metadata for WT item {event_id} after GCal merge.", save_err, user_id=user_id)

            # Add the (potentially updated) merged data to the context list
            aggregated_context_list.append(merged_data)

        else:
            # --- External GCal Event (Not in our DB) ---
            external_event_data = gcal_data.copy() # Start with GCal data
            external_event_data["type"] = "external_event" # Mark its type
            external_event_data["user_id"] = user_id # Ensure user_id is present
            # Ensure required fields for formatting have some value?
            external_event_data.setdefault("status", None) # External events don't have WT status
            aggregated_context_list.append(external_event_data)

    # 6. Add WT Tasks Not Found in GCal Fetch Window
    for event_id, task_data in wt_tasks_map.items():
        if event_id not in processed_wt_ids:
            # This is a WT item (task/reminder) that wasn't in the GCal list for this window.
            # Could be a local-only task, or GCal event outside window, or deleted from GCal.
            # We still want it in the context if it's relevant (e.g., active status).
            log_info("sync_service", fn_name, f"Including WT item {event_id} (status: {task_data.get('status')}) which was not found in GCal fetch window.")
            # Make sure session IDs are list (should be from DB layer)
            if isinstance(task_data.get("session_event_ids"), str):
                 try: task_data["session_event_ids"] = json.loads(task_data["session_event_ids"])
                 except: task_data["session_event_ids"] = []
            aggregated_context_list.append(task_data) # Add the DB data as is

    log_info("sync_service", fn_name, f"Generated aggregated context with {len(aggregated_context_list)} items for {user_id}.")
    # Sort the final aggregated list before returning? Good for routines.
    sorted_aggregated_context = _sort_tasks(aggregated_context_list) # Use the existing sort helper
    return sorted_aggregated_context

# --- Placeholder for future full sync ---
def perform_full_sync(user_id: str):
    """(NOT IMPLEMENTED) Placeholder for a more complex two-way sync."""
    log_warning("sync_service", "perform_full_sync", f"Full two-way sync not implemented. User: {user_id}")
    # This would involve:
    # 1. Fetching *all* relevant GCal events (wider date range? or use sync tokens?)
    # 2. Fetching *all* non-cancelled WT tasks from DB.
    # 3. Complex diffing logic to identify:
    #    - New GCal events -> Create corresponding 'external_event' metadata in DB? (Optional)
    #    - New WT tasks -> Create in GCal? (Maybe only if scheduled?)
    #    - Updated GCal events -> Update corresponding WT task metadata in DB.
    #    - Updated WT tasks -> Update corresponding GCal event? (Be careful!)
    #    - Deleted GCal events -> Update status or delete WT task metadata?
    #    - Deleted WT tasks (marked cancelled) -> Delete GCal event?
    # 4. Handling conflicts gracefully.
    # 5. Updating last_sync timestamp in user preferences.
    pass

# Helper from task_query_service might be needed here if not importing that module
def _sort_tasks(task_list: List[Dict]) -> List[Dict]:
    """Sorts task list robustly by date/time."""
    fn_name = "_sort_tasks_sync" # Different name to avoid potential conflicts if imported elsewhere
    def sort_key(item):
        gcal_start = item.get("gcal_start_datetime")
        if gcal_start and isinstance(gcal_start, str):
             try:
                 if 'T' in gcal_start: dt_aware = datetime.fromisoformat(gcal_start.replace('Z', '+00:00')); return dt_aware.replace(tzinfo=None)
                 elif len(gcal_start) == 10: dt_date = datetime.strptime(gcal_start, '%Y-%m-%d').date(); return datetime.combine(dt_date, datetime.min.time())
             except ValueError: pass
        meta_date_str = item.get("date"); meta_time_str = item.get("time")
        sort_dt = datetime.max
        if meta_date_str:
            try:
                if meta_time_str:
                    time_part = meta_time_str + ':00' if len(meta_time_str.split(':')) == 2 else meta_time_str
                    sort_dt = datetime.strptime(f"{meta_date_str} {time_part}", "%Y-%m-%d %H:%M:%S")
                else: sort_dt = datetime.strptime(meta_date_str, "%Y-%m-%d")
            except (ValueError, TypeError): pass
        return sort_dt
    try:
        return sorted(task_list, key=lambda item: (sort_key(item), item.get("created_at", ""), item.get("title", "").lower()))
    except Exception as e:
        log_error("sync_service", fn_name, f"Error during task sorting: {e}", e)
        return task_list


# --- END OF REFACTORED services/sync_service.py ---

# --- END OF FILE services/sync_service.py ---



================================================================================
ðŸ“„ services/notification_service.py
================================================================================

# --- START OF FILE services/notification_service.py ---

# --- START OF FULL services/notification_service.py ---
"""
Handles checking for upcoming events and sending notifications.
Uses an in-memory set within agent_state to track sent notifications for the day.
"""
import traceback
from datetime import datetime, timedelta, timezone
import pytz

from tools.logger import log_info, log_error, log_warning
from users.user_registry import get_registry, get_user_preferences
from services.sync_service import get_synced_context_snapshot
from services.agent_state_manager import get_notified_event_ids, add_notified_event_id
from bridge.request_router import send_message # Direct import for sending
from services.task_manager import _parse_duration_to_minutes # For parsing lead time

try:
    from tools.activity_db import update_task_fields
    DB_IMPORTED = True
except ImportError:
    DB_IMPORTED = False
    def update_task_fields(*args, **kwargs): return False # Dummy
    log_error("notification_service", "import", "Failed to import activity_db.update_task_fields")
# --- End Imports ---

NOTIFICATION_CHECK_INTERVAL_MINUTES = 5
DEFAULT_NOTIFICATION_LEAD_TIME_MINUTES = 15

def generate_event_notification_message(event_data, user_timezone_str="UTC"):
    """
    Formats a simple notification message for an event, converting the start time
    to the user's local timezone.
    """
    fn_name = "generate_event_notification_message"
    title = event_data.get('title', '(No Title)')
    start_time_str = event_data.get('gcal_start_datetime')
    if not start_time_str:
        log_warning("notification_service", fn_name, f"Cannot generate notification for event '{title}' - missing gcal_start_datetime.")
        return None
    try:
        user_tz = pytz.utc
        try:
            if user_timezone_str: user_tz = pytz.timezone(user_timezone_str)
        except pytz.UnknownTimeZoneError:
            log_warning("notification_service", fn_name, f"Unknown timezone '{user_timezone_str}'. Using UTC for notification message for event '{title}'.")
            user_timezone_str = "UTC"
        dt_aware = datetime.fromisoformat(start_time_str.replace('Z', '+00:00'))
        dt_local = dt_aware.astimezone(user_tz)
        time_str = dt_local.strftime('%H:%M %Z')
        return f"ðŸ”” Reminder: '{title}' is starting soon at {time_str}."
    except (ValueError, TypeError) as parse_err:
        log_error("notification_service", fn_name, f"Error parsing/converting start time '{start_time_str}' for event '{title}'. Error: {parse_err}", parse_err)
        return None
    except Exception as e:
        log_error("notification_service", fn_name, f"General error formatting notification for event '{title}': {e}", e)
        return None

def check_event_notifications():
    """
    Scheduled job function. Checks all users for upcoming events needing notification.
    Updates 'internal_reminder_sent' in the database for notified WT items.
    """
    fn_name = "check_event_notifications"
    log_info("notification_service", fn_name, "Running scheduled check for event notifications...")
    now_utc = datetime.now(timezone.utc)

    # --- Outermost Try ---
    try:
        registry = get_registry()
        if not registry:
            log_warning("notification_service", fn_name, "User registry is empty. Skipping check.")
            return

        user_ids = list(registry.keys())
        log_info("notification_service", fn_name, f"Checking notifications for {len(user_ids)} users.")

        for user_id in user_ids:
            prefs = None
            # --- Inner Try (User Level) ---
            try:
                prefs = get_user_preferences(user_id)
                if not prefs or prefs.get("status") != "active" or not prefs.get("Calendar_Enabled"):
                    continue

                lead_time_str = prefs.get("Notification_Lead_Time", "15m")
                lead_time_minutes = _parse_duration_to_minutes(lead_time_str)
                if lead_time_minutes is None:
                    log_warning("notification_service", fn_name, f"Invalid Notification_Lead_Time '{lead_time_str}' for user {user_id}. Using default.", user_id=user_id)
                    lead_time_minutes = DEFAULT_NOTIFICATION_LEAD_TIME_MINUTES

                notification_window_end_utc = now_utc + timedelta(minutes=lead_time_minutes)
                today_date_str = now_utc.strftime("%Y-%m-%d")
                aggregated_context = get_synced_context_snapshot(user_id, today_date_str, today_date_str)
                if not aggregated_context: continue

                notified_today_set = get_notified_event_ids(user_id)

                for item in aggregated_context:
                    event_id = item.get("event_id")
                    start_time_iso = item.get("gcal_start_datetime")

                    if not event_id or not start_time_iso or 'T' not in start_time_iso: continue
                    if event_id in notified_today_set: continue

                    # --- Innermost Try (Item Level) ---
                    try:
                        start_dt_aware = datetime.fromisoformat(start_time_iso.replace('Z', '+00:00'))

                        # Corrected check logic
                        if start_dt_aware > now_utc and start_dt_aware <= notification_window_end_utc:
                            log_info("notification_service", fn_name, f"Triggering notification for user {user_id}, event: {event_id} ('{item.get('title')}')")

                            notification_message = generate_event_notification_message(item, prefs.get("TimeZone", "UTC"))

                            if notification_message:
                                send_message(user_id, notification_message)
                                add_notified_event_id(user_id, event_id)
                                log_info("notification_service", fn_name, f"Sent notification and marked as notified (memory) for event {event_id}, user {user_id}")

                                if item.get("type") in ["task", "reminder"] and DB_IMPORTED:
                                    sent_time_utc_str = datetime.now(timezone.utc).isoformat(timespec='seconds')+'Z'
                                    update_payload = {"internal_reminder_sent": sent_time_utc_str}
                                    db_update_success = update_task_fields(event_id, update_payload)
                                    if db_update_success:
                                        log_info("notification_service", fn_name, f"Updated internal_reminder_sent in DB for WT item {event_id}")
                                    else:
                                        log_warning("notification_service", fn_name, f"Failed to update internal_reminder_sent in DB for WT item {event_id}", user_id=user_id)
                            else:
                                log_warning("notification_service", fn_name, f"Failed to generate notification message for event {event_id}.", user_id=user_id)

                    except ValueError:
                        log_warning("notification_service", fn_name, f"Could not parse start time '{start_time_iso}' for event {event_id}. Skipping.", user_id=user_id)
                    except Exception as item_err:
                        log_error("notification_service", fn_name, f"Error processing item {event_id} for user {user_id}", item_err, user_id=user_id)
                    # --- End Innermost Try ---

            except Exception as user_err: # <-- Correctly indented for Inner Try
                user_context = user_id if prefs else f"Unknown User (Error before prefs load)"
                log_error("notification_service", fn_name, f"Error processing notifications for user {user_id}", user_err, user_id=user_context)
            # --- End Inner Try ---

    # --- <<< CORRECTED INDENTATION FOR FINAL EXCEPT >>> ---
    except Exception as main_err: # <-- Correctly indented for Outermost Try
        log_error("notification_service", fn_name, f"General error during notification check run", main_err)
    # --- <<< END CORRECTION >>> ---

    # --- Correctly indented final log ---
    log_info("notification_service", fn_name, "Finished scheduled check for event notifications.")

# --- END OF FILE services/notification_service.py ---

# --- END OF FILE services/notification_service.py ---



================================================================================
ðŸ“„ services/routine_service.py
================================================================================

# --- START OF FILE services/routine_service.py ---

# --- START OF FILE services/routine_service.py ---
"""
Handles scheduled generation of Morning and Evening summaries/reviews.
Includes timezone handling and daily cleanup tasks.
"""

import traceback
from datetime import datetime, timedelta, timezone
import pytz # For timezone handling

from tools.logger import log_info, log_error, log_warning
from users.user_registry import get_registry, get_user_preferences
from services import sync_service # Import the whole module
from services.config_manager import update_preferences # To update last trigger date
from services.agent_state_manager import clear_notified_event_ids, get_agent_state # For daily cleanup
from services.task_query_service import _format_task_line, _sort_tasks # Use helper for formatting

# Define time window for fetching context for routines (e.g., yesterday to 14 days ahead)
ROUTINE_CONTEXT_HISTORY_DAYS = 1
ROUTINE_CONTEXT_FUTURE_DAYS = 14

def get_local_time(user_timezone_str):
    """Gets the current time in the user's specified timezone."""
    fn_name = "get_local_time"
    if not user_timezone_str: user_timezone_str = 'UTC' # Default if None/empty
    try:
        user_tz = pytz.timezone(user_timezone_str)
        return datetime.now(user_tz)
    except pytz.UnknownTimeZoneError:
        log_warning("routine_service", fn_name, f"Unknown timezone '{user_timezone_str}'. Using UTC.")
        return datetime.now(pytz.utc)
    except Exception as e:
        log_error("routine_service", fn_name, f"Error getting local time for tz '{user_timezone_str}'", e)
        return datetime.now(pytz.utc) # Default to UTC on error


def generate_morning_summary(user_id, context):
    """Generates the morning summary message including GCal events and WT tasks."""
    fn_name = "generate_morning_summary"
    prefs = get_user_preferences(user_id)
    user_tz_str = prefs.get("TimeZone", "UTC") if prefs else "UTC"
    user_tz = pytz.timezone(user_tz_str) # Get timezone object
    now_local = get_local_time(user_tz_str)
    today_local_str = now_local.strftime("%Y-%m-%d")

    log_info("routine_service", fn_name, f"Generating morning summary for user {user_id} for date {today_local_str}")

    items_today = []
    for item in context:
        item_date_local_str = None
        start_dt_str = item.get("gcal_start_datetime")
        is_all_day = item.get("is_all_day", False)

        if start_dt_str: # Prioritize GCal time
             try:
                 # Parse ISO string (can be date or datetime, potentially with Z or offset)
                 if 'T' in start_dt_str: # It's likely a datetime
                      dt_aware = datetime.fromisoformat(start_dt_str.replace('Z', '+00:00'))
                      item_date_local_str = dt_aware.astimezone(user_tz).strftime("%Y-%m-%d")
                 elif len(start_dt_str) == 10: # It's likely just a date (all-day event)
                      item_date_local_str = start_dt_str
             except ValueError:
                 log_warning("routine_service", fn_name, f"Could not parse gcal_start_datetime '{start_dt_str}' for item {item.get('event_id')}")
                 pass # Ignore parse errors for this item's date check
        elif item.get("date"): # Fallback to WT date field
             item_date_local_str = item.get("date")

        # Check if the derived local date matches today
        if item_date_local_str == today_local_str:
             # Include tasks, reminders, and external events if they are not completed/cancelled
             current_status = item.get("status", "pending") # Default WT items to pending
             item_type = item.get("type")
             # Include external events, or WT items not completed/cancelled
             if item_type == "external_event" or current_status not in ["completed", "cancelled"]:
                items_today.append(item)

    if not items_today:
        return f"Good morning! â˜€ï¸ Looks like a clear schedule today ({today_local_str}). Anything you'd like to add?"

    # Sort items for display
    sorted_items = _sort_tasks(items_today) # Use the existing sort helper

    message_lines = [f"Good morning! â˜€ï¸ Here's your overview for today, {today_local_str}:"]
    for item in sorted_items:
        # Pass user_tz_str for potential use in formatting (though _format_task_line needs update)
        # Add timezone info to item temporarily for formatting function
        item['_user_timezone_for_display'] = user_tz_str
        formatted_line = _format_task_line(item)
        message_lines.append(f"- {formatted_line}")
        item.pop('_user_timezone_for_display', None) # Clean up temporary key

    message_lines.append("\nHave a productive day!")
    return "\n".join(message_lines)


def generate_evening_review(user_id, context):
    """Generates the evening review message, listing only active WT items for the day."""
    fn_name = "generate_evening_review"
    prefs = get_user_preferences(user_id)
    user_tz_str = prefs.get("TimeZone", "UTC") if prefs else "UTC"
    user_tz = pytz.timezone(user_tz_str)
    now_local = get_local_time(user_tz_str)
    today_local_str = now_local.strftime("%Y-%m-%d")

    log_info("routine_service", fn_name, f"Generating evening review for user {user_id} for date {today_local_str}")

    wt_items_today_active = []
    for item in context:
        item_type = item.get("type")
        # *** Filter for WT items ONLY ***
        if item_type in ["task", "reminder"]:
            item_date_local_str = None
            start_dt_str = item.get("gcal_start_datetime")
            is_all_day = item.get("is_all_day", False)

            if start_dt_str:
                 try:
                     if 'T' in start_dt_str:
                          dt_aware = datetime.fromisoformat(start_dt_str.replace('Z', '+00:00'))
                          item_date_local_str = dt_aware.astimezone(user_tz).strftime("%Y-%m-%d")
                     elif len(start_dt_str) == 10:
                          item_date_local_str = start_dt_str
                 except ValueError: pass
            elif item.get("date"):
                 item_date_local_str = item.get("date")

            # Include if it's for today AND its status is pending or in progress
            if item_date_local_str == today_local_str and item.get("status") in ["pending", "in_progress", "in progress"]:
                 wt_items_today_active.append(item)

    if not wt_items_today_active:
        return f"Good evening! ðŸ‘‹ No active tasks or reminders were scheduled for today ({today_local_str}). Time to relax or plan for tomorrow?"

    # Sort items for display
    sorted_items = _sort_tasks(wt_items_today_active)

    message_lines = [f"Good evening! ðŸ‘‹ Let's review your day ({today_local_str}). Here are the tasks/reminders still marked as active:"]
    for i, item in enumerate(sorted_items):
        # Pass user_tz_str for potential use in formatting
        item['_user_timezone_for_display'] = user_tz_str
        formatted_line = _format_task_line(item)
        message_lines.append(f"{i+1}. {formatted_line}")
        item.pop('_user_timezone_for_display', None)

    message_lines.append("\nHow did it go? You can update items by replying (e.g., 'complete 1', 'cancel 2') or add new ones for tomorrow.")
    return "\n".join(message_lines)


def check_routine_triggers(): # Remove -> List[Tuple[str, str]] hint if not allowed
    """
    Scheduled job function. Checks all users if morning/evening routines should run.
    Calls sync service before generating summaries.
    Returns a list of (user_id, message_content) tuples for messages to be sent.
    """
    fn_name = "check_routine_triggers"
    log_info("routine_service", fn_name, "Running scheduled check for routine triggers...")
    messages_to_send = [] # Initialize list to store messages

    try:
        registry = get_registry()
        if not registry:
            log_warning("routine_service", fn_name, "User registry is empty. Skipping check.")
            return []

        user_ids = list(registry.keys())
        log_info("routine_service", fn_name, f"Checking routines for {len(user_ids)} users.")

        for user_id in user_ids:
            prefs = None
            try:
                prefs = get_user_preferences(user_id)
                if not prefs or prefs.get("status") != "active":
                    continue

                user_tz_str = prefs.get("TimeZone")
                if not user_tz_str:
                    continue

                now_local = get_local_time(user_tz_str)
                today_local_str = now_local.strftime("%Y-%m-%d")
                current_local_hm = now_local.strftime("%H:%M")

                aggregated_context = None
                context_fetched = False

                # --- Check Morning Routine ---
                morning_time_str = prefs.get("Morning_Summary_Time")
                if prefs.get("Enable_Morning") and morning_time_str:
                    last_triggered_morning = prefs.get("last_morning_trigger_date")
                    if current_local_hm >= morning_time_str and last_triggered_morning != today_local_str:
                        log_info("routine_service", fn_name, f"Triggering Morning Summary for user {user_id} at {current_local_hm} {user_tz_str}")
                        if not context_fetched:
                            context_start_date = (now_local - timedelta(days=ROUTINE_CONTEXT_HISTORY_DAYS)).strftime("%Y-%m-%d")
                            context_end_date = (now_local + timedelta(days=ROUTINE_CONTEXT_FUTURE_DAYS)).strftime("%Y-%m-%d")
                            log_info("routine_service", fn_name, f"Getting synced context for routines (User: {user_id})...")
                            aggregated_context = sync_service.get_synced_context_snapshot(user_id, context_start_date, context_end_date)
                            context_fetched = True

                        summary_msg = generate_morning_summary(user_id, aggregated_context or [])
                        if summary_msg:
                            # --- REPLACE send_message WITH append ---
                            messages_to_send.append((user_id, summary_msg))
                            # ----------------------------------------
                            update_success = update_preferences(user_id, {"last_morning_trigger_date": today_local_str})
                            if not update_success: log_error("routine_service", fn_name, f"Failed to update last_morning_trigger_date for {user_id}")
                        else:
                            log_warning("routine_service", fn_name, f"Morning summary generated empty message for {user_id}")

                # --- Check Evening Routine ---
                evening_time_str = prefs.get("Evening_Summary_Time")
                if prefs.get("Enable_Evening") and evening_time_str:
                    last_triggered_evening = prefs.get("last_evening_trigger_date")
                    if current_local_hm >= evening_time_str and last_triggered_evening != today_local_str:
                        log_info("routine_service", fn_name, f"Triggering Evening Review for user {user_id} at {current_local_hm} {user_tz_str}")
                        if not context_fetched:
                            context_start_date = (now_local - timedelta(days=ROUTINE_CONTEXT_HISTORY_DAYS)).strftime("%Y-%m-%d")
                            context_end_date = (now_local + timedelta(days=ROUTINE_CONTEXT_FUTURE_DAYS)).strftime("%Y-%m-%d")
                            log_info("routine_service", fn_name, f"Getting synced context for routines (User: {user_id})...")
                            aggregated_context = sync_service.get_synced_context_snapshot(user_id, context_start_date, context_end_date)
                            context_fetched = True

                        review_msg = generate_evening_review(user_id, aggregated_context or [])
                        if review_msg:
                            # --- REPLACE send_message WITH append ---
                            messages_to_send.append((user_id, review_msg))
                            # ----------------------------------------
                            update_success = update_preferences(user_id, {"last_evening_trigger_date": today_local_str})
                            if not update_success: log_error("routine_service", fn_name, f"Failed to update last_evening_trigger_date for {user_id}")
                        else:
                            log_warning("routine_service", fn_name, f"Evening review generated empty message for {user_id}")

            except Exception as user_err:
                 log_error("routine_service", fn_name, f"Error processing routines for user {user_id}. Prefs: {prefs}", user_err)
                 traceback.print_exc()

    except Exception as main_err:
        log_error("routine_service", fn_name, f"General error during routine check run", main_err)
        traceback.print_exc()

    log_info("routine_service", fn_name, f"Finished scheduled check for routine triggers. Found {len(messages_to_send)} messages to send.")
    return messages_to_send # Return the list

def daily_cleanup():
    """Scheduled job to perform daily cleanup tasks (e.g., reset notification tracker)."""
    fn_name = "daily_cleanup"
    log_info("routine_service", fn_name, "Running daily cleanup job...")

    try:
        registry = get_registry()
        user_ids = list(registry.keys())
        if not user_ids:
            log_info("routine_service", fn_name, "No users found for daily cleanup.")
            return

        log_info("routine_service", fn_name, f"Performing daily cleanup for {len(user_ids)} users...")
        cleared_count = 0
        for user_id in user_ids:
            try:
                if AGENT_STATE_IMPORTED:
                    # Check if state exists before trying to clear
                    if get_agent_state(user_id): # Use get_agent_state to check existence
                       clear_notified_event_ids(user_id)
                       cleared_count += 1
                    # else: No state in memory, nothing to clear
                # else: Cannot clear if state manager not imported
            except Exception as e:
                 log_error("routine_service", fn_name, f"Error during daily cleanup for user {user_id}", e)

        log_info("routine_service", fn_name, f"Daily cleanup finished. Cleared notification sets for {cleared_count} users.")

    except Exception as e:
        log_error("routine_service", fn_name, "Error during daily cleanup main loop", e)

# --- END OF FILE services/routine_service.py ---

# --- END OF FILE services/routine_service.py ---



================================================================================
ðŸ“„ tools/google_calendar_api.py
================================================================================

# --- START OF FILE tools/google_calendar_api.py ---

# --- START OF FULL tools/google_calendar_api.py ---

import os
import re  # <--- ADD THIS IMPORT

from datetime import datetime, timedelta
from typing import Dict, List, Any, TYPE_CHECKING # Removed Optional

# --- Try importing Google libraries ---
Credentials = None
build = None
HttpError = Exception
GoogleAuthRequest = None
RefreshError = Exception
GOOGLE_LIBS_AVAILABLE = False

try:
    from tools.logger import log_info, log_error, log_warning
except ImportError:
    import logging
    logging.basicConfig(level=logging.INFO, format='%(levelname)s:google_calendar_api:%(message)s')
    log_info = logging.info; log_error = logging.error; log_warning = logging.warning
    log_error("google_calendar_api", "import", "Failed to import project logger.")

try:
    from google.oauth2.credentials import Credentials as ImportedCredentials
    from googleapiclient.discovery import build as imported_build
    from googleapiclient.errors import HttpError as ImportedHttpError
    from google.auth.transport.requests import Request as ImportedGoogleAuthRequest
    from google.auth.exceptions import RefreshError as ImportedRefreshError

    Credentials = ImportedCredentials
    build = imported_build
    HttpError = ImportedHttpError
    GoogleAuthRequest = ImportedGoogleAuthRequest
    RefreshError = ImportedRefreshError
    GOOGLE_LIBS_AVAILABLE = True
    log_info("google_calendar_api", "import", "Successfully imported Google API libraries.")
except ImportError as import_error_exception:
    log_error("google_calendar_api", "import", f"Failed to import one or more Google API libraries: {import_error_exception}. GoogleCalendarAPI will be non-functional.", import_error_exception)

# --- Other Local Project Imports ---
try:
    from tools.token_store import get_user_token, save_user_token_encrypted
except ImportError as e:
     log_error("google_calendar_api", "import", f"Failed to import from token_store: {e}", e)
     # Define dummy functions if import fails
     def get_user_token(*args, **kwargs): return None
     def save_user_token_encrypted(*args, **kwargs): return False

if TYPE_CHECKING:
    from googleapiclient.discovery import Resource
    if Credentials:
         from google.oauth2.credentials import Credentials

# --- Configuration ---
GOOGLE_CLIENT_ID = os.getenv("GOOGLE_CLIENT_ID")
GOOGLE_CLIENT_SECRET = os.getenv("GOOGLE_CLIENT_SECRET")
if not GOOGLE_CLIENT_ID: log_error("google_calendar_api", "config", "CRITICAL: GOOGLE_CLIENT_ID not set.")
if not GOOGLE_CLIENT_SECRET: log_error("google_calendar_api", "config", "CRITICAL: GOOGLE_CLIENT_SECRET not set.")
DEFAULT_TIMEZONE = "Asia/Jerusalem" # TODO: Make user-specific via preferences
GOOGLE_TOKEN_URI = "https://oauth2.googleapis.com/token"


class GoogleCalendarAPI:
    """Handles interactions with the Google Calendar API for a specific user."""
    def __init__(self, user_id: str):
        fn_name = "__init__"
        self.user_id = user_id
        self.service = None # Initialize service to None
        self.user_timezone = DEFAULT_TIMEZONE # TODO: Load from user prefs eventually

        log_info("GoogleCalendarAPI", fn_name, f"Initializing for user {self.user_id}")
        if not GOOGLE_LIBS_AVAILABLE:
            log_error("GoogleCalendarAPI", fn_name, "Google API libraries not available. Initialization skipped.")
            return

        credentials = self._load_credentials() # This now returns Credentials or None

        if credentials is not None: # Explicit check for None
            try:
                if build is None:
                    raise ImportError("Build function ('googleapiclient.discovery.build') not available.")
                # Assign the built service to self.service
                self.service = build("calendar", "v3", credentials=credentials, cache_discovery=False)
                log_info("GoogleCalendarAPI", fn_name, f"GCal service built successfully for {self.user_id}")
            except ImportError as e:
                 log_error("GoogleCalendarAPI", fn_name, f"Import error during service build: {e}", e)
                 self.service = None # Ensure service is None on error
            except Exception as e:
                log_error("GoogleCalendarAPI", fn_name, f"Failed to build GCal service: {e}", e)
                self.service = None # Ensure service is None on error
        else:
            log_warning("GoogleCalendarAPI", fn_name, f"Initialization incomplete for {self.user_id} due to credential failure.")
            self.service = None # Ensure service is None if creds fail


    # Return type is now 'Credentials | None', but we remove the hint as requested
    # The function still returns None on failure.
    def _load_credentials(self):
        fn_name = "_load_credentials"
        log_info("GoogleCalendarAPI", fn_name, f"Attempting credentials load for {self.user_id}")

        if not GOOGLE_LIBS_AVAILABLE or Credentials is None:
            log_error("GoogleCalendarAPI", fn_name, "Google libraries or Credentials class not available.")
            return None

        if not GOOGLE_CLIENT_ID or not GOOGLE_CLIENT_SECRET:
             log_error("GoogleCalendarAPI", fn_name, "Client ID or Secret missing in environment config.")
             return None

        token_data = get_user_token(self.user_id)
        if token_data is None: # Explicit check
             log_info("GoogleCalendarAPI", fn_name, f"No token data found for user {self.user_id}.")
             return None

        if "refresh_token" not in token_data:
             log_error("GoogleCalendarAPI", fn_name, f"FATAL: refresh_token missing in stored data for {self.user_id}. Re-auth needed.")
             # Consider deleting the bad token file here?
             # delete_token_file(self.user_id) # Hypothetical function
             return None

        credential_info_for_lib = {
            'token': token_data.get('access_token'),
            'refresh_token': token_data.get('refresh_token'),
            'token_uri': GOOGLE_TOKEN_URI,
            'client_id': GOOGLE_CLIENT_ID,
            'client_secret': GOOGLE_CLIENT_SECRET,
            'scopes': token_data.get('scopes', [])
        }
        # Ensure scopes is a list
        if isinstance(credential_info_for_lib['scopes'], str):
            credential_info_for_lib['scopes'] = credential_info_for_lib['scopes'].split()

        creds = None
        try:
            creds = Credentials.from_authorized_user_info(credential_info_for_lib)

            if not creds.valid:
                log_warning("GoogleCalendarAPI", fn_name, f"Credentials invalid/expired for {self.user_id}. Checking refresh token.")
                if creds.refresh_token:
                    log_info("GoogleCalendarAPI", fn_name, f"Attempting explicit token refresh for {self.user_id}...")
                    try:
                        if GoogleAuthRequest is None: raise ImportError("GoogleAuthRequest class not available for refresh.")
                        creds.refresh(GoogleAuthRequest())
                        log_info("GoogleCalendarAPI", fn_name, f"Token refresh successful for {self.user_id}.")
                        # Prepare data for saving (including expiry)
                        refreshed_token_data_to_save = {
                            'access_token': creds.token,
                            'refresh_token': creds.refresh_token,
                            'token_uri': creds.token_uri,
                            'client_id': creds.client_id,
                            'client_secret': creds.client_secret,
                            'scopes': creds.scopes,
                            'expiry_iso': creds.expiry.isoformat() if creds.expiry else None
                        }
                        # Attempt to save the refreshed token
                        if save_user_token_encrypted is None:
                            log_error("GoogleCalendarAPI", fn_name, "save_user_token_encrypted function not available.")
                        elif not save_user_token_encrypted(self.user_id, refreshed_token_data_to_save):
                            log_warning("GoogleCalendarAPI", fn_name, f"Failed to save refreshed token for {self.user_id}.")
                        # else: Saved successfully (no log needed unless verbose)

                    except RefreshError as refresh_err:
                        log_error("GoogleCalendarAPI", fn_name, f"Token refresh FAILED for {self.user_id} (RefreshError): {refresh_err}. Re-authentication required.", refresh_err)
                        token_file_path = os.path.join("data", f"tokens_{self.user_id}.json.enc")
                        if os.path.exists(token_file_path):
                            log_warning("GoogleCalendarAPI", fn_name, f"Deleting invalid token file due to refresh failure: {token_file_path}")
                            try: os.remove(token_file_path)
                            except OSError as rm_err: log_error("GoogleCalendarAPI", fn_name, f"Failed to remove token file: {rm_err}")
                        return None # Return None as refresh failed
                    except ImportError as imp_err:
                         log_error("GoogleCalendarAPI", fn_name, f"Import error during refresh for {self.user_id}: {imp_err}")
                         return None
                    except Exception as refresh_err:
                        log_error("GoogleCalendarAPI", fn_name, f"Unexpected error during token refresh for {self.user_id}: {refresh_err}", refresh_err)
                        return None # Return None on unexpected error
                else:
                     log_error("GoogleCalendarAPI", fn_name, f"Credentials invalid for {self.user_id}, and no refresh token available. Re-authentication needed.")
                     return None # Return None as creds invalid and no refresh possible

            # Final check after potential refresh attempt
            if creds and creds.valid:
                 log_info("GoogleCalendarAPI", fn_name, f"Credentials loaded and valid for {self.user_id}.")
                 return creds # Return the valid credentials object
            else:
                 log_error("GoogleCalendarAPI", fn_name, f"Failed to obtain valid credentials for {self.user_id} after potential refresh.")
                 return None # Return None as still not valid
        except Exception as e:
            log_error("GoogleCalendarAPI", fn_name, f"Error creating/validating credentials object for {self.user_id}: {e}", e)
            return None # Return None on error


    def is_active(self):
        """Checks if the Google Calendar service object was successfully initialized."""
        return self.service is not None

    # Returns event ID string or None
    def create_event(self, event_data: Dict):
        fn_name = "create_event"
        if not self.is_active():
            log_error("GoogleCalendarAPI", fn_name, f"Service not active for user {self.user_id}.")
            return None
        assert self.service is not None # Should be active if is_active() passed

        try:
            event_date = event_data.get('date')
            event_time = event_data.get('time')
            duration_minutes = None
            if event_data.get('duration'):
                try:
                    # Improved duration parsing logic
                    duration_str = str(event_data['duration']).lower().replace(' ','')
                    total_minutes = 0.0
                    hour_match = re.search(r'(\d+(\.\d+)?)\s*h', duration_str)
                    minute_match = re.search(r'(\d+)\s*m', duration_str)
                    if hour_match: total_minutes += float(hour_match.group(1)) * 60
                    if minute_match: total_minutes += int(minute_match.group(1))
                    # Handle plain numbers (assume minutes) only if no h/m found
                    if total_minutes == 0 and hour_match is None and minute_match is None:
                         if duration_str.replace('.','',1).isdigit():
                              total_minutes = float(duration_str)
                         else: raise ValueError("Unrecognized duration format")
                    duration_minutes = int(round(total_minutes)) if total_minutes > 0 else None
                except (ValueError, TypeError, AttributeError):
                    log_warning("GoogleCalendarAPI", fn_name, f"Could not parse duration: {event_data.get('duration')}, using default.")
                    duration_minutes = 30 # Default to 30 mins if parse fails

            if not event_date:
                 log_error("GoogleCalendarAPI", fn_name, f"Missing mandatory 'date' field.")
                 return None

            start_obj = {}
            end_obj = {}
            time_zone = self.user_timezone # Use the instance timezone

            if event_time:
                 # Handle timed event
                 try:
                     start_dt = datetime.strptime(f"{event_date} {event_time}", "%Y-%m-%d %H:%M")
                     # Use parsed duration or default
                     delta = timedelta(minutes=duration_minutes if duration_minutes is not None else 30)
                     end_dt = start_dt + delta
                     start_obj = {"dateTime": start_dt.isoformat(), "timeZone": time_zone}
                     end_obj = {"dateTime": end_dt.isoformat(), "timeZone": time_zone}
                 except ValueError as time_err:
                      log_error("GoogleCalendarAPI", fn_name, f"Invalid date/time format: {event_date} {event_time}", time_err)
                      return None
            else: # Handle all-day event
                try:
                    start_dt_date = datetime.strptime(event_date, "%Y-%m-%d").date()
                    # All-day events end on the *next* day according to GCal API
                    end_date_dt = start_dt_date + timedelta(days=1)
                    start_obj = {"date": start_dt_date.strftime("%Y-%m-%d")}
                    end_obj = {"date": end_date_dt.strftime("%Y-%m-%d")}
                except ValueError as date_err:
                     log_error("GoogleCalendarAPI", fn_name, f"Invalid date format '{event_date}' for all-day event", date_err)
                     return None

            # Construct the event body
            google_event_body = {
                "summary": event_data.get("title", event_data.get("description", "New Item")),
                "description": event_data.get("description", ""),
                "start": start_obj,
                "end": end_obj
            }

            log_info("GoogleCalendarAPI", fn_name, f"Creating GCal event for user {self.user_id}: {google_event_body.get('summary')}")
            created_event = self.service.events().insert(calendarId='primary', body=google_event_body).execute()
            google_event_id = created_event.get("id")

            if google_event_id:
                log_info("GoogleCalendarAPI", fn_name, f"Successfully created GCal event ID: {google_event_id}")
                return google_event_id # Return the ID string
            else:
                log_error("GoogleCalendarAPI", fn_name, f"GCal API response missing 'id'. Response: {created_event}")
                return None # Return None on failure
        except HttpError as http_err:
             log_error("GoogleCalendarAPI", fn_name, f"HTTP error creating event for user {self.user_id}: Status {http_err.resp.status}", http_err)
             return None
        except Exception as e:
            log_error("GoogleCalendarAPI", fn_name, f"Unexpected error creating event for user {self.user_id}", e)
            return None

    # Returns bool
    def update_event(self, event_id: str, updates: Dict):
        fn_name = "update_event"
        if not self.is_active():
            log_error("GoogleCalendarAPI", fn_name, f"Service not active for user {self.user_id}, cannot update event {event_id}.")
            return False
        assert self.service is not None

        try:
            # Get the existing event first to determine current times/duration if needed
            try:
                  existing_event = self.service.events().get(calendarId='primary', eventId=event_id).execute()
            except HttpError as get_err:
                 # If event not found, cannot update
                 if get_err.resp.status == 404:
                     log_warning("GoogleCalendarAPI", fn_name, f"Cannot update event {event_id}: Not found.")
                     return False
                 else:
                      log_error("GoogleCalendarAPI", fn_name, f"HTTP error getting event {event_id} before update: {get_err}", get_err)
                      return False

            update_payload = {}
            needs_update = False
            time_zone = self.user_timezone

            # Update simple fields
            if "title" in updates: update_payload["summary"] = updates["title"]; needs_update = True
            if "description" in updates: update_payload["description"] = updates["description"]; needs_update = True

            # Handle date/time updates carefully
            new_date_str = updates.get("date")
            new_time_str = updates.get("time") # Can be None if time is cleared

            # Check if date or time is being explicitly modified
            if new_date_str is not None or "time" in updates:
                 current_start_info = existing_event.get('start', {})
                 current_end_info = existing_event.get('end', {})
                 is_currently_all_day = 'date' in current_start_info and 'dateTime' not in current_start_info

                 # Determine the target date
                 target_date_str = new_date_str
                 if target_date_str is None: # Date not provided in update, use existing
                      if is_currently_all_day:
                           target_date_str = current_start_info.get('date')
                      elif current_start_info.get('dateTime'):
                           try: target_date_str = datetime.fromisoformat(current_start_info['dateTime']).strftime('%Y-%m-%d')
                           except ValueError: target_date_str = None # Fallback if parse fails
                      else: target_date_str = None # Cannot determine existing date

                 # Determine the target time (can be None)
                 target_time_str = new_time_str if "time" in updates else (datetime.fromisoformat(current_start_info['dateTime']).strftime('%H:%M') if current_start_info.get('dateTime') and not is_currently_all_day else None)

                 if target_date_str:
                      if target_time_str: # Update to a timed event
                           try:
                               start_dt = datetime.strptime(f"{target_date_str} {target_time_str}", "%Y-%m-%d %H:%M")
                               # Preserve duration if possible
                               duration = timedelta(minutes=30) # Default fallback
                               if current_start_info.get('dateTime') and current_end_info.get('dateTime'):
                                    try: duration = datetime.fromisoformat(current_end_info['dateTime']) - datetime.fromisoformat(current_start_info['dateTime'])
                                    except ValueError: pass # Use default if parse fails
                               end_dt = start_dt + duration
                               update_payload["start"] = {"dateTime": start_dt.isoformat(), "timeZone": time_zone}
                               update_payload["end"] = {"dateTime": end_dt.isoformat(), "timeZone": time_zone}
                               needs_update = True
                           except ValueError as e:
                                log_error("GoogleCalendarAPI", fn_name, f"Invalid date/time '{target_date_str} {target_time_str}' on update: {e}")
                                # Don't proceed with this part of the update if format is bad
                      else: # Update to an all-day event
                           try:
                               start_dt_date = datetime.strptime(target_date_str, "%Y-%m-%d").date()
                               end_date_dt = start_dt_date + timedelta(days=1)
                               update_payload["start"] = {"date": start_dt_date.strftime("%Y-%m-%d")}
                               update_payload["end"] = {"date": end_date_dt.strftime("%Y-%m-%d")}
                               # Clear any existing dateTime fields if changing to all-day
                               if 'dateTime' in update_payload.get("start",{}): del update_payload["start"]["dateTime"]
                               if 'dateTime' in update_payload.get("end",{}): del update_payload["end"]["dateTime"]
                               needs_update = True
                           except ValueError as e:
                                log_error("GoogleCalendarAPI", fn_name, f"Invalid date '{target_date_str}' for all-day update: {e}")

            if not needs_update:
                 log_info("GoogleCalendarAPI", fn_name, f"No fields require patching for GCal event {event_id}")
                 return True # No change needed, considered success

            log_info("GoogleCalendarAPI", fn_name, f"Patching GCal event {event_id} for user {self.user_id}. Fields: {list(update_payload.keys())}")
            self.service.events().patch(calendarId='primary', eventId=event_id, body=update_payload).execute()
            log_info("GoogleCalendarAPI", fn_name, f"Successfully updated GCal event {event_id}")
            return True # Return True on success
        except HttpError as http_err:
             log_error("GoogleCalendarAPI", fn_name, f"HTTP error updating event {event_id}: Status {http_err.resp.status}", http_err)
             return False
        except Exception as e:
            log_error("GoogleCalendarAPI", fn_name, f"Unexpected error updating event {event_id}", e)
            return False

    # Returns bool
    def delete_event(self, event_id: str):
        fn_name = "delete_event"
        if not self.is_active():
            log_error("GoogleCalendarAPI", fn_name, f"Service not active for user {self.user_id}, cannot delete event {event_id}.")
            return False
        assert self.service is not None
        try:
            log_info("GoogleCalendarAPI", fn_name, f"Attempting delete GCal event {event_id} for user {self.user_id}")
            # sendNotifications=False might be useful if you handle reminders internally
            self.service.events().delete(calendarId='primary', eventId=event_id, sendNotifications=False).execute()
            log_info("GoogleCalendarAPI", fn_name, f"Successfully deleted GCal event {event_id}.")
            return True # Return True on success
        except HttpError as http_err:
            if http_err.resp.status in [404, 410]: # Not Found or Gone
                log_warning("GoogleCalendarAPI", fn_name, f"GCal event {event_id} not found or already gone (Status {http_err.resp.status}). Assuming deleted.")
                return True # Consider deletion successful if it's already gone
            else:
                 log_error("GoogleCalendarAPI", fn_name, f"HTTP error deleting event {event_id}: Status {http_err.resp.status}", http_err)
                 return False # Return False on other HTTP errors
        except Exception as e:
            log_error("GoogleCalendarAPI", fn_name, f"Unexpected error deleting event {event_id}", e)
            return False

    # Returns list of dicts or empty list
    def list_events(self, start_date: str, end_date: str):
        fn_name = "list_events"
        if not self.is_active():
            log_error("GoogleCalendarAPI", fn_name, f"Service not active for user {self.user_id}, cannot list events.")
            return []
        assert self.service is not None

        try:
            # Format dates for API (inclusive start, exclusive end)
            start_dt = datetime.strptime(start_date, "%Y-%m-%d").replace(hour=0, minute=0, second=0, microsecond=0)
            end_dt_exclusive = datetime.strptime(end_date, "%Y-%m-%d").replace(hour=0, minute=0, second=0, microsecond=0) + timedelta(days=1)
            # Use UTC 'Z' for timeMin/timeMax as recommended by Google API
            time_min = start_dt.isoformat() + "Z"
            time_max = end_dt_exclusive.isoformat() + "Z"
            log_info("GoogleCalendarAPI", fn_name, f"Listing GCal events for user {self.user_id} from {time_min} to {time_max}")
        except ValueError as date_err:
            log_error("GoogleCalendarAPI", fn_name, f"Invalid date format for listing events: {start_date} / {end_date}", date_err)
            return [] # Return empty list on bad date format

        try:
            all_items = []
            page_token = None
            while True:
                events_result = self.service.events().list(
                    calendarId='primary',
                    timeMin=time_min,
                    timeMax=time_max,
                    maxResults=250, # Max allowed by API per page
                    singleEvents=True, # Expand recurring events
                    orderBy='startTime',
                    pageToken=page_token
                ).execute()
                items = events_result.get("items", [])
                all_items.extend(items)
                page_token = events_result.get('nextPageToken')
                if not page_token: break # Exit loop when no more pages
            log_info("GoogleCalendarAPI", fn_name, f"Found {len(all_items)} GCal events for user {self.user_id} in range.")
            # Parse *after* collecting all items
            return [self._parse_google_event(e) for e in all_items]
        except HttpError as http_err:
             log_error("GoogleCalendarAPI", fn_name, f"HTTP error listing events for user {self.user_id}: Status {http_err.resp.status}", http_err)
             return []
        except Exception as e:
            log_error("GoogleCalendarAPI", fn_name, f"Unexpected error listing events for user {self.user_id}", e)
            return []

    # Returns dict or None
    def _get_single_event(self, event_id: str):
        fn_name = "_get_single_event"
        if not self.is_active():
            log_error("GoogleCalendarAPI", fn_name, f"Service not active for user {self.user_id}.")
            return None
        assert self.service is not None
        try:
            event = self.service.events().get(calendarId='primary', eventId=event_id).execute()
            return event # Return the raw GCal event dictionary
        except HttpError as http_err:
             if http_err.resp.status in [404, 410]: # Not Found or Gone
                  log_warning("GoogleCalendarAPI", fn_name, f"Event {event_id} not found (Status {http_err.resp.status}).")
                  return None # Return None if not found
             else:
                  log_error("GoogleCalendarAPI", fn_name, f"HTTP error getting event {event_id}: Status {http_err.resp.status}", http_err)
                  return None # Return None on other errors
        except Exception as e:
            log_error("GoogleCalendarAPI", fn_name, f"Unexpected error getting event {event_id}", e)
            return None

    # Returns dict
    def _parse_google_event(self, event: Dict):
        # This function parses the raw GCal event into our standard format
        start_info = event.get("start", {})
        end_info = event.get("end", {})
        # Get dateTime first, fallback to date for all-day events
        start_datetime_str = start_info.get("dateTime", start_info.get("date"))
        end_datetime_str = end_info.get("dateTime", end_info.get("date"))
        # Determine if it's an all-day event
        is_all_day = "date" in start_info and "dateTime" not in start_info

        # Basic parsing
        parsed = {
            "event_id": event.get("id"),
            "title": event.get("summary", ""),
            "description": event.get("description", ""),
            "gcal_start_datetime": start_datetime_str, # Store the full string
            "gcal_end_datetime": end_datetime_str,     # Store the full string
            "is_all_day": is_all_day,
            "gcal_link": event.get("htmlLink", ""),
            "status_gcal": event.get("status", ""), # e.g., 'confirmed', 'tentative', 'cancelled'
            "created_gcal": event.get("created"), # ISO timestamp
            "updated_gcal": event.get("updated"), # ISO timestamp
            # Add any other relevant fields we might want later
        }
        return parsed

# --- END OF CLASS GoogleCalendarAPI ---

# --- END OF FULL tools/google_calendar_api.py ---

# --- END OF FILE tools/google_calendar_api.py ---



================================================================================
ðŸ“„ tools/calendar_tool.py
================================================================================

# --- START OF FILE tools/calendar_tool.py ---

# --- START OF FULL tools/calendar_tool.py ---

import os
import requests
import json # For logging potentially
from fastapi import APIRouter, Request, Response
from fastapi.responses import HTMLResponse
from tools.logger import log_info, log_error, log_warning
from tools.encryption import encrypt_data # Only need encrypt_data from here
import jwt
import requests.compat # Needed for urlencode in authenticate
from datetime import datetime # Keep if used elsewhere

# --- REMOVE service layer import attempt from module level ---
# --- Keep ONLY direct registry update as fallback ---
from users.user_registry import update_preferences as update_prefs_direct
log_warning("calendar_tool", "import", "Using direct registry update for preferences in callback.")
# ----------------------------------------------

router = APIRouter()

# --- Configuration ---
GOOGLE_CLIENT_SECRET = os.getenv("GOOGLE_CLIENT_SECRET")
CLIENT_ID = os.getenv("GOOGLE_CLIENT_ID")
REDIRECT_URI = os.getenv("GOOGLE_REDIRECT_URI", "http://localhost:8000/oauth2callback")
SCOPE = "https://www.googleapis.com/auth/calendar"
TOKEN_URL = "https://oauth2.googleapis.com/token"
AUTH_URL_BASE = "https://accounts.google.com/o/oauth2/auth"

if not GOOGLE_CLIENT_SECRET or not CLIENT_ID:
    log_error("calendar_tool", "config", "CRITICAL: GOOGLE_CLIENT_ID or GOOGLE_CLIENT_SECRET env var not set.")

log_info("calendar_tool", "config", f"Calendar Tool loaded. Client ID starts with: {str(CLIENT_ID)[:10]}")

# --- Core Authentication Check Function ---
def authenticate(user_id: str, prefs: dict) -> dict:
    """
    Checks user's calendar auth status based on provided preferences.
    Returns status and auth URL if needed, or status/message if token exists.
    """
    log_info("calendar_tool", "authenticate", f"Checking auth status for user {user_id}")
    from tools.token_store import get_user_token # Import here
    token_data = get_user_token(user_id)
    calendar_enabled = prefs.get("Calendar_Enabled", False)

    if token_data is not None and calendar_enabled:
        log_info("calendar_tool", "authenticate", f"Token data exists and enabled flag is True for {user_id}.")
        return {"status": "token_exists", "message": "Stored calendar credentials found. Attempting to use..."}
    else:
        # ... (rest of auth URL generation logic remains the same) ...
        if token_data is not None and not calendar_enabled:
             log_info("calendar_tool", "authenticate", f"Token data exists but Calendar_Enabled=False for {user_id}. Initiating re-auth/enable.")
        else: # token_data is None
             log_info("calendar_tool", "authenticate", f"No valid token data found for {user_id}. Initiating auth.")

        if not CLIENT_ID or not REDIRECT_URI:
             log_error("calendar_tool", "authenticate", f"Client ID or Redirect URI missing for auth URL generation.")
             return {"status": "fails", "message": "Server configuration error prevents authentication."}

        normalized_state = user_id.replace("@c.us", "").replace("+","")
        params = {
            "client_id": CLIENT_ID, "redirect_uri": REDIRECT_URI, "scope": SCOPE,
            "response_type": "code", "access_type": "offline", "state": normalized_state, "prompt": "consent"
        }
        try:
             encoded_params = requests.compat.urlencode(params)
             auth_url = f"{AUTH_URL_BASE}?{encoded_params}"
             log_info("calendar_tool", "authenticate", f"Generated auth URL for {user_id}")
             return {"status": "pending", "message": f"Please authenticate your calendar by visiting this URL: {auth_url}"}
        except Exception as url_e:
             log_error("calendar_tool", "authenticate", f"Failed to build auth URL for {user_id}", url_e)
             return {"status": "fails", "message": "Failed to generate authentication URL."}


# --- OAuth Callback Endpoint (Corrected Scope) ---
@router.get("/oauth2callback", response_class=HTMLResponse)
async def oauth2callback(request: Request, code: str | None = None, state: str | None = None, error: str | None = None):
    """
    Handles the OAuth2 callback from Google.
    Exchanges code, saves token, updates preferences.
    """
    # --- Define flag and function placeholder INSIDE the function scope ---
    config_manager_imported_locally = False
    update_prefs_service = None
    # --- Attempt import locally ---
    try:
        from services.config_manager import update_preferences
        update_prefs_service = update_preferences
        config_manager_imported_locally = True
    except ImportError:
         # Error/warning already logged at module level, no need to repeat
         pass # Keep flag as False, function as None
    # ---------------------------------------------------------------------

    html_error_template = "<html><body><h1>Authentication Error</h1><p>Details: {details}</p><p>Please try authenticating again or contact support if the issue persists.</p></body></html>"
    html_success_template = "<html><body><h1>Authentication Successful!</h1><p>Your credentials have been saved. The connection will be fully tested when first used. You can close this window and return to the chat.</p></body></html>"

    if error:
        log_error("calendar_tool", "oauth2callback", f"OAuth error received from Google: {error}")
        return HTMLResponse(content=html_error_template.format(details=f"Google reported an error: {error}"), status_code=400)
    if not code or not state:
        log_error("calendar_tool", "oauth2callback", "Callback missing code or state.")
        return HTMLResponse(content=html_error_template.format(details="Invalid response received from Google (missing code or state)."), status_code=400)

    user_id = state
    log_info("calendar_tool", "oauth2callback", f"Callback received for user {user_id}.")

    if not GOOGLE_CLIENT_SECRET or not CLIENT_ID:
        log_error("calendar_tool", "oauth2callback", "Server configuration error: Client ID/Secret not set.")
        return HTMLResponse(content=html_error_template.format(details="Server configuration error."), status_code=500)

    try:
        # --- 1. Exchange Code for Tokens ---
        payload = {
            "code": code, "client_id": CLIENT_ID, "client_secret": GOOGLE_CLIENT_SECRET,
            "redirect_uri": REDIRECT_URI, "grant_type": "authorization_code"
        }
        log_info("calendar_tool", "oauth2callback", f"Exchanging authorization code for tokens for user {user_id}.")
        token_response = requests.post(TOKEN_URL, data=payload)
        token_response.raise_for_status()
        tokens = token_response.json()
        log_info("calendar_tool", "oauth2callback", f"Tokens received successfully. Keys: {list(tokens.keys())}")

        if 'access_token' not in tokens:
            log_error("calendar_tool", "oauth2callback", f"Access token *NOT* received for user {user_id}.")
            return HTMLResponse(content=html_error_template.format(details="Failed to obtain access token from Google."), status_code=500)
        if 'refresh_token' not in tokens:
            log_warning("calendar_tool", "oauth2callback", f"Refresh token *NOT* received for user {user_id}. Offline access might fail later.")

        # --- 2. Extract Email ---
        email = ""
        id_token = tokens.get("id_token")
        if id_token:
            try:
                decoded = jwt.decode(id_token, options={"verify_signature": False, "verify_aud": False})
                email = decoded.get("email", "")
                log_info("calendar_tool", "oauth2callback", f"Extracted email '{email}' for user {user_id}.")
            except jwt.exceptions.DecodeError as jwt_e:
                log_warning("calendar_tool", "oauth2callback", f"Failed decode id_token for {user_id}, proceeding without email.", jwt_e)

        # --- 3. Save Tokens Encrypted ---
        from tools.token_store import save_user_token_encrypted # Import locally
        if not save_user_token_encrypted(user_id, tokens):
             log_error("calendar_tool", "oauth2callback", f"Failed to save token via token_store for {user_id}.")
             return HTMLResponse(content=html_error_template.format(details="Failed to save credentials securely."), status_code=500)
        log_info("calendar_tool", "oauth2callback", f"Tokens stored successfully via token_store for {user_id}.")

        # --- 4. Update Preferences ---
        prefs_update = { "email": email, "Calendar_Enabled": True }
        pref_update_success = False
        # --- Use locally checked flag and function ---
        if config_manager_imported_locally and update_prefs_service:
            pref_update_success = update_prefs_service(user_id, prefs_update)
            if pref_update_success:
                 log_info("calendar_tool", "oauth2callback", f"Preferences updated via ConfigManager for {user_id}: {prefs_update}")
            else:
                 log_error("calendar_tool", "oauth2callback", f"ConfigManager failed update preferences for {user_id} after token save.")
        else:
             # Fallback to direct update
             # Warning already logged at module level
             pref_update_success = update_prefs_direct(user_id, prefs_update) # Use direct update
             if pref_update_success:
                  log_info("calendar_tool", "oauth2callback", f"Preferences updated DIRECTLY for {user_id}: {prefs_update}")
             else:
                  log_error("calendar_tool", "oauth2callback", f"Direct registry update failed for {user_id} after token save.")
        # -----------------------------------------

        if pref_update_success:
             return HTMLResponse(content=html_success_template, status_code=200)
        else:
             # Tokens saved, but profile update failed
             return HTMLResponse(content=html_error_template.format(details="Credentials saved, but failed to update user profile. Contact support."), status_code=500)

    except requests.exceptions.HTTPError as http_e:
        response_text = http_e.response.text; status_code = http_e.response.status_code
        error_details = f"Error {status_code} during token exchange.";
        try: error_json = http_e.response.json(); error_details = error_json.get('error_description', error_json.get('error', f"HTTP {status_code}"))
        except ValueError: pass
        log_error("calendar_tool", "oauth2callback", f"HTTP error {status_code} during token exchange for {user_id}. Details: {error_details}", http_e)
        return HTMLResponse(content=html_error_template.format(details=f"Could not get authorization from Google: {error_details}."), status_code=status_code)
    except Exception as e:
        log_error("calendar_tool", "oauth2callback", f"Generic unexpected error during callback for {user_id}", e)
        return HTMLResponse(content=html_error_template.format(details=f"An unexpected server error occurred: {e}."), status_code=500)

# --- END OF FULL tools/calendar_tool.py ---

# --- END OF FILE tools/calendar_tool.py ---



================================================================================
ðŸ“„ tools/token_store.py
================================================================================

# --- START OF FILE tools/token_store.py ---

# --- START OF FULL tools/token_store.py ---

import os
import json
from dotenv import load_dotenv # Added for .env loading
from tools.encryption import decrypt_data, encrypt_data # Ensure both are imported
from tools.logger import log_info, log_error, log_warning

# --- Load Environment Variables ---
# Load variables from .env file in the current directory or parent directories
load_dotenv()

# --- Configuration ---
# Get the data suffix (_cli or empty) from environment variables
DATA_SUFFIX = os.getenv("DATA_SUFFIX", "") # Default to empty string if not set

# Define the specific subdirectory for tokens
TOKEN_BASE_DIR = "data"
TOKEN_SUB_DIR = "tokens" # New subdirectory name
TOKEN_DIR_PATH = os.path.join(TOKEN_BASE_DIR, TOKEN_SUB_DIR)

# --- Helper Function to Construct Full Path ---
def _get_token_path(user_id: str) -> str:
    """Constructs the absolute path for a user's token file including the suffix."""
    # Construct filename with suffix: e.g., tokens_1234_cli.json.enc or tokens_1234.json.enc
    filename = f"tokens_{user_id}{DATA_SUFFIX}.json.enc"
    # Construct the relative path including the subdirectory
    relative_path = os.path.join(TOKEN_DIR_PATH, filename)
    # Return the absolute path
    return os.path.abspath(relative_path)

# --- Core Functions ---

def get_user_token(user_id: str) -> dict | None:
    """Loads and decrypts the user's token from the encrypted file in data/tokens."""
    fn_name = "get_user_token"
    absolute_path = _get_token_path(user_id) # Get the full path

    # Log the path being checked
    log_info("token_store", fn_name, f"Attempting to load token from: '{absolute_path}'")

    # Check if the file exists
    if not os.path.exists(absolute_path):
        log_info("token_store", fn_name, f"Token file not found for user {user_id} at '{absolute_path}'.")
        return None

    # Proceed if file exists
    log_info("token_store", fn_name, f"Token file found at '{absolute_path}'. Attempting to read and decrypt.")
    try:
        with open(absolute_path, "rb") as f:
            encrypted_data = f.read()

        # Decrypt the data
        token_data = decrypt_data(encrypted_data) # Assumes decrypt_data returns dict or None

        if token_data:
            log_info("token_store", fn_name, f"Successfully decrypted token data for user {user_id}. Keys: {list(token_data.keys())}")
            return token_data
        else:
            # decrypt_data should log its own errors
            log_error("token_store", fn_name, f"Decryption failed for token file: '{absolute_path}'. Check encryption logs.")
            return None

    except FileNotFoundError:
         # Should ideally not happen after os.path.exists, but handles race conditions
         log_info("token_store", fn_name, f"Token file disappeared before reading for user {user_id} at '{absolute_path}'.")
         return None
    except PermissionError as pe:
         log_error("token_store", fn_name, f"Permission denied reading token file '{absolute_path}'", pe)
         return None
    except Exception as e:
        log_error("token_store", fn_name, f"Unexpected error loading token from '{absolute_path}'", e)
        return None


def save_user_token_encrypted(user_id: str, token_data: dict) -> bool:
    """
    Encrypts and saves the user's token data to a file in data/tokens using atomic write.
    """
    fn_name = "save_user_token_encrypted"
    path = _get_token_path(user_id) # Get the full target path
    temp_path = path + ".tmp"      # Temporary file for atomic write

    log_info("token_store", fn_name, f"Attempting to save encrypted token for user {user_id} to '{path}'")

    try:
        # --- Data Validation ---
        # Ensure necessary tokens are present
        data_to_save = token_data.copy()
        # Handle potential 'token' key from oauth callback -> 'access_token'
        if 'token' in data_to_save and 'access_token' not in data_to_save:
            data_to_save['access_token'] = data_to_save.pop('token')

        if not data_to_save.get('access_token'):
            log_error("token_store", fn_name, f"Cannot save token: Missing 'access_token' for user {user_id}.")
            return False
        # It's critical for Google OAuth to have a refresh token for offline access
        if not data_to_save.get('refresh_token'):
            log_warning("token_store", fn_name, f"Saving token data missing 'refresh_token' for user {user_id}. Offline access/refresh will fail.")
        # -----------------------

        # Encrypt the validated data
        encrypted_tokens = encrypt_data(data_to_save)
        if not encrypted_tokens:
            # encrypt_data should log its own errors
            log_error("token_store", fn_name, f"Encryption failed for user {user_id}'s tokens during save.")
            return False

        # --- Atomic Write ---
        # Ensure the target directory exists (data/tokens/)
        os.makedirs(os.path.dirname(path), exist_ok=True)

        # Write to temporary file first
        with open(temp_path, "wb") as f:
            f.write(encrypted_tokens)

        # Atomically replace the old file with the new one
        os.replace(temp_path, path)
        # --------------------

        log_info("token_store", fn_name, f"Token stored successfully for {user_id} at '{path}'.")
        return True

    except PermissionError as pe:
        log_error("token_store", fn_name, f"Permission denied writing token file to '{path}' or temp file '{temp_path}'.", pe)
        return False
    except Exception as e:
        log_error("token_store", fn_name, f"Unexpected error saving token file to '{path}'", e)
        # Clean up temporary file if it exists after an error
        if os.path.exists(temp_path):
            try:
                os.remove(temp_path)
                log_info("token_store", fn_name, f"Removed temporary token file '{temp_path}' after error.")
            except OSError as rm_err:
                log_error("token_store", fn_name, f"Failed to remove temporary token file '{temp_path}' after error: {rm_err}")
        return False

# --- END OF FULL tools/token_store.py ---

# --- END OF FILE tools/token_store.py ---



================================================================================
ðŸ“„ tools/encryption.py
================================================================================

# --- START OF FILE tools/encryption.py ---

# tools/encryption.py

import os
import json
from cryptography.fernet import Fernet, InvalidToken
from tools.logger import log_error, log_info

# --- Key Management ---
# Load the encryption key from environment variables
# CRITICAL: This key MUST be kept secret and secure.
# Generate one using generate_key() below and set it as an environment variable.
ENCRYPTION_KEY_ENV_VAR = "ENCRYPTION_KEY"
_encryption_key = os.getenv(ENCRYPTION_KEY_ENV_VAR)

if not _encryption_key:
    log_error("encryption", "__init__",
              f"CRITICAL ERROR: Environment variable '{ENCRYPTION_KEY_ENV_VAR}' not set. Encryption disabled.")
    # You might want to raise an exception here to halt execution
    # raise ValueError(f"Environment variable '{ENCRYPTION_KEY_ENV_VAR}' is required for encryption.")
    _fernet = None
else:
    try:
        # Ensure the key is bytes
        _key_bytes = _encryption_key.encode('utf-8')
        _fernet = Fernet(_key_bytes)
        log_info("encryption", "__init__", "Fernet encryption service initialized successfully.")
    except Exception as e:
        log_error("encryption", "__init__", f"Failed to initialize Fernet. Invalid key format? Error: {e}", e)
        _fernet = None
        # raise ValueError(f"Invalid encryption key format: {e}") # Optional: Halt execution

# --- Encryption/Decryption Functions ---

def encrypt_data(data: dict) -> bytes | None:
    """
    Encrypts a dictionary using Fernet.

    Args:
        data: The dictionary to encrypt.

    Returns:
        Encrypted bytes if successful, None otherwise.
    """
    if not _fernet:
        log_error("encryption", "encrypt_data", "Encryption service not available (key missing or invalid).")
        return None
    try:
        # Serialize the dictionary to a JSON string, then encode to bytes
        data_bytes = json.dumps(data).encode('utf-8')
        encrypted_data = _fernet.encrypt(data_bytes)
        return encrypted_data
    except Exception as e:
        log_error("encryption", "encrypt_data", f"Encryption failed: {e}", e)
        return None

def decrypt_data(encrypted_data: bytes) -> dict | None:
    """
    Decrypts data encrypted with Fernet back into a dictionary.

    Args:
        encrypted_data: The encrypted bytes to decrypt.

    Returns:
        The original dictionary if successful, None otherwise.
    """
    if not _fernet:
        log_error("encryption", "decrypt_data", "Encryption service not available (key missing or invalid).")
        return None
    try:
        decrypted_bytes = _fernet.decrypt(encrypted_data)
        # Decode bytes back to JSON string, then parse into dictionary
        decrypted_json = decrypted_bytes.decode('utf-8')
        original_data = json.loads(decrypted_json)
        return original_data
    except InvalidToken:
        log_error("encryption", "decrypt_data", "Decryption failed: Invalid token (key mismatch or data corrupted).")
        return None
    except Exception as e:
        log_error("encryption", "decrypt_data", f"Decryption failed: {e}", e)
        return None

# --- Key Generation Utility ---

def generate_key() -> str:
    """Generates a new Fernet key (URL-safe base64 encoded)."""
    return Fernet.generate_key().decode('utf-8')

# Example usage for generating a key (run this file directly: python -m tools.encryption)
if __name__ == "__main__":
    new_key = generate_key()
    print("Generated Fernet Key (set this as your ENCRYPTION_KEY environment variable):")
    print(new_key)
    print("\nWARNING: Keep this key secure and secret!")

# --- END OF FILE tools/encryption.py ---



================================================================================
ðŸ“„ tools/logger.py
================================================================================

# --- START OF FILE tools/logger.py ---

# --- START OF FULL tools/logger.py ---
import os
import pytz
from datetime import datetime, timezone
import traceback
import json

# --- Database Logging Import ---
# REMOVE the direct import attempt from here
# import tools.activity_db as activity_db # REMOVE THIS LINE
ACTIVITY_DB_IMPORTED = False # Assume not imported initially
_activity_db_log_func = None # Placeholder for the function

# Try to import ONLY the function we need, later, within the logging calls
try:
    # This import happens when the logger *module* is loaded.
    # We want to defer accessing the function until it's actually called.
    pass # We will import inside the function call instead
except ImportError:
    print(f"[{datetime.now(timezone.utc).isoformat(timespec='seconds')+'Z'}] [ERROR] [logger:import_check] Failed initial check for activity_db module. DB logging disabled.")


# === Config ===
DEBUG_MODE = os.getenv("DEBUG_MODE", "True").lower() in ('true', '1', 't', 'yes', 'y')
LOG_DIR = "logs"
LOG_FILE = os.path.join(LOG_DIR, "whats_tasker.log")
LOG_TIMEZONE = pytz.utc

try:
    os.makedirs(LOG_DIR, exist_ok=True)
except OSError as e:
    print(f"[{datetime.now(timezone.utc).isoformat(timespec='seconds')+'Z'}] [ERROR] [logger:init] Failed to create log directory '{LOG_DIR}': {e}")

# === Helpers ===
def _timestamp():
    return datetime.now(LOG_TIMEZONE).isoformat(timespec='seconds').replace('+00:00', 'Z')

def _format_log_entry(level: str, module: str, func: str, message: str):
    ts = datetime.now(LOG_TIMEZONE).strftime("%Y-%m-%d %H:%M:%S %Z")
    return f"[{ts}] [{level}] [{module}:{func}] {message}"

# === Log functions ===
def log_info(module: str, func: str, message: str):
    """Logs informational messages. Prints only if DEBUG_MODE is True."""
    if DEBUG_MODE:
        entry = _format_log_entry("INFO", module, func, message)
        print(entry)
    # Optionally write INFO to file log even in production?
    # else:
    #     try:
    #         with open(LOG_FILE, "a", encoding="utf-8") as f:
    #             f.write(entry + "\n")
    #     except Exception: pass # Avoid errors during info logging

def _try_log_to_db(level: str, module: str, function: str, message: str, traceback_str: str | None = None, user_id_context: str | None = None, timestamp: str | None = None):
    """Internal helper to attempt DB logging dynamically."""
    global _activity_db_log_func, ACTIVITY_DB_IMPORTED

    # Try to import and get the function only once if not already done
    if not ACTIVITY_DB_IMPORTED and _activity_db_log_func is None:
        try:
            from tools.activity_db import log_system_event
            _activity_db_log_func = log_system_event
            ACTIVITY_DB_IMPORTED = True
            # Use internal print as logger might not be fully ready
            print(f"[{_timestamp()}] [INFO] [logger:_try_log_to_db] Successfully linked activity_db.log_system_event.")
        except ImportError:
            # This is expected if called before activity_db is importable
            # print(f"[{_timestamp()}] [INFO] [logger:_try_log_to_db] activity_db not yet available for import.")
            _activity_db_log_func = None # Ensure it's None
            ACTIVITY_DB_IMPORTED = False # Ensure flag is false
        except Exception as e:
            # Catch other potential import errors
             print(f"[{_timestamp()}] [ERROR] [logger:_try_log_to_db] Unexpected error linking activity_db.log_system_event: {e}")
             _activity_db_log_func = None
             ACTIVITY_DB_IMPORTED = False


    # If the function is available, call it
    if _activity_db_log_func:
        try:
            _activity_db_log_func(
                level=level,
                module=module,
                function=function,
                message=message,
                traceback_str=traceback_str,
                user_id_context=user_id_context,
                timestamp=timestamp # Use provided or let DB func generate
            )
        except Exception as db_log_err:
             # Fallback to print if the DB call fails *after* import succeeded
             ts_iso = timestamp or _timestamp()
             print(f"[{ts_iso}] [CRITICAL DB LOG FAIL] [{level}] [{module}:{function}] DB log failed: {db_log_err} | Original Msg: {message}")
    else:
        # Fallback print if DB import failed or function link failed
        ts_iso = timestamp or _timestamp()
        print(f"[{ts_iso}] [DB_LOG_SKIP] [{level}] [{module}:{function}] {message}")


def log_error(module: str, func: str, message: str, exception: Exception = None, user_id: str | None = None):
    """Logs error messages. Prints/logs to file AND attempts to log to database."""
    level = "ERROR"
    ts_iso = _timestamp()
    traceback_str = None
    if exception:
        traceback_str = traceback.format_exc()

    # Log to Console/File
    entry = _format_log_entry(level, module, func, message)
    if DEBUG_MODE:
        print(entry)
        if traceback_str: print(traceback_str)
    else:
        try:
            with open(LOG_FILE, "a", encoding="utf-8") as f:
                f.write(entry + "\n")
                if traceback_str: f.write(traceback_str + "\n")
        except Exception as file_log_e:
            print(f"CRITICAL: Failed to write ERROR to log file {LOG_FILE}: {file_log_e}")

    # Attempt to log to Database
    _try_log_to_db(level, module, func, message, traceback_str, user_id, ts_iso)


def log_warning(module: str, func: str, message: str, exception: Exception = None, user_id: str | None = None):
    """Logs warning messages. Prints/logs to file AND attempts to log to database."""
    level = "WARNING"
    ts_iso = _timestamp()
    traceback_str = None
    if exception:
        traceback_str = traceback.format_exc()

    # Log to Console/File
    entry = _format_log_entry(level, module, func, message)
    if DEBUG_MODE:
        print(entry)
        if traceback_str and exception: print(f"Warning Exception Info:\n{traceback_str}")
    else:
        try:
            with open(LOG_FILE, "a", encoding="utf-8") as f:
                f.write(entry + "\n")
                if traceback_str and exception: f.write(f"Warning Exception Info:\n{traceback_str}\n")
        except Exception as file_log_e:
            print(f"CRITICAL: Failed to write WARNING to log file {LOG_FILE}: {file_log_e}")

    # Attempt to log to Database
    _try_log_to_db(level, module, func, message, traceback_str, user_id, ts_iso)

# --- END OF FULL tools/logger.py ---

# --- END OF FILE tools/logger.py ---



================================================================================
ðŸ“„ tools/activity_db.py
================================================================================

# --- START OF FILE tools/activity_db.py ---

# --- START OF FULL tools/activity_db.py ---

import sqlite3
import os
import json
import threading
from datetime import datetime, timezone
from typing import Dict, List, Any, Tuple # <--- ADD Dict, List, Any, Tuple HERE
from tools.logger import log_info, log_error, log_warning

# --- Configuration ---
DATA_SUFFIX = os.getenv("DATA_SUFFIX", "")
DB_DIR = "data"
DB_FILE = os.path.join(DB_DIR, f"whatstasker_activity{DATA_SUFFIX}.db") # Dynamic DB file name
DB_LOCK = threading.Lock() # Lock for thread-safe writes if needed later

# --- Initialization ---
def init_db():
    """Initializes the database: creates directory, file, tables, and indexes if they don't exist."""
    fn_name = "init_db"
    try:
        os.makedirs(DB_DIR, exist_ok=True)
        log_info("activity_db", fn_name, f"Connecting to database: {DB_FILE}")
        # Use 'with' statement for automatic connection management
        with sqlite3.connect(DB_FILE, check_same_thread=False) as conn: # Allow access from different threads if needed by scheduler/web server
            cursor = conn.cursor()
            log_info("activity_db", fn_name, "Ensuring tables and indexes exist...")

            # === Create users_tasks Table ===
            cursor.execute("""
            CREATE TABLE IF NOT EXISTS users_tasks (
                event_id TEXT PRIMARY KEY NOT NULL,
                user_id TEXT NOT NULL,
                type TEXT NOT NULL,
                status TEXT NOT NULL,
                title TEXT NOT NULL,
                description TEXT,
                date TEXT,
                time TEXT,
                estimated_duration TEXT,
                sessions_planned INTEGER DEFAULT 0,
                sessions_completed INTEGER DEFAULT 0,
                progress_percent INTEGER DEFAULT 0,
                session_event_ids TEXT DEFAULT '[]',
                project TEXT,
                series_id TEXT,
                gcal_start_datetime TEXT,
                gcal_end_datetime TEXT,
                duration TEXT,
                created_at TEXT NOT NULL,
                completed_at TEXT,
                internal_reminder_sent TEXT,
                original_date TEXT,
                progress TEXT
            )
            """)
            # === Create Indexes for users_tasks ===
            cursor.execute("CREATE INDEX IF NOT EXISTS idx_tasks_user_id ON users_tasks (user_id)")
            cursor.execute("CREATE INDEX IF NOT EXISTS idx_tasks_user_id_status ON users_tasks (user_id, status)")
            cursor.execute("CREATE INDEX IF NOT EXISTS idx_tasks_user_id_date ON users_tasks (user_id, date)")
            cursor.execute("CREATE INDEX IF NOT EXISTS idx_tasks_project ON users_tasks (project)")
            cursor.execute("CREATE INDEX IF NOT EXISTS idx_tasks_status ON users_tasks (status)")

            # === Create messages Table ===
            cursor.execute("""
            CREATE TABLE IF NOT EXISTS messages (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                timestamp TEXT NOT NULL,
                user_id TEXT NOT NULL,
                direction TEXT NOT NULL,
                content TEXT NOT NULL,
                raw_user_id TEXT,
                bridge_message_id TEXT
            )
            """)
            cursor.execute("CREATE INDEX IF NOT EXISTS idx_messages_timestamp ON messages (timestamp)")
            cursor.execute("CREATE INDEX IF NOT EXISTS idx_messages_user_id ON messages (user_id)")

            # === Create llm_activity Table ===
            cursor.execute("""
            CREATE TABLE IF NOT EXISTS llm_activity (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                timestamp TEXT NOT NULL,
                user_id TEXT NOT NULL,
                agent_type TEXT NOT NULL,
                activity_type TEXT NOT NULL,
                tool_name TEXT,
                tool_call_id TEXT,
                content_summary TEXT,
                details_json TEXT
            )
            """)
            cursor.execute("CREATE INDEX IF NOT EXISTS idx_llm_activity_timestamp ON llm_activity (timestamp)")
            cursor.execute("CREATE INDEX IF NOT EXISTS idx_llm_activity_user_id ON llm_activity (user_id)")
            cursor.execute("CREATE INDEX IF NOT EXISTS idx_llm_activity_type ON llm_activity (activity_type)")

            # === Create system_logs Table ===
            cursor.execute("""
            CREATE TABLE IF NOT EXISTS system_logs (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                timestamp TEXT NOT NULL,
                level TEXT NOT NULL,
                module TEXT NOT NULL,
                function TEXT NOT NULL,
                message TEXT NOT NULL,
                traceback TEXT,
                user_id_context TEXT
            )
            """)
            cursor.execute("CREATE INDEX IF NOT EXISTS idx_system_logs_timestamp ON system_logs (timestamp)")
            cursor.execute("CREATE INDEX IF NOT EXISTS idx_system_logs_level ON system_logs (level)")

            conn.commit() # Commit table/index creations
            log_info("activity_db", fn_name, "Database initialization check complete.")

    except sqlite3.Error as e:
        log_error("activity_db", fn_name, f"Database initialization failed: {e}", e)
        raise # Re-raise critical error

# --- users_tasks Table Functions ---

# Define the expected fields for consistency (matches FIELDNAMES from metadata_store)
TASK_FIELDS = [
    "event_id", "user_id", "type", "status", "title", "description", "date", "time",
    "estimated_duration", "sessions_planned", "sessions_completed", "progress_percent",
    "session_event_ids", "project", "series_id", "gcal_start_datetime",
    "gcal_end_datetime", "duration", "created_at", "completed_at",
    "internal_reminder_sent", "original_date", "progress"
]

def _dict_factory(cursor, row):
    """Converts DB rows into dictionaries."""
    fields = [column[0] for column in cursor.description]
    return {key: value for key, value in zip(fields, row)}

def add_or_update_task(task_data: dict) -> bool:
    """Adds a new task or updates an existing one based on event_id."""
    fn_name = "add_or_update_task"
    event_id = task_data.get("event_id")
    if not event_id:
        log_error("activity_db", fn_name, "Cannot save task: 'event_id' is missing.")
        return False

    # Prepare data: ensure only valid fields, handle JSON, default numerics
    db_params = []
    placeholders = []
    columns = []
    for field in TASK_FIELDS:
        columns.append(field)
        value = task_data.get(field)
        if field == "session_event_ids":
            # Ensure it's a JSON string, default to '[]'
            if isinstance(value, (list, tuple)):
                value = json.dumps(value)
            elif not isinstance(value, str) or not value.strip():
                value = '[]'
        elif field in ["sessions_planned", "sessions_completed", "progress_percent"]:
            # Ensure it's an integer, default to 0
            try:
                value = int(value) if value is not None else 0
            except (ValueError, TypeError):
                value = 0
        # Convert None to NULL for DB, handle other types simply
        db_params.append(value)
        placeholders.append("?")

    sql = f"""
    INSERT INTO users_tasks ({', '.join(columns)})
    VALUES ({', '.join(placeholders)})
    ON CONFLICT(event_id) DO UPDATE SET
    {', '.join([f'{col}=excluded.{col}' for col in columns if col != 'event_id'])}
    """
    # Note: UPSERT syntax `ON CONFLICT...` requires SQLite 3.24.0+

    try:
        with DB_LOCK: # Use lock for write operations if needed later
            with sqlite3.connect(DB_FILE, check_same_thread=False) as conn:
                cursor = conn.cursor()
                cursor.execute(sql, db_params)
                conn.commit()
        log_info("activity_db", fn_name, f"Successfully added/updated task {event_id}")
        return True
    except sqlite3.Error as e:
        log_error("activity_db", fn_name, f"Database error saving task {event_id}: {e}", e)
        return False
    except Exception as e: # Catch other potential errors like JSON issues
        log_error("activity_db", fn_name, f"Unexpected error saving task {event_id}: {e}", e)
        return False

def get_task(event_id: str) -> dict | None:
    """Retrieves a single task by event_id."""
    fn_name = "get_task"
    sql = "SELECT * FROM users_tasks WHERE event_id = ?"
    try:
        with sqlite3.connect(DB_FILE, check_same_thread=False) as conn:
            conn.row_factory = _dict_factory # Return rows as dicts
            cursor = conn.cursor()
            cursor.execute(sql, (event_id,))
            row = cursor.fetchone()
            if row:
                # Decode JSON field
                try:
                    row['session_event_ids'] = json.loads(row.get('session_event_ids', '[]') or '[]')
                except (json.JSONDecodeError, TypeError):
                    log_warning("activity_db", fn_name, f"Failed to decode session_event_ids JSON for task {event_id}. Using empty list.")
                    row['session_event_ids'] = []
            return row # Returns dict or None if not found
    except sqlite3.Error as e:
        log_error("activity_db", fn_name, f"Database error getting task {event_id}: {e}", e)
        return None

def delete_task(event_id: str) -> bool:
    """Deletes a task by event_id."""
    fn_name = "delete_task"
    sql = "DELETE FROM users_tasks WHERE event_id = ?"
    try:
        with DB_LOCK:
            with sqlite3.connect(DB_FILE, check_same_thread=False) as conn:
                cursor = conn.cursor()
                cursor.execute(sql, (event_id,))
                conn.commit()
                if cursor.rowcount > 0:
                    log_info("activity_db", fn_name, f"Successfully deleted task {event_id}")
                    return True
                else:
                    log_warning("activity_db", fn_name, f"Task {event_id} not found for deletion.")
                    return False # Return False if not found
    except sqlite3.Error as e:
        log_error("activity_db", fn_name, f"Database error deleting task {event_id}: {e}", e)
        return False

def list_tasks_for_user(
    user_id: str,
    status_filter: list[str] | None = None, # Allows filtering by multiple statuses
    date_range: tuple[str, str] | None = None,
    project_filter: str | None = None
) -> list[dict]:
    """Lists tasks for a user, optionally filtering by status, date range, and project."""
    fn_name = "list_tasks_for_user"
    sql = "SELECT * FROM users_tasks WHERE user_id = ?"
    params = [user_id]
    conditions = []

    if status_filter:
        # Ensure status_filter is a list or tuple
        if isinstance(status_filter, str):
            status_filter = [status_filter] # Convert single string to list
        if isinstance(status_filter, (list, tuple)) and status_filter:
            # Sanitize statuses just in case
            clean_statuses = [s.lower().strip() for s in status_filter if isinstance(s, str)]
            if clean_statuses:
                 placeholders = ','.join('?' * len(clean_statuses))
                 conditions.append(f"status IN ({placeholders})")
                 params.extend(clean_statuses)
            else:
                 log_warning("activity_db", fn_name, f"Received empty or invalid status_filter list for user {user_id}.")
                 # Decide behavior: fetch all? fetch none? Let's fetch none to be safe.
                 return []

    # If NO status filter is provided, we fetch ALL statuses by default currently.
    # <<<--- This is where the change would happen if we altered the default

    if date_range and len(date_range) == 2:
        conditions.append("date BETWEEN ? AND ?")
        params.extend(date_range)

    if project_filter:
        conditions.append("LOWER(project) = LOWER(?)")
        params.append(project_filter)

    if conditions:
        sql += " AND " + " AND ".join(conditions)

    sql += " ORDER BY date, time"

    results = []
    try:
        with sqlite3.connect(DB_FILE, check_same_thread=False) as conn:
            conn.row_factory = _dict_factory
            cursor = conn.cursor()
            cursor.execute(sql, params)
            rows = cursor.fetchall()
            for row in rows:
                try:
                    row['session_event_ids'] = json.loads(row.get('session_event_ids', '[]') or '[]')
                except (json.JSONDecodeError, TypeError):
                    log_warning("activity_db", fn_name, f"Failed decode session_event_ids JSON for task {row.get('event_id')}. Using empty list.")
                    row['session_event_ids'] = []
                results.append(row)
        return results
    except sqlite3.Error as e:
        log_error("activity_db", fn_name, f"Database error listing tasks for user {user_id}: {e}", e)
        return []


# --- Logging Table Functions ---

def log_system_event(level: str, module: str, function: str, message: str, traceback_str: str | None = None, user_id_context: str | None = None, timestamp: str | None = None): # Added timestamp param
    """Logs errors and warnings to the system_logs table."""
    fn_name = "log_system_event"
    sql = """
    INSERT INTO system_logs (timestamp, level, module, function, message, traceback, user_id_context)
    VALUES (?, ?, ?, ?, ?, ?, ?)
    """
    ts = timestamp or datetime.now(timezone.utc).isoformat(timespec='seconds')+'Z' # Use provided or generate new
    params = (ts, level, module, function, message, traceback_str, user_id_context)
    try:
        with DB_LOCK:
            with sqlite3.connect(DB_FILE, check_same_thread=False) as conn:
                cursor = conn.cursor()
                cursor.execute(sql, params)
                conn.commit()
        # Avoid logging the log success itself to prevent loops
    except sqlite3.Error as e:
        # Fallback to print if DB logging fails
        print(f"CRITICAL DB LOG ERROR: {e} while logging: {params}")

def log_message_db(direction: str, user_id: str, content: str, raw_user_id: str | None = None, bridge_message_id: str | None = None):
    """Logs incoming/outgoing messages to the messages table."""
    fn_name = "log_message_db"
    sql = """
    INSERT INTO messages (timestamp, user_id, direction, content, raw_user_id, bridge_message_id)
    VALUES (?, ?, ?, ?, ?, ?)
    """
    ts = datetime.now(timezone.utc).isoformat(timespec='seconds')+'Z'
    params = (ts, user_id, direction, content, raw_user_id, bridge_message_id)
    try:
        with DB_LOCK:
            with sqlite3.connect(DB_FILE, check_same_thread=False) as conn:
                cursor = conn.cursor()
                cursor.execute(sql, params)
                conn.commit()
    except sqlite3.Error as e:
        print(f"CRITICAL DB LOG ERROR: {e} while logging message: {params}")

def log_llm_activity_db(user_id: str, agent_type: str, activity_type: str, tool_name: str | None = None, tool_call_id: str | None = None, content_summary: str | None = None, details: dict | list | None = None):
    """Logs LLM interactions to the llm_activity table."""
    fn_name = "log_llm_activity_db"
    sql = """
    INSERT INTO llm_activity
    (timestamp, user_id, agent_type, activity_type, tool_name, tool_call_id, content_summary, details_json)
    VALUES (?, ?, ?, ?, ?, ?, ?, ?)
    """
    ts = datetime.now(timezone.utc).isoformat(timespec='seconds')+'Z'
    details_str = None
    if isinstance(details, (dict, list)):
        try:
            details_str = json.dumps(details)
        except TypeError as json_err:
            log_warning("activity_db", fn_name, f"Could not serialize details to JSON for LLM activity: {json_err}. Details: {details}")
            details_str = json.dumps({"error": "Serialization failed", "original_type": str(type(details))})
    elif isinstance(details, str): # Allow passing pre-serialized JSON
        details_str = details

    params = (ts, user_id, agent_type, activity_type, tool_name, tool_call_id, content_summary, details_str)
    try:
        with DB_LOCK:
             with sqlite3.connect(DB_FILE, check_same_thread=False) as conn:
                 cursor = conn.cursor()
                 cursor.execute(sql, params)
                 conn.commit()
    except sqlite3.Error as e:
        print(f"CRITICAL DB LOG ERROR: {e} while logging LLM activity: {params}")

# --- Add this function ---
def update_task_fields(event_id: str, updates: Dict[str, Any]) -> bool:
    """Updates specific fields for a task given its event_id."""
    fn_name = "update_task_fields"
    if not event_id or not updates:
        log_warning("activity_db", fn_name, "Missing event_id or updates dictionary.")
        return False

    # Ensure we only try to update valid columns
    allowed_fields = {f for f in TASK_FIELDS if f != 'event_id'} # Cannot update primary key
    update_cols = []
    update_params = []
    for key, value in updates.items():
        if key in allowed_fields:
            update_cols.append(f"{key} = ?")
            # Handle potential JSON encoding for session_event_ids if needed
            if key == "session_event_ids" and isinstance(value, list):
                 update_params.append(json.dumps(value))
            else:
                 update_params.append(value)
        else:
            log_warning("activity_db", fn_name, f"Ignoring invalid field '{key}' in update for {event_id}.")

    if not update_cols:
        log_warning("activity_db", fn_name, f"No valid fields to update for {event_id}.")
        return False # Nothing to update

    update_params.append(event_id) # Add event_id for the WHERE clause
    sql = f"UPDATE users_tasks SET {', '.join(update_cols)} WHERE event_id = ?"

    try:
        with DB_LOCK:
            with sqlite3.connect(DB_FILE, check_same_thread=False) as conn:
                cursor = conn.cursor()
                cursor.execute(sql, update_params)
                conn.commit()
                if cursor.rowcount > 0:
                    log_info("activity_db", fn_name, f"Successfully updated fields {list(updates.keys())} for task {event_id}")
                    return True
                else:
                    # This could happen if the event_id doesn't exist
                    log_warning("activity_db", fn_name, f"Task {event_id} not found for update or no changes needed.")
                    return False # Return False if no row was updated
    except sqlite3.Error as e:
        log_error("activity_db", fn_name, f"Database error updating task {event_id}: {e}", e)
        return False
    except Exception as e:
        log_error("activity_db", fn_name, f"Unexpected error updating task {event_id}: {e}", e)
        return False

# --- End of added function ---


# --- Initialize DB on module load ---
init_db()

# --- END OF tools/activity_db.py ---

# --- END OF FILE tools/activity_db.py ---



================================================================================
ðŸ“„ users/user_manager.py
================================================================================

# --- START OF FILE users/user_manager.py ---

# --- START OF FILE users/user_manager.py ---

import os
import re
from datetime import datetime, timedelta
from typing import Dict, Any, Optional, List
import traceback
from tools.logger import log_info, log_error, log_warning
from users.user_registry import get_registry, register_user, get_user_preferences

# --- Database Import ---
try:
    import tools.activity_db as activity_db
    DB_IMPORTED = True
    log_info("user_manager", "import", "Successfully imported activity_db.")
except ImportError:
    DB_IMPORTED = False
    class activity_db:
        @staticmethod
        def list_tasks_for_user(*args, **kwargs): return []
    log_error("user_manager", "import", "activity_db not found. Task preloading disabled.")

# --- State Manager Import ---
try:
    from services.agent_state_manager import (
        register_agent_instance,
        get_agent_state,
        initialize_state_store
    )
    AGENT_STATE_MANAGER_IMPORTED = True
except ImportError:
     log_error("user_manager", "import", "AgentStateManager not found.")
     AGENT_STATE_MANAGER_IMPORTED = False
     _user_agents_in_memory: Dict[str, Dict[str, Any]] = {}
     def register_agent_instance(uid, state): _user_agents_in_memory[uid] = state
     def get_agent_state(uid): return _user_agents_in_memory.get(uid)
     def initialize_state_store(ref): global _user_agents_in_memory; _user_agents_in_memory = ref

# --- Service/Tool Imports ---
try:
    from tools.google_calendar_api import GoogleCalendarAPI
    GCAL_API_IMPORTED = True
except ImportError:
    GCAL_API_IMPORTED = False
    GoogleCalendarAPI = None
    log_warning("user_manager","import", "GoogleCalendarAPI not found.")

# --- Token Store Import (NEEDED FOR CHECK) ---
try:
    from tools.token_store import get_user_token
    TOKEN_STORE_IMPORTED = True
except ImportError:
     TOKEN_STORE_IMPORTED = False
     log_error("user_manager", "import", "Failed to import token_store.get_user_token. GCalAPI check will fail.")
     def get_user_token(*args, **kwargs): return None
# ---------------------------------------------

# --- In-Memory State Dictionary Reference ---
_user_agents_in_memory: Dict[str, Dict[str, Any]] = {}
if AGENT_STATE_MANAGER_IMPORTED:
    initialize_state_store(_user_agents_in_memory)


# --- Preload Context ---
def _preload_initial_context(user_id: str) -> list[dict]:
    """Loads initial context (all tasks) for a user from the SQLite database."""
    fn_name = "_preload_initial_context"
    log_info("user_manager", fn_name, f"Preloading initial context for {user_id} from activity_db.")

    if not DB_IMPORTED:
        log_error("user_manager", fn_name, "Database module not imported. Cannot preload context.")
        return []

    try:
        task_list = activity_db.list_tasks_for_user(user_id=user_id)
        log_info("user_manager", fn_name, f"Preloaded {len(task_list)} tasks from DB for {user_id}.")
        return task_list
    except Exception as e:
        log_error("user_manager", fn_name, f"Error preloading context for {user_id} from DB", e, user_id=user_id)
        return []


# --- Agent State Creation ---
def create_and_register_agent_state(user_id: str): # REMOVED TYPE HINT
    """Creates the full agent state dictionary and registers it."""
    fn_name = "create_and_register_agent_state"
    log_info("user_manager", fn_name, f"Creating FULL agent state for {user_id}")
    norm_user_id = re.sub(r'\D', '', user_id)
    if not norm_user_id:
        log_error("user_manager", fn_name, f"Invalid user_id after normalization: '{user_id}'")
        return None

    register_user(norm_user_id)
    preferences = get_user_preferences(norm_user_id)
    if not preferences:
        log_error("user_manager", fn_name, f"Failed to get/create prefs for {norm_user_id} after registration attempt.")
        return None

    # --- Refined GCal Initialization ---
    calendar_api_instance = None
    # Check if GCal enabled in prefs AND necessary libraries/functions are loaded
    if preferences.get("Calendar_Enabled") and GCAL_API_IMPORTED and GoogleCalendarAPI is not None and TOKEN_STORE_IMPORTED:
        # --- MODIFIED CHECK: Try loading token data ---
        token_data = get_user_token(norm_user_id)
        if token_data is not None:
            # --- END MODIFIED CHECK ---
            log_info("user_manager", fn_name, f"Valid token data found for {norm_user_id}. Attempting GCalAPI init.")
            temp_cal_api = None
            try:
                temp_cal_api = GoogleCalendarAPI(norm_user_id)
                if temp_cal_api.is_active():
                    calendar_api_instance = temp_cal_api
                    log_info("user_manager", fn_name, f"GCalAPI initialized and active for {norm_user_id}")
                else:
                    log_warning("user_manager", fn_name, f"GCalAPI initialized but NOT active for {norm_user_id}. Calendar features disabled.")
                    calendar_api_instance = None
            except Exception as cal_e:
                 tb_str = traceback.format_exc()
                 log_error("user_manager", fn_name, f"Exception during GCalAPI initialization or is_active() check for {norm_user_id}. Traceback:\n{tb_str}", cal_e)
                 calendar_api_instance = None
        else:
            # Token data not found (logged by get_user_token)
            log_warning("user_manager", fn_name, f"GCal enabled for {norm_user_id} but no valid token data found via token_store.")
    elif not preferences.get("Calendar_Enabled"):
         log_info("user_manager", fn_name, f"Calendar not enabled for {norm_user_id}, skipping GCal init.")
    # Log if libs were the issue
    elif not GCAL_API_IMPORTED:
         log_warning("user_manager", fn_name, f"GoogleCalendarAPI library not imported, skipping calendar init for {norm_user_id}.")
    elif not TOKEN_STORE_IMPORTED:
         log_warning("user_manager", fn_name, f"token_store not imported, skipping calendar init for {norm_user_id}.")
    # --- End Refined GCal Initialization ---

    initial_context = _preload_initial_context(norm_user_id)

    agent_state = {
        "user_id": norm_user_id,
        "preferences": preferences,
        "active_tasks_context": initial_context,
        "calendar": calendar_api_instance,
        "conversation_history": [],
        "notified_event_ids_today": set()
    }

    try:
        register_agent_instance(norm_user_id, agent_state)
        log_info("user_manager", fn_name, f"Successfully registered agent state for {norm_user_id}")
        return agent_state
    except Exception as e:
        log_error("user_manager", fn_name, f"Failed state registration for {norm_user_id}", e)
        return None

# --- Initialize All Agents ---
def init_all_agents():
    """Initializes states for all users found in the registry."""
    fn_name = "init_all_agents"
    log_info("user_manager", fn_name, "Initializing states for all registered users...")
    registry_data = get_registry()
    registered_users = list(registry_data.keys())
    initialized_count = 0
    failed_count = 0

    if not registered_users:
        log_info("user_manager", fn_name, "No users found in registry.")
        return

    log_info("user_manager", fn_name, f"Found {len(registered_users)} users. Initializing...")
    for user_id in registered_users:
        norm_user_id = re.sub(r'\D', '', user_id)
        if not norm_user_id:
             log_warning("user_manager", fn_name, f"Skipping invalid user_id found in registry: '{user_id}'")
             failed_count += 1
             continue
        try:
            created_state = create_and_register_agent_state(norm_user_id)
            if created_state:
                initialized_count += 1
            else:
                failed_count += 1
        except Exception as e:
            log_error("user_manager", fn_name, f"Unexpected error initializing agent state for user {norm_user_id}", e)
            failed_count += 1

    log_info("user_manager", fn_name, f"Agent state initialization complete. Success: {initialized_count}, Failed: {failed_count}")


# --- Get Agent State ---
def get_agent(user_id: str) -> Optional[Dict]:
    """Retrieves or creates and registers the agent state for a user."""
    fn_name = "get_agent"
    norm_user_id = re.sub(r'\D', '', user_id)
    if not norm_user_id:
         log_error("user_manager", fn_name, f"Cannot get agent state for invalid normalized user_id from '{user_id}'")
         return None

    agent_state = None
    try:
        agent_state = get_agent_state(norm_user_id)
        if not agent_state:
            log_warning("user_manager", fn_name, f"State for {norm_user_id} not in memory. Creating now.")
            agent_state = create_and_register_agent_state(norm_user_id)
    except Exception as e:
         log_error("user_manager", fn_name, f"Error retrieving/creating agent state for {norm_user_id}", e)
         agent_state = None

    return agent_state

# --- END OF FILE users/user_manager.py ---

# --- END OF FILE users/user_manager.py ---



================================================================================
ðŸ“„ users/user_registry.py
================================================================================

# --- START OF FILE users/user_registry.py ---

# --- START OF FILE users/user_registry.py ---

import json
import os
from datetime import datetime
from tools.logger import log_info, log_error, log_warning

DATA_SUFFIX = os.getenv("DATA_SUFFIX", "") # Default to empty for whatsapp mode
USER_REGISTRY_PATH = f"data/users/registry{DATA_SUFFIX}.json" # Dynamic path

# --- UPDATED Default Preferences ---
DEFAULT_PREFERENCES = {
    "status": "new", # 'new', 'onboarding', 'active'
    # Time & Scheduling Preferences
    "TimeZone": None, # REQUIRED during onboarding (e.g., "Asia/Jerusalem", "America/New_York")
    "Work_Start_Time": None, # REQUIRED during onboarding (HH:MM)
    "Work_End_Time": None,   # REQUIRED during onboarding (HH:MM)
    "Work_Days": ["Monday", "Tuesday", "Wednesday", "Thursday", "Friday"], # Default, modifiable
    "Preferred_Session_Length": None, # REQUIRED during onboarding (e.g., "60m", "1.5h")
    # Routine Preferences
    "Morning_Summary_Time": None , # User local time (HH:MM), default None
    "Evening_Summary_Time": None , # User local time (HH:MM), default None
    "Enable_Morning": True, # Default enabled if time is set
    "Enable_Evening": True, # Default enabled if time is set
    "Enable_Weekly_Reflection": False, # Future use
    # Notification Preferences (NEW)
    "Notification_Lead_Time": "15m", # Default lead time for event notifications
    # Calendar Integration
    "Calendar_Enabled": False, # Flag if GCal connected
    "Calendar_Type": "", # "Google" or potentially others later
    "email": "", # User's Google email (extracted during auth)
    "token_file": None, # Path to encrypted token file
    # Internal Tracking
    "Last_Sync": "", # ISO 8601 UTC timestamp (e.g., "2025-04-22T15:30:00Z")
    "last_morning_trigger_date": "", # YYYY-MM-DD string
    "last_evening_trigger_date": "", # YYYY-MM-DD string
    # Misc/Future Use
    "Holiday_Dates": [], # List of YYYY-MM-DD strings
}
# --- END OF UPDATED DEFAULT PREFERENCES ---


# Global in-memory registry variable.
_registry = {}

def load_registry():
    """Loads the registry from disk into memory."""
    global _registry
    if os.path.exists(USER_REGISTRY_PATH):
        try:
            with open(USER_REGISTRY_PATH, "r", encoding="utf-8") as f:
                # Handle empty file case
                content = f.read()
                if not content.strip():
                    _registry = {}
                else:
                    f.seek(0) # Go back to start if not empty
                    _registry = json.load(f)
            # Ensure existing users have all default keys
            updated_registry = False
            for user_id, user_data in _registry.items():
                 if "preferences" not in user_data:
                      user_data["preferences"] = DEFAULT_PREFERENCES.copy()
                      updated_registry = True
                 else:
                      for key, default_value in DEFAULT_PREFERENCES.items():
                           if key not in user_data["preferences"]:
                                user_data["preferences"][key] = default_value
                                updated_registry = True
            if updated_registry:
                 log_info("user_registry", "load_registry", "Added missing default preference keys to existing users.")
                 save_registry() # Save immediately if defaults were added

        except (json.JSONDecodeError, IOError) as e:
            log_error("user_registry", "load_registry", f"Failed to load or parse registry file {USER_REGISTRY_PATH}", e)
            _registry = {} # Fallback to empty registry on error
    else:
        _registry = {}
    log_info("user_registry", "load_registry", f"Registry loaded with {len(_registry)} users.")
    return _registry

def get_registry():
    """Returns the in-memory registry. Loads it if not already loaded."""
    global _registry
    # Check if registry is empty dictionary, load only if file exists
    if not _registry and os.path.exists(USER_REGISTRY_PATH):
         load_registry()
    # If still empty after attempt, it's genuinely empty or failed load
    return _registry

# Compatibility alias
def load_registered_users():
    return get_registry()

def save_registry():
    """Saves the current in-memory registry to disk."""
    global _registry
    try:
        # Ensure directory exists
        os.makedirs(os.path.dirname(USER_REGISTRY_PATH), exist_ok=True)
        # Use atomic write pattern
        temp_path = USER_REGISTRY_PATH + ".tmp"
        with open(temp_path, "w", encoding="utf-8") as f:
            json.dump(_registry, f, indent=2, ensure_ascii=False)
        os.replace(temp_path, USER_REGISTRY_PATH)
        # log_info("user_registry", "save_registry", "Registry saved to disk.") # Can be noisy
    except IOError as e:
        log_error("user_registry", "save_registry", f"Failed to write registry file {USER_REGISTRY_PATH}", e)
        if os.path.exists(temp_path):
            try: os.remove(temp_path)
            except OSError: pass
    except Exception as e:
        log_error("user_registry", "save_registry", f"Unexpected error saving registry", e)
        if os.path.exists(temp_path):
             try: os.remove(temp_path)
             except OSError: pass


def register_user(user_id):
    """Registers a new user with default preferences if not already present."""
    reg = get_registry() # Ensures registry is loaded
    if user_id not in reg:
        log_info("user_registry", "register_user", f"Registering new user {user_id}...")
        # Use deep copy to avoid modifying the original DEFAULT_PREFERENCES
        reg[user_id] = {"preferences": DEFAULT_PREFERENCES.copy()}
        save_registry() # Save after adding the new user
        log_info("user_registry", "register_user", f"Registered new user {user_id} with default preferences.")
    # else: User already exists, do nothing silently


def update_preferences(user_id, new_preferences):
    """Updates preferences for a given user and saves the registry."""
    reg = get_registry()
    if user_id in reg:
        # Ensure the preferences key exists and is a dict
        if not isinstance(reg[user_id].get("preferences"), dict):
             reg[user_id]["preferences"] = DEFAULT_PREFERENCES.copy()

        # Validate keys before updating? Optional, but good practice.
        valid_updates = {k: v for k, v in new_preferences.items() if k in DEFAULT_PREFERENCES}
        invalid_keys = set(new_preferences.keys()) - set(valid_updates.keys())
        if invalid_keys:
            log_warning("user_registry", "update_preferences", f"Ignoring invalid preference keys for user {user_id}: {invalid_keys}")

        if not valid_updates:
             log_warning("user_registry", "update_preferences", f"No valid preference keys provided for update for user {user_id}.")
             return False # Or True if ignoring invalid keys is considered success? Let's say False.

        reg[user_id]["preferences"].update(valid_updates)
        save_registry() # Save after updating
        log_info("user_registry", "update_preferences", f"Updated preferences for user {user_id}: {list(valid_updates.keys())}")
        return True
    else:
        log_error("user_registry", "update_preferences", f"User {user_id} not registered, cannot update preferences.")
        return False

def get_user_preferences(user_id):
    """Gets preferences for a user, returns None if user not found."""
    reg = get_registry()
    user_data = reg.get(user_id)
    if user_data:
        # Ensure preferences key exists and return a copy with all defaults ensured
        prefs = user_data.get("preferences", {})
        if not isinstance(prefs, dict):
             prefs = {} # Reset if not a dict

        # Create a copy of defaults, update with user's saved prefs
        # This ensures all keys exist in the returned dict
        full_prefs = DEFAULT_PREFERENCES.copy()
        full_prefs.update(prefs)
        return full_prefs
    else:
        return None

# Load registry into memory on module import.
load_registry()

# (Keep __main__ block for testing if desired)

# --- END OF FILE users/user_registry.py ---

# --- END OF FILE users/user_registry.py ---



================================================================================
ðŸ“„ tests/mock_browser_chat.py
================================================================================

# --- START OF FILE tests/mock_browser_chat.py ---

# tests/mock_browser_chat.py
# Simplified version using only print() for errors/info

import os
import requests
import json
import time
import threading
from flask import Flask, render_template, request, jsonify
from collections import deque
from datetime import datetime
from dotenv import load_dotenv
import logging # <--- Added missing import for logging module

# --- Load Environment Variables ---
load_dotenv()

# --- Configuration ---
VIEWER_PORT = int(os.getenv("VIEWER_PORT", "5001"))
MAX_MESSAGES = 100

# --- Main Backend Configuration ---
MAIN_BACKEND_PORT = os.getenv("PORT", "8001")
MAIN_BACKEND_BASE_URL = f"http://localhost:{MAIN_BACKEND_PORT}"
MAIN_BACKEND_INCOMING_URL = f"{MAIN_BACKEND_BASE_URL}/incoming"
MAIN_BACKEND_OUTGOING_URL = f"{MAIN_BACKEND_BASE_URL}/outgoing"
MAIN_BACKEND_ACK_URL = f"{MAIN_BACKEND_BASE_URL}/ack"

# --- Mock User ID ---
MOCK_USER_ID = os.getenv("MOCK_SENDER_DEFAULT_USER", "1234")

# --- In-memory message store (bot messages only) ---
message_store_bot = deque(maxlen=MAX_MESSAGES)
message_lock = threading.Lock()
_stop_polling_event = threading.Event()

# --- Flask App Setup ---
script_dir = os.path.dirname(os.path.abspath(__file__))
template_dir = os.path.join(script_dir, 'templates')
app = Flask(__name__, template_folder=template_dir)
app.secret_key = os.getenv("FLASK_SECRET_KEY", os.urandom(24))

# --- Background Polling Function (Simplified Output) ---
def poll_main_backend():
    """Polls the main backend for outgoing messages and sends ACKs. Prints only errors."""
    print(f"[Polling Thread] Started. Target: {MAIN_BACKEND_OUTGOING_URL}")
    session = requests.Session()
    connection_lost = False
    last_successful_poll = time.time()

    while not _stop_polling_event.is_set():
        try:
            res = session.get(MAIN_BACKEND_OUTGOING_URL, timeout=10)
            res.raise_for_status()
            if connection_lost: print("[Polling Thread] Connection restored."); connection_lost = False
            last_successful_poll = time.time()
            data = res.json()
            messages = data.get("messages", [])

            if messages:
                timestamp = datetime.now().strftime("%H:%M:%S")
                with message_lock:
                    for msg in reversed(messages):
                         message_content = msg.get('message', '[No message content]')
                         message_id = msg.get('message_id', f'nomockid-{time.time()}')
                         message_store_bot.append({
                             "sender": "bot", "timestamp": timestamp,
                             "content": message_content, "id": message_id
                         })
                         try:
                             ack_payload = {"message_id": message_id, "user_id": msg.get("user_id")}
                             ack_res = session.post(MAIN_BACKEND_ACK_URL, json=ack_payload, timeout=3)
                             if ack_res.status_code != 200:
                                 print(f"[Polling Thread WARNING] Failed ACK for {message_id}. Status: {ack_res.status_code}")
                         except Exception as ack_e:
                             print(f"[Polling Thread ERROR] Error sending ACK for {message_id}: {ack_e}")

        except requests.exceptions.Timeout:
            if not connection_lost and (time.time() - last_successful_poll > 30):
                print(f"[Polling Thread WARNING] Connection lost? Repeated timeouts polling {MAIN_BACKEND_OUTGOING_URL}.")
                connection_lost = True
        except requests.exceptions.RequestException as e:
             error_msg = f"Connection error polling {MAIN_BACKEND_OUTGOING_URL}: {e}"
             if isinstance(e, requests.exceptions.ConnectionError) and "actively refused it" in str(e).lower():
                  error_msg = f"Connection error polling {MAIN_BACKEND_OUTGOING_URL}: Target refused connection. Is main backend running?"
             if not connection_lost: print(f"[Polling Thread ERROR] {error_msg}"); connection_lost = True
        except json.JSONDecodeError as e:
             response_text_snippet = res.text[:100] if 'res' in locals() else 'N/A'
             print(f"[Polling Thread ERROR] Failed JSON decode from {MAIN_BACKEND_OUTGOING_URL}. Response: {response_text_snippet}. Error: {e}")
        except Exception as e:
            print(f"[Polling Thread ERROR] Unexpected error: {e}")
            connection_lost = True
        finally:
            sleep_time = 0.5 if not connection_lost else 2.0
            time.sleep(sleep_time)

    print("[Polling Thread] Stopped.")


# --- Flask Routes (Simplified Output) ---
@app.route('/')
def index():
    return render_template('browser_chat.html', title=f"WhatsTasker Chat (User: {MOCK_USER_ID})")

@app.route('/send_message', methods=['POST'])
def send_message():
    """Receives message from browser, forwards to main backend."""
    try:
        data = request.get_json()
        message_text = data.get('message')
        if not message_text: return jsonify({"status": "error", "message": "No message content"}), 400

        user_id_to_send = MOCK_USER_ID
        # print(f"Forwarding message from {user_id_to_send}: {message_text[:50]}...") # Removed info print

        backend_payload = {"user_id": user_id_to_send, "message": message_text}
        backend_timeout = 60

        try:
            response = requests.post(MAIN_BACKEND_INCOMING_URL, json=backend_payload, timeout=backend_timeout)
            response.raise_for_status()
            try:
                ack_data = response.json();
                if not ack_data.get("ack"): print(f"[Flask WARNING] Main backend response ({response.status_code}) missing ACK.")
            except ValueError: print(f"[Flask WARNING] Main backend response ({response.status_code}) not JSON.")
            return jsonify({"status": "ok", "message": "Forwarded to backend"}), 200

        except requests.exceptions.Timeout: print(f"[Flask ERROR] Timeout sending to {MAIN_BACKEND_INCOMING_URL}"); return jsonify({"status": "error", "message": f"Timeout sending to backend"}), 503
        except requests.exceptions.ConnectionError: print(f"[Flask ERROR] Conn refused by {MAIN_BACKEND_INCOMING_URL}"); return jsonify({"status": "error", "message": f"Connection refused by backend"}), 503
        except requests.exceptions.RequestException as e: print(f"[Flask ERROR] Failed forward to backend: {e}"); return jsonify({"status": "error", "message": f"Failed to forward: {e}"}), 500

    except Exception as e: print(f"[Flask ERROR] Error in /send_message: {e}"); return jsonify({"status": "error", "message": str(e)}), 500

@app.route('/get_messages')
def get_messages():
    """Provides ONLY BOT messages to the frontend."""
    with message_lock:
        bot_messages = list(message_store_bot)
    sorted_bot_messages = sorted(bot_messages, key=lambda x: x.get('timestamp', ''))
    return jsonify({"messages": sorted_bot_messages})

@app.route('/clear_messages', methods=['POST'])
def clear_messages():
    """Clears the server's BOT message store."""
    with message_lock:
        message_store_bot.clear()
    print("[Flask INFO] Browser chat BOT messages cleared on server.")
    return jsonify({"status": "ok"}), 200

# --- Main Execution ---
if __name__ == '__main__':
    print(f"--- Starting Mock Browser Chat Interface ---")
    print(f"Serving chat UI on: http://localhost:{VIEWER_PORT}")
    print(f"Acting as User ID:  {MOCK_USER_ID}")
    print(f"Talking to Backend: {MAIN_BACKEND_INCOMING_URL}")
    print(f"Polling Backend at: {MAIN_BACKEND_OUTGOING_URL}")
    print(f"--------------------------------------------")

    polling_thread = threading.Thread(target=poll_main_backend, daemon=True)
    polling_thread.start()

    try:
        # Disable Flask's default verbose logging by getting the logger
        log = logging.getLogger('werkzeug') # <--- Get Flask's logger
        log.setLevel(logging.WARNING)      # <--- Set level to WARNING

        app.run(host='0.0.0.0', port=VIEWER_PORT, debug=False, use_reloader=False)
    except KeyboardInterrupt:
        print("\nCtrl+C received, shutting down...")
    except Exception as e:
        print(f"[ERROR] Flask server crashed: {e}")
    finally:
        _stop_polling_event.set()
        print("Waiting for polling thread to stop...")
        polling_thread.join(timeout=2)
        print("Mock browser chat server stopped.")

# --- END OF FILE tests/mock_browser_chat.py ---



================================================================================
ðŸ“„ tests/templates/browser_chat.html
================================================================================

# --- START OF FILE tests/templates/browser_chat.html ---

<!-- tests/templates/browser_chat.html -->
<!doctype html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>{{ title }}</title>
    <style>
        /* Styles remain the same */
        body { font-family: sans-serif; margin: 0; padding: 0; display: flex; flex-direction: column; height: 100vh; background-color: #f4f4f4; }
        h1 { text-align: center; color: #333; margin: 10px 0; }
        #chat-container { flex-grow: 1; border: 1px solid #ccc; background-color: #fff; margin: 0 10px 10px 10px; overflow-y: auto; padding: 10px; }
        #messages { list-style-type: none; padding: 0; margin: 0; }
        #messages li { margin-bottom: 10px; padding: 8px; border-radius: 5px; word-wrap: break-word; max-width: 80%; clear: both; }
        #messages li.user { background-color: #dcf8c6; margin-left: auto; float: right; text-align: right; }
        #messages li.bot { background-color: #e0e0e0; margin-right: auto; float: left; text-align: left; }
        #messages li.system { background-color: #f0e68c; margin-left: auto; margin-right: auto; text-align: center; font-style: italic; color: #555; max-width: 90%; float: none; font-size: 0.9em;}
        #messages li[dir="rtl"] { text-align: right; }
        #messages li[dir="ltr"] { text-align: left; }
        .msg-meta { font-size: 0.8em; color: #888; display: block; margin-top: 4px; }
        .msg-content { white-space: pre-wrap; }
        #input-area { display: flex; padding: 10px; border-top: 1px solid #ccc; background-color: #eee; }
        #messageInput { flex-grow: 1; padding: 10px; border: 1px solid #ccc; border-radius: 3px; margin-right: 5px;}
        #sendButton { padding: 10px 15px; cursor: pointer; }
        #controls { text-align: right; padding: 0 10px 5px 0; font-size: 0.8em; }
    </style>
</head>
<body>

    <h1>{{ title }}</h1>
    <div id="controls">
        <button id="clearButton" title="Clear messages displayed in this browser window">Clear Display</button>
    </div>

    <div id="chat-container">
        <ul id="messages">
            <!-- Messages will be added dynamically -->
        </ul>
    </div>

    <div id="input-area">
        <input type="text" id="messageInput" placeholder="Type your message..." autocomplete="off">
        <button id="sendButton">Send</button>
    </div>

    <script>
        const messagesContainer = document.getElementById('messages');
        const messageInput = document.getElementById('messageInput');
        const sendButton = document.getElementById('sendButton');
        const clearButton = document.getElementById('clearButton');

        let displayedMessageIds = new Set(); // Track IDs shown in browser
        let isSending = false;
        let isFetching = false;

        function containsHebrew(text) {
            if (!text) return false;
            return /[\u0590-\u05FF]/.test(text);
        }

        // Function to add a single message object to the display UL
        function addMessageToDisplay(msg) {
             if (!msg || !msg.id || displayedMessageIds.has(msg.id)) {
                 return false; // Don't add if no message, no ID, or already displayed
             }

             const li = document.createElement('li');
             const senderClass = msg.sender || 'system';
             li.classList.add(senderClass);

             const isRtl = containsHebrew(msg.content);
             li.setAttribute('dir', isRtl ? 'rtl' : 'ltr');

             const contentSpan = document.createElement('span');
             contentSpan.className = 'msg-content';
             contentSpan.textContent = msg.content;

             const metaSpan = document.createElement('span');
             metaSpan.className = 'msg-meta';
             // Use sender from message object now
             metaSpan.textContent = `[${msg.timestamp}] ${senderClass.toUpperCase()}`;

             li.appendChild(contentSpan);
             li.appendChild(metaSpan);

             messagesContainer.appendChild(li);
             displayedMessageIds.add(msg.id); // Mark as displayed
             return true;
        }

        // Fetches ONLY BOT messages and adds them if not already displayed
        async function fetchAndUpdateMessages() {
            if (isFetching) return;
            isFetching = true;
            let addedNew = false;
             try {
                const response = await fetch('/get_messages'); // Fetches BOT messages from server store
                if (!response.ok) throw new Error(`HTTP error! status: ${response.status}`);
                const result = await response.json();
                const botMessages = result.messages || [];

                botMessages.forEach(msg => {
                    // addMessageToDisplay checks displayedMessageIds
                    if(addMessageToDisplay(msg)) {
                        addedNew = true;
                    }
                });

            } catch (error) {
                console.error('Error fetching messages:', error);
            } finally {
                 isFetching = false;
                 if (addedNew) {
                     messagesContainer.scrollTop = messagesContainer.scrollHeight;
                 }
             }
        }

       async function sendMessage() {
            const messageText = messageInput.value.trim();
            if (!messageText || isSending) return;
            isSending = true;
            sendButton.disabled = true;
            messageInput.disabled = true;

            // 1. Create and display user message OBJECT immediately
             const userTimestamp = new Date().toLocaleTimeString([], { hour: '2-digit', minute: '2-digit', second: '2-digit' });
             const localUserId = `user-${Date.now()}`;
             const userMsg = {
                 sender: 'user',
                 timestamp: userTimestamp,
                 content: messageText,
                 id: localUserId
             };
             if(addMessageToDisplay(userMsg)){ // Add user message to display
                 messagesContainer.scrollTop = messagesContainer.scrollHeight;
             }
             messageInput.value = '';

            // 2. Send message to viewer backend to forward to main backend
            try {
                const response = await fetch('/send_message', { // Send to viewer backend
                    method: 'POST',
                    headers: { 'Content-Type': 'application/json' },
                    body: JSON.stringify({ message: messageText })
                });
                if (!response.ok) {
                    const errorData = await response.json().catch(() => ({ message: response.statusText }));
                    console.error('Error sending message via viewer:', errorData.message);
                    // Add error message to display
                    addMessageToDisplay({ sender: 'system', timestamp: new Date().toLocaleTimeString(), content: `Error sending: ${errorData.message}`, id:`err-${Date.now()}`});
                    messagesContainer.scrollTop = messagesContainer.scrollHeight;
                }
                 // Bot response will arrive via the fetchAndUpdateMessages polling
            } catch (error) {
                console.error('Network error sending message via viewer:', error);
                 addMessageToDisplay({ sender: 'system', timestamp: new Date().toLocaleTimeString(), content: `Network Error: ${error}`, id:`neterr-${Date.now()}`});
                 messagesContainer.scrollTop = messagesContainer.scrollHeight;
            } finally {
                 isSending = false;
                 sendButton.disabled = false;
                 messageInput.disabled = false;
                 messageInput.focus();
            }
        }

       async function clearMessages() {
             displayedMessageIds.clear(); // Clear JS tracking
             messagesContainer.innerHTML = '<li>Clearing...</li>'; // Update display
            try {
                await fetch('/clear_messages', { method: 'POST' }); // Tell server to clear its bot store
                 messagesContainer.innerHTML = '<li>Messages cleared.</li>';
            } catch (error) {
                console.error('Error signaling viewer to clear messages:', error);
                messagesContainer.innerHTML = '<li>Error clearing messages.</li>';
            }
        }

        // Event Listeners
        sendButton.addEventListener('click', sendMessage);
        messageInput.addEventListener('keypress', (e) => {
            if (e.key === 'Enter') { sendMessage(); }
        });
        clearButton.addEventListener('click', clearMessages);

        // Fetch messages periodically
        setInterval(fetchAndUpdateMessages, 1500);

        // Initial fetch
        // No initial fetch needed, or fetch then clear display?
        // Let's start clean
        messagesContainer.innerHTML = '<li>Connecting...</li>'; // Initial message

    </script>

</body>
</html>

# --- END OF FILE tests/templates/browser_chat.html ---



================================================================================
ðŸ“¦ Node.js Dependencies Note
================================================================================

# The 'package.json' file lists Node.js dependencies.
# The 'package-lock.json' file (not included) locks specific versions.
# Run 'npm install' in the project root to install these dependencies (including whatsapp-web.js, axios, qrcode-terminal, dotenv, nodemailer).
# The 'node_modules/' directory containing the installed packages is NOT included in this dump.

