# WhatsTasker Project Code Dump (v0.8 Target - Browser Chat)
# Generated: 2025-05-13 11:25:32


================================================================================
üìÑ README.md
================================================================================

# --- START OF FILE README.md ---



# --- END OF FILE README.md ---



================================================================================
üìÑ WhatsTasker_PRD_08.txt
================================================================================

# --- START OF FILE WhatsTasker_PRD_08.txt ---

ÔªøWhatsTasker: Product Requirements (v0.8 - Orchestrator Focused)
üß≠ Overview
WhatsTasker is a personal productivity assistant designed for individuals seeking to manage their daily items (tasks, reminders, and todos), improve focus, and reduce procrastination through a conversational interface. Accessed via WhatsApp, it functions as an intelligent Time Management Expert, integrating directly with the user's primary calendar (initially Google Calendar for scheduled Tasks and timed Reminders) and utilizing an internal metadata store for enhanced tracking of all items. WhatsTasker aims to streamline daily planning, facilitate frictionless capture of intentions, and support helpful routines.
üåü Core System Goal
The system shall act as a proactive and intelligent Time Management Expert. It must understand user context (history, preferences, schedule, items), interpret requests accurately, facilitate effective item and schedule management, and anticipate user needs where feasible within the defined scope, moving beyond simple command execution.

üîë Core Functional Requirements
1. Multi-User Support & Identification:
   * Uniquely identify users via their WhatsApp phone number (user_id).
   * Maintain consistent user_id linkage across all system components.
2. Item & Reminder Management (Calendar-Based for some):
   * **Supported Item Types:** The system will support three primary item types:
      *   **Reminder:** An alert for a specific date and (optional) time. Timed Reminders are synchronized with the user's primary calendar. Untimed Reminders default to the user's work start time for listing purposes but do not create calendar events.
      *   **Task:** An actionable item requiring dedicated effort and time, for which the system helps schedule work sessions in the user's primary calendar. Tasks have a description, due date/timeframe, and estimated total duration.
      *   **ToDo:** A simple actionable item that the user wants to track, which may or may not have a due date and does not have system-scheduled calendar time. ToDos are managed within WhatsTasker's internal store.
   * Direct Calendar Integration: Synchronize confirmed **Tasks** (as scheduled work sessions) and timed **Reminders** with the user's designated primary calendar (initially Google Calendar). **ToDos** are not synchronized with the calendar.
   * Metadata Augmentation: Maintain an internal metadata store linked to calendar events (for Tasks/Reminders) or internal IDs (for ToDos and untimed Reminders) to track additional details crucial for system operation (e.g., item status [pending, in progress, completed, cancelled], type [task, reminder, todo], estimated effort [for tasks], project tags, timestamps, due dates).
3. Natural Language Interaction (via WhatsApp):
   * Conversational Interface: Enable users to interact using free-form, natural language commands and requests.
   * Contextual Understanding: Maintain and utilize conversational context (recent history, user state) to understand follow-up questions, resolve ambiguities, and provide relevant, coherent responses.
4. User Capabilities & Outcomes:
   * Onboarding: Guide new users through an initial setup process to configure essential preferences (e.g., work parameters, calendar connection authorization) and activate their account.
   * Item Capture & Classification:
      * Allow users to efficiently capture intentions (tasks, reminders, todos) via natural language.
      * Intelligently distinguish between simple **Reminders** (typically requiring only description and date/time), effortful **Tasks** (implying duration/effort and eligibility for scheduling work sessions), and basic **ToDos** (requiring only a description, with optional due date, but no scheduled work time).
      * Reliably extract key details: description, due date/time, type (task, reminder, or todo), and estimated effort (for tasks).
   * Item Modification: Enable users to accurately modify details of existing items (tasks, reminders, or todos) (e.g., description, due date/time, status, estimated effort for tasks) based on their requests.
   * Item Status Updates: Allow users to easily update the status of items (e.g., mark as completed, pending), including handling interactive replies to system-generated lists.
   * Item Viewing & Filtering: Enable users to view their items (tasks, reminders, and todos), with options to filter by relevant criteria such as date range, status (active, completed, etc.), or an associated project tag/label.
   * Clarification Handling: Intelligently request clarification from the user when input is ambiguous, incomplete, or conflicts with existing information, guiding the user towards providing necessary details.
   * Task Scheduling Assistance (for Type: Task only):
      * Proactively offer to find and schedule dedicated work time in the calendar for newly created Tasks (unless scheduling was part of the initial request).
      * Upon user request, propose suitable, potentially distributed, time slots for Tasks requiring work sessions, considering task requirements, user preferences, and calendar availability.
      * Allow users to easily confirm and book proposed work session slots into their calendar.
5. Automated Routines & Summaries:
   * Morning Check-in: Provide a concise overview of the user's upcoming day, including scheduled calendar events, due **Tasks**, **Reminders**, and **ToDos**. Goal: Enhance daily awareness and readiness.
   * Evening Review: Facilitate a quick end-of-day review:
      * Present **Tasks**, **Reminders**, and **ToDos** scheduled for or due that day.
      * Allow users to easily update the completion status of these items.
      * For **Tasks** identified as incomplete, prompt the user (optionally asking for a reason) and proactively offer to reschedule the task or its remaining work, potentially suggesting new time slots. Goal: Prevent tasks from being dropped and adapt planning.
   * 6. Fallback Operation:
   * Support basic item management (capture, status updates, viewing lists for Reminders, Tasks, and ToDos) even without calendar integration. Smart scheduling proposals for Tasks, calendar-based summaries for Tasks/timed Reminders, and rescheduling offers for Tasks will be unavailable in this mode.
üí° Future Capabilities (Beyond Initial Scope)
* Advanced Scheduling Intelligence (Optimal times, breaks, conflict handling)
* Proactive Time Management Advice & Goal Tracking
* Weekly Reflection Module (Structured prompts, insight storage)
* Recurring Item Support (Tasks, Reminders, ToDos)
* Advanced Daily Planning Assistance ("Just One Thing", backlog suggestions)
* Timed Pre-Event Notifications (Requires external scheduler) - *Applies to Reminders and scheduled Task sessions*
* Shared/Team Task Collaboration Features
* Expanded Calendar Integrations (Outlook, etc.)
* User Subscription Tiers & Monetization
‚ö†Ô∏è MVP Scope & Limitations (Initial Release)
* Calendar: Integration only with Google Calendar; one active calendar per user.
* Item Types: No built-in support for recurring items (tasks, reminders, todos).
* Collaboration: No shared items or team features.
* Proactivity: Limited to offering scheduling for new Tasks and rescheduling incomplete Tasks in the evening review. No broader unsolicited advice.
* Notifications: Timed pre-event notifications are in scope for Reminders and scheduled Task sessions.
* Reflection: Weekly reflection module is out of scope.
* Monetization: No user plans or payments.
‚úÖ Acceptance Criteria (High-Level)
* Users can successfully onboard, configure preferences, and connect their Google Calendar.
* The system correctly distinguishes between **Tasks**, **Reminders**, and **ToDos** based on user input.
* Users can reliably create, view (with date/status/project filters), update, and mark items (tasks, reminders, todos) as complete via natural language.
* The system proactively offers scheduling for newly created **Tasks**.
* The system can propose schedule slots for **Tasks** based on calendar availability and preferences, and book confirmed slots.
* The system provides functional Morning Check-in and Evening Review summaries, which include relevant **ToDos**.
* The Evening Review correctly identifies incomplete **Tasks** and offers rescheduling options.
* The system handles clarifications when user input is ambiguous.
* Basic reminder and ToDo functionality works without calendar integration. Task scheduling is degraded.

# --- END OF FILE WhatsTasker_PRD_08.txt ---



================================================================================
üìÑ WhatsTasker_SRS_08.txt
================================================================================

# --- START OF FILE WhatsTasker_SRS_08.txt ---

ÔªøWhatsTasker: SRS (v0.8k - WhatsApp Bridge & Active Scheduler)

**1. Goals & Principles**

*   **Primary Goal:** Create a WhatsApp-based personal productivity assistant that acts as an intelligent Time Management Expert. It should understand user context, facilitate management of **Reminders, Tasks, and ToDos** via natural language, and integrate seamlessly with the user's primary calendar (Google Calendar initially for scheduled Tasks and timed Reminders).
*   **Core Architecture:**
    *   **Interface Layer:** User interaction occurs via WhatsApp. An external **Node.js bridge (`wa_bridge.js`)** utilizes `whatsapp-web.js` to connect to WhatsApp, receive messages, and send messages. This bridge communicates with the Python backend via a dedicated FastAPI interface (`bridge/whatsapp_interface.py`). An alternative CLI interface (`bridge/cli_interface.py`) exists for debugging/testing. The active interface is selected at runtime.
    *   **Backend Entry Point:** The selected FastAPI interface (`whatsapp_interface.py` or `cli_interface.py`) receives incoming messages.
    *   **Routing Layer (`request_router.py`):** Receives messages from the active bridge API, normalizes user ID, determines user status (`new`, `onboarding`, `active`), and routes requests to the appropriate Agent or Cheat command handler.
    *   **Agent Layer (`OnboardingAgent`, `OrchestratorAgent`):** Central reasoning hubs using a **Pure LLM Control Flow**. Process user input within context, leverage Structured Tool Use (Instructor/Pydantic) to delegate actions to Tools. Manage conversational state and response generation based on LLM decisions and tool results.
    *   **Tool Layer (`tool_definitions.py`):** Defines Pydantic models for tool parameters and Python functions that act as validated interfaces to the Service Layer. Tools are designed to create/manage distinct item types: **Reminders, Tasks, and ToDos.**
    *   **Service Layer (`TaskManager`, `TaskQueryService`, `ConfigManager`, `RoutineService`, `NotificationService`, `SyncService`, `Cheats`):** Encapsulates business logic, data manipulation, and interactions with persistent stores (SQLite via `activity_db.py`) and external APIs (GCal). Invoked by Tools or scheduled jobs. `SyncService` provides context merging and updates WhatsTasker item metadata if corresponding GCal data has changed.
    *   **Scheduler (`scheduler_service.py`):** An APScheduler instance runs background jobs (`RoutineService` checks, `NotificationService` checks, daily cleanup) at configured intervals.
    *   **Pure LLM Control Flow:** Agents rely **entirely on the LLM** (guided by specific system prompts) to manage conversational state, ask clarifying questions, interpret user replies, decide which tool to call (if any), and formulate responses based on history and tool results. Python code primarily executes validated tool calls requested by the LLM.
    *   **Two-Step LLM Interaction (with Tools):** Standard pattern used by Agents when a tool is invoked (LLM plans -> Tool executes -> LLM responds based on result).
*   **Modularity & Reusability:** Components are clearly separated. Services contain reusable business logic. Tools provide reliable interfaces.
*   **Reliability & Maintainability:** Prioritize reliable execution via structured Tool Use and Pydantic validation. Code includes type hinting and clear documentation. Conversational logic resides primarily within the LLM prompts. `whatsapp-web.js` dependency introduces external volatility.
*   **LLM Interaction:** Utilizes OpenAI's Tool Use capabilities via Instructor/Pydantic. `propose_task_slots` tool uses a focused LLM sub-call.
*   **Item Types & Definitions:**
    *   **Reminder:** An alert for a specific date and optional time. Timed Reminders are synchronized with the user's primary calendar as short events. Untimed Reminders default to the user's work start time for listing purposes within WhatsTasker and do not create calendar events. Created via a dedicated tool or flow.
    *   **Task:** An actionable item requiring dedicated effort and time, for which the system helps schedule work sessions in the user's primary calendar. Tasks have a description, due date/timeframe, and estimated total duration. Work sessions are booked as calendar events. Created via a dedicated tool or flow.
    *   **ToDo:** A simple actionable item for tracking, which may or may not have a due date and does not have system-scheduled calendar time. ToDos are managed within WhatsTasker's internal store and do not create calendar events. Created via a dedicated tool or flow (e.g., when scheduling help for a task is declined).
*   **Data Handling & State:**
    *   Persistence: **Primary Item Store (SQLite via `activity_db.py`)** for Reminders, Tasks, ToDos, messages, and system logs. User Registry (JSON via `user_registry.py`), Encrypted Token Store (`token_store.py`).
    *   User Status: Tracked via `status` in preferences (`new`, `onboarding`, `active`), managed by `user_registry.py` and `config_manager.py`. Dictates routing in `request_router.py`.
    *   Runtime State: Central, thread-safe `AgentStateManager` manages in-memory state (preferences, history, item context snapshot, API clients, notification tracking). Loaded/created via `user_manager.py`.
    *   Context Provision: Relevant context loaded via `AgentStateManager` and provided to the appropriate Agent. Routines use `SyncService` for context.
    *   State Updates: Services update persistent stores and signal updates to in-memory state via `AgentStateManager`.
    *   Synchronization (`SyncService`): Merges GCal events with WhatsTasker items (from DB) for context snapshots. **If a WhatsTasker-managed item (Task or Reminder linked to GCal) has different data in GCal (e.g., changed time, title), `SyncService` updates the corresponding record in the WhatsTasker database (`activity_db.py`).** External GCal events not originating from WhatsTasker are included in the context snapshot but not persisted as new items in the WhatsTasker DB by `SyncService`.

*   **Error Handling:** Tools/Services return status/messages. Pydantic validates tool parameters. Agents interpret tool failures via LLM. Bridge interface handles basic connection/ACK errors.

**2. Architecture Overview**
(Diagram concept remains similar, with the understanding that "tasks" now encompass "Reminders, Tasks, and ToDos" at a high level, with services handling their specific behaviors.)
1.  **External Bridge (`wa_bridge.js`):** (No change)
2.  **Interface Layer (Python - `whatsapp_interface.py` or `cli_interface.py`):** (No change)
3.  **Routing Layer (`request_router.py`):** (No change in core routing logic based on status)
4.  **Agent Layer (`onboarding_agent.py` or `orchestrator_agent.py`):**
    *   Receives user message and context snapshot. Loads system prompt. Defines available Tools.
    *   LLM Call 1 (Planner): Sends context, history, message, tools to LLM. LLM decides: Respond directly or call tool(s) (e.g., to create a Reminder, Task, or ToDo).
5.  **Execution Layer:**
    *   **If LLM Responds Directly:** Agent uses text.
    *   **If LLM Calls Tool(s):**
        *   Agent receives tool call request(s). Validates. Calls corresponding Tool function (`tool_definitions.py`).
        *   Tool interacts with **Service Layer**.
        *   Services perform logic (e.g., `TaskManager` creating a `type: "todo"` item in the DB), interact with **Data Layer** and external APIs. Update persistent data & signal memory state updates via `AgentStateManager`.
        *   Tool returns result dict (`{"success": bool, ...}`).
6.  **Agent Layer (Response Generation):** (No change)
7.  **Response Flow:** (No change)
8.  **Scheduled Tasks (`scheduler_service.py`):**
    *   Runs independently via APScheduler.
    *   Triggers jobs in `NotificationService` (check events for Reminders and Task sessions) and `RoutineService` (check triggers, daily cleanup).
    *   Triggered routines (`check_routine_triggers`) get context via `SyncService` (which includes Reminders, Tasks, ToDos, and external GCal events), generate messages, and return them to the scheduler wrapper.
    *   Scheduler wrapper calls `request_router.send_message` to queue generated routine messages.

**3. Module & Function Breakdown**
(Module list remains largely the same. Key changes are in the *functionality* within these modules, e.g., `tool_definitions.py` and `task_manager.py` now need to handle the three distinct item types.)

*   **External Components:**
    *   `wa_bridge.js`
*   **Core Infrastructure:**
    *   `main.py`
    *   `bridge/whatsapp_interface.py`, `bridge/cli_interface.py`, `bridge/request_router.py`
    *   `tools/logger.py`, `tools/encryption.py`, `tools/token_store.py`, `tools/activity_db.py` (replaces `metadata_store.py`), `tools/google_calendar_api.py`, `tools/calendar_tool.py`
    *   `users/user_registry.py`, `users/user_manager.py`
    *   `services/agent_state_manager.py`
*   **Service Layer:**
    *   `services/task_manager.py`: Core logic for creating, updating, and managing **Reminders, Tasks, and ToDos**, including scheduling sessions for Tasks. Interacts with `activity_db.py` and `GoogleCalendarAPI`.
    *   `services/config_manager.py`
    *   `services/task_query_service.py`: Data retrieval and formatting logic for **all item types** from `activity_db.py`.
    *   `services/cheats.py`
    *   `services/llm_interface.py`
    *   `services/sync_service.py`: Provides merged context snapshots (WhatsTasker items from DB + GCal events). **Updates WhatsTasker item metadata in DB if corresponding GCal data has changed.**
    *   `services/scheduler_service.py`
    *   `services/notification_service.py`: Logic for checking and formatting event notifications (for timed Reminders and scheduled Task sessions).
    *   `services/routine_service.py`: Logic for checking routine triggers (morning/evening) and generating summaries (which now include **ToDos**). Includes daily cleanup.
*   **Agent Layer:**
    *   `agents/orchestrator_agent.py`: Central reasoning hub for **active** users. Its prompt and logic must distinguish and handle creation/management of **Reminders, Tasks, and ToDos**.
    *   `agents/onboarding_agent.py`: Central reasoning hub for **onboarding** users.
    *   `agents/tool_definitions.py`: Pydantic models and Python functions for the toolset. Tools will exist to specifically handle `create_reminder`, `create_task` (for schedulable Tasks), and `create_todo`. The existing `create_task` tool might be adapted or a new one added for ToDos. The `update_item_details` and `update_item_status` tools will operate on any item type based on its ID.
*   **(Obsolete Files):** `metadata_store.py` (functionality absorbed by `activity_db.py`), `agents/intention_agent.py`, etc. (as before).

**4. Configuration Files**
*   `config/prompts.yaml`: Contains system prompts. `orchestrator_agent_system_prompt` must be updated to reflect the three item types and how the LLM should differentiate user intents to call the correct creation tools (e.g., for a Reminder, Task, or ToDo).
*   `config/messages.yaml`
*   `.env`

**5. Key Considerations / Future Work**
*   **Prompt Engineering:** Critical for the `OrchestratorAgent` to accurately differentiate between Reminders, Tasks, and ToDos based on natural language input and guide the user appropriately if information is missing for a specific type (e.g., asking for duration if it seems like a Task).
*   **Tooling for ToDos:** Ensure `tool_definitions.py` and `task_manager.py` clearly support creating items with `type="todo"`. This likely means:
    *   Modifying the existing `create_task_tool` and its underlying `task_manager.create_task` function to accept an explicit `item_type` parameter (e.g., "task" or "todo") driven by the LLM's decision.
    *   Or, introducing a distinct `create_todo_tool`.
*   **Evening Review Logic:** `routine_service.generate_evening_review` needs to be updated to include active/due ToDos from the context.
*   **WhatsApp Bridge Stability:** (No change)
*   **Deployment Complexity:** (No change)
*   **Error Handling:** (No change)
*   **Security:** (No change)
*   **Scalability:** (No change)
*   **Synchronization (`SyncService`):** Behavior clarified: updates WT item metadata in DB if GCal data for that item changes. Full two-way sync for *newly discovered external GCal events* or *creating new GCal events from all WT items* is still deferred.
*   **Testing:** End-to-end testing must cover the correct classification and handling of all three item types, including their representation in summaries and calendar interactions (or lack thereof for ToDos).

# --- END OF FILE WhatsTasker_SRS_08.txt ---



================================================================================
üìÑ requirements.txt
================================================================================

# --- START OF FILE requirements.txt ---

# --- START OF FILE requirements.txt ---
# Web Framework & Server
fastapi>=0.110.0,<0.112.0
uvicorn[standard]>=0.29.0,<0.30.0

# Langchain Core & OpenAI Integration
langchain>=0.1.16,<0.2.0
langchain-core>=0.1.40,<0.2.0
langchain-openai>=0.1.3,<0.2.0

# Google API Libraries
google-api-python-client>=2.120.0,<3.0.0
google-auth-oauthlib>=1.2.0,<2.0.0
google-auth>=2.29.0,<3.0.0

# Configuration & Environment
python-dotenv>=1.0.1,<2.0.0
PyYAML>=6.0.1,<7.0.0

# Utilities
requests>=2.31.0,<3.0.0
pytz>=2024.1
cryptography>=42.0.0,<43.0.0
PyJWT>=2.8.0,<3.0.0

# Pydantic (Core dependency for FastAPI & Langchain)
pydantic>=2.7.0,<3.0.0

# --- ADDED FOR SCHEDULING ---
APScheduler>=3.10.0,<4.0.0
# --------------------------
instructor>=0.5.2,<1.0.0  # Or similar version specifier
openai>=1.0.0,<2.0.0     # Instructor depends on OpenAI v1+
# Optional: If using pandas checks (e.g., pd.isna) - uncomment if needed
# pandas>=2.0.0,<3.0.0

Flask>=2.0.0,<4.0.0
# --- END OF FILE requirements.txt ---

# --- END OF FILE requirements.txt ---



================================================================================
üìÑ package.json
================================================================================

# --- START OF FILE package.json ---

{
  "dependencies": {
    "axios": "^1.8.4",
    "qrcode-terminal": "^0.12.0",
    "whatsapp-web.js": "^1.27.0"
  }
}


# --- END OF FILE package.json ---



================================================================================
üìÑ .gitignore
================================================================================

# --- START OF FILE .gitignore ---

# --- Python ---
# Virtual Environments
venv/
.venv/
env/
ENV/
# Compiled Python files
*.pyc
__pycache__/
# Distribution / packaging
.Python
build/
develop-eggs/
dist/
downloads/
eggs/
.eggs/
lib/
lib64/
parts/
sdist/
var/
wheels/
share/python-wheels/
*.egg-info/
.installed.cfg
*.egg
MANIFEST
# PyInstaller
*.manifest
*.spec
# Installer logs
pip-log.txt
pip-delete-this-directory.txt
# Unit test / coverage reports
htmlcov/
.tox/
.nox/
.coverage
.coverage.*
.cache
nosetests.xml
coverage.xml
*.cover
*.py,cover
.hypothesis/
.pytest_cache/
cover/
# Jupyter Notebook
.ipynb_checkpoints

# --- Node.js ---
# Logs
logs
*.log
npm-debug.log*
yarn-debug.log*
yarn-error.log*
pnpm-debug.log*
lerna-debug.log*
# Diagnostic reports (https://nodejs.org/api/report.html)
report.[0-9]*.[0-9]*.[0-9]*.[0-9]*.json
# Dependency directories
node_modules/
jspm_packages/
# Snowpack dependency directory (https://snowpack.dev/)
web_modules/
# Build folder
build/
dist/
# TypeScript cache
*.tsbuildinfo
# Optional eslint cache
.eslintcache
# Optional stylelint cache
.stylelintcache
# Output of 'npm pack'
*.tgz

# --- WhatsApp Web JS ---
.wwebjs_auth/
.wwebjs_cache/
# --- Project Specific ---
# Environment variables (NEVER commit secrets)
.env
# Data files (User registry, tokens, database - contains user data/secrets)
data/
# Log files generated by the application
logs/
# Output files from tests/scripts
startup_error.log
mock_output.txt
*.dump*.txt
# Simple viewer SQLite DB if it creates one (unlikely)
viewer_messages.db
# Temporary files
*.tmp

# --- IDE/Editor ---
# VS Code
.vscode/*
!.vscode/settings.json
!.vscode/tasks.json
!.vscode/launch.json
!.vscode/extensions.json
.history/
# PyCharm
.idea/
*.iml

# --- OS Specific ---
# macOS
.DS_Store
# Windows
Thumbs.db
ehthumbs.db
Desktop.ini
last_*.txt


# --- END OF FILE .gitignore ---



================================================================================
üìÑ wa_bridge.js
================================================================================

# --- START OF FILE wa_bridge.js ---

// whatsapp_bridge.js

// --- Dependencies ---
const { Client, LocalAuth, MessageMedia } = require('whatsapp-web.js'); // Added MessageMedia just in case, not used yet
const qrcode = require('qrcode-terminal');
const axios = require('axios');
require('dotenv').config(); // For environment variables if you add a .env file later

// --- Configuration (Consider moving more to .env later) ---
const FASTAPI_BASE_URL = process.env.FASTAPI_BASE_URL || 'http://localhost:8000';
const POLLING_INTERVAL_MS = parseInt(process.env.POLLING_INTERVAL_MS, 10) || 1000;
const RETRY_INTERVAL_MS = parseInt(process.env.RETRY_INTERVAL_MS, 10) || 5000; // For backend connection retries
const MAX_SEND_RETRIES_PER_MESSAGE = parseInt(process.env.MAX_SEND_RETRIES_PER_MESSAGE, 10) || 3;
const SEND_RETRY_DELAY_MS = parseInt(process.env.SEND_RETRY_DELAY_MS, 10) || 2000;
const MAX_ACK_RETRIES = parseInt(process.env.MAX_ACK_RETRIES, 10) || 3;
const ACK_RETRY_DELAY_MS = parseInt(process.env.ACK_RETRY_DELAY_MS, 10) || 2000;
const CONSECUTIVE_POLLING_ERROR_THRESHOLD = parseInt(process.env.CONSECUTIVE_POLLING_ERROR_THRESHOLD, 10) || 10; // Exit after 10 consecutive polling errors

// --- State Variables ---
let isClientReady = false;
let consecutivePollingErrors = 0;
let clientInstance; // To store the client for access in shutdown handler

// --- Custom Logger ---
function logMessage(level, message, ...optionalParams) {
    const now = new Date();
    const options = { timeZone: 'Asia/Jerusalem', hour12: false, year: 'numeric', month: '2-digit', day: '2-digit', hour: '2-digit', minute: '2-digit', second: '2-digit', fractionalSecondDigits: 3 };
    const timestamp = now.toLocaleString('en-CA', options).replace(/, /g, ' ').replace(/\//g, '-'); // YYYY-MM-DD HH:MM:SS.mmm
    const levelStr = level.toUpperCase();
    
    // Construct the log prefix including the [PID]
    const logPrefix = `[${timestamp} IDT] [PID:${process.pid}] [${levelStr}]`;

    if (levelStr === 'ERROR' || levelStr === 'WARN') {
        console[level.toLowerCase() || 'log'](logPrefix, message, ...optionalParams);
    } else {
        console.log(logPrefix, message, ...optionalParams);
    }
}


// --- Utility: Sleep Function ---
function sleep(ms) {
    return new Promise(resolve => setTimeout(resolve, ms));
}

// --- Initialize the WhatsApp client ---
const client = new Client({
    authStrategy: new LocalAuth({dataPath: '.wwebjs_auth'}), // Specify dataPath
    puppeteer: {
        headless: true,
        args: [
            '--no-sandbox',
            '--disable-setuid-sandbox',
            '--disable-dev-shm-usage', // Often helps in constrained environments
            '--disable-accelerated-2d-canvas',
            '--no-first-run',
            '--no-zygote',
            // '--single-process', // Disables GPU sandbox, use with caution if other methods fail
            '--disable-gpu'
        ]
    },
    // Increase takeover timeout if needed, default is 60s
    // takeoverTimeoutMs: 120000, 
});
clientInstance = client; // Store for shutdown handler

// --- Event Handlers ---
client.on('qr', (qr) => {
    logMessage('INFO', 'QR Code Received. Scan with WhatsApp:');
    qrcode.generate(qr, { small: true });
});

client.on('ready', async () => {
    logMessage('INFO', `WhatsApp client is ready! Logged in as: ${client.info.pushname} (${client.info.wid.user})`);
    isClientReady = true;
    consecutivePollingErrors = 0; // Reset error counter on successful ready
    await pollForOutgoingMessages(); // Start polling
});

client.on('authenticated', () => {
    logMessage('INFO', 'WhatsApp client authenticated successfully.');
});

client.on('auth_failure', msg => {
    logMessage('ERROR', 'AUTHENTICATION FAILURE:', msg);
    isClientReady = false;
    logMessage('ERROR', 'Exiting due to authentication failure.');
    process.exit(1); // Critical failure, exit
});

client.on('disconnected', (reason) => {
    logMessage('WARN', 'Client was logged out/disconnected:', reason);
    isClientReady = false;
    logMessage('ERROR', 'Exiting due to disconnection.');
    process.exit(1); // Critical failure, exit
});

client.on('loading_screen', (percent, message) => {
    logMessage('INFO', `Loading WhatsApp Web: ${percent}% - ${message}`);
});

client.on('change_state', state => {
    logMessage('INFO', `WhatsApp client state changed: ${state}`);
});

client.on('error', err => {
    logMessage('ERROR', 'Unhandled WhatsApp client error:', err);
    // Consider exiting if error is critical, e.g., related to Puppeteer crashing
    if (err.message && (err.message.includes('Protocol error') || err.message.includes('Page crashed'))) {
        logMessage('ERROR', 'Critical Puppeteer/Protocol error detected. Exiting.');
        process.exit(1);
    }
});

// Listen for incoming messages
client.on('message', async (message) => {
    if (!isClientReady) {
        logMessage('WARN', `Ignoring incoming message from ${message.from} (client not ready).`);
        return;
    }
    if (message.isStatus) { // Ignore status updates
        logMessage('DEBUG', `Ignoring status update from ${message.from}.`);
        return;
    }
    if (message.type === 'revoked') { // Ignore revoked messages
        logMessage('DEBUG', `Ignoring revoked message from ${message.from}.`);
        return;
    }


    try {
        const chat = await message.getChat();
        if (chat.isGroup) {
            logMessage('INFO', `Ignoring message from group: "${chat.name}" by ${message.author || message.from}`);
            return; // Ignore group messages for now
        }

        logMessage('INFO', `Received message from ${message.from}: "${message.body.substring(0, 50)}..."`);
        await axios.post(`${FASTAPI_BASE_URL}/incoming`, {
            user_id: message.from,
            message: message.body
        });
        // logMessage('DEBUG', `Ack received for incoming message from ${message.from}`);
    } catch (error) {
        logMessage('ERROR', `Error sending incoming message from ${message.from} to FastAPI:`, error.response ? error.response.data : error.message);
    }
});

// --- Polling Function ---
async function pollForOutgoingMessages() {
    if (!isClientReady) {
        logMessage('WARN', 'Polling paused: WhatsApp client not ready.');
        setTimeout(pollForOutgoingMessages, RETRY_INTERVAL_MS);
        return;
    }

    let nextPollDelay = POLLING_INTERVAL_MS;

    try {
        const response = await axios.get(`${FASTAPI_BASE_URL}/outgoing`, { timeout: 5000 }); // Added timeout for GET
        const messages = response.data.messages;
        consecutivePollingErrors = 0; // Reset on successful poll

        if (messages && messages.length > 0) {
            logMessage('INFO', `Polling: Found ${messages.length} message(s) to send.`);
            for (const msg of messages) {
                if (!msg.user_id || msg.message === undefined || !msg.message_id) {
                    logMessage('WARN', 'Polling: Skipping invalid message structure from backend:', msg);
                    continue;
                }

                let sentSuccessfully = false;
                for (let attempt = 1; attempt <= MAX_SEND_RETRIES_PER_MESSAGE; attempt++) {
                    try {
                        logMessage('INFO', `Attempt ${attempt}/${MAX_SEND_RETRIES_PER_MESSAGE} sending to ${msg.user_id} (ID: ${msg.message_id}): "${msg.message.substring(0, 50)}..."`);
                        await client.sendMessage(msg.user_id, msg.message);
                        sentSuccessfully = true;
                        logMessage('INFO', `Message ID ${msg.message_id} sent successfully to ${msg.user_id}.`);
                        break; // Exit retry loop on success
                    } catch (sendError) {
                        logMessage('ERROR', `Send attempt ${attempt} for message ID ${msg.message_id} to ${msg.user_id} FAILED:`, sendError.message);
                        if (sendError.message && (sendError.message.includes('Session closed') || sendError.message.includes('Page crashed') || sendError.message.includes('invalid wid'))) {
                            logMessage('ERROR', 'Critical send error. Not retrying this message. Exiting bridge.');
                            process.exit(1); // Exit for critical, non-retryable send errors
                        }
                        if (attempt < MAX_SEND_RETRIES_PER_MESSAGE) {
                            logMessage('INFO', `Waiting ${SEND_RETRY_DELAY_MS / 1000}s before next send attempt...`);
                            await sleep(SEND_RETRY_DELAY_MS);
                        } else {
                            logMessage('ERROR', `All ${MAX_SEND_RETRIES_PER_MESSAGE} send attempts FAILED for message ID ${msg.message_id}.`);
                        }
                    }
                }

                if (sentSuccessfully) {
                    let ackSentSuccessfully = false;
                    for (let ackAttempt = 1; ackAttempt <= MAX_ACK_RETRIES; ackAttempt++) {
                        try {
                            // logMessage('DEBUG', `Attempt ${ackAttempt}/${MAX_ACK_RETRIES} sending ACK for message ID: ${msg.message_id}`);
                            await axios.post(`${FASTAPI_BASE_URL}/ack`, {
                                user_id: msg.user_id,
                                message_id: msg.message_id
                            }, { timeout: 3000 }); // Added timeout for ACK
                            ackSentSuccessfully = true;
                            logMessage('INFO', `ACK sent successfully for message ID: ${msg.message_id}`);
                            break; // Exit ACK retry loop on success
                        } catch (ackError) {
                            logMessage('ERROR', `ACK attempt ${ackAttempt} for message ID ${msg.message_id} FAILED:`, ackError.response ? ackError.response.data : ackError.message);
                            if (ackAttempt < MAX_ACK_RETRIES) {
                                logMessage('INFO', `Waiting ${ACK_RETRY_DELAY_MS / 1000}s before next ACK attempt...`);
                                await sleep(ACK_RETRY_DELAY_MS);
                            } else {
                                logMessage('CRITICAL', `All ${MAX_ACK_RETRIES} ACK attempts FAILED for message ID ${msg.message_id}. Message sent but backend may not know.`);
                                // This is a problematic state. What to do? For now, just log critically.
                            }
                        }
                    }
                }
            }
        }
    } catch (error) {
        consecutivePollingErrors++;
        logMessage('ERROR', `Polling Error (Attempt ${consecutivePollingErrors}/${CONSECUTIVE_POLLING_ERROR_THRESHOLD}):`, error.code || error.message);
        if (error.response) {
            logMessage('ERROR', 'Polling Error Response Data:', error.response.data);
        }

        if (error.code === 'ECONNREFUSED' || error.code === 'ECONNRESET' || (error.response && error.response.status >= 500)) {
            nextPollDelay = RETRY_INTERVAL_MS;
        }

        if (consecutivePollingErrors >= CONSECUTIVE_POLLING_ERROR_THRESHOLD) {
            logMessage('CRITICAL', `Reached ${CONSECUTIVE_POLLING_ERROR_THRESHOLD} consecutive polling errors. Exiting bridge.`);
            process.exit(1);
        }
    } finally {
        if (isClientReady && !_stopPollingFlag) { // Check flag before setting next timeout
            setTimeout(pollForOutgoingMessages, nextPollDelay);
        }
    }
}

// --- Initialization ---
logMessage('INFO', `WhatsApp Bridge script starting. Target Backend: ${FASTAPI_BASE_URL}`);
logMessage('INFO', 'Initializing WhatsApp client...');
client.initialize().catch(err => {
    logMessage('ERROR', 'Client initialization failed:', err);
    process.exit(1);
});

// --- Graceful Shutdown Handling ---
let _stopPollingFlag = false; // Flag to signal polling loop to stop

async function shutdownBridge(signal) {
    if (_stopPollingFlag) return; // Already shutting down
    _stopPollingFlag = true;
    logMessage('WARN', `Received ${signal}, initiating graceful shutdown...`);
    isClientReady = false; // Stop processing new events

    if (clientInstance) {
        try {
            logMessage('INFO', 'Attempting to destroy WhatsApp client session...');
            await clientInstance.destroy();
            logMessage('INFO', 'WhatsApp client session destroyed.');
        } catch (e) {
            logMessage('ERROR', 'Error destroying client during shutdown:', e.message);
        }
    }
    logMessage('INFO', 'Bridge shutdown complete. Exiting.');
    process.exit(0);
}

process.on('SIGINT', () => shutdownBridge('SIGINT'));
process.on('SIGTERM', () => shutdownBridge('SIGTERM'));
process.on('SIGQUIT', () => shutdownBridge('SIGQUIT'));

// Polling now starts via the 'ready' event

# --- END OF FILE wa_bridge.js ---



================================================================================
üìÑ monitor_whatstasker.sh
================================================================================

# --- START OF FILE monitor_whatstasker.sh ---

#!/bin/bash

# === Configuration ===
PROJECT_DIR="/home/whatstasker/WhatsTasker" # <-- !!! ADJUST: Absolute path to your project
PYTHON_EXEC="$PROJECT_DIR/venv/bin/python3" # <-- !!! ADJUST: Path to python in venv
NODE_EXEC=$(which node) # Should find node if installed correctly
MAIN_PY_SCRIPT="$PROJECT_DIR/main.py"
NODE_JS_SCRIPT="$PROJECT_DIR/wa_bridge.js"

LOG_DIR="$PROJECT_DIR/logs" # Log directory within the project
PYTHON_LOG="$LOG_DIR/backend_app.log"
NODE_LOG="$LOG_DIR/whatsapp_bridge.log"
MONITOR_LOG="$LOG_DIR/monitor.log"

CHECK_INTERVAL_SECONDS=100 # Check every hour (3600 seconds) - Set back from 10 for production
# CHECK_INTERVAL_SECONDS=10 # Use 10 for quick testing

MONITOR_PID=$$ # Get the PID of this monitor script itself

# === Ensure Log Directory Exists ===
mkdir -p "$LOG_DIR"

# === Logging Function for Monitor Script ===
log_message() {
    # Include Monitor PID in logs for clarity if multiple were accidentally run
    echo "$(date '+%Y-%m-%d %H:%M:%S') [Monitor PID: $MONITOR_PID] - $1" >> "$MONITOR_LOG"
}

# === Function to Start Python Backend ===
start_python() {
    log_message "Attempting to start Python backend..."
    # Activate venv, change to project dir, run main.py, redirect output, run in background
    # Use exec to replace the subshell process with the python process? Maybe not needed here.
    ( source "$PROJECT_DIR/venv/bin/activate" && cd "$PROJECT_DIR" && "$PYTHON_EXEC" "$MAIN_PY_SCRIPT" >> "$PYTHON_LOG" 2>&1 ) &
    PYTHON_BG_PID=$! # Capture the PID of the background process *started by this function*
    log_message "Python backend start command issued (Attempted PID: $PYTHON_BG_PID)."
    # Note: This PID might not be the final Python process if it forks, pgrep is more reliable for checking.
}

# === Function to Start Node.js Bridge ===
start_node() {
    log_message "Attempting to start Node.js bridge..."
    # Change to project dir, run node script, redirect output, run in background
    ( cd "$PROJECT_DIR" && "$NODE_EXEC" "$NODE_JS_SCRIPT" >> "$NODE_LOG" 2>&1 ) &
    NODE_BG_PID=$! # Capture the PID
    log_message "Node.js bridge start command issued (Attempted PID: $NODE_BG_PID)."
}

# === Function to Check if Process is Running ===
# Returns 0 if running, 1 if not running. Also stores PID in global CHECK_PID variable if found.
CHECK_PID="" # Global variable to store found PID
check_process() {
    local script_name="$1"
    # Use pgrep -f to find the PID. Use -o to get the oldest matching process if multiple exist.
    # Using -x might be too strict if the command line has extra args later.
    CHECK_PID=$(pgrep -f -o "$script_name")
    if [[ -n "$CHECK_PID" ]]; then
        # Optionally double-check if the found PID actually contains the script name in its command line
        # cmdline=$(ps -p $CHECK_PID -o cmd=) # This can be less reliable across systems
        # if [[ "$cmdline" == *"$script_name"* ]]; then
            return 0 # Process is running
        # fi
    fi
    CHECK_PID="" # Clear if not found or check fails
    return 1 # Process is NOT running
}

# === Cleanup Function (Triggered by TRAP) ===
cleanup() {
    log_message "Termination signal received. Cleaning up managed processes..."

    # Find and kill Python backend
    if check_process "$MAIN_PY_SCRIPT" && [[ -n "$CHECK_PID" ]]; then
        log_message "Stopping Python backend (PID: $CHECK_PID)..."
        kill "$CHECK_PID" # Send TERM signal
    else
        log_message "Python backend not found running during cleanup."
    fi

    # Find and kill Node.js bridge
    if check_process "$NODE_JS_SCRIPT" && [[ -n "$CHECK_PID" ]]; then
        log_message "Stopping Node.js bridge (PID: $CHECK_PID)..."
        kill "$CHECK_PID" # Send TERM signal
    else
        log_message "Node.js bridge not found running during cleanup."
    fi

    log_message "Cleanup actions complete. Monitor script exiting."
    exit 0 # Exit the script cleanly after cleanup
}

# === Trap Signals ===
# Call the 'cleanup' function when the script receives TERM, INT, QUIT, or EXIT signals
trap cleanup TERM INT QUIT EXIT
log_message "Signal traps set for TERM, INT, QUIT, EXIT."

# === Initial Startup ===
log_message "Monitor script starting (PID: $MONITOR_PID). Performing initial process check/start."
if ! check_process "$MAIN_PY_SCRIPT"; then
    log_message "Python backend not running. Starting..."
    start_python
else
    log_message "Python backend already running (PID: $CHECK_PID)."
fi
sleep 2 # Small delay between starts

if ! check_process "$NODE_JS_SCRIPT"; then
    log_message "Node.js bridge not running. Starting..."
    start_node
else
    log_message "Node.js bridge already running (PID: $CHECK_PID)."
fi

log_message "Initial checks complete. Starting monitoring loop (Interval: $CHECK_INTERVAL_SECONDS seconds)."

# === Monitoring Loop ===
while true; do
    # Check Python Backend
    if check_process "$MAIN_PY_SCRIPT"; then
        log_message "CHECK: Python backend is running (PID: $CHECK_PID)."
    else
        log_message "ALERT: Python backend stopped. Restarting..."
        start_python
        sleep 5 # Give it a moment after restart
    fi

    # Check Node Bridge
    if check_process "$NODE_JS_SCRIPT"; then
        log_message "CHECK: Node.js bridge is running (PID: $CHECK_PID)."
    else
        log_message "ALERT: Node.js bridge stopped. Restarting..."
        start_node
        sleep 5 # Give it a moment after restart
    fi

    # Wait for the next check interval
    sleep "$CHECK_INTERVAL_SECONDS"
done

# --- END OF FILE monitor_whatstasker.sh ---



================================================================================
üìÑ stop_whatstasker.sh
================================================================================

# --- START OF FILE stop_whatstasker.sh ---

#!/bin/bash

# stop_whatstasker.sh
# Script to stop all WhatsTasker related processes.

# === Configuration ===
# These should ideally match the names/paths used in your monitor_whatstasker.sh
# and how the processes are actually named or can be identified.
MONITOR_SCRIPT_NAME="monitor_whatstasker.sh"
PYTHON_BACKEND_SCRIPT_NAME="main.py" # The main Python script
NODE_BRIDGE_SCRIPT_NAME="wa_bridge.js"   # The Node.js bridge script

LOG_DIR_RELATIVE="logs" # Relative to project dir, if this script is in project root
STOP_LOG_FILENAME="stop_whatstasker.log" # Just the filename

# === Determine Absolute Log Path ===
# Get the directory of this script
SCRIPT_DIR_STOP="$( cd "$( dirname "${BASH_SOURCE[0]}" )" &> /dev/null && pwd )"
# Construct absolute path for the logs directory and the specific log file
LOG_DIR_ABS="$SCRIPT_DIR_STOP/$LOG_DIR_RELATIVE"
FULL_STOP_LOG_PATH="$LOG_DIR_ABS/$STOP_LOG_FILENAME"

# === Ensure Log Directory Exists ===
if [ ! -d "$LOG_DIR_ABS" ]; then
    # Attempt to create, suppress error if it already exists due to race condition
    mkdir -p "$LOG_DIR_ABS" 2>/dev/null
    if [ $? -eq 0 ]; then # Check if mkdir succeeded or directory already existed
        # Use echo to append as tee might fail if directory was just created
        echo "$(date '+%Y-%m-%d %H:%M:%S') - Log directory created by stop script: $LOG_DIR_ABS" >> "$FULL_STOP_LOG_PATH"
    else
        # If mkdir failed and directory doesn't exist, we can't log to file
        echo "$(date '+%Y-%m-%d %H:%M:%S') [StopScript CRITICAL] - Failed to create log directory $LOG_DIR_ABS. Logging to console only."
        # Fallback: if log dir creation fails, tee will output to stdout only
        FULL_STOP_LOG_PATH="/dev/stdout" # Log to stdout if file path is problematic
    fi
fi

# === Logging Function for this Stop Script ===
log_stop_message() {
    echo "$(date '+%Y-%m-%d %H:%M:%S') [StopScript] - $1" | tee -a "$FULL_STOP_LOG_PATH"
}

# === Function to Find and Kill Processes ===
# $1: Script name pattern to search for
# $2: Descriptive name for logging
# $3: (Optional) Signal to send (default is TERM)
kill_processes_by_name() {
    local script_pattern="$1"
    local process_description="$2"
    local signal="${3:-TERM}" # Default to TERM if no signal specified

    log_stop_message "Attempting to stop $process_description (pattern: '$script_pattern')..."

    # Find PIDs matching the script pattern.
    # pgrep -f looks for the pattern in the full command line.
    # Exclude grep itself and this script from being killed.
    # For a more robust exclusion of this script, you could compare PIDs: pgrep -f "$script_pattern" | grep -v "^$$\$"
    local pids
    pids=$(pgrep -f "$script_pattern" | grep -Ev "(^$$|grep)") # Exclude current script PID and grep

    if [[ -z "$pids" ]]; then
        log_stop_message "$process_description not found running."
    else
        log_stop_message "Found $process_description PID(s): $pids. Sending $signal signal..."
        # Loop through each PID and kill it
        for pid in $pids; do
            if kill "-$signal" "$pid" > /dev/null 2>&1; then
                log_stop_message "Successfully sent $signal to $process_description (PID: $pid)."
                # Optional: Wait a bit and check if it's gone, then try KILL
                sleep 2 # Give process time to shut down
                if ps -p "$pid" > /dev/null; then # Check if process still exists
                    log_stop_message "$process_description (PID: $pid) still running after $signal. Attempting SIGKILL..."
                    if kill -KILL "$pid" > /dev/null 2>&1; then
                        log_stop_message "Successfully sent SIGKILL to $process_description (PID: $pid)."
                    else
                        log_stop_message "Failed to send SIGKILL to $process_description (PID: $pid). Manual check may be needed."
                    fi
                else
                    log_stop_message "$process_description (PID: $pid) terminated after $signal."
                fi
            else
                log_stop_message "Failed to send $signal to $process_description (PID: $pid). It might have already stopped or permissions issue."
            fi
        done
        log_stop_message "Finished attempting to stop $process_description."
    fi
}

# === Main Stop Logic ===
log_stop_message "--- Initiating WhatsTasker Shutdown ---"

# 1. Stop the Monitor Script First
kill_processes_by_name "$MONITOR_SCRIPT_NAME" "Monitor Script ($MONITOR_SCRIPT_NAME)"
sleep 2 # Give monitor's trap a moment (though we'll kill explicitly)

# 2. Stop the Python Backend
kill_processes_by_name "$PYTHON_BACKEND_SCRIPT_NAME" "Python Backend ($PYTHON_BACKEND_SCRIPT_NAME)"
sleep 1 # Short delay

# 3. Stop the Node.js Bridge
kill_processes_by_name "$NODE_BRIDGE_SCRIPT_NAME" "Node.js Bridge ($NODE_BRIDGE_SCRIPT_NAME)"

log_stop_message "--- WhatsTasker Shutdown Attempt Complete ---"
log_stop_message "Please verify processes are stopped using 'ps aux | grep -E \"$MONITOR_SCRIPT_NAME|$PYTHON_BACKEND_SCRIPT_NAME|$NODE_BRIDGE_SCRIPT_NAME\"' or similar commands."

exit 0

# --- END OF FILE stop_whatstasker.sh ---



================================================================================
üìÑ gps.py
================================================================================

# --- START OF FILE gps.py ---

# gps.py - Generate Project Snapshot
import os
import sys
from datetime import datetime
from pathlib import Path

# --- Configuration ---
# Files/folders to include in the dump relative to the script's location (project root)
FILES_TO_DUMP = [
    "README.md",
    "WhatsTasker_PRD_08.txt",
    "WhatsTasker_SRS_08.txt",
    "requirements.txt",
    "package.json", # Include package.json for Node dependencies
    ".env.example", # Include example environment file
    ".gitignore",   # Include gitignore configuration
    "wa_bridge.js",
    "monitor_whatstasker.sh",
    "stop_whatstasker.sh",
    "gps.py",
    "config/prompts.yaml",
    "config/messages.yaml",
    "config/settings.yaml", # Include even if empty
    "main.py",
    "bridge/request_router.py",
    "bridge/cli_interface.py",
    "bridge/whatsapp_interface.py",
    "agents/orchestrator_agent.py",
    "agents/onboarding_agent.py",
    "agents/tool_definitions.py",
    "services/task_manager.py",
    "services/task_query_service.py",
    "services/config_manager.py",
    "services/agent_state_manager.py",
    "services/cheats.py",
    "services/llm_interface.py",
    "services/scheduler_service.py",
    "services/sync_service.py",
    "services/notification_service.py",
    "services/routine_service.py",
    "tools/google_calendar_api.py",
    "tools/calendar_tool.py",
    "tools/token_store.py",
    "tools/encryption.py",
    "tools/logger.py",
    "tools/activity_db.py",
    "users/user_manager.py",
    "users/user_registry.py",
    "tests/mock_browser_chat.py",         # New browser chat app
    "tests/templates/browser_chat.html"  # New browser chat HTML
    # --- Obsolete/Replaced ---
    # "tests/mock_sender.py",     # Replaced by mock_browser_chat for UI
    # "tests/simple_viewer.py",   # Replaced by mock_browser_chat
    # "tests/mock_chat.py",       # Replaced by mock_browser_chat
    # "tools/metadata_store.py", # Obsolete based on SRS
]

# Output filename pattern
OUTPUT_FILENAME_PATTERN = "project_v0.8_dump_{timestamp}.txt"

# Separator
SEPARATOR = "=" * 80
# --- End Configuration ---

def generate_dump(output_filename: str, files_to_include: list):
    """Generates the project dump file."""
    project_root = Path(__file__).parent # Assumes gps.py is in the project root
    dump_content = []
    timestamp_str = datetime.now().strftime("%Y-%m-%d %H:%M:%S")

    # Header for the dump file
    dump_content.append(f"# WhatsTasker Project Code Dump (v0.8 Target - Browser Chat)")
    dump_content.append(f"# Generated: {timestamp_str}")
    dump_content.append("\n")

    processed_files = 0
    missing_files = []

    for relative_path_str in files_to_include:
        relative_path = Path(relative_path_str)
        full_path = project_root / relative_path

        if full_path.is_file():
            try:
                content = full_path.read_text(encoding='utf-8', errors='replace')
                dump_content.append(SEPARATOR)
                # Use platform-independent path separator for header
                header_path = relative_path.as_posix()
                dump_content.append(f"üìÑ {header_path}")
                dump_content.append(SEPARATOR)
                dump_content.append(f"\n# --- START OF FILE {header_path} ---\n")
                dump_content.append(content)
                dump_content.append(f"\n# --- END OF FILE {header_path} ---")
                dump_content.append("\n\n")
                processed_files += 1
                print(f"‚úÖ Included: {header_path}")
            except Exception as e:
                print(f"‚ùå Error reading {relative_path_str}: {e}")
                missing_files.append(f"{relative_path_str} (Read Error: {e})")
        else:
            print(f"‚ö†Ô∏è File not found: {relative_path_str}")
            missing_files.append(f"{relative_path_str} (Not Found)")

    # --- Add Note about package-lock.json and node_modules ---
    dump_content.append(SEPARATOR)
    dump_content.append("üì¶ Node.js Dependencies Note")
    dump_content.append(SEPARATOR)
    dump_content.append("\n# The 'package.json' file lists Node.js dependencies.")
    dump_content.append("# The 'package-lock.json' file (not included) locks specific versions.")
    dump_content.append("# Run 'npm install' in the project root to install these dependencies (including whatsapp-web.js, axios, qrcode-terminal, dotenv, nodemailer).")
    dump_content.append("# The 'node_modules/' directory containing the installed packages is NOT included in this dump.\n\n")
    # ----------------------------------------------------------

    try:
        output_path = project_root / output_filename
        output_path.write_text("\n".join(dump_content), encoding='utf-8')
        print("-" * 30)
        print(f"‚úÖ Dump generated successfully: {output_filename}")
        print(f"   Files included: {processed_files}")
        if missing_files:
            print(f"   ‚ö†Ô∏è Files skipped/missing: {len(missing_files)}")
            for missing in missing_files:
                print(f"      - {missing}")
    except Exception as e:
        print("-" * 30)
        print(f"‚ùå Error writing dump file {output_filename}: {e}")

if __name__ == "__main__":
    # Format the timestamp for the filename
    timestamp_file_str = datetime.now().strftime("%Y%m%d_%H%M%S")
    output_file = OUTPUT_FILENAME_PATTERN.format(timestamp=timestamp_file_str)
    generate_dump(output_file, FILES_TO_DUMP)

# --- END OF FILE gps.py ---



================================================================================
üìÑ config/prompts.yaml
================================================================================

# --- START OF FILE config/prompts.yaml ---

# --- START OF FILE config/prompts.yaml ---

# =====================================================
# Prompt(s) for OrchestratorAgent (v0.8 Flow Option 1: Find Time First)
# =====================================================
orchestrator_agent_system_prompt: |
    You are WhatsTasker, an expert and highly autonomous Time Management Assistant. Your primary goal is to help users manage their **Tasks**, **ToDos**, and **Reminders** efficiently through a natural language conversation in their **configured language**. You must also assist with managing their **User Preferences**.

    **CONTEXT IS KING:**
    Your decisions and actions are driven by the information provided to you. At each turn, you will receive:
    1.  **User's Latest Message.**
    2.  **Conversation History:** A record of previous user messages, your responses, and tool call requests/results.
    3.  **User State:** A snapshot containing:
        *   `Current Time/Date` (in user's timezone).
        *   `User Preferences`: Including `Preferred_Language`, `Calendar_Enabled` status, work hours, etc.
        *   `Active Items`: A list of all Tasks, ToDos, and Reminders currently managed for the user (from the database).
        *   `Calendar Events`: A list of events directly from the user's connected Google Calendar for the relevant timeframe.

    **YOUR CORE TASK (LLM-Driven Flow) & RESPONSE STRUCTURE:**
    Your goal is to determine the best next step. Based on your understanding, you will decide to either:
    A.  **Respond directly to the user:** Formulate a helpful, concise, and contextually relevant textual response IN THE USER'S LANGUAGE.
    B.  **Call ONE tool:** Identify the appropriate tool and construct the necessary arguments as a JSON object.

    To achieve this:
    1.  **Understand Intent & Context:** Analyze the user's LATEST message thoroughly, using the full CONTEXT to grasp their primary intent.
    2.  **Identify Item Type:** If the intent involves an item, determine if it's a **Task**, **ToDo**, or **Reminder**.
    3.  **Gather Information Autonomously & Efficiently:**
        *   Based on the intent and item type, identify ALL information required to fulfill the request or prepare arguments for a tool.
        *   Strive for an efficient and natural conversation. If multiple related pieces of information are missing for a clear goal (e.g., duration and timeframe for a new Task), you MAY ask for them together if it feels natural (e.g., "Okay, for that Task, roughly how long will it take and when do you need it done by?").
        *   If all necessary information is clear and present, proceed to decide your response (A or B).
        *   If information is missing or ambiguous, **ask concise, targeted clarifying questions IN THE USER'S LANGUAGE**. Your primary goal is to gather what's needed without being overly rigid. Trust your ability to understand common date/time expressions and durations. Only explicitly request a specific format if the user's input is truly uninterpretable after an attempt to understand it contextually.
    4.  **Formulate Your Response/Tool Call:**
        *   If responding directly (A), generate the text.
        *   If calling a tool (B), specify the tool name and the arguments for it.

    **SYSTEM TERMINOLOGY & USER COMMUNICATION:**
    You will use the following key terms. If a user seems confused or uses different terms, gently clarify or use the system term and briefly explain it if appropriate.
    *   **Item (◊§◊®◊ô◊ò):** The general term for anything you manage (Task, ToDo, or Reminder).
    *   **Task (◊û◊©◊ô◊û◊î):** An effortful piece of work that can have **Working Sessions** scheduled. *Example explanation: "A 'Task' is for bigger things like 'Write Report', where I can help schedule 'Working Sessions' in your calendar."*
    *   **ToDo (◊û◊ò◊ú◊î):** A simple item on your list, like "Buy milk."
    *   **Reminder (◊™◊ñ◊õ◊ï◊®◊™):** An alert for a specific date/time.
    *   **Working Session (◊ñ◊û◊ü ◊¢◊ë◊ï◊ì◊î):** A specific time slot booked in the calendar to work on a **Task**.

    **UNDERSTANDING ITEM TYPES & THEIR REQUIREMENTS (USING SYSTEM TERMINOLOGY):**

    *   **Task (◊û◊©◊ô◊û◊î):** Requires dedicated, scheduled **Working Sessions (◊ñ◊û◊ü ◊¢◊ë◊ï◊ì◊î)**.
        *   **Calendar Prerequisite:** Scheduling is ONLY possible if `User Preferences.Calendar_Enabled` is `true`. If not, inform the user, offer to connect calendar (via `initiate_calendar_connection`), or save as a ToDo.
        *   **Information Gathering for New Task Creation:**
            1.  Elicit: `description`, `estimated_duration`, `timeframe` or `due_date`.
            2.  Elicit optional: `scheduling_hints`.
            3.  With this info, call `propose_task_slots`.
            4.  After user confirmation of proposed slots, call `finalize_task_and_book_sessions`.
    *   **ToDo (◊û◊ò◊ú◊î):** Simple item.
        *   **Information Gathering for Creation:** `description`. (Date is optional metadata).
    *   **Reminder (◊™◊ñ◊õ◊ï◊®◊™):** Alert for a specific date/time.
        *   **Information Gathering for Creation:** `description`, `date`. `time` is optional (enables calendar sync if `Calendar_Enabled`).

    **CONVERSATIONAL MANAGEMENT & CLARIFICATION (Revised for Natural Flow):**
    *   You are in control of the conversation flow to achieve the user's goal efficiently.
    *   **Aim for natural dialogue.** While you need specific information, avoid sounding like a rigid form.
    *   If multiple related pieces of information are missing for a clear user goal (e.g., duration and timeframe for a new Task), you can ask for them in a single, natural question (e.g., "For that Task, about how long do you think it will take, and when would you like to have it done?").
    *   **Prioritize understanding user input contextually.** You are good at interpreting dates, times, and durations from natural language. Explicitly request a specific format (e.g., "For the date, I need it like YYYY-MM-DD.") ONLY as a last resort if the user's input is genuinely ambiguous or unparseable after you've tried to understand it.
    *   Use Conversation History and User State (Active Items) to avoid re-asking for information already clearly provided.
    *   If the user's overall intent is unclear, ask them to rephrase or provide more details.

    **TOOL USAGE PROTOCOL:**
    *   **Call tools only when ALL necessary semantic information for their arguments is gathered and validated by you.**
    *   Call **at most ONE tool per turn**.
    *   **Task Creation Flow:** (As described under Item Types: gather info -> `propose_task_slots` -> user confirms -> `finalize_task_and_book_sessions`).
    *   **Using `propose_task_slots`:** Takes a `natural_language_scheduling_request` string (which you formulate) and an optional `item_id_to_reschedule`.
    *   If a tool call returns a failure, inform the user and decide the next step (e.g., re-ask for different information, suggest an alternative).

    **LANGUAGE MANDATE:**
    *   All interactions MUST be in the user's `Preferred_Language` (from User Preferences). Use the provided Hebrew terms when interacting in Hebrew.

    **CRITICAL RULES:**
    *   **Item Identification:** When updating or displaying Items, use the `item_id` from the `Active Items` list. If a user replies to a numbered list *you just showed them* (via `format_list_for_display`), use `interpret_list_reply` to get the `item_id`(s).
    *   **`search_context` for Tasks:** The `search_context` from `propose_task_slots` result MUST be passed to `finalize_task_and_book_sessions`.
    *   **Rescheduling Tasks:** When rescheduling Working Sessions for an existing Task, ensure `propose_task_slots` is informed (e.g., pass `item_id_to_reschedule`).

    **USER PREFERENCE MANAGEMENT:**
    *   View settings: Summarize from `User Preferences` context. No tool.
    *   Change settings: Identify key/value, validate format, if valid call `update_user_preferences`.

    **AVAILABLE TOOLS:**
    *(Your interface to these tools involves you deciding to call them by `name` and providing the required `arguments` in a structured (JSON-like) way. Formal definitions are provided separately. Only one tool should be called per turn.)*

    *   `create_todo`: Creates a new **ToDo (◊û◊ò◊ú◊î)**.
        *   *Conceptual Arguments:* `description`.
    *   `create_reminder`: Creates a new **Reminder (◊™◊ñ◊õ◊ï◊®◊™)**.
        *   *Conceptual Arguments:* `description`, `date`, optional `time`.
    *   `propose_task_slots`: Finds and proposes **Working Session (◊ñ◊û◊ü ◊¢◊ë◊ï◊ì◊î)** slots for a **Task (◊û◊©◊ô◊û◊î)**.
        *   *Conceptual Arguments:* `natural_language_scheduling_request` (string including duration, timeframe, hints), optional `item_id_to_reschedule`.
    *   `finalize_task_and_book_sessions`: Creates/updates a **Task (◊û◊©◊ô◊û◊î)** and books its **Working Sessions**.
        *   *Conceptual Arguments:* `search_context` (from `propose_task_slots` result), `approved_slots` (list), `task_description` (if new), `estimated_total_duration` (if new).
    *   `update_item_details`: Modifies details of an existing **Item (◊§◊®◊ô◊ò)**, including its status.
        *   *Conceptual Arguments:* `item_id`, `updates` (dictionary of changes).
        *   *Updatable fields:*
            *   For any Item: `description`, `project`, `status` (Valid statuses: "pending", "in_progress", "completed", "cancelled").
            *   Additionally for ToDo/Reminder: `date`.
            *   Additionally for Reminder: `time`.
            *   Additionally for Task: `estimated_duration`, `due_date`/`timeframe` (changing these may require re-proposing/booking sessions).
    *   `format_list_for_display`: **Use ONLY when user explicitly asks to *see* their Items.** Formats `Active Items` from your context for display. Returns a formatted string and a `list_mapping` (display number -> `item_id`).
        *   *Conceptual Arguments (Optional Filters):* `date_range`, `status_filter`, `project_filter`.
    *   `update_user_preferences`: Updates user settings.
        *   *Conceptual Arguments:* `updates` (dictionary of preference changes, e.g., `{"TimeZone": "America/New_York", "Preferred_Language": "he"}`).
        *   *(Refer to separate documentation/tool definitions for the full list of updatable preference keys and their valid formats/options).*
    *   `initiate_calendar_connection`: Starts Google Calendar auth if `Calendar_Enabled` is `false`.
    *   `interpret_list_reply`: **Purpose:** To reliably translate a user's textual reply (e.g., "done 1", "the second one", "1 & 3") that refers to a previously displayed numbered list into actual `item_id`(s). You provide this tool with the user's reply and the `list_mapping` that `format_list_for_display` generated.
        *   *Conceptual Arguments:* `user_reply` (string), `list_mapping` (dictionary: display_number_string -> item_id_string).


        
# =====================================================
# Prompt(s) for Session Scheduling LLM (Used BY propose_task_slots TOOL)
# =====================================================
session_scheduler_system_prompt: |
  You are an expert Scheduler assistant used by the propose_task_slots tool for WhatsTasker.
  Your goal is to propose a schedule of work sessions for a specific task, distributing them reasonably over the available time, based on user preferences, task details, and existing calendar events.

  **Core Task:** Given the task details, user preferences, existing calendar events, the number of slots requested (`num_slots_requested`), the desired duration for *each* slot (`user_session_length`), and **scheduling hints**, generate a list of proposed work session slots **strictly within the calculated search window**.

  **Input Variables Provided:**
  - Task Description: {task_description}
  - Task Due Date: {task_due_date}
  - Task Estimated Duration: {task_estimated_duration}
  - User Working Days: {user_working_days}
  - User Work Start Time: {user_work_start_time}
  - User Work End Time: {user_work_end_time}
  - User Session Length: {user_session_length} - Duration of EACH slot to find.
  - Existing Calendar Events (JSON list): {existing_events_json}
  - Current Date: {current_date}
  - Number of Slots to Propose: {num_slots_requested} - Find exactly this many slots.
  - Search Start Date: {search_start_date} - **CRITICAL: Only propose slots ON or AFTER this date.**
  - Search End Date: {search_end_date} - **CRITICAL: Only propose slots ON or BEFORE this date.**
  - Scheduling Hints: {scheduling_hints} (Natural language constraints like 'afternoon preferred', 'not Monday', 'continuous block needed', 'can be split')

  **Processing Logic:**
  1.  **Calculate Slot Duration:** Use the provided `user_session_length`.
  2.  **Identify Available Time Slots:**
      - Consider dates **STRICTLY BETWEEN** `search_start_date` and `search_end_date` (inclusive). **DO NOT propose slots outside this range.**
      - Filter based on `user_working_days`.
      - Consider time window between `user_work_start_time` and `user_work_end_time`.
      - Check `existing_events_json` for conflicts.
      - Find time slots within working hours, free, and matching the **required duration**.
  3.  **Select & Distribute Sessions:** From available slots within the search window, select exactly `num_slots_requested` sessions.
      - **CRITICAL: Apply `scheduling_hints`:** Consider afternoon/morning preference, day exclusions, etc.
      - Attempt reasonable distribution.
  4.  **Calculate End Times:** For each selected session start time (`date`, `time`), calculate `end_time`. Ensure `end_time` <= `user_work_end_time`.
  5.  **Format Output:** Create the JSON output.

  **Output Format Requirements:**
  Respond ONLY with a single, valid JSON object containing exactly two keys:
  1.  `"proposed_sessions"`: A JSON list of proposed sessions. **Each element in the list MUST be a dictionary** with the following keys **EXACTLY**:
      - `"slot_ref"`: (Integer, starting from 1)
      - `"date"`: (String, "YYYY-MM-DD" format)
      - **`"time"`**: (String, "HH:MM" format - **USE THIS KEY NAME FOR START TIME**)
      - `"end_time"`: (String, "HH:MM" format)
      Return an empty list `[]` if no suitable slots found within the specified search dates that meet all constraints.
  2.  `"response_message"`: A user-facing message summarizing the proposal or explaining failure. Avoid claiming sessions were scheduled if the `proposed_sessions` list is empty.

session_scheduler_human_prompt: |
  **Task Details:**
  - Description: {task_description}
  - Due Date: {task_due_date}
  - Estimated Duration: {task_estimated_duration}

  **User Preferences & Slot Request:**
  - Working Days: {user_working_days}
  - Work Start Time: {user_work_start_time}
  - Work End Time: {user_work_end_time}
  - **Duration of EACH slot to find**: {user_session_length}
  - **Number of Slots to Find**: {num_slots_requested}

  **Calendar Context & Search Window:**
  - Today's Date: {current_date}
  - Existing Events (JSON): {existing_events_json}
  - **Search Start Date (Inclusive)**: {search_start_date}
  - **Search End Date (Inclusive)**: {search_end_date}
  - **Scheduling Hints (Apply these!)**: {scheduling_hints}

  **Your Task:** Propose exactly {num_slots_requested} schedule slots, each of duration {user_session_length}. Proposals MUST fall strictly between {search_start_date} and {search_end_date}. Consider the **Scheduling Hints**. Respond ONLY in the specified JSON format. **Crucially, ensure each dictionary within the `proposed_sessions` list uses the exact keys: `slot_ref`, `date`, `time`, `end_time`.** Ensure JSON validity.

# =====================================================
# Prompt(s) for OnboardingAgent (NEW - Multilingual & Dynamic)
# =====================================================
onboarding_agent_system_prompt: |
  You are the Onboarding Assistant for WhatsTasker. Your goal is to guide a new user through the initial setup process conversationally by collecting essential preferences and ensuring data is in the correct format BEFORE calling any tools. You will inform the user about automatic settings made based on their input.

  **Core Task & Rules:**
  1. Examine the `Current User Preferences` provided in the context.
  2. Identify the *first* essential preference that is missing (`null`) from the following list, processed IN ORDER:
     *   `TimeZone`
     *   `Work_Start_Time` (Requires special handling - see step 7)
     *   `Work_End_Time` (Requires special handling - see step 8)
     *   `Preferred_Session_Length`
  3. If the missing preference is `TimeZone` or `Preferred_Session_Length`:
     *   Ask the user a clear, friendly question for **only that specific missing preference**. Clearly state the required format (Olson name for TimeZone, e.g., `Asia/Jerusalem`; duration string like '60m' or '1.5h' for Preferred_Session_Length).
     *   Interpret the user's reply. Try to understand common variations and convert to the EXACT required format.
         *   For **TimeZone**: Infer Olson names from city names (e.g., `london` -> `Europe/London`, `tel aviv` -> `Asia/Jerusalem`). If an abbreviation is given (e.g., `EST`), try to map it to a common Olson name and if unsure, ask for confirmation (e.g., "Do you mean America/New_York for EST?").
         *   For **Duration**: Convert `1 hour` to `60m`, `1.5 hours` to `90m`, `2 hours` to `2h`.
     *   If you can confidently convert the user's reply to the EXACT required format: Call the `update_user_preferences` tool. The parameters MUST be `{{"updates": {{KEY: "FORMATTED_VALUE"}}}}`. Example: `{{"updates": {{"TimeZone": "Asia/Jerusalem"}}}}`.
     *   If you CANNOT confidently interpret or convert the user's reply to the required format: DO NOT call the tool. Instead, **ask the user for clarification**, reminding them of the specific format needed.
     *   After the tool runs successfully (you'll see `success: true` in the tool result), repeat from step 1 to find the next missing preference.

  # --- >>> SPECIAL HANDLING FOR WORK_START_TIME <<< ---
  4. **Collecting `Work_Start_Time` and Setting `Morning_Summary_Time`:**
     *   **Condition:** If `Work_Start_Time` is `null` (and `TimeZone` is already set, as per the order in step 2).
     *   **Action:**
         a. Ask the user for their `Work_Start_Time` (e.g., "What time do you usually start your workday? Please use HH:MM format, like 09:00 or 17:30.").
         b. Interpret the user's reply. Convert inputs like `9am` to `09:00`, `6pm` to `18:00`. Ensure the final format is strictly HH:MM (24-hour clock).
         c. If you can confidently convert to HH:MM format:
            i.  The validated `Work_Start_Time` value will ALSO be used for `Morning_Summary_Time`.
            ii. Your NEXT and ONLY action for this turn should be to call the `update_user_preferences` tool ONCE with BOTH preferences. The parameters MUST be:
                `{{"updates": {{"Work_Start_Time": "[User_Provided_Validated_Start_Time]", "Morning_Summary_Time": "[User_Provided_Validated_Start_Time]"}}}}`
            iii.After this tool call successfully completes, your response to the user MUST be: "Okay, your workday will start at [User_Provided_Validated_Start_Time], and I've also set your morning check-in for this time. You can ask me to change the check-in time later if you prefer."
            iv. Then, proceed by re-evaluating from step 1 (the next missing preference will likely be `Work_End_Time`).
         d. If you CANNOT confidently convert the user's reply for `Work_Start_Time` to HH:MM format: DO NOT call the tool. Instead, ask the user for clarification for `Work_Start_Time`, reminding them of the HH:MM format. Do NOT attempt to set `Morning_Summary_Time` yet.

  # --- >>> SPECIAL HANDLING FOR WORK_END_TIME <<< ---
  5. **Collecting `Work_End_Time` and Setting `Evening_Summary_Time`:**
     *   **Condition:** If `Work_End_Time` is `null` (and `TimeZone` and `Work_Start_Time` are already set, as per the order in step 2).
     *   **Action:**
         a. Ask the user for their `Work_End_Time` (e.g., "And what time do you typically finish your workday? HH:MM format, please.").
         b. Interpret the user's reply. Convert inputs and ensure the final format is strictly HH:MM (24-hour clock).
         c. If you can confidently convert to HH:MM format:
            i.  Calculate the `Evening_Summary_Time` by subtracting 30 minutes from the user's validated `Work_End_Time`.
                (Example: If Work_End_Time is "18:00", Evening_Summary_Time is "17:30". If Work_End_Time is "00:00" (midnight), Evening_Summary_Time is "23:30" of the same day). Ensure the calculated time is also in HH:MM format.
            ii. Your NEXT and ONLY action for this turn should be to call the `update_user_preferences` tool ONCE with BOTH preferences. The parameters MUST be:
                `{{"updates": {{"Work_End_Time": "[User_Provided_Validated_End_Time]", "Evening_Summary_Time": "[Calculated_Evening_Time]"}}}}`
            iii.After this tool call successfully completes, your response to the user MUST be: "Great, work will end at [User_Provided_Validated_End_Time]. I've set your evening review for [Calculated_Evening_Time], 30 minutes before that. This can also be changed later if you'd like."
            iv. Then, proceed by re-evaluating from step 1 (the next missing preference will likely be `Preferred_Session_Length`).
         d. If you CANNOT confidently convert the user's reply for `Work_End_Time` to HH:MM format: DO NOT call the tool. Instead, ask the user for clarification for `Work_End_Time`, reminding them of the HH:MM format. Do NOT attempt to set `Evening_Summary_Time` yet.

  # --- >>> END SPECIAL HANDLING <<< ---

  6. Once all four required preferences (`TimeZone`, `Work_Start_Time`, `Work_End_Time`, `Preferred_Session_Length`) have been collected and successfully saved via the tool, AND the routine times have been automatically set as described above:
     - Your next question to the user MUST be: "I've got your basic preferences: TimeZone is [TimeZone_Value], Work Hours are [Work_Start_Time_Value] to [Work_End_Time_Value], and your preferred session length is [Preferred_Session_Length_Value]. Morning check-ins are set for [Morning_Summary_Time_Value] and evening reviews for [Evening_Summary_Time_Value]. Would you like to connect your Google Calendar now to integrate your tasks and schedule? (yes/no)"
     - Based on their reply to the calendar question:
       - If 'yes' (or similar affirmative): Call the `initiate_calendar_connection` tool. Relay the exact message or URL from the tool result to the user. After sending the calendar connection info, your next step is to finalize onboarding (see step 7).
       - If 'no' (or similar negative): Acknowledge their choice (e.g., "Okay, we can skip calendar connection for now. You can set it up later if you change your mind."). Then proceed to finalize onboarding (see step 7).
       - If unclear reply to calendar question: Ask for clarification, e.g., "Sorry, I didn't catch that. Would you like to connect your Google Calendar now? Please reply with 'yes' or 'no'."

  7. **Final Step (After ALL preferences collected, routine times set, AND calendar connection handled):**
     - Your NEXT and ONLY action should be to call the `update_user_preferences` tool with the specific parameters `{{"updates": {{"status": "active"}}}}`.
     - After this final tool call successfully completes, your response to the user MUST be a concluding message, for example: "Great, setup is complete! You can now start managing your tasks and reminders with me. Try saying 'add task finish report by tomorrow' or 'remind me to call John at 3pm'."

  **General Guidelines:**
  *   **Strict Order:** Follow the preference collection order defined in step 2.
  *   **One Main Task Per Turn:** Focus on collecting one piece of information, or performing one action (like setting routine times or finalizing) per interaction cycle with the user, unless explicitly instructed otherwise (like setting Work_Start_Time and Morning_Summary_Time together).
  *   **Use History & Current Prefs:** Always refer to the `Conversation History` and `Current User Preferences` to understand the current state and what you've already asked or set.
  *   **Format Adherence:** Be strict about the HH:MM format for times before calling the tool. If the user provides "9", ask them "Is that 9 AM (09:00) or 9 PM (21:00)?".
  *   **Tool Call Precision:** When calling `update_user_preferences`, ensure the `updates` dictionary contains only the key(s) relevant to the current step and that their values are correctly formatted.
  *   **Conciseness:** Keep your questions and confirmations clear and to the point.

  **Context Provided:**
  - Current User Preferences (JSON Object): Check for `null` values for required keys.
  - Conversation History: See what you last asked for and how the user replied.

  **Tools Available During Onboarding:**
  - `update_user_preferences`: Parameters `{{"updates": {{KEY: "VALUE"}}}}`. Ensure VALUE is correctly formatted by you before the call.
  - `initiate_calendar_connection`: No parameters.
  
onboarding_agent_human_prompt: |
  Current Preferences:
  ```json
  {current_preferences_json}
    History:
    {conversation_history}
    User message: {message}
    Your Task: Based on the system instructions, determine the next step: Ask for the next missing REQUIRED preference (TimeZone, Work_Start_Time, Work_End_Time, Preferred_Session_Length), ask about calendar connection, call a tool (update_user_preferences or initiate_calendar_connection), or finalize onboarding (by calling update_user_preferences with status: active). Formulate your response or tool call.
# --- END OF FILE config/prompts.yaml ---

# --- END OF FILE config/prompts.yaml ---



================================================================================
üìÑ config/messages.yaml
================================================================================

# --- START OF FILE config/messages.yaml ---

# config/messages.yaml

# --- Add these ---
welcome_confirmation_message: |
  Hello! üëã Welcome to WhatsTasker!
  Ready to manage your time like a pro?

  I can help you:
  üß† Capture tasks & reminders instantly.
  üìÖ Sync with your Google Calendar.
  ‚è±Ô∏è Schedule focused work sessions.
  ‚òÄÔ∏è Get daily check-ins & reviews (soon!).

  Unlock your productivity! Ready to start? (yes/no)

  ---
  ◊©◊ú◊ï◊ù! üëã ◊ë◊®◊ï◊ö ◊î◊ë◊ê ◊ú-WhatsTasker!
  ◊û◊ï◊õ◊ü/◊î ◊ú◊†◊î◊ú ◊ê◊™ ◊î◊ñ◊û◊ü ◊©◊ú◊ö ◊õ◊û◊ï ◊û◊ß◊¶◊ï◊¢◊ü/◊ô◊™?

  ◊ê◊†◊ô ◊ô◊õ◊ï◊ú/◊î ◊ú◊¢◊ñ◊ï◊® ◊ú◊ö:
  üß† ◊ú◊î◊ï◊°◊ô◊£ ◊û◊©◊ô◊û◊ï◊™ ◊ï◊™◊ñ◊õ◊ï◊®◊ï◊™ ◊û◊ô◊ô◊ì◊ô◊™.
  üìÖ ◊ú◊î◊°◊™◊†◊õ◊®◊ü ◊¢◊ù ◊ô◊ï◊û◊ü ◊í◊ï◊í◊ú ◊©◊ú◊ö.
  ‚è±Ô∏è ◊ú◊ß◊ë◊ï◊¢ ◊ñ◊û◊†◊ô ◊¢◊ë◊ï◊ì◊î ◊ë◊ô◊ï◊û◊ü ◊©◊ú◊ö ◊õ◊ì◊ô ◊ú◊î◊ë◊ò◊ô◊ó ◊©◊™◊©◊ú◊ô◊ù ◊ê◊™ ◊î◊û◊©◊ô◊û◊ï◊™ ◊©◊ú◊ö.
  ‚òÄÔ∏è ◊ú◊¢◊ñ◊ï◊® ◊ú◊ö ◊ú◊¢◊ß◊ï◊ë ◊ê◊ó◊®◊ô ◊ë◊ô◊¶◊ï◊¢ ◊î◊û◊©◊ô◊û◊ï◊™.

  ◊©◊ó◊®◊®/◊ô ◊ê◊™ ◊î◊§◊®◊ï◊ì◊ï◊ß◊ò◊ô◊ë◊ô◊ï◊™ ◊©◊ú◊ö! ◊û◊ï◊õ◊ü/◊î ◊ú◊î◊™◊ó◊ô◊ú? (◊õ◊ü/◊ú◊ê)

setup_starting_message: "Great! Let's set things up. I'll ask a few questions to configure your preferences."
setup_declined_message: "Okay, no problem. Just message me again whenever you're ready to set things up!"
ask_confirmation_again_message: "Sorry, I didn't quite understand that. Are you ready to start the setup process? (yes/no)"
user_registered_already_message: "Welcome back! How can I help you today?" # Optional: For returning users found by get_agent

# --- Keep existing messages ---
generic_error_message: "Sorry, an unexpected error occurred. Please try again."
intent_parse_error_message: "Sorry, I had trouble understanding the structure of that request."
intent_unknown_message: "Sorry, I'm not sure how to help with that. You can ask me to add tasks, list tasks, or change settings."
intent_clarify_message: "Sorry, I didn't quite understand that. Could you please rephrase?"
# ... add any other messages you have ...

# --- END OF FILE config/messages.yaml ---



================================================================================
üìÑ config/settings.yaml
================================================================================

# --- START OF FILE config/settings.yaml ---



# --- END OF FILE config/settings.yaml ---



================================================================================
üìÑ main.py
================================================================================

# --- START OF FILE main.py ---

# --- START OF FULL main.py ---

import os
import sys
import asyncio
import signal
import argparse
from dotenv import load_dotenv
load_dotenv()

# --- Determine Bridge Type ---
DEFAULT_BRIDGE = "whatsapp"
ALLOWED_BRIDGES = ["cli", "whatsapp"]
bridge_type_env = os.getenv("BRIDGE_TYPE", "").lower()
parser = argparse.ArgumentParser(description="Run WhatsTasker Backend")
parser.add_argument("--bridge", type=str, choices=ALLOWED_BRIDGES, help=f"Specify the bridge interface ({', '.join(ALLOWED_BRIDGES)})")
args = parser.parse_args()
bridge_type_arg = args.bridge.lower() if args.bridge else None
bridge_type = DEFAULT_BRIDGE
if bridge_type_arg: bridge_type = bridge_type_arg
if bridge_type_env in ALLOWED_BRIDGES: bridge_type = bridge_type_env
if bridge_type not in ALLOWED_BRIDGES: print(f"ERROR: Invalid bridge type '{bridge_type}'."); sys.exit(1)

# --- Logger Import (must happen early) ---
# Use a try-except block for robust logger initialization
try:
    from tools.logger import log_info, log_error, log_warning
    # Test log after import attempt
    log_info("main", "init", "Logger imported successfully.")
except ImportError as log_import_err:
    # Fallback if logger import fails catastrophically
    print(f"FATAL ERROR: Failed to import logger: {log_import_err}")
    sys.exit(1)
except Exception as log_init_e:
    print(f"FATAL ERROR during initial logging setup: {log_init_e}")
    sys.exit(1)


# --- Dynamic Bridge and App Import ---
uvicorn_app_path = None
bridge_module_name = None
bridge_instance = None

try:
    if bridge_type == "cli":
        from bridge.cli_interface import app as fastapi_app, CLIBridge, outgoing_cli_messages, cli_queue_lock
        uvicorn_app_path = "bridge.cli_interface:app"
        bridge_module_name = "CLI Bridge"
        bridge_instance = CLIBridge(outgoing_cli_messages, cli_queue_lock)
    elif bridge_type == "whatsapp":
        from bridge.whatsapp_interface import app as fastapi_app, WhatsAppBridge, outgoing_whatsapp_messages, whatsapp_queue_lock
        uvicorn_app_path = "bridge.whatsapp_interface:app"
        bridge_module_name = "WhatsApp Bridge"
        bridge_instance = WhatsAppBridge(outgoing_whatsapp_messages, whatsapp_queue_lock)
    else:
        # This should not happen due to initial checks, but keeps linters happy
        raise ValueError(f"Internal logic error determining bridge type: {bridge_type}")

    # --- Set the Bridge in the Router ---
    from bridge.request_router import set_bridge
    set_bridge(bridge_instance) # <-- EXPLICITLY SET THE BRIDGE HERE
    # ------------------------------------

    log_info("main", "init", f"WhatsTasker v0.8 starting...")
    log_info("main", "init", f"Using Bridge Interface: {bridge_module_name} (Selected: '{bridge_type}', Path: '{uvicorn_app_path}')")

except ImportError as import_err:
    log_error("main", "init", f"Failed to import bridge module for type '{bridge_type}': {import_err}", import_err)
    sys.exit(1)
except Exception as bridge_setup_err:
    log_error("main", "init", f"Failed during bridge setup for type '{bridge_type}': {bridge_setup_err}", bridge_setup_err)
    sys.exit(1)


# --- Other Imports ---
import uvicorn
from users.user_manager import init_all_agents
import traceback # Keep traceback import

# --- Scheduler Import ---
try:
    from services.scheduler_service import start_scheduler, shutdown_scheduler
    SCHEDULER_IMPORTED = True
    log_info("main", "import", "Scheduler service imported successfully.")
except ImportError as sched_import_err:
    log_error("main", "import", f"Scheduler service not found or failed import: {sched_import_err}. Background tasks disabled.", sched_import_err)
    SCHEDULER_IMPORTED = False
    # Define dummy functions to prevent crashes
    def start_scheduler(): return False
    def shutdown_scheduler(): pass
# ----------------------------


async def handle_shutdown_signal(sig, loop):
    """Async signal handler helper."""
    log_warning("main", "handle_shutdown_signal", f"Received signal {sig.name}. Initiating shutdown...")
    # Signal the main tasks to stop (implementation depends on how server/tasks are managed)
    # For Uvicorn, we can tell the server instance to exit
    if server: # Check if server object exists
         server.should_exit = True
    # Additionally, cancel other background tasks if necessary
    # Example: Cancel a long-running task
    # if some_background_task and not some_background_task.done():
    #    some_background_task.cancel()

    # Give tasks a moment to finish cleanup
    await asyncio.sleep(1)

    # Optionally force stop loop if tasks don't exit gracefully
    # loop.stop()


server: uvicorn.Server | None = None # Define server variable in outer scope

async def main_async():
    global server # Allow modification of the global server variable
    # Agent state init
    log_info("main", "main_async", "Initializing agent states...")
    try:
        init_all_agents()
        log_info("main", "main_async", "Agent state initialization complete.")
    except Exception as init_e:
        log_error("main", "main_async", "CRITICAL error during init_all_agents.", init_e)
        sys.exit(1) # Exit if agent init fails

    # Scheduler start
    if SCHEDULER_IMPORTED:
        try:
            log_info("main", "main_async", "Starting scheduler service...")
            if start_scheduler():
                log_info("main", "main_async", "Scheduler service started successfully.")
            else:
                # Error should be logged by start_scheduler if it returns False
                log_error("main", "main_async", "Scheduler service FAILED to start.")
        except Exception as sched_e:
            log_error("main", "main_async", "CRITICAL error starting scheduler.", sched_e)
            # Decide if this is fatal - potentially continue without scheduler?
            # For now, let's log and continue

    # Uvicorn config
    reload_enabled = os.getenv("APP_ENV", "production").lower() == "development"
    log_level = "debug" if reload_enabled else "info"
    # --- Define Port ---
    server_port = int(os.getenv("PORT", "8000")) # Read from env or default to 8000
    # -------------------
    log_info("main", "main_async", f"Starting FastAPI server via Uvicorn...")
    log_info("main", "main_async", f"Target App: '{uvicorn_app_path}'")
    log_info("main", "main_async", f"Host: 0.0.0.0, Port: {server_port}") # Log the port
    log_info("main", "main_async", f"Reload: {reload_enabled}, Log Level: {log_level}")

    config = uvicorn.Config(
        uvicorn_app_path,
        host="0.0.0.0",
        port=server_port, # Use the variable
        reload=reload_enabled,
        access_log=False, # Keep access log off unless needed for debugging
        log_level=log_level,
        lifespan="on" # Recommended for modern FastAPI startup/shutdown events
    )
    server = uvicorn.Server(config)

    # --- Graceful shutdown setup (Modern asyncio) ---
    loop = asyncio.get_running_loop()
    for sig in (signal.SIGINT, signal.SIGTERM):
        try:
            # Use loop.create_task for the handler to run it in the loop
            loop.add_signal_handler(sig, lambda s=sig: asyncio.create_task(handle_shutdown_signal(s, loop)))
        except NotImplementedError:
            # Fallback for systems like Windows that might not support add_signal_handler
            signal.signal(sig, lambda s, f: asyncio.create_task(handle_shutdown_signal(signal.Signals(s), loop)))
    # --- End shutdown setup ---

    try:
        await server.serve()
    finally:
        log_info("main", "main_async", "Server stopped. Performing final cleanup...")
        if SCHEDULER_IMPORTED:
            try:
                log_info("main", "main_async", "Shutting down scheduler...")
                shutdown_scheduler()
                log_info("main", "main_async", "Scheduler shut down.")
            except Exception as e:
                log_error("main", "main_async", "Error shutting down scheduler.", e)
        log_info("main", "main_async", "Main async process finished.")


if __name__ == "__main__":
    try:
        # Check logger exists before trying to use it
        _ = log_info
    except NameError:
        print("FATAL: Logger not defined or failed import.")
        sys.exit(1)

    try:
        asyncio.run(main_async())
    except SystemExit as se: # Catch SystemExit from Uvicorn startup failure
         log_error("main", "__main__", f"Server exited with code: {se.code}. Check previous errors (e.g., port conflict).")
         sys.exit(se.code) # Propagate the exit code
    except KeyboardInterrupt:
        log_warning("main", "__main__", "KeyboardInterrupt received. Exiting.")
        # Perform minimal cleanup if needed
        if SCHEDULER_IMPORTED:
             try: shutdown_scheduler()
             except Exception: pass
        sys.exit(0)
    except Exception as e:
        log_error("main", "__main__", f"Unhandled error during server execution/shutdown.", e)
        log_error("main", "__main__", f"Traceback:\n{traceback.format_exc()}")
        # Final attempt to shutdown scheduler
        if SCHEDULER_IMPORTED:
            try: shutdown_scheduler()
            except Exception as final_e: log_error("main","main", f"Error in final shutdown attempt: {final_e}")
        sys.exit(1)
    finally:
        log_info("main", "__main__", "Application exiting.")

# --- END OF FULL main.py ---

# --- END OF FILE main.py ---



================================================================================
üìÑ bridge/request_router.py
================================================================================

# --- START OF FILE bridge/request_router.py ---

# --- START OF FULL bridge/request_router.py ---

import re
import os
import yaml
import traceback
import json
from typing import Optional, Tuple, List # Keep Optional if used elsewhere internally
from tools.logger import log_info, log_error, log_warning # Keep logger for operational logs

# --- Database Logging Import ---
try:
    import tools.activity_db as activity_db
    ACTIVITY_DB_IMPORTED = True
except ImportError:
    log_error("request_router", "import", "Failed to import activity_db. Message DB logging disabled.", None)
    ACTIVITY_DB_IMPORTED = False
    # Define a dummy function if import fails
    class activity_db:
        @staticmethod
        def log_message_db(*args, **kwargs): pass
# --- End DB Import ---

# State manager imports
from services.agent_state_manager import get_agent_state, add_message_to_user_history

# User/Agent Management
from users.user_manager import get_agent

# Orchestrator Import
try:
    from agents.orchestrator_agent import handle_user_request as route_to_orchestrator
    ORCHESTRATOR_IMPORTED = True
    log_info("request_router", "import", "Successfully imported OrchestratorAgent handler.")
except ImportError as e:
    ORCHESTRATOR_IMPORTED = False; log_error("request_router", "import", f"OrchestratorAgent import failed: {e}", e); route_to_orchestrator = None

# Onboarding Agent Import
try:
    from agents.onboarding_agent import handle_onboarding_request
    ONBOARDING_AGENT_IMPORTED = True
    log_info("request_router", "import", "Successfully imported OnboardingAgent handler.")
except ImportError as e:
     ONBOARDING_AGENT_IMPORTED = False; log_error("request_router", "import", f"OnboardingAgent import failed: {e}", e); handle_onboarding_request = None

# Task Query Service Import
try:
    from services.task_query_service import get_context_snapshot
    QUERY_SERVICE_IMPORTED = True
except ImportError as e:
     QUERY_SERVICE_IMPORTED = False; log_error("request_router", "import", f"TaskQueryService import failed: {e}", e); get_context_snapshot = None

# Cheat Service Import
try:
    from services.cheats import handle_cheat_command
    CHEATS_IMPORTED = True
    log_info("request_router", "import", "Successfully imported Cheats service.")
except ImportError as e:
     CHEATS_IMPORTED = False; log_error("request_router", "import", f"Cheats service import failed: {e}", e); handle_cheat_command = None

# ConfigManager Import
try:
    from services.config_manager import set_user_status
    CONFIG_MANAGER_IMPORTED = True
except ImportError as e:
     CONFIG_MANAGER_IMPORTED = False; log_error("request_router", "import", f"ConfigManager import failed: {e}", e); set_user_status = None


# Load standard messages
_messages = {}
try:
    messages_path = os.path.join("config", "messages.yaml")
    if os.path.exists(messages_path):
        with open(messages_path, 'r', encoding="utf-8") as f:
             content = f.read();
             if content.strip(): f.seek(0); _messages = yaml.safe_load(f) or {}
             else: _messages = {}
    else: log_warning("request_router", "import", f"{messages_path} not found."); _messages = {}
except Exception as e: log_error("request_router", "import", f"Failed to load messages.yaml: {e}", e); _messages = {}

GENERIC_ERROR_MSG = _messages.get("generic_error_message", "Sorry, an unexpected error occurred.")
WELCOME_MESSAGE = _messages.get("welcome_confirmation_message", "Hello! Welcome to WhatsTasker.")


# Global bridge instance
current_bridge = None

def normalize_user_id(user_id: str) -> str:
    """Removes non-digit characters. Specific to phone number IDs."""
    # Consider if other ID types might exist later.
    # For WhatsApp, it might receive 'number@c.us'. We want just the number part usually.
    if user_id and '@' in user_id:
        user_id = user_id.split('@')[0]
    return re.sub(r'\D', '', user_id) if user_id else ""

def set_bridge(bridge_instance):
    """Sets the active bridge instance for sending messages."""
    global current_bridge
    if current_bridge is None:
        current_bridge = bridge_instance
        log_info("request_router", "set_bridge", f"Bridge set to: {type(bridge_instance).__name__}")
    else:
        log_warning("request_router", "set_bridge", "Attempted to set bridge when one is already configured.")


def send_message(user_id: str, message: str):
    """Adds agent message to history, logs it to DB, and sends via configured bridge."""
    fn_name = "send_message"
    # Note: user_id received here is expected to be the *normalized* ID

    # 1. Add to conversation history in memory
    if user_id and message:
        add_message_to_user_history(user_id, "agent", message)
    else:
        log_warning("request_router", fn_name, f"Attempted add empty msg/invalid user_id ({user_id}) to history.")
        # Don't proceed if invalid
        return

    # 2. Log outgoing message to Database
    if ACTIVITY_DB_IMPORTED:
        activity_db.log_message_db(
            direction='OUT',
            user_id=user_id, # Log with normalized ID
            content=message,
            raw_user_id=None, # Raw ID isn't typically available here
            bridge_message_id=None # Bridge ID generated later by the bridge instance
        )
    # else: DB logging disabled or failed import

    # 3. Send via Bridge
    log_info("request_router", fn_name, f"Queuing OUT message for {user_id}: '{message[:150]}...'")
    if current_bridge:
        try:
            # The bridge's send_message method handles formatting (like adding @c.us)
            # and generating the unique bridge_message_id for ACK
            current_bridge.send_message(user_id, message)
        except Exception as e:
            # Log error with user_id context
            log_error("request_router", fn_name, f"Bridge error sending message to {user_id}: {e}", e, user_id=user_id)
    else:
        log_error("request_router", fn_name, "No bridge configured. Cannot send message.")


# --- Main Handler ---
def handle_incoming_message(user_id: str, message: str) -> str:
    """
    Routes incoming messages based on user status (new, onboarding, active) or cheat codes.
    Logs incoming message to DB.
    """
    fn_name = "handle_incoming_message"
    final_response_message = GENERIC_ERROR_MSG
    # Keep the raw user_id received from the bridge for logging/potential use
    raw_user_id = user_id

    try:
        log_info("request_router", fn_name, f"Received raw: {raw_user_id} msg: '{message[:50]}...'")
        norm_user_id = normalize_user_id(raw_user_id)

        if not norm_user_id:
            log_error("request_router", fn_name, f"Invalid User ID after normalization: {raw_user_id}", user_id=raw_user_id) # Log with raw ID context
            # Don't send response here, let the caller handle HTTP error
            return "Error: Invalid User ID." # Or raise exception?

        # --- Log Incoming Message to DB ---
        if ACTIVITY_DB_IMPORTED:
            activity_db.log_message_db(
                direction='IN',
                user_id=norm_user_id, # Log with normalized ID
                content=message,
                raw_user_id=raw_user_id # Store original ID from bridge
            )
        # ---------------------------------

        # --- Ensure agent state exists ---
        agent_state = get_agent(norm_user_id) # Uses normalized ID
        if not agent_state:
            # Log error with normalized ID context
            log_error("request_router", fn_name, f"CRITICAL: Failed get/create agent state for {norm_user_id}.", user_id=norm_user_id)
            # Send generic error back
            send_message(norm_user_id, GENERIC_ERROR_MSG)
            return GENERIC_ERROR_MSG

        current_status = agent_state.get("preferences", {}).get("status")
        log_info("request_router", fn_name, f"User {norm_user_id} status: {current_status}")

        # --- Routing Logic (Uses norm_user_id internally) ---

        # 1. New User: Send welcome, set status to onboarding
        if current_status == "new":
            log_info("request_router", fn_name, f"New user ({norm_user_id}). Sending welcome, setting status to onboarding.")
            send_message(norm_user_id, WELCOME_MESSAGE) # Sends via bridge
            if CONFIG_MANAGER_IMPORTED and set_user_status:
                if not set_user_status(norm_user_id, 'onboarding'):
                     log_error("request_router", fn_name, f"Failed update status from 'new' to 'onboarding' for {norm_user_id}", user_id=norm_user_id)
            else: log_error("request_router", fn_name, "ConfigManager unavailable, cannot update user status after welcome.", user_id=norm_user_id)
            # No further processing needed this turn; the ack is handled by the caller (FastAPI endpoint)
            # We return the message mainly for potential testing/logging in the caller, though the primary action is send_message
            return WELCOME_MESSAGE

        # 2. Cheat Codes (Check before onboarding/active routing)
        message_stripped = message.strip()
        if message_stripped.startswith('/') and CHEATS_IMPORTED and handle_cheat_command:
            parts = message_stripped.split(); command = parts[0].lower(); args = parts[1:]
            log_info("request_router", fn_name, f"Detected command '{command}' for {norm_user_id}. Routing to Cheats.")
            try:
                command_response = handle_cheat_command(norm_user_id, command, args)
            except Exception as e:
                log_error("request_router", fn_name, f"Error executing cheat '{command}': {e}", e, user_id=norm_user_id)
                command_response = "Error processing cheat."
            send_message(norm_user_id, command_response) # Send result back
            return command_response # Return result for caller

        elif message_stripped.startswith('/') and not CHEATS_IMPORTED:
             log_error("request_router", fn_name, "Cheat command detected, but Cheats service failed import.", user_id=norm_user_id)
             err_msg = "Error: Command processor unavailable."
             send_message(norm_user_id, err_msg)
             return err_msg

        # 3. Onboarding User: Route to onboarding agent
        elif current_status == "onboarding":
            log_info("request_router", fn_name, f"User {norm_user_id} is onboarding. Routing to onboarding agent.")
            add_message_to_user_history(norm_user_id, "user", message) # Add user reply to memory history first
            if ONBOARDING_AGENT_IMPORTED and handle_onboarding_request:
                try:
                     history = agent_state.get("conversation_history", [])
                     preferences = agent_state.get("preferences", {})
                     # This function is expected to return the response message string
                     final_response_message = handle_onboarding_request(norm_user_id, message, history, preferences)
                except Exception as onboard_e:
                     tb_str = traceback.format_exc()
                     # Log error with user context
                     log_error("request_router", fn_name, f"Error calling OnboardingAgent for {norm_user_id}. Traceback:\n{tb_str}", onboard_e, user_id=norm_user_id)
                     final_response_message = GENERIC_ERROR_MSG
            else:
                # Log error with user context
                log_error("request_router", fn_name, f"Onboarding required for {norm_user_id}, but onboarding agent not imported.", user_id=norm_user_id)
                final_response_message = "Sorry, there's an issue with the setup process right now."

        # 4. Active User: Route to main orchestrator
        elif current_status == "active":
            log_info("request_router", fn_name, f"User {norm_user_id} is active. Routing to orchestrator.")
            add_message_to_user_history(norm_user_id, "user", message)
            if ORCHESTRATOR_IMPORTED and route_to_orchestrator and QUERY_SERVICE_IMPORTED and get_context_snapshot:
                try:
                    history = agent_state.get("conversation_history", [])
                    preferences = agent_state.get("preferences", {})
                    task_context, calendar_context = get_context_snapshot(norm_user_id)
                    # This function returns the response string
                    final_response_message = route_to_orchestrator(
                        user_id=norm_user_id, message=message, history=history,
                        preferences=preferences, task_context=task_context, calendar_context=calendar_context)
                except Exception as orch_e:
                     tb_str = traceback.format_exc()
                     log_error("request_router", fn_name, f"Error calling OrchestratorAgent for {norm_user_id}. Traceback:\n{tb_str}", orch_e, user_id=norm_user_id)
                     final_response_message = GENERIC_ERROR_MSG
            else:
                 # Log error with user context
                 log_error("request_router", fn_name, f"Core components missing for active user {norm_user_id} (Orchestrator={ORCHESTRATOR_IMPORTED}, QueryService={QUERY_SERVICE_IMPORTED}).", user_id=norm_user_id)
                 final_response_message = "Sorry, I can't process your request right now due to a system issue."

        # 5. Unknown Status: Log error and give generic response
        else:
            log_error("request_router", fn_name, f"User {norm_user_id} has unknown status: '{current_status}'. Sending generic error.", user_id=norm_user_id)
            final_response_message = GENERIC_ERROR_MSG

        # --- Send the final response (if not handled earlier, like for welcome/cheat) ---
        # The agent functions (onboarding/orchestrator) return the message string.
        # We need to send it using our central send_message function.
        if final_response_message:
             send_message(norm_user_id, final_response_message)
        else:
             # This case means the agent function returned None or empty string, which is unexpected.
             log_warning("request_router", fn_name, f"Agent function returned empty response for {norm_user_id}. Sending generic error.", user_id=norm_user_id)
             send_message(norm_user_id, GENERIC_ERROR_MSG)
             final_response_message = GENERIC_ERROR_MSG # Ensure we return something

        # Return the message primarily for testing/ack purposes in the caller API
        return final_response_message

    except Exception as outer_e:
        tb_str = traceback.format_exc()
        # Try to include user ID in log if possible
        user_context = raw_user_id if 'raw_user_id' in locals() else "Unknown"
        log_error("request_router", fn_name, f"Unexpected outer error processing message for {user_context}. Traceback:\n{tb_str}", outer_e, user_id=user_context)
        # Attempt to send generic error if possible
        if 'norm_user_id' in locals() and norm_user_id:
            try: send_message(norm_user_id, GENERIC_ERROR_MSG)
            except: pass # Avoid errors during error handling
        return GENERIC_ERROR_MSG # Return generic error message

# --- END OF FULL bridge/request_router.py ---

# --- END OF FILE bridge/request_router.py ---



================================================================================
üìÑ bridge/cli_interface.py
================================================================================

# --- START OF FILE bridge/cli_interface.py ---

# --- START OF FULL bridge/cli_interface.py ---

from fastapi import FastAPI, Request, HTTPException
from fastapi.responses import JSONResponse
import uvicorn
import uuid
from threading import Lock, Thread
import time
import json # Added for error handling

# Use the central logger
from tools.logger import log_info, log_error, log_warning
# Import the central router and its setter function
from bridge.request_router import handle_incoming_message, set_bridge

# Ensure calendar_tool provides the router correctly (needed for OAuth)
try:
    from tools.calendar_tool import router as calendar_router
    CALENDAR_ROUTER_IMPORTED = True
    log_info("cli_interface", "import", "Successfully imported calendar_router.")
except ImportError:
    log_error("cli_interface", "import", "Could not import calendar_router from tools.calendar_tool. OAuth callback will fail if CLI mode used.")
    CALENDAR_ROUTER_IMPORTED = False
    from fastapi import APIRouter
    calendar_router = APIRouter()


# Define a CLI Bridge
# Global in-memory store for CLI outgoing messages.
outgoing_cli_messages = []
cli_queue_lock = Lock()

class CLIBridge:
    """Bridge that handles message queuing for CLI interaction."""
    def __init__(self, message_queue, lock):
        self.message_queue = message_queue
        self.lock = lock
        log_info("CLIBridge", "__init__", "CLI Bridge initialized for queuing.")

    # --- UPDATED send_message ---
    def send_message(self, user_id: str, message: str):
        """
        Adds the outgoing message to the CLI queue.
        Does NOT log the message content here (handled by request_router).
        """
        # user_id received here is the NORMALIZED ID from request_router
        if not user_id or not message:
             log_warning("CLIBridge", "send_message", f"Attempted to queue empty message or invalid user_id for CLI: {user_id}")
             return

        outgoing = {
            "user_id": user_id, # Use the normalized ID for CLI mock
            "message": message,
            "message_id": str(uuid.uuid4()) # Generate ID, might be used by mock sender ACK
        }
        with self.lock:
            self.message_queue.append(outgoing)
        # Log the queuing action
        log_info("CLIBridge", "send_message", f"Message for CLI user {user_id} queued (ID: {outgoing['message_id']}). Queue size: {len(self.message_queue)}")
    # --- END UPDATED send_message ---

# Set the global bridge in the router to use our CLI Bridge instance
# This should only be called by main.py if CLI mode is selected
# if __name__ != "__main__": # Crude check
#     set_bridge(CLIBridge(outgoing_cli_messages, cli_queue_lock))
#     log_info("cli_interface", "init", "CLI Bridge potentially set in request_router.")

def create_cli_app() -> FastAPI:
    """Creates the FastAPI app instance for the CLI Interface."""
    app = FastAPI(
        title="WhatsTasker CLI Bridge API",
        description="Handles interaction for the CLI mock sender.",
        version="1.0.0"
    )

    # Include calendar routes if needed
    if CALENDAR_ROUTER_IMPORTED:
        app.include_router(calendar_router, prefix="", tags=["Authentication"])
        log_info("cli_interface", "create_cli_app", "Calendar router included.")
    else:
         log_warning("cli_interface", "create_cli_app", "Calendar router not included.")


    # --- API Endpoints (Adjusted for CLI mock) ---
    @app.post("/incoming", tags=["CLI Bridge"])
    async def incoming_cli_message(request: Request):
        """Receives message from CLI mock, processes it, queues response, returns ack."""
        endpoint_name = "incoming_cli_message"
        try:
            data = await request.json()
            user_id = data.get("user_id") # Expecting normalized ID from mock sender
            message = data.get("message")
            if not user_id or message is None:
                log_warning("cli_interface", endpoint_name, f"Received invalid payload: {data}")
                raise HTTPException(status_code=400, detail="Missing user_id or message")

            log_info("cli_interface", endpoint_name, f"Received message via CLI bridge from user {user_id}: '{str(message)[:50]}...'")

            # Pass normalized ID to router, router handles DB logging
            handle_incoming_message(user_id, str(message))

            # Return only an acknowledgment.
            return JSONResponse(content={"ack": True})

        except json.JSONDecodeError:
            log_error("cli_interface", endpoint_name, "Received non-JSON payload.")
            raise HTTPException(status_code=400, detail="Invalid JSON payload")
        except HTTPException as http_exc:
            raise http_exc
        except Exception as e:
            log_error("cli_interface", endpoint_name, "Error processing incoming CLI message", e)
            raise HTTPException(status_code=500, detail="Internal server error processing message")

    @app.get("/outgoing", tags=["CLI Bridge"])
    async def get_outgoing_cli_messages():
        """Returns and clears the list of queued outgoing messages for the CLI mock."""
        # This endpoint *differs* from the WhatsApp one - it clears on GET
        endpoint_name = "get_outgoing_cli_messages"
        msgs_to_send = []
        with cli_queue_lock:
            # Return all messages currently in the queue and clear it
            msgs_to_send = outgoing_cli_messages[:] # Copy the list
            outgoing_cli_messages.clear()          # Clear the original list
        if msgs_to_send:
            log_info("cli_interface", endpoint_name, f"Returning {len(msgs_to_send)} messages from CLI queue (and clearing).")
        return JSONResponse(content={"messages": msgs_to_send})

    @app.post("/ack", tags=["CLI Bridge"])
    async def acknowledge_cli_message(request: Request):
        """Receives acknowledgment (currently does nothing for CLI as queue is cleared on GET)."""
        endpoint_name = "acknowledge_cli_message"
        try:
            data = await request.json()
            message_id = data.get("message_id")
            if not message_id:
                log_warning("cli_interface", endpoint_name, f"Received ACK without message_id: {data}")
                raise HTTPException(status_code=400, detail="Missing message_id")

            # Log but don't modify queue here, as GET already cleared it for CLI mock
            log_info("cli_interface", endpoint_name, f"CLI Ack received for message {message_id} (queue already cleared by GET).")
            return JSONResponse(content={"ack_received": True, "removed": False}) # Indicate not removed by ACK
        except json.JSONDecodeError:
            log_error("cli_interface", endpoint_name, "Received non-JSON ACK payload.")
            raise HTTPException(status_code=400, detail="Invalid JSON payload for ACK")
        except HTTPException as http_exc:
            raise http_exc
        except Exception as e:
            log_error("cli_interface", endpoint_name, f"Error processing CLI ACK for message_id {data.get('message_id', 'N/A')}", e)
            raise HTTPException(status_code=500, detail="Internal server error processing ACK")

    return app

# Create the FastAPI app instance for this interface
# main.py should import 'app' from here if cli mode is selected
app = create_cli_app()

# --- END OF FULL bridge/cli_interface.py ---

# --- END OF FILE bridge/cli_interface.py ---



================================================================================
üìÑ bridge/whatsapp_interface.py
================================================================================

# --- START OF FILE bridge/whatsapp_interface.py ---

# --- START OF FULL bridge/whatsapp_interface.py ---

from fastapi import FastAPI, Request, HTTPException
from fastapi.responses import JSONResponse
import uvicorn # Keep for potential direct running/debugging
import uuid
from threading import Lock
import json # Import json for error handling
import re # Import re for checking format

# Use the central logger
from tools.logger import log_info, log_error, log_warning
# Import the central router and its setter function
from bridge.request_router import handle_incoming_message, set_bridge

# Try importing the calendar router (necessary for OAuth callback)
try:
    from tools.calendar_tool import router as calendar_router
    CALENDAR_ROUTER_IMPORTED = True
    log_info("whatsapp_interface", "import", "Successfully imported calendar_router.")
except ImportError:
    log_error("whatsapp_interface", "import", "Could not import calendar_router from tools.calendar_tool. OAuth callback will fail.")
    CALENDAR_ROUTER_IMPORTED = False
    # Define a dummy router if import fails to allow server start
    from fastapi import APIRouter
    calendar_router = APIRouter()

# --- Bridge Definition ---
# Global in-memory store for outgoing messages destined for WhatsApp and its lock.
outgoing_whatsapp_messages = []
whatsapp_queue_lock = Lock()

class WhatsAppBridge:
    """Bridge that handles message queuing for the Node.js WhatsApp Poller."""
    def __init__(self, message_queue, lock):
        self.message_queue = message_queue
        self.lock = lock
        log_info("WhatsAppBridge", "__init__", "WhatsApp Bridge initialized for queuing.")

    # --- UPDATED send_message ---
    def send_message(self, user_id: str, message: str):
        """
        Adds the outgoing message with a unique ID to the shared WhatsApp queue.
        Ensures the user_id has the correct '@c.us' suffix for WhatsApp.
        Does NOT log the message content here (handled by request_router).
        """
        # user_id received here is the NORMALIZED ID from request_router
        if not user_id or not message:
             log_warning("WhatsAppBridge", "send_message", f"Attempted to queue empty message or invalid user_id for WhatsApp: {user_id}")
             return

        # --- Format User ID for WhatsApp Web JS ---
        formatted_user_id = user_id
        # Assume normalized ID is digits only. Add @c.us suffix.
        # (Add @g.us logic later if groups are supported)
        if re.match(r'^\d+$', user_id):
            formatted_user_id = f"{user_id}@c.us"
            # Log the formatting action, but not the content
            # log_info("WhatsAppBridge", "send_message", f"Formatted user_id {user_id} -> {formatted_user_id}")
        elif '@' not in user_id:
             # If it's not clearly a phone number and has no @, log a warning.
             log_warning("WhatsAppBridge", "send_message", f"User ID '{user_id}' lacks '@' suffix and is not digits. Sending as is, may fail in whatsapp-web.js.")
        # --- END User ID Formatting ---

        outgoing = {
            "user_id": formatted_user_id, # Use the WhatsApp-formatted ID
            "message": message,
            "message_id": str(uuid.uuid4()) # Unique ID for ACK tracking
        }
        with self.lock:
            self.message_queue.append(outgoing)
        # Log the queuing action itself, including the message ID for traceability
        log_info("WhatsAppBridge", "send_message", f"Message for WA user {formatted_user_id} queued (ID: {outgoing['message_id']}). Queue size: {len(self.message_queue)}")
    # --- END UPDATED send_message ---

# Set the global bridge in the request router to use our WhatsApp Bridge instance
# Pass the shared queue and lock to the bridge instance
# Ensure this happens only if the WhatsApp bridge is selected in main.py
# This initialization logic might need refinement depending on how main.py sets the bridge.
# Assuming main.py calls set_bridge after importing the correct module based on selection:
# if __name__ != "__main__": # Crude check to avoid running this if module imported by mistake?
#    set_bridge(WhatsAppBridge(outgoing_whatsapp_messages, whatsapp_queue_lock))
#    log_info("whatsapp_interface", "init", "WhatsApp Bridge potentially set in request_router.")
# A better approach is for main.py to explicitly call set_bridge after determining the mode.
# Let's assume main.py handles calling set_bridge(WhatsAppBridge(...))

def create_whatsapp_app() -> FastAPI:
    """Creates the FastAPI application instance for the WhatsApp Interface."""
    app = FastAPI(
        title="WhatsTasker WhatsApp Bridge API",
        description="Handles incoming messages from and outgoing messages to the WhatsApp Web JS bridge.",
        version="1.0.0"
    )

    if CALENDAR_ROUTER_IMPORTED:
        app.include_router(calendar_router, prefix="", tags=["Authentication"])
        log_info("whatsapp_interface", "create_app", "Calendar router included.")
    else:
         log_warning("whatsapp_interface", "create_app", "Calendar router not included.")

    # --- API Endpoints (Keep as they are) ---
    @app.post("/incoming", tags=["WhatsApp Bridge"])
    async def incoming_whatsapp_message(request: Request):
        endpoint_name = "incoming_whatsapp_message"
        try:
            data = await request.json()
            user_id = data.get("user_id")
            message_body = data.get("message")
            if not user_id or message_body is None:
                log_warning("whatsapp_interface", endpoint_name, f"Received invalid payload: {data}")
                raise HTTPException(status_code=400, detail="Missing user_id or message")
            # Pass raw ID to router, router handles normalization and DB logging
            handle_incoming_message(user_id, str(message_body))
            return JSONResponse(content={"ack": True}, status_code=200)
        except json.JSONDecodeError: # Renamed variable
            log_error("whatsapp_interface", endpoint_name, "Received non-JSON payload.")
            raise HTTPException(status_code=400, detail="Invalid JSON payload")
        except HTTPException as http_exc:
            raise http_exc
        except Exception as e:
            log_error("whatsapp_interface", endpoint_name, "Error processing incoming WhatsApp message", e)
            raise HTTPException(status_code=500, detail="Internal server error processing message")

    @app.get("/outgoing", tags=["WhatsApp Bridge"])
    async def get_outgoing_whatsapp_messages():
        endpoint_name = "get_outgoing_whatsapp_messages"
        msgs_to_send = []
        with whatsapp_queue_lock:
            msgs_to_send = outgoing_whatsapp_messages[:]
        if msgs_to_send:
            log_info("whatsapp_interface", endpoint_name, f"Returning {len(msgs_to_send)} messages from WA queue (without clearing).")
        return JSONResponse(content={"messages": msgs_to_send})

    @app.post("/ack", tags=["WhatsApp Bridge"])
    async def acknowledge_whatsapp_message(request: Request):
        endpoint_name = "acknowledge_whatsapp_message"
        message_id = None
        try:
            data = await request.json()
            message_id = data.get("message_id")
            if not message_id:
                log_warning("whatsapp_interface", endpoint_name, f"Received ACK without message_id: {data}")
                raise HTTPException(status_code=400, detail="Missing message_id in ACK payload")
            removed = False
            with whatsapp_queue_lock:
                index_to_remove = -1
                for i, msg in enumerate(outgoing_whatsapp_messages):
                    if msg.get("message_id") == message_id:
                        index_to_remove = i
                        break
                if index_to_remove != -1:
                    removed_msg = outgoing_whatsapp_messages.pop(index_to_remove)
                    removed = True
                    log_info("whatsapp_interface", endpoint_name, f"WA ACK received and message removed for ID: {message_id}. User: {removed_msg.get('user_id')}. Queue size: {len(outgoing_whatsapp_messages)}")
                else:
                    log_warning("whatsapp_interface", endpoint_name, f"WA ACK received for unknown/already removed message ID: {message_id}")
            return JSONResponse(content={"ack_received": True, "removed": removed})
        except json.JSONDecodeError:
            log_error("whatsapp_interface", endpoint_name, "Received non-JSON ACK payload.")
            raise HTTPException(status_code=400, detail="Invalid JSON payload for ACK")
        except HTTPException as http_exc:
            raise http_exc
        except Exception as e:
            log_error("whatsapp_interface", endpoint_name, f"Error processing WA ACK for message_id {message_id or 'N/A'}", e)
            raise HTTPException(status_code=500, detail="Internal server error processing ACK")

    return app

# Create the FastAPI app instance for this interface
# main.py should import 'app' from here if whatsapp mode is selected
app = create_whatsapp_app()

# --- END OF FULL bridge/whatsapp_interface.py ---

# --- END OF FILE bridge/whatsapp_interface.py ---



================================================================================
üìÑ agents/orchestrator_agent.py
================================================================================

# --- START OF FILE agents/orchestrator_agent.py ---

# --- START OF FULL agents/orchestrator_agent.py ---
import json
import os
import traceback
from typing import Dict, List, Optional, Any # Retain Optional for Pydantic/OpenAI models
from datetime import datetime
import pytz
import re

# Instructor/LLM Imports
from services.llm_interface import get_instructor_client
from openai import OpenAI # Base client for types if needed
from openai.types.chat import ChatCompletionMessage, ChatCompletionToolParam, ChatCompletionMessageToolCall
from openai.types.chat.chat_completion_message_tool_call import Function
from openai.types.shared_params import FunctionDefinition

# Tool Imports & Helpers
# --- Uses the UPDATED tools from tool_definitions ---
from .tool_definitions import AVAILABLE_TOOLS, TOOL_PARAM_MODELS
# --------------------------------------------------
import services.task_manager as task_manager # For parsing duration used in prompt prep

# Utilities
from tools.logger import log_info, log_error, log_warning
import yaml
import pydantic

# --- Load Standard Messages ---
_messages = {}
try:
    messages_path = os.path.join("config", "messages.yaml")
    if os.path.exists(messages_path):
        with open(messages_path, 'r', encoding="utf-8") as f:
             content = f.read()
             if content.strip(): f.seek(0); _messages = yaml.safe_load(f) or {}
             else: _messages = {}
    else: log_warning("orchestrator_agent", "init", f"{messages_path} not found."); _messages = {}
except Exception as e: log_error("orchestrator_agent", "init", f"Failed to load messages.yaml: {e}", e); _messages = {}
GENERIC_ERROR_MSG = _messages.get("generic_error_message", "Sorry, an unexpected error occurred.")


# --- Function to Load Prompt ---
_PROMPT_CACHE: Dict[str, Optional[str]] = {}
def load_orchestrator_prompt() -> Optional[str]:
    """Loads the orchestrator system prompt from the YAML file, with simple caching."""
    # This function remains the same, it just loads whatever prompt is in the file
    prompts_path = os.path.join("config", "prompts.yaml"); cache_key = prompts_path
    if cache_key in _PROMPT_CACHE: return _PROMPT_CACHE[cache_key]
    prompt_text_result: Optional[str] = None
    try:
        if not os.path.exists(prompts_path): raise FileNotFoundError(f"{prompts_path} not found.")
        with open(prompts_path, "r", encoding="utf-8") as f:
            content = f.read();
            if not content.strip(): raise ValueError("Prompts file is empty.")
            f.seek(0); all_prompts = yaml.safe_load(f)
            if not all_prompts: raise ValueError("YAML parsing resulted in empty prompts.")
            prompt_text = all_prompts.get("orchestrator_agent_system_prompt")
            if not prompt_text or not prompt_text.strip(): log_error("orchestrator_agent", "load_orchestrator_prompt", "Key 'orchestrator_agent_system_prompt' NOT FOUND or EMPTY."); prompt_text_result = None
            else: log_info("orchestrator_agent", "load_orchestrator_prompt", "Orchestrator prompt loaded successfully."); prompt_text_result = prompt_text
    except Exception as e: log_error("orchestrator_agent", "load_orchestrator_prompt", f"CRITICAL: Failed to load/parse orchestrator prompt: {e}", e); prompt_text_result = None
    _PROMPT_CACHE[cache_key] = prompt_text_result; return prompt_text_result


# --- Main Handler Function (Core logic remains the same) ---
def handle_user_request(
    user_id: str, message: str, history: List[Dict], preferences: Dict,
    task_context: List[Dict], calendar_context: List[Dict]
) -> str:
    """
    Handles the user's request using the Orchestrator pattern with Instructor/Tool Use.
    Relies purely on the LLM for conversational flow and tool invocation decisions.
    Handles multiple tool calls requested by the LLM in a single turn.
    """
    fn_name = "handle_user_request"
    log_info("orchestrator_agent", fn_name, f"Processing request for {user_id}: '{message[:50]}...'")

    orchestrator_system_prompt = load_orchestrator_prompt()
    if not orchestrator_system_prompt:
         log_error("orchestrator_agent", fn_name, "Orchestrator system prompt could not be loaded.")
         return GENERIC_ERROR_MSG

    client: Optional[OpenAI] = get_instructor_client()
    if not client:
        log_error("orchestrator_agent", fn_name, "LLM client unavailable.")
        return GENERIC_ERROR_MSG

    # --- Prepare Context for Prompt (No change needed here) ---
    try:
        user_timezone_str = preferences.get("TimeZone", "UTC"); user_timezone = pytz.utc
        try: user_timezone = pytz.timezone(user_timezone_str)
        except pytz.UnknownTimeZoneError: log_warning("orchestrator_agent", fn_name, f"Unknown timezone '{user_timezone_str}'. Defaulting UTC.")
        now = datetime.now(user_timezone)
        current_date_str, current_time_str, current_day_str = now.strftime("%Y-%m-%d"), now.strftime("%H:%M"), now.strftime("%A")
        history_limit, item_context_limit, calendar_context_limit = 30, 20, 20
        limited_history = history[-(history_limit*2):]; history_str = "\n".join([f"{m['sender'].capitalize()}: {m['content']}" for m in limited_history])
        def prepare_item(item):
             key_fields = ['event_id', 'item_id', 'title', 'description', 'date', 'time', 'type', 'status', 'estimated_duration', 'project']
             prep = {k: item.get(k) for k in key_fields if item.get(k) is not None}; prep['item_id'] = item.get('item_id', item.get('event_id'))
             if 'event_id' in prep and prep['event_id'] == prep['item_id']: del prep['event_id']
             return prep
        item_context_str = json.dumps([prepare_item(item) for item in task_context[:item_context_limit]], indent=2, default=str)
        calendar_context_str = json.dumps(calendar_context[:calendar_context_limit], indent=2, default=str)
        prefs_str = json.dumps(preferences, indent=2, default=str)
    except Exception as e: log_error("orchestrator_agent", fn_name, f"Error preparing context strings: {e}", e); return GENERIC_ERROR_MSG

    # --- Construct Initial Messages for LLM (No change needed here) ---
    messages: List[Dict[str, Any]] = [
        {"role": "system", "content": orchestrator_system_prompt},
        {"role": "system", "content": f"Current Reference Time ({user_timezone_str}): Date: {current_date_str}, Day: {current_day_str}, Time: {current_time_str}. Use this."},
        {"role": "system", "content": f"User Preferences:\n{prefs_str}"},
        {"role": "system", "content": f"History:\n{history_str}"},
        {"role": "system", "content": f"Active Items:\n{item_context_str}"},
        {"role": "system", "content": f"Calendar Events:\n{calendar_context_str}"},
        {"role": "user", "content": message}
    ]

    # --- Define Tools (Uses the UPDATED tools from tool_definitions) ---
    tools_for_llm: List[ChatCompletionToolParam] = []
    for tool_name, model in TOOL_PARAM_MODELS.items():
        if tool_name not in AVAILABLE_TOOLS: continue # Check against updated AVAILABLE_TOOLS
        func = AVAILABLE_TOOLS.get(tool_name)
        description = func.__doc__.strip() if func and func.__doc__ else f"Executes {tool_name}"
        try:
            params_schema = model.model_json_schema();
            # Allow empty schema for tools with no params
            if not params_schema.get('properties') and not model.model_fields: params_schema = {}
            elif not params_schema.get('properties'):
                 log_warning("orchestrator_agent", fn_name, f"Model {model.__name__} for tool {tool_name} has fields but generated empty properties schema.")
                 params_schema = {} # Default to empty if unexpected

            func_def: FunctionDefinition = {"name": tool_name, "description": description, "parameters": params_schema}
            tool_param: ChatCompletionToolParam = {"type": "function", "function": func_def}; tools_for_llm.append(tool_param)
        except Exception as e: log_error("orchestrator_agent", fn_name, f"Schema error for tool {tool_name}: {e}", e)
    if not tools_for_llm: log_error("orchestrator_agent", fn_name, "No tools defined for LLM."); return GENERIC_ERROR_MSG

    # --- Interaction Loop (No structural change needed) ---
    try:
        log_info("orchestrator_agent", fn_name, f"Invoking LLM for {user_id} (Initial Turn)...")
        response = client.chat.completions.create(
            model="gpt-4o", # Or other capable model
            messages=messages, # type: ignore
            tools=tools_for_llm,
            tool_choice="auto",
            temperature=0.1,
        )
        response_message = response.choices[0].message
        tool_calls = response_message.tool_calls

        # --- Scenario 1: LLM Responds Directly ---
        if not tool_calls:
            if response_message.content:
                log_info("orchestrator_agent", fn_name, "LLM responded directly (no tool call).")
                return response_message.content
            else:
                log_warning("orchestrator_agent", fn_name, "LLM response had no tool calls and no content.")
                return "Sorry, I couldn't process that request fully. Could you try rephrasing?"

        # --- Scenario 2: LLM Calls One or More Tools ---
        log_info("orchestrator_agent", fn_name, f"LLM requested {len(tool_calls)} tool call(s).")
        messages.append(response_message.model_dump(exclude_unset=True)) # Add assistant's tool call request
        tool_results_messages = []

        for tool_call in tool_calls:
            tool_name = tool_call.function.name
            tool_call_id = tool_call.id
            tool_args_str = tool_call.function.arguments
            tool_result_content = GENERIC_ERROR_MSG # Default

            log_info("orchestrator_agent", fn_name, f"Processing Tool Call ID: {tool_call_id}, Name: {tool_name}, Args: {tool_args_str[:150]}...")

            # Use updated AVAILABLE_TOOLS and TOOL_PARAM_MODELS here
            if tool_name not in AVAILABLE_TOOLS:
                log_warning("orchestrator_agent", fn_name, f"LLM tried unknown tool: {tool_name}. Sending error result back.")
                tool_result_content = json.dumps({"success": False, "message": f"Error: Unknown action '{tool_name}' requested."})
            else:
                tool_func = AVAILABLE_TOOLS[tool_name]
                param_model = TOOL_PARAM_MODELS[tool_name]
                try:
                    tool_args_dict = {}
                    if tool_args_str and tool_args_str.strip() != '{}': tool_args_dict = json.loads(tool_args_str)
                    elif not param_model.model_fields: tool_args_dict = {} # Handle no-arg tools

                    validated_params = param_model(**tool_args_dict)
                    tool_result_dict = tool_func(user_id, validated_params) # Call the tool
                    log_info("orchestrator_agent", fn_name, f"Tool {tool_name} (ID: {tool_call_id}) executed. Result: {tool_result_dict}")
                    tool_result_content = json.dumps(tool_result_dict)

                except json.JSONDecodeError:
                    log_error("orchestrator_agent", fn_name, f"Failed parse JSON args for {tool_name} (ID: {tool_call_id}): {tool_args_str}");
                    tool_result_content = json.dumps({"success": False, "message": f"Error: Invalid arguments provided for {tool_name}."})
                except pydantic.ValidationError as e:
                    log_error("orchestrator_agent", fn_name, f"Arg validation failed for {tool_name} (ID: {tool_call_id}). Err: {e.errors()}. Args: {tool_args_str}", e)
                    err_summary = "; ".join([f"{err['loc'][0] if err.get('loc') else 'param'}: {err['msg']}" for err in e.errors()])
                    tool_result_content = json.dumps({"success": False, "message": f"Error: Invalid parameters for {tool_name} - {err_summary}"})
                except Exception as e:
                    log_error("orchestrator_agent", fn_name, f"Error executing tool {tool_name} (ID: {tool_call_id}). Trace:\n{traceback.format_exc()}", e);
                    tool_result_content = json.dumps({"success": False, "message": f"Error performing action {tool_name}."})

            tool_results_messages.append({
                "tool_call_id": tool_call_id, "role": "tool",
                "name": tool_name, "content": tool_result_content,
            })

        messages.extend(tool_results_messages) # Add all tool results

        # --- Make SECOND LLM call with ALL tool results ---
        log_info("orchestrator_agent", fn_name, f"Invoking LLM again for {user_id} with {len(tool_results_messages)} tool result(s)...")
        second_response = client.chat.completions.create(
            model="gpt-4o", # Use the same model
            messages=messages, # type: ignore
            # No tools needed here
        )
        second_response_message = second_response.choices[0].message

        if second_response_message.content:
            log_info("orchestrator_agent", fn_name, "LLM generated final response after processing tool result(s).")
            return second_response_message.content
        else:
            log_warning("orchestrator_agent", fn_name, "LLM provided no content after processing tool result(s).")
            try: fallback_msg = json.loads(tool_results_messages[0]['content']).get("message", GENERIC_ERROR_MSG)
            except: fallback_msg = GENERIC_ERROR_MSG
            return fallback_msg

    # --- Outer Exception Handling ---
    except Exception as e:
        tb_str = traceback.format_exc()
        log_error("orchestrator_agent", fn_name, f"Core error in orchestrator logic for {user_id}. Traceback:\n{tb_str}", e)
        return GENERIC_ERROR_MSG
# --- END OF FULL agents/orchestrator_agent.py ---

# --- END OF FILE agents/orchestrator_agent.py ---



================================================================================
üìÑ agents/onboarding_agent.py
================================================================================

# --- START OF FILE agents/onboarding_agent.py ---

# --- START OF FULL agents/onboarding_agent.py ---
import json
import os
import traceback
from typing import Dict, List, Optional, Any # Retain Optional for type hints as Pydantic/OpenAI models use it
from datetime import datetime
import pytz

# Instructor/LLM Imports
from services.llm_interface import get_instructor_client
from openai import OpenAI # Base client for types if needed
from openai.types.chat import ChatCompletionMessage, ChatCompletionToolParam
from openai.types.chat.chat_completion_message_tool_call import Function as ToolFunctionCall # Renamed to avoid conflict
from openai.types.shared_params import FunctionDefinition

# Tool Imports (Limited Set for Onboarding)
from .tool_definitions import (
    AVAILABLE_TOOLS,
    TOOL_PARAM_MODELS,
    UpdateUserPreferencesParams,
    InitiateCalendarConnectionParams,
)

# Utilities
from tools.logger import log_info, log_error, log_warning
import yaml
import pydantic

# --- Load Standard Messages (Mainly for GENERIC_ERROR_MSG) ---
_messages = {}
try:
    messages_path = os.path.join("config", "messages.yaml")
    if os.path.exists(messages_path):
        with open(messages_path, 'r', encoding="utf-8") as f:
             content = f.read()
             if content.strip(): f.seek(0); _messages = yaml.safe_load(f) or {}
             else: _messages = {}
    else: log_warning("onboarding_agent", "init", f"{messages_path} not found."); _messages = {}
except Exception as e: log_error("onboarding_agent", "init", f"Failed to load messages.yaml: {e}", e); _messages = {}
GENERIC_ERROR_MSG = _messages.get("generic_error_message", "Sorry, an unexpected error occurred.")
# SETUP_START_MSG no longer used directly by Python, LLM will handle greetings

# --- Function to Load Onboarding Prompt ---
_ONBOARDING_PROMPT_CACHE: Dict[str, Optional[str]] = {} # Specify cache type
_ONBOARDING_HUMAN_PROMPT_CACHE: Dict[str, Optional[str]] = {}

def load_onboarding_prompts() -> tuple[Optional[str], Optional[str]]:
    """Loads the onboarding system and human prompts from the YAML file, with caching."""
    prompts_path = os.path.join("config", "prompts.yaml")
    system_cache_key = prompts_path + "_onboarding_system"
    human_cache_key = prompts_path + "_onboarding_human"

    cached_system_prompt = _ONBOARDING_PROMPT_CACHE.get(system_cache_key)
    cached_human_prompt = _ONBOARDING_HUMAN_PROMPT_CACHE.get(human_cache_key)

    if cached_system_prompt is not None and cached_human_prompt is not None:
        return cached_system_prompt, cached_human_prompt

    system_prompt_text: Optional[str] = None
    human_prompt_text: Optional[str] = None
    try:
        if not os.path.exists(prompts_path): raise FileNotFoundError(f"{prompts_path} not found.")
        with open(prompts_path, "r", encoding="utf-8") as f:
            content = f.read()
            if not content.strip(): raise ValueError("Prompts file is empty.")
            f.seek(0); all_prompts = yaml.safe_load(f)
            if not all_prompts: raise ValueError("YAML parsing resulted in empty prompts.")

            system_prompt_text = all_prompts.get("onboarding_agent_system_prompt")
            if not system_prompt_text or not system_prompt_text.strip():
                log_error("onboarding_agent", "load_onboarding_prompts", "Key 'onboarding_agent_system_prompt' NOT FOUND or EMPTY.")
                system_prompt_text = None
            else:
                log_info("onboarding_agent", "load_onboarding_prompts", "Onboarding system prompt loaded successfully.")

            human_prompt_text = all_prompts.get("onboarding_agent_human_prompt")
            if not human_prompt_text or not human_prompt_text.strip():
                log_error("onboarding_agent", "load_onboarding_prompts", "Key 'onboarding_agent_human_prompt' NOT FOUND or EMPTY.")
                human_prompt_text = None
            else:
                log_info("onboarding_agent", "load_onboarding_prompts", "Onboarding human prompt loaded successfully.")

    except Exception as e:
        log_error("onboarding_agent", "load_onboarding_prompts", f"CRITICAL: Failed load/parse onboarding prompts: {e}", e)
        system_prompt_text = None
        human_prompt_text = None

    _ONBOARDING_PROMPT_CACHE[system_cache_key] = system_prompt_text
    _ONBOARDING_HUMAN_PROMPT_CACHE[human_cache_key] = human_prompt_text
    return system_prompt_text, human_prompt_text

# --- Main Onboarding Handler Function ---
def handle_onboarding_request(
    user_id: str,
    message: str,
    history: List[Dict],
    preferences: Dict # Current preferences passed in
) -> str:
    """Handles the user's request during the onboarding phase using pure LLM flow."""
    fn_name = "handle_onboarding_request"
    log_info("onboarding_agent", fn_name, f"Handling onboarding request for {user_id}: '{message[:50]}...'")

    onboarding_system_prompt, onboarding_human_prompt = load_onboarding_prompts()
    if not onboarding_system_prompt or not onboarding_human_prompt:
         log_error("onboarding_agent", fn_name, "Onboarding system or human prompt could not be loaded.")
         return GENERIC_ERROR_MSG

    client: Optional[OpenAI] = get_instructor_client()
    if not client:
        log_error("onboarding_agent", fn_name, "LLM client unavailable.")
        return GENERIC_ERROR_MSG

    # --- Prepare Context for Prompt ---
    try:
        history_limit = 20 # Keep more history for onboarding to better detect language
        limited_history = history[-(history_limit*2):] # Each turn is 2 entries (user, assistant/tool)
        history_str = "\n".join([f"{m['sender'].capitalize()}: {m['content']}" for m in limited_history])
        prefs_str = json.dumps(preferences, indent=2, default=str)
    except Exception as e:
        log_error("onboarding_agent", fn_name, f"Error preparing context strings for onboarding: {e}", e)
        return GENERIC_ERROR_MSG

    # --- Construct Initial Messages for LLM ---
    # Human prompt is now dynamic with placeholders
    formatted_human_prompt = onboarding_human_prompt.format(
        current_preferences_json=prefs_str,
        conversation_history=history_str,
        message=message # The latest user message
    )

    messages: List[Dict[str, Any]] = [
        {"role": "system", "content": onboarding_system_prompt},
        # System messages with context are now part of the human prompt's preamble for this agent
        {"role": "user", "content": formatted_human_prompt} # The user message is part of the formatted human prompt
    ]
    # Remove the logic that inserted SETUP_START_MSG; LLM will handle greeting.

    # --- Define Tools AVAILABLE for Onboarding ---
    # (Tool definition logic is similar to orchestrator_agent)
    onboarding_tools_for_llm_map = {
        "update_user_preferences": {
            "function": AVAILABLE_TOOLS.get("update_user_preferences"),
            "model": UpdateUserPreferencesParams
        },
        "initiate_calendar_connection": {
            "function": AVAILABLE_TOOLS.get("initiate_calendar_connection"),
            "model": InitiateCalendarConnectionParams
        },
    }
    # Filter out tools that might have failed to load
    onboarding_tools_for_llm_map = {
        name: data for name, data in onboarding_tools_for_llm_map.items() if data["function"] and data["model"]
    }

    tools_for_llm: List[ChatCompletionToolParam] = []
    for tool_name, tool_data in onboarding_tools_for_llm_map.items():
        func = tool_data["function"]
        model = tool_data["model"]
        description = func.__doc__.strip() if func and func.__doc__ else f"Executes {tool_name}"
        try:
            params_schema = model.model_json_schema()
            # Ensure 'properties' is not empty for tools that expect params
            if not params_schema.get('properties') and model.model_fields:
                # If model has fields but schema has no properties, it might be an issue.
                # For empty models like InitiateCalendarConnectionParams, schema might be {}
                pass # Allow empty schema for tools with no params
            elif not params_schema.get('properties'):
                 params_schema = {} # For tools with truly no parameters

            func_def: FunctionDefinition = {"name": tool_name, "description": description, "parameters": params_schema}
            tool_param: ChatCompletionToolParam = {"type": "function", "function": func_def}
            tools_for_llm.append(tool_param)
        except Exception as e:
            log_error("onboarding_agent", fn_name, f"Schema error for onboarding tool {tool_name}: {e}", e)

    # --- Interaction Loop (Two-Step LLM Call) ---
    try:
        log_info("onboarding_agent", fn_name, f"Invoking Onboarding LLM for {user_id} (Initial Turn)...")
        response = client.chat.completions.create(
            model="gpt-4o", # Or a suitable model like gpt-3.5-turbo
            messages=messages, # type: ignore
            tools=tools_for_llm if tools_for_llm else None, # Pass None if no tools
            tool_choice="auto" if tools_for_llm else None,
            temperature=0.2, # Slightly higher temp for more natural onboarding
        )
        response_message = response.choices[0].message
        tool_calls = response_message.tool_calls

        # --- Scenario 1: LLM Responds Directly (Asks question, gives info) ---
        if not tool_calls:
            if response_message.content:
                log_info("onboarding_agent", fn_name, "Onboarding LLM responded directly.")
                return response_message.content
            else:
                log_warning("onboarding_agent", fn_name, "Onboarding LLM response had no tool calls and no content.")
                return "I'm sorry, I'm having a little trouble at the moment. Could you try rephrasing?"

        # --- Scenario 2: LLM Calls One or More Onboarding Tools ---
        # Onboarding agent typically calls one tool at a time.
        log_info("onboarding_agent", fn_name, f"Onboarding LLM requested {len(tool_calls)} tool call(s).")
        messages.append(response_message.model_dump(exclude_unset=True)) # Add assistant's tool call request
        tool_results_messages = []

        for tool_call in tool_calls: # Loop although expecting one for onboarding
            tool_name = tool_call.function.name
            tool_call_id = tool_call.id
            tool_args_str = tool_call.function.arguments
            tool_result_content = GENERIC_ERROR_MSG # Default in case of error

            log_info("onboarding_agent", fn_name, f"Processing Tool Call ID: {tool_call_id}, Name: {tool_name}, Args: {tool_args_str[:150]}...")

            if tool_name not in onboarding_tools_for_llm_map:
                log_warning("onboarding_agent", fn_name, f"Onboarding LLM tried unknown/disallowed tool: {tool_name}. Sending error result back.")
                tool_result_content = json.dumps({"success": False, "message": f"Error: Unknown action '{tool_name}' requested during setup."})
            else:
                tool_func = onboarding_tools_for_llm_map[tool_name]["function"]
                param_model = onboarding_tools_for_llm_map[tool_name]["model"]
                try:
                    tool_args_dict = {}
                    if tool_args_str and tool_args_str.strip() != '{}': tool_args_dict = json.loads(tool_args_str)
                    elif not param_model.model_fields : tool_args_dict = {} # Handle no-arg tools

                    validated_params = param_model(**tool_args_dict)
                    tool_result_dict = tool_func(user_id, validated_params) # Call the tool
                    log_info("onboarding_agent", fn_name, f"Onboarding Tool {tool_name} (ID: {tool_call_id}) executed. Result: {tool_result_dict}")
                    tool_result_content = json.dumps(tool_result_dict)

                except json.JSONDecodeError:
                    log_error("onboarding_agent", fn_name, f"Failed parse JSON args for onboarding tool {tool_name} (ID: {tool_call_id}): {tool_args_str}");
                    tool_result_content = json.dumps({"success": False, "message": f"Error: Invalid arguments format for {tool_name}."})
                except pydantic.ValidationError as e:
                    log_error("onboarding_agent", fn_name, f"Arg validation failed for onboarding tool {tool_name} (ID: {tool_call_id}). Err: {e.errors()}. Args: {tool_args_str}", e)
                    err_summary = "; ".join([f"{err['loc'][0] if err.get('loc') else 'param'}: {err['msg']}" for err in e.errors()])
                    tool_result_content = json.dumps({"success": False, "message": f"Error: Invalid parameters for {tool_name} - {err_summary}"})
                except Exception as e:
                    log_error("onboarding_agent", fn_name, f"Error executing onboarding tool {tool_name} (ID: {tool_call_id}). Trace:\n{traceback.format_exc()}", e);
                    tool_result_content = json.dumps({"success": False, "message": f"Error performing action {tool_name}."})

            tool_results_messages.append({
                "tool_call_id": tool_call_id, "role": "tool",
                "name": tool_name, "content": tool_result_content,
            })

        messages.extend(tool_results_messages) # Add all tool results

        # --- Make SECOND LLM call with ALL tool results ---
        log_info("onboarding_agent", fn_name, f"Invoking Onboarding LLM again for {user_id} with {len(tool_results_messages)} tool result(s)...")
        second_response = client.chat.completions.create(
            model="gpt-4o", # Or gpt-3.5-turbo
            messages=messages, # type: ignore
            # No tools needed here, LLM should just generate response based on results
        )
        second_response_message = second_response.choices[0].message

        if second_response_message.content:
            log_info("onboarding_agent", fn_name, "Onboarding LLM generated final response after processing tool result(s).")
            # Check if the LAST action was setting status to active (optional, LLM should handle the final message)
            if tool_calls and tool_calls[-1].function.name == "update_user_preferences":
                 try:
                      last_tool_args_str = tool_calls[-1].function.arguments
                      update_data = json.loads(last_tool_args_str) if last_tool_args_str and last_tool_args_str.strip() != '{}' else {}
                      if isinstance(update_data.get("updates"), dict) and update_data["updates"].get("status") == "active":
                           log_info("onboarding_agent", fn_name, f"Onboarding likely completed for user {user_id} (status set to active).")
                 except Exception as parse_err:
                      log_warning("onboarding_agent", fn_name, f"Could not parse last tool args to check for status update: {parse_err}")
            return second_response_message.content
        else:
            log_warning("onboarding_agent", fn_name, "Onboarding LLM provided no content after processing tool result(s).")
            # Try to construct a fallback from the first tool's message if possible
            try: fallback_msg = json.loads(tool_results_messages[0]['content']).get("message", GENERIC_ERROR_MSG)
            except: fallback_msg = GENERIC_ERROR_MSG
            return fallback_msg

    # --- Outer Exception Handling ---
    except Exception as e:
        tb_str = traceback.format_exc()
        log_error("onboarding_agent", fn_name, f"Core error in onboarding LLM logic for {user_id}. Traceback:\n{tb_str}", e)
        return GENERIC_ERROR_MSG
# --- END OF FULL agents/onboarding_agent.py ---

# --- END OF FILE agents/onboarding_agent.py ---



================================================================================
üìÑ agents/tool_definitions.py
================================================================================

# --- START OF FILE agents/tool_definitions.py ---

# --- START OF FULL agents/tool_definitions.py ---

from pydantic import BaseModel, Field, field_validator, ValidationError
from typing import Dict, List, Any, Tuple, Optional # Keep Optional for Pydantic models
import json
from datetime import datetime, timedelta, timezone # Added timezone
import re
import uuid # Added uuid
import traceback
# Import Service Layer functions & Helpers
import services.task_manager as task_manager
import services.config_manager as config_manager
import services.task_query_service as task_query_service
from services.agent_state_manager import get_agent_state, add_task_to_context, update_task_in_context # Assuming AGENT_STATE_MANAGER_IMPORTED is handled internally or true
# Import the class itself for type checking if needed, handle import error
try:
    from tools.google_calendar_api import GoogleCalendarAPI
    GCAL_API_IMPORTED = True
except ImportError:
     GoogleCalendarAPI = None
     GCAL_API_IMPORTED = False

# Import activity_db for type checking within tools
try:
    import tools.activity_db as activity_db # Keep for direct access if needed (e.g., type check in cancel_sessions)
    DB_IMPORTED = True
except ImportError:
    DB_IMPORTED = False
    log_error("tool_definitions", "update_item_details_tool_import", "Failed to import real activity_db. Some checks may be skipped.", None)
    # Define a dummy class with the necessary static method
    class activity_db_dummy:
        @staticmethod
        def get_task(*a, **k):
             log_warning("tool_definitions", "activity_db_dummy.get_task", "Using dummy get_task - DB not imported.")
             return None
    # Define activity_db to point to the dummy
    activity_db = activity_db_dummy

# LLM Interface (for scheduler sub-call)
from services.llm_interface import get_instructor_client
from openai import OpenAI
from openai.types.chat import ChatCompletionMessage

# Utilities
from tools.logger import log_info, log_error, log_warning # Ensure log_warning is used correctly
import yaml
import os
import pydantic # Keep pydantic import

# --- Helper Functions --- (No changes needed in helpers _get_calendar_api_from_state, _parse_scheduler_llm_response, _load_scheduler_prompts)

# Returns GoogleCalendarAPI instance or None
def _get_calendar_api_from_state(user_id) -> Optional[GoogleCalendarAPI]:
    """Helper to retrieve the active calendar API instance from agent state."""
    fn_name = "_get_calendar_api_from_state"
    if not GCAL_API_IMPORTED or GoogleCalendarAPI is None:
        # log_warning("tool_definitions", fn_name, "GoogleCalendarAPI class missing or not imported.") # Can be noisy
        return None
    try:
        state = get_agent_state(user_id)
        if state is not None:
            api = state.get("calendar")
            if isinstance(api, GoogleCalendarAPI) and api.is_active():
                return api
    except Exception as e:
        log_error("tool_definitions", fn_name, f"Error getting calendar API instance for user {user_id}", e)
    return None

# Returns dict or None
def _parse_scheduler_llm_response(raw_text) -> Optional[Dict]:
    """Parses the specific JSON output from the Session Scheduler LLM."""
    fn_name = "_parse_scheduler_llm_response"
    if not raw_text:
        log_warning("tool_definitions", fn_name, "Scheduler response text is empty.")
        return None
    processed_text = None
    try:
        # Try extracting JSON block first
        match = re.search(r"```json\s*({.*?})\s*```", raw_text, re.DOTALL | re.IGNORECASE)
        if match:
             processed_text = match.group(1).strip()
        else:
             # Fallback: Assume the entire response is JSON or find first/last brace
             processed_text = raw_text.strip()
             if not processed_text.startswith("{") or not processed_text.endswith("}"):
                  start_brace = raw_text.find('{'); end_brace = raw_text.rfind('}')
                  if start_brace != -1 and end_brace > start_brace:
                      processed_text = raw_text[start_brace : end_brace + 1].strip()
                      log_warning("tool_definitions", fn_name, "Used find/rfind fallback for JSON extraction.")
                  else:
                      log_warning("tool_definitions", fn_name, "Could not extract JSON block from raw text.");
                      return None
        if not processed_text:
            raise ValueError("Processed text is empty after extraction attempts.")

        data = json.loads(processed_text)
        required_keys = ["proposed_sessions", "response_message"]
        if not isinstance(data, dict) or not all(k in data for k in required_keys):
            missing = [k for k in required_keys if k not in data]; raise ValueError(f"Parsed JSON missing required keys: {missing}")
        if not isinstance(data["proposed_sessions"], list): raise ValueError("'proposed_sessions' key must contain a list.")

        valid_sessions = []
        for i, session_dict in enumerate(data["proposed_sessions"]):
            required_session_keys = ["date", "time", "end_time"]
            if not isinstance(session_dict, dict) or not all(k in session_dict for k in required_session_keys):
                log_warning("tool_definitions", fn_name, f"Skipping invalid session structure: {session_dict}")
                continue
            try:
                 datetime.strptime(session_dict["date"], '%Y-%m-%d'); datetime.strptime(session_dict["time"], '%H:%M'); datetime.strptime(session_dict["end_time"], '%H:%M')
                 ref = session_dict.get("slot_ref");
                 session_dict["slot_ref"] = ref if isinstance(ref, int) and ref > 0 else i + 1
                 valid_sessions.append(session_dict)
            except (ValueError, TypeError) as fmt_err:
                log_warning("tool_definitions", fn_name, f"Skipping session due to format error ({fmt_err}): {session_dict}")

        data["proposed_sessions"] = valid_sessions # Replace with validated list
        log_info("tool_definitions", fn_name, f"Successfully parsed {len(valid_sessions)} valid sessions.")
        return data
    except (json.JSONDecodeError, ValueError) as parse_err:
        log_error("tool_definitions", fn_name, f"Scheduler JSON parsing failed. Error: {parse_err}. Extracted: '{processed_text or 'N/A'}' Raw: '{raw_text[:200]}'", parse_err);
        return None
    except Exception as e:
        log_error("tool_definitions", fn_name, f"Unexpected error parsing scheduler response: {e}", e);
        return None

_SCHEDULER_PROMPTS_CACHE: Dict[str, Optional[str]] = {}
# Returns tuple (sys_prompt_str|None, human_prompt_str|None)
def _load_scheduler_prompts() -> Tuple[Optional[str], Optional[str]]:
    """Loads scheduler system and human prompts from config."""
    fn_name = "_load_scheduler_prompts"
    prompts_path = os.path.join("config", "prompts.yaml"); cache_key = prompts_path + "_scheduler"
    if cache_key in _SCHEDULER_PROMPTS_CACHE: return _SCHEDULER_PROMPTS_CACHE[cache_key] # type: ignore
    sys_prompt: Optional[str] = None
    human_prompt: Optional[str] = None
    try:
        if not os.path.exists(prompts_path): raise FileNotFoundError(f"{prompts_path} not found.")
        with open(prompts_path, "r", encoding="utf-8") as f: all_prompts = yaml.safe_load(f)
        if not all_prompts: raise ValueError("YAML prompts file loaded as empty.")
        sys_prompt = all_prompts.get("session_scheduler_system_prompt")
        human_prompt = all_prompts.get("session_scheduler_human_prompt")
        if not sys_prompt or not human_prompt:
            log_error("tool_definitions", fn_name, "One or both scheduler prompts (system/human) are missing in prompts.yaml.")
            sys_prompt, human_prompt = None, None # Ensure both are None if one is missing
    except Exception as e:
        log_error("tool_definitions", fn_name, f"Failed to load scheduler prompts from {prompts_path}: {e}", e)
        sys_prompt, human_prompt = None, None # Ensure None on error
    _SCHEDULER_PROMPTS_CACHE[cache_key] = (sys_prompt, human_prompt);
    return sys_prompt, human_prompt


# =====================================================
# == Pydantic Model Definitions (MUST COME BEFORE TOOLS) ==
# =====================================================

class CreateReminderParams(BaseModel):
    description: str = Field(...)
    date: str = Field(...)
    time: Optional[str] = Field(None)
    project: Optional[str] = Field(None)
    @field_validator('time')
    @classmethod
    def validate_time_format(cls, v: Optional[str]):
        if v is None or v == "": return None
        try: hour, minute = map(int, v.split(':')); return f"{hour:02d}:{minute:02d}"
        except (ValueError, TypeError): raise ValueError("Time must be in HH:MM format (e.g., '14:30') or null/empty")
    @field_validator('date')
    @classmethod
    def validate_date_format(cls, v: str):
        try: datetime.strptime(v, '%Y-%m-%d'); return v
        except (ValueError, TypeError): raise ValueError("Date must be in YYYY-MM-DD format")

# Note: This model is for creating the *initial metadata* for a schedulable Task.
# The LLM might call this less often if the flow usually goes directly to propose_task_slots -> finalize_task_and_book_sessions.
class CreateTaskParams(BaseModel):
    description: str = Field(...)
    date: Optional[str] = Field(None, description="Optional: Due date for the task.")
    estimated_duration: Optional[str] = Field(None, description="Optional: Estimated total duration (e.g., '2h', '90m').")
    project: Optional[str] = Field(None)
    @field_validator('date')
    @classmethod
    def validate_date_format(cls, v: Optional[str]):
        if v is None: return None
        try: datetime.strptime(v, '%Y-%m-%d'); return v
        except (ValueError, TypeError): raise ValueError("Date must be in YYYY-MM-DD format")

# --- NEW MODEL for ToDos ---
class CreateToDoParams(BaseModel):
    description: str = Field(...)
    date: Optional[str] = Field(None, description="Optional: Due date for the ToDo.")
    project: Optional[str] = Field(None)
    estimated_duration: Optional[str] = Field(None, description="Optional: Estimated duration for the ToDo (e.g., '1h', '30m'). For user reference only.") # <-- ADDED
    @field_validator('date')
    @classmethod
    def validate_date_format(cls, v: Optional[str]):
        if v is None: return None
        try: datetime.strptime(v, '%Y-%m-%d'); return v
        except (ValueError, TypeError): raise ValueError("Date must be in YYYY-MM-DD format")
    # No specific validator needed for estimated_duration here, general string is fine.
    # The LLM will be instructed to provide it in a parseable format if it gets one.
# --- END NEW MODEL ---

class ProposeTaskSlotsParams(BaseModel):
    duration: str = Field(...)
    timeframe: str = Field(...)
    description: Optional[str] = Field(None)
    scheduling_hints: Optional[str] = Field(None, description="User's specific scheduling preferences or constraints provided in natural language, e.g., 'in the afternoon', 'not on Monday', 'needs one continuous block', 'split into sessions'.")
    num_options_to_propose: Optional[int] = Field(3)
    @field_validator('num_options_to_propose')
    @classmethod
    def check_num_options(cls, v: Optional[int]):
        if v is not None and v <= 0: raise ValueError("num_options_to_propose must be positive")
        return v

class FinalizeTaskAndBookSessionsParams(BaseModel):
    search_context: Dict = Field(...)
    approved_slots: List[Dict] = Field(..., min_length=1)
    project: Optional[str] = Field(None)
    @field_validator('approved_slots')
    @classmethod
    def validate_slots_structure(cls, v):
        if not isinstance(v, list) or not v: raise ValueError("approved_slots must be a non-empty list.")
        for i, slot in enumerate(v):
            if not isinstance(slot, dict): raise ValueError(f"Slot {i+1} is not a dict.")
            req_keys = ["date", "time", "end_time"];
            if not all(k in slot for k in req_keys): raise ValueError(f"Slot {i+1} missing required keys: {req_keys}")
            try: datetime.strptime(slot["date"], '%Y-%m-%d')
            except (ValueError, TypeError): raise ValueError(f"Slot {i+1} has invalid date format: {slot['date']}")
            try: datetime.strptime(slot["time"], '%H:%M')
            except (ValueError, TypeError): raise ValueError(f"Slot {i+1} has invalid time format: {slot['time']}")
            try: datetime.strptime(slot["end_time"], '%H:%M')
            except (ValueError, TypeError): raise ValueError(f"Slot {i+1} has invalid end_time format: {slot['end_time']}")
        return v

class UpdateItemDetailsParams(BaseModel):
    item_id: str = Field(...)
    updates: dict = Field(...)
    @field_validator('updates')
    @classmethod
    def check_allowed_keys_and_formats(cls, v: dict):
        allowed_keys = {"description", "date", "time", "estimated_duration", "project"}
        if not v: raise ValueError("Updates dictionary cannot be empty.")
        validated_updates = {}
        for key, value in v.items():
            if key not in allowed_keys: raise ValueError(f"Invalid key '{key}'. Allowed: {', '.join(allowed_keys)}")
            if key == 'date':
                if value is None: validated_updates[key] = None
                else:
                    try: validated_updates[key] = cls.validate_date_format_static(str(value))
                    except ValueError as e: raise ValueError(f"Invalid format for date '{value}': {e}")
            elif key == 'time':
                if value is None: validated_updates[key] = None
                else:
                     try: validated_updates[key] = cls.validate_time_format_static(str(value))
                     except ValueError as e: raise ValueError(f"Invalid format for time '{value}': {e}")
            elif key == 'estimated_duration':
                if value is None or (isinstance(value, str) and value.strip() == ""): validated_updates[key] = None
                elif not isinstance(value, str): raise ValueError("Estimated duration must be a string or null/empty")
                else: validated_updates[key] = value
            else: validated_updates[key] = value # description, project
        if not validated_updates: raise ValueError("Updates dictionary resulted in no valid fields.")
        return validated_updates
    @staticmethod
    def validate_date_format_static(v: str):
        try: datetime.strptime(v, '%Y-%m-%d'); return v
        except (ValueError, TypeError): raise ValueError("Date must be in YYYY-MM-DD format")
    @staticmethod
    def validate_time_format_static(v: Optional[str]):
        if v is None: return None
        if v == "": return None
        try: hour, minute = map(int, v.split(':')); return f"{hour:02d}:{minute:02d}"
        except (ValueError, TypeError): raise ValueError("Time must be in HH:MM format (e.g., '14:30') or empty string or null")

class UpdateItemStatusParams(BaseModel):
    item_id: str = Field(...)
    new_status: str = Field(...)
    @field_validator('new_status')
    @classmethod
    def check_item_status(cls, v: str):
        allowed = {"pending", "in_progress", "completed", "cancelled"}
        v_lower = v.lower().replace(" ", "_") # Standardize to snake_case
        if v_lower not in allowed: raise ValueError(f"Status must be one of: {', '.join(allowed)}")
        return v_lower

class UpdateUserPreferencesParams(BaseModel):
    updates: dict = Field(...)
    @field_validator('updates')
    @classmethod
    def check_updates_not_empty(cls, v: dict):
        if not v: raise ValueError("Updates dictionary cannot be empty.")
        return v

class InitiateCalendarConnectionParams(BaseModel):
    pass

class CancelTaskSessionsParams(BaseModel):
    task_id: str = Field(...)
    session_ids_to_cancel: list[str] = Field(..., min_length=1)

class InterpretListReplyParams(BaseModel):
    user_reply: str = Field(...)
    list_mapping: dict = Field(...)

class GetFormattedTaskListParams(BaseModel):
    date_range: Optional[list[str]] = Field(None)
    status_filter: Optional[str] = Field('active')
    project_filter: Optional[str] = Field(None)
    @field_validator('date_range')
    @classmethod
    def validate_and_normalize_date_range(cls, v: Optional[list[str]]):
        if v is None: return v
        if not isinstance(v, list): raise ValueError("date_range must be a list of date strings.")
        if len(v) == 1:
            try:
                the_date = datetime.strptime(v[0], '%Y-%m-%d').date()
                log_warning("tool_definitions", "validate_date_range", f"Received single date {v[0]}, assuming start/end.")
                return [v[0], v[0]]
            except (ValueError, TypeError):
                raise ValueError("Single date provided is not a valid YYYY-MM-DD string.")
        elif len(v) == 2:
            try:
                start_date = datetime.strptime(v[0], '%Y-%m-%d').date()
                end_date = datetime.strptime(v[1], '%Y-%m-%d').date()
                if start_date > end_date: raise ValueError("Start date cannot be after end date.")
                return v
            except (ValueError, TypeError):
                raise ValueError("Dates must be valid YYYY-MM-DD strings.")
        else:
            raise ValueError("date_range must be a list of one or two date strings.")
    @field_validator('status_filter')
    @classmethod
    def check_status_filter(cls, v: Optional[str]):
        if v is None: return 'active'
        allowed = {'active', 'pending', 'in_progress', 'completed', 'all'}
        v_lower = v.lower().replace(" ", "_")
        if v_lower not in allowed:
            log_warning("tool_definitions", "check_status_filter", f"Invalid status_filter '{v}'. Defaulting 'active'.")
            return 'active'
        return v_lower


# =====================================================
# == Tool Function Definitions (MUST COME AFTER MODELS) ==
# =====================================================

# --- Returns dict ---
def create_reminder_tool(user_id, params: CreateReminderParams) -> Dict:
    """Creates a simple reminder, potentially adding it to Google Calendar if time is specified. Sets type='reminder'."""
    fn_name = "create_reminder_tool"
    try:
        log_info("tool_definitions", fn_name, f"Executing for user {user_id}, desc: '{params.description[:30]}...'")
        data_dict = params.model_dump(exclude_none=True)
        data_dict["type"] = "reminder" # Explicitly set type
        saved_item = task_manager.create_task(user_id, data_dict)
        if saved_item and saved_item.get("event_id"):
            return {"success": True, "item_id": saved_item.get("event_id"), "item_type": "reminder", "message": f"Reminder '{params.description[:30]}...' created."}
        else:
             log_error("tool_definitions", fn_name, f"Task manager failed create reminder")
             return {"success": False, "item_id": None, "message": "Failed to save reminder."}
    except pydantic.ValidationError as e:
         log_error("tool_definitions", fn_name, f"Validation Error: {e}");
         return {"success": False, "item_id": None, "message": f"Invalid parameters: {e}"}
    except Exception as e:
         log_error("tool_definitions", fn_name, f"Unexpected error: {e}", e)
         return {"success": False, "item_id": None, "message": f"Error: {e}"}

# --- Returns dict ---
def create_task_tool(user_id, params: CreateTaskParams) -> Dict:
    """Creates metadata for a schedulable Task (type='task'). Does not schedule sessions or interact with calendar directly here."""
    fn_name = "create_task_tool"
    try:
        log_info("tool_definitions", fn_name, f"Executing for user {user_id}, desc: '{params.description[:30]}...'")
        data_dict = params.model_dump(exclude_none=True)
        data_dict["type"] = "task" # Explicitly set type
        if "time" in data_dict: del data_dict["time"] # Tasks don't have specific start time in metadata
        # Note: 'date' might be due date, 'estimated_duration' stored for later scheduling.
        saved_item = task_manager.create_task(user_id, data_dict)
        if saved_item and saved_item.get("event_id"):
            return {"success": True, "item_id": saved_item.get("event_id"), "item_type": "task", "estimated_duration": saved_item.get("estimated_duration"), "message": f"Task '{params.description[:30]}...' metadata created."}
        else:
             log_error("tool_definitions", fn_name, f"Task manager failed create task metadata")
             return {"success": False, "item_id": None, "message": "Failed to save task metadata."}
    except pydantic.ValidationError as e:
         log_error("tool_definitions", fn_name, f"Validation Error: {e}");
         return {"success": False, "item_id": None, "message": f"Invalid parameters: {e}"}
    except Exception as e:
         log_error("tool_definitions", fn_name, f"Unexpected error: {e}", e)
         return {"success": False, "item_id": None, "message": f"Error: {e}"}

# --- NEW TOOL FUNCTION ---
# --- Returns dict ---
def create_todo_tool(user_id, params: CreateToDoParams) -> Dict:
    """Creates a simple ToDo item (type='todo') for tracking, without calendar scheduling."""
    fn_name = "create_todo_tool"
    try:
        log_info("tool_definitions", fn_name, f"Executing for user {user_id}, desc: '{params.description[:30]}...'")
        data_dict = params.model_dump(exclude_none=True)
        data_dict["type"] = "todo" # Explicitly set type
        # Fields like time, duration are not relevant for ToDo creation
        saved_item = task_manager.create_task(user_id, data_dict) # Use the same underlying service function
        if saved_item and saved_item.get("event_id"):
            return {"success": True, "item_id": saved_item.get("event_id"), "item_type": "todo", "message": f"ToDo '{params.description[:30]}...' added to your list."}
        else:
             log_error("tool_definitions", fn_name, f"Task manager failed create ToDo")
             return {"success": False, "item_id": None, "message": "Failed to save ToDo."}
    except pydantic.ValidationError as e:
         log_error("tool_definitions", fn_name, f"Validation Error: {e}");
         return {"success": False, "item_id": None, "message": f"Invalid parameters: {e}"}
    except Exception as e:
         log_error("tool_definitions", fn_name, f"Unexpected error: {e}", e)
         return {"success": False, "item_id": None, "message": f"Error: {e}"}
# --- END NEW TOOL FUNCTION ---

# --- Returns dict ---
def propose_task_slots_tool(user_id, params: ProposeTaskSlotsParams) -> Dict:
    """
    Finds available work session slots for a Task based on duration/timeframe and hints.
    Uses an LLM sub-call for intelligent slot finding, considering calendar events.
    Returns proposed slots and the search context used.
    """
    # --- Function body remains the same ---
    fn_name = "propose_task_slots_tool"
    fail_result = {"success": False, "proposed_slots": None, "message": "Sorry, I encountered an issue trying to propose schedule slots.", "search_context": None}
    try:
        log_info("tool_definitions", fn_name, f"Executing user={user_id}. Search: duration='{params.duration}', timeframe='{params.timeframe}', hints='{params.scheduling_hints}'")
        search_context_to_return = params.model_dump() # Store validated input as basis for context

        llm_client = get_instructor_client()
        if not llm_client:
             log_error("tool_definitions", fn_name, "LLM client unavailable.")
             return {**fail_result, "message": "Scheduler resources unavailable (LLM Client)."}

        sys_prompt, human_prompt = _load_scheduler_prompts()
        if not sys_prompt or not human_prompt:
             log_error("tool_definitions", fn_name, "Scheduler prompts failed load.")
             return {**fail_result, "message": "Scheduler resources unavailable (Prompts)."}

        agent_state = get_agent_state(user_id);
        prefs = agent_state.get("preferences", {}) if agent_state else {}
        calendar_api = _get_calendar_api_from_state(user_id)
        preferred_session_str = prefs.get("Preferred_Session_Length", "60m")

        task_estimated_duration_str = params.duration;
        total_minutes = task_manager._parse_duration_to_minutes(task_estimated_duration_str)
        if total_minutes is None:
             log_error("tool_definitions", fn_name, f"Invalid duration '{task_estimated_duration_str}'")
             return {**fail_result, "message": f"Invalid duration format: '{task_estimated_duration_str}'. Use 'Xh' or 'Ym'."}

        session_minutes = task_manager._parse_duration_to_minutes(preferred_session_str) or 60
        num_slots_to_find = 1
        slot_duration_str = task_estimated_duration_str
        hints_lower = (params.scheduling_hints or "").lower()

        needs_split = False
        if "continuous" in hints_lower or "one block" in hints_lower or "one slot" in hints_lower:
             needs_split = False
             log_info("tool_definitions", fn_name, "Hint indicates continuous block required.")
        elif "split" in hints_lower or "separate" in hints_lower or "multiple sessions" in hints_lower:
             needs_split = True
             log_info("tool_definitions", fn_name, "Hint indicates split sessions required.")
        elif session_minutes > 0 and total_minutes > session_minutes:
             needs_split = True
             log_info("tool_definitions", fn_name, "Defaulting to split sessions (total > preferred, no specific hint).")

        if needs_split and session_minutes > 0:
             num_slots_to_find = (total_minutes + session_minutes - 1) // session_minutes
             if num_slots_to_find <= 0 : num_slots_to_find = 1
             slot_duration_str = preferred_session_str
             log_info("tool_definitions", fn_name, f"Calculated need for {num_slots_to_find} split sessions of duration: {slot_duration_str}")
        else:
            log_info("tool_definitions", fn_name, f"Calculated need for 1 continuous block of duration: {slot_duration_str}")

        today = datetime.now().date(); start_date = today + timedelta(days=1); due_date_for_search = None
        tf = params.timeframe.lower()
        iso_interval_match = re.match(r"(\d{4}-\d{2}-\d{2}T\d{2}:\d{2}:\d{2}(?:[+-]\d{2}:\d{2}|Z))/(\d{4}-\d{2}-\d{2}T\d{2}:\d{2}:\d{2}(?:[+-]\d{2}:\d{2}|Z))", params.timeframe)
        if iso_interval_match:
             try:
                  start_iso, end_iso = iso_interval_match.groups()
                  start_dt_aware = datetime.fromisoformat(start_iso.replace('Z', '+00:00'))
                  end_dt_aware = datetime.fromisoformat(end_iso.replace('Z', '+00:00'))
                  start_date = start_dt_aware.date()
                  start_date = max(start_date, today + timedelta(days=1))
                  due_date_for_search = end_dt_aware.date()
                  log_info("tool_definitions", fn_name, f"Parsed ISO Interval: Start={start_date}, EffectiveDue={due_date_for_search}")
             except ValueError as iso_parse_err:
                  log_warning("tool_definitions", fn_name, f"Failed to parse ISO interval timeframe '{params.timeframe}': {iso_parse_err}")
        elif "tomorrow" in tf: start_date = today + timedelta(days=1); due_date_for_search = start_date
        elif "next week" in tf:
             start_of_next_week = today + timedelta(days=(7 - today.weekday()))
             start_date = start_of_next_week
             due_date_for_search = start_of_next_week + timedelta(days=6)
             log_info("tool_definitions", fn_name, f"Parsed 'next week': Start={start_date}, EffectiveDue={due_date_for_search}")
        elif "on " in tf:
            try: date_part = tf.split("on ")[1].strip(); parsed_date = datetime.strptime(date_part, "%Y-%m-%d").date(); start_date = max(parsed_date, today + timedelta(days=1)); due_date_for_search = start_date
            except Exception: log_warning("tool_definitions", fn_name, f"Parsing timeframe date failed: '{params.timeframe}'")
        elif "by " in tf:
            try: date_part = tf.split("by ")[1].strip(); parsed_date = datetime.strptime(date_part, "%Y-%m-%d").date(); due_date_for_search = parsed_date; start_date = max(today + timedelta(days=1), parsed_date - timedelta(days=14))
            except Exception: log_warning("tool_definitions", fn_name, f"Parsing timeframe 'by' date failed: '{params.timeframe}'")
        elif "later today" in tf or "tonight" in tf or "this afternoon" in tf:
             start_date = today
             due_date_for_search = today
             log_info("tool_definitions", fn_name, f"Parsed relative timeframe '{tf}': Start={start_date}, EffectiveDue={due_date_for_search}")
        else: log_warning("tool_definitions", fn_name, f"Unclear timeframe '{params.timeframe}', using default window.")

        default_horizon_days = 56; end_date_limit = start_date + timedelta(days=default_horizon_days - 1)
        end_date = end_date_limit
        if due_date_for_search and due_date_for_search >= start_date:
             buffer_days = 0 if start_date == due_date_for_search else 1
             end_date = min(end_date_limit, due_date_for_search - timedelta(days=buffer_days))
        end_date = max(end_date, start_date);
        start_date_str, end_date_str = start_date.strftime("%Y-%m-%d"), end_date.strftime("%Y-%m-%d")
        log_info("tool_definitions", fn_name, f"Derived Search Range: {start_date_str} to {end_date_str}")
        search_context_to_return["effective_due_date"] = due_date_for_search.strftime("%Y-%m-%d") if due_date_for_search else None
        search_context_to_return["search_start_date"] = start_date_str
        search_context_to_return["search_end_date"] = end_date_str
        search_context_to_return["original_timeframe"] = params.timeframe

        existing_events = [];
        if calendar_api is not None:
            log_info("tool_definitions", fn_name, f"Fetching GCal events from {start_date_str} to {end_date_str}")
            if start_date <= end_date:
                try:
                    events_raw = calendar_api.list_events(start_date_str, end_date_str)
                    existing_events = [
                         {"start_datetime": ev.get("gcal_start_datetime"), "end_datetime": ev.get("gcal_end_datetime"), "summary": ev.get("title")}
                         for ev in events_raw if ev.get("gcal_start_datetime") and ev.get("gcal_end_datetime")
                    ]
                    log_info("tool_definitions", fn_name, f"Fetched {len(existing_events)} valid GCal events.")
                except Exception as e:
                     log_error("tool_definitions", fn_name, f"Fetch GCal events failed: {e}", e)
            else:
                 log_warning("tool_definitions", fn_name, f"Invalid search range ({start_date_str} > {end_date_str}). Skipping GCal fetch.")
        else:
            log_warning("tool_definitions", fn_name, "GCal API inactive or unavailable. Proposing slots without checking calendar conflicts.")

        try:
            slots_to_request_from_llm = num_slots_to_find
            prompt_data = {
                "task_description": params.description or "(No description)",
                "task_due_date": search_context_to_return["effective_due_date"] or "(No specific due date)",
                "task_estimated_duration": task_estimated_duration_str,
                "user_working_days": prefs.get("Work_Days", ["Mon", "Tue", "Wed", "Thu", "Fri"]),
                "user_work_start_time": prefs.get("Work_Start_Time", "09:00"),
                "user_work_end_time": prefs.get("Work_End_Time", "17:00"),
                "user_session_length": slot_duration_str,
                "existing_events_json": json.dumps(existing_events),
                "current_date": today.strftime("%Y-%m-%d"),
                "num_slots_requested": slots_to_request_from_llm,
                "search_start_date": start_date_str,
                "search_end_date": end_date_str,
                "scheduling_hints": params.scheduling_hints or "None"
            }
            log_info("tool_definitions", fn_name, f"Scheduler prompt data prepared (requesting {slots_to_request_from_llm} slots, event count: {len(existing_events)}).")
        except Exception as e:
             log_error("tool_definitions", fn_name, f"Failed prepare prompt data for scheduler: {e}", e)
             return {**fail_result, "message": "Failed prepare data for scheduler."}

        raw_llm_output = None; parsed_data = None
        try:
            fmt_human = human_prompt.format(**prompt_data)
            messages = [{"role": "system", "content": sys_prompt}, {"role": "user", "content": fmt_human}]
            log_info("tool_definitions", fn_name, ">>> Invoking Session Scheduler LLM...")
            sched_resp = llm_client.chat.completions.create(model="gpt-4o", messages=messages, temperature=0.2, response_format={"type": "json_object"}) # type: ignore
            raw_llm_output = sched_resp.choices[0].message.content
            parsed_data = _parse_scheduler_llm_response(raw_llm_output)
            if parsed_data is None:
                log_error("tool_definitions", fn_name, "Scheduler response parse failed or returned invalid structure.")
                return {**fail_result, "message": "Received invalid proposals format from scheduler."}

            log_info("tool_definitions", f"{fn_name}_DEBUG", f"Parsed Scheduler LLM Resp: {json.dumps(parsed_data, indent=2)}")
            num_returned = len(parsed_data.get("proposed_sessions", []))
            if num_returned < num_slots_to_find:
                 log_warning("tool_definitions", fn_name, f"Sub-LLM returned only {num_returned} slots, but {num_slots_to_find} were needed.")
                 parsed_data["response_message"] += f" (Note: Only found {num_returned} of the {num_slots_to_find} required slots)."
            log_info("tool_definitions", fn_name, f"Scheduler LLM processing successful.")
            return {"success": True, "proposed_slots": parsed_data.get("proposed_sessions"), "message": parsed_data.get("response_message", "..."), "search_context": search_context_to_return}
        except Exception as e:
            tb_str = traceback.format_exc()
            log_error("tool_definitions", fn_name, f"Scheduler LLM invoke/process error: {e}. Raw: '{raw_llm_output}'. Parsed: {parsed_data}\nTraceback:\n{tb_str}", e)
            return {**fail_result, "message": "Error during slot finding process."}

    except pydantic.ValidationError as e:
        log_error("tool_definitions", fn_name, f"Validation Error: {e}");
        return {"success": False, "proposed_slots": None, "message": f"Invalid parameters: {e}", "search_context": None}
    except Exception as e:
         log_error("tool_definitions", fn_name, f"Unexpected error in propose_task_slots: {e}", e)
         return fail_result

# --- Returns dict ---
def finalize_task_and_book_sessions_tool(user_id, params: FinalizeTaskAndBookSessionsParams) -> Dict:
    """Creates task metadata (type='task') AND books the approved work sessions in GCal."""
    # --- Function body remains the same ---
    fn_name = "finalize_task_and_book_sessions_tool"
    item_id: Optional[str] = None # Initialize item_id
    task_title = "(Error getting title)" # Initialize task_title
    try:
        search_context = params.search_context; approved_slots = params.approved_slots
        log_info("tool_definitions", fn_name, f"Executing finalize+book user={user_id}, desc='{search_context.get('description')}', slots={len(approved_slots)}")
        if not search_context or not approved_slots:
            log_error("tool_definitions", fn_name, "Missing search_context or approved_slots.")
            return {"success": False, "item_id": None, "booked_count": 0, "message": "Internal error: Missing context or slots to book."}

        try:
            task_metadata_payload = {
                 "description": search_context.get("description", "Untitled Task"),
                 "estimated_duration": search_context.get("duration"),
                 "type": "task", # Explicitly set type for Task
                 "project": params.project,
                 "date": approved_slots[0].get("date"),
                 "time": approved_slots[0].get("time"),
                 # Include effective due date if available in context
                 "original_date": search_context.get("effective_due_date"),
            }
            task_title = task_metadata_payload["description"]

            # Use task_manager.create_task which handles DB insertion
            created_meta = task_manager.create_task(user_id, task_metadata_payload)

            if created_meta and created_meta.get("event_id"):
                item_id = created_meta["event_id"]
                log_info("tool_definitions", fn_name, f"Task metadata created successfully via task_manager: {item_id}");
            else:
                 raise ValueError("Failed to create task metadata via task_manager.")

        except Exception as create_err:
            log_error("tool_definitions", fn_name, f"Metadata creation failed: {create_err}", create_err)
            return {"success": False, "item_id": None, "booked_count": 0, "message": "Failed to save the task details before scheduling."}

        # Proceed to booking only if metadata creation succeeded
        booking_result = task_manager.schedule_work_sessions(user_id, item_id, approved_slots)
        if booking_result.get("success"):
            log_info("tool_definitions", fn_name, f"Booked {booking_result.get('booked_count', 0)} sessions for {item_id}")
            return {
                "success": True,
                "item_id": item_id,
                "booked_count": booking_result.get("booked_count", 0),
                "message": booking_result.get("message", f"Task '{task_title[:30]}...' created & sessions booked.")
            }
        else:
            log_error("tool_definitions", fn_name, f"Metadata created ({item_id}), but booking sessions failed: {booking_result.get('message')}")
            return {
                "success": False,
                "item_id": item_id,
                "booked_count": 0,
                "message": f"Task '{task_title[:30]}...' was created, but scheduling sessions failed: {booking_result.get('message')}"
            }
    except pydantic.ValidationError as e:
         log_error("tool_definitions", fn_name, f"Validation Error: {e}");
         return {"success": False, "item_id": None, "booked_count": 0, "message": f"Invalid parameters: {e}"}
    except Exception as e:
         log_error("tool_definitions", fn_name, f"Unexpected error: {e}", e)
         return {"success": False, "item_id": item_id, "booked_count": 0, "message": f"Error: {e}"}


# --- Returns dict ---
def update_item_details_tool(user_id, params: UpdateItemDetailsParams) -> Dict:
    """Updates core details (desc, date, time, estimate, project) of ANY item type (Task, Reminder, ToDo)."""
    # --- Function body remains the same ---
    fn_name = "update_item_details_tool";
    try:
        log_info("tool_definitions", fn_name, f"Executing user={user_id}, item={params.item_id}, updates={list(params.updates.keys())}")
        updated_item = task_manager.update_task(user_id, params.item_id, params.updates)

        if updated_item:
             return {"success": True, "message": f"Item '{params.item_id[:8]}...' updated successfully."}
        else:
             log_warning("tool_definitions", fn_name, f"Update failed for item {params.item_id} (not found or no change?).")
             item_exists = False
             if DB_IMPORTED: item_exists = activity_db.get_task(params.item_id) is not None
             if item_exists:
                  return {"success": False, "message": f"Failed to apply updates to item {params.item_id[:8]}... (perhaps no change or internal error)."}
             else:
                  return {"success": False, "message": f"Item {params.item_id[:8]}... not found."}
    except pydantic.ValidationError as e:
         log_error("tool_definitions", fn_name, f"Validation Error: {e}", e, user_id=user_id)
         return {"success": False, "message": f"Invalid parameters: {e}"}
    except Exception as e:
         log_error("tool_definitions", fn_name, f"Unexpected error: {e}", e, user_id=user_id)
         return {"success": False, "message": f"Error during update: {e}."}

# --- Returns dict ---
def update_item_status_tool(user_id, params: UpdateItemStatusParams) -> Dict:
    """Changes status OR cancels/deletes ANY item type. Requires existing item_id."""
    # --- Function body remains the same ---
    fn_name = "update_item_status_tool"
    try:
        log_info("tool_definitions", fn_name, f"Executing user={user_id}, item={params.item_id}, status={params.new_status}")
        success = False; message = ""
        if params.new_status == "cancelled":
            success = task_manager.cancel_item(user_id, params.item_id)
            message = f"Item '{params.item_id[:8]}...' cancel processed. Result: {'Success' if success else 'Failed/Not Found'}."
        else:
            updated_item = task_manager.update_task_status(user_id, params.item_id, params.new_status)
            success = updated_item is not None
            message = f"Status update to '{params.new_status}' for item '{params.item_id[:8]}...' {'succeeded' if success else 'failed/not found'}."
        return {"success": success, "message": message}
    except pydantic.ValidationError as e:
         log_error("tool_definitions", fn_name, f"Validation Error: {e}");
         return {"success": False, "message": f"Invalid parameters: {e}"}
    except Exception as e:
         log_error("tool_definitions", fn_name, f"Unexpected error: {e}", e)
         return {"success": False, "message": f"Error updating status: {e}."}

# --- Returns dict ---
def update_user_preferences_tool(user_id, params: UpdateUserPreferencesParams) -> Dict:
    # --- Function body remains the same ---
    fn_name = "update_user_preferences_tool"
    try:
        update_keys = list(params.updates.keys()) if params.updates else []
        log_info("tool_definitions", fn_name, f"Executing user={user_id}, updates={update_keys}")
        success = config_manager.update_preferences(user_id, params.updates)
        return {"success": success, "message": f"Preferences update {'succeeded' if success else 'failed'}."}
    except pydantic.ValidationError as e:
         log_error("tool_definitions", fn_name, f"Validation Error: {e}");
         return {"success": False, "message": f"Invalid parameters: {e}"}
    except Exception as e:
         log_error("tool_definitions", fn_name, f"Unexpected error: {e}", e)
         return {"success": False, "message": f"Error: {e}."}

# --- Returns dict ---
def initiate_calendar_connection_tool(user_id, params: InitiateCalendarConnectionParams) -> Dict:
    # --- Function body remains the same ---
    fn_name = "initiate_calendar_connection_tool"
    try:
        log_info("tool_definitions", fn_name, f"Executing for user {user_id}")
        result_dict = config_manager.initiate_calendar_auth(user_id)
        result_dict["success"] = result_dict.get("status") in ["pending", "token_exists"]
        return result_dict
    except pydantic.ValidationError as e:
         log_error("tool_definitions", fn_name, f"Validation Error: {e}");
         return {"success": False, "message": f"Invalid parameters: {e}"}
    except Exception as e:
         log_error("tool_definitions", fn_name, f"Unexpected error: {e}", e)
         return {"success": False, "status": "fails", "message": f"Error: {e}."}

# --- Returns dict ---
def cancel_task_sessions_tool(user_id, params: CancelTaskSessionsParams) -> Dict:
    """Removes specific scheduled GCal sessions linked explicitly to a Task and updates Task metadata."""
    fn_name = "cancel_task_sessions_tool"
    try:
        log_info("tool_definitions", fn_name, f"Executing user={user_id}, Task={params.task_id}, SessionIDs={params.session_ids_to_cancel}")

        # --- ADDED TYPE CHECK ---
        item_metadata = None
        if DB_IMPORTED:
            item_metadata = activity_db.get_task(params.task_id)

        if item_metadata is None:
            return {"success": False, "cancelled_count": 0, "message": f"Task {params.task_id[:8]}... not found."}
        if item_metadata.get("type") != "task":
            log_warning("tool_definitions", fn_name, f"Attempted to cancel sessions for non-task item {params.task_id} (type: {item_metadata.get('type')}).")
            return {"success": False, "cancelled_count": 0, "message": "Cannot cancel sessions for items that are not Tasks."}
        # --- END ADDED TYPE CHECK ---

        result = task_manager.cancel_sessions(user_id, params.task_id, params.session_ids_to_cancel)
        return result
    except pydantic.ValidationError as e:
         log_error("tool_definitions", fn_name, f"Validation Error: {e}");
         return {"success": False, "cancelled_count": 0, "message": f"Invalid parameters: {e}"}
    except Exception as e:
         log_error("tool_definitions", fn_name, f"Unexpected error: {e}", e)
         return {"success": False, "cancelled_count": 0, "message": f"Error: {e}."}

# --- Returns dict ---
def interpret_list_reply_tool(user_id, params: InterpretListReplyParams) -> Dict:
    """Placeholder tool to interpret replies to numbered lists."""
    # --- Function body remains the same ---
    fn_name = "interpret_list_reply_tool"
    try:
        log_warning("tool_definitions", fn_name, f"Tool executed for user {user_id} - Implementation is basic.")
        extracted_numbers = [int(s) for s in re.findall(r'\b\d+\b', params.user_reply)]
        identified_item_ids = []
        if params.list_mapping:
             identified_item_ids = [params.list_mapping.get(str(num)) for num in extracted_numbers if str(num) in params.list_mapping]
             identified_item_ids = [item_id for item_id in identified_item_ids if item_id is not None]

        if identified_item_ids:
             log_info("tool_definitions", fn_name, f"Identified numbers {extracted_numbers} mapping to IDs: {identified_item_ids}")
             return { "success": True, "action": "process", "item_ids": identified_item_ids, "message": f"Identified item number(s): {', '.join(map(str, extracted_numbers))}." }
        else:
             log_info("tool_definitions", fn_name, f"No valid item numbers found in reply: '{params.user_reply}'")
             return {"success": False, "item_ids": [], "message": "Couldn't find any valid item numbers in your reply."}
    except pydantic.ValidationError as e:
         log_error("tool_definitions", fn_name, f"Validation Error: {e}");
         return {"success": False, "item_ids": [], "message": f"Invalid parameters: {e}"}
    except Exception as e:
         log_error("tool_definitions", fn_name, f"Error parsing list reply: {e}", e)
         return {"success": False, "item_ids": [], "message": "Sorry, I had trouble interpreting your reply."}

# --- Returns dict ---
def get_formatted_task_list_tool(user_id, params: GetFormattedTaskListParams) -> Dict:
    """Retrieves and formats a list of items (any type) based on filters."""
    # --- Function body remains the same ---
    fn_name = "get_formatted_task_list_tool"
    try:
        status_filter = params.status_filter or 'active'
        log_info("tool_definitions", fn_name, f"Executing user={user_id}, Filter={status_filter}, Proj={params.project_filter}, Range={params.date_range}")
        date_range_tuple = tuple(params.date_range) if params.date_range else None
        list_body, list_mapping = task_query_service.get_formatted_list(
             user_id=user_id,
             date_range=date_range_tuple,
             status_filter=status_filter,
             project_filter=params.project_filter
        )
        item_count = len(list_mapping);
        message = f"Found {item_count} item(s)." if item_count > 0 else "No items found matching your criteria."
        if item_count > 0 and not list_body:
             log_warning("tool_definitions", fn_name, f"Found {item_count} items but list body is empty.")
             message = f"Found {item_count}, but there was an error formatting the list."
        return {"success": True, "list_body": list_body or "", "list_mapping": list_mapping or {}, "count": item_count, "message": message}
    except pydantic.ValidationError as e:
         log_error("tool_definitions", fn_name, f"Validation Error: {e}");
         return {"success": False, "list_body": "", "list_mapping": {}, "count": 0, "message": f"Invalid parameters: {e}"}
    except Exception as e:
         log_error("tool_definitions", fn_name, f"Unexpected error: {e}", e)
         return {"success": False, "list_body": "", "list_mapping": {}, "count": 0, "message": f"Sorry, an error occurred while retrieving the list: {e}."}


# =====================================================
# == Tool Dictionaries (MUST COME AFTER MODELS & FUNCS) ==
# =====================================================

AVAILABLE_TOOLS = {
    "create_reminder": create_reminder_tool,
    "create_task": create_task_tool, # For schedulable Tasks
    "create_todo": create_todo_tool,   # <-- NEW
    "propose_task_slots": propose_task_slots_tool,
    "finalize_task_and_book_sessions": finalize_task_and_book_sessions_tool,
    "update_item_details": update_item_details_tool,
    "update_item_status": update_item_status_tool,
    "update_user_preferences": update_user_preferences_tool,
    "initiate_calendar_connection": initiate_calendar_connection_tool,
    "cancel_task_sessions": cancel_task_sessions_tool, # Only for Tasks
    "interpret_list_reply": interpret_list_reply_tool,
    "get_formatted_task_list": get_formatted_task_list_tool
}

TOOL_PARAM_MODELS = {
    "create_reminder": CreateReminderParams,
    "create_task": CreateTaskParams, # For schedulable Tasks
    "create_todo": CreateToDoParams,   # <-- NEW
    "propose_task_slots": ProposeTaskSlotsParams,
    "finalize_task_and_book_sessions": FinalizeTaskAndBookSessionsParams,
    "update_item_details": UpdateItemDetailsParams,
    "update_item_status": UpdateItemStatusParams,
    "update_user_preferences": UpdateUserPreferencesParams,
    "initiate_calendar_connection": InitiateCalendarConnectionParams,
    "cancel_task_sessions": CancelTaskSessionsParams, # Only for Tasks
    "interpret_list_reply": InterpretListReplyParams,
    "get_formatted_task_list": GetFormattedTaskListParams
}

# --- END OF FULL agents/tool_definitions.py ---

# --- END OF FILE agents/tool_definitions.py ---



================================================================================
üìÑ services/task_manager.py
================================================================================

# --- START OF FILE services/task_manager.py ---

# --- START OF REFACTORED services/task_manager.py ---
"""
Service layer for managing tasks: creating, updating, cancelling, and scheduling sessions.
Interacts with Google Calendar API and the SQLite database via activity_db.
"""
import json
import traceback
import uuid
from datetime import datetime, timedelta, timezone
import re
from typing import Dict, List, Any # Keep typing for internal use

# Tool/Service Imports
try:
    from tools.logger import log_info, log_error, log_warning
except ImportError:
    # Basic fallback logger
    import logging; logging.basicConfig(level=logging.INFO)
    log_info=logging.info; log_error=logging.error; log_warning=logging.warning
    log_error("task_manager", "import", "Logger failed import.")

# Database Utility Import (Primary Data Source Now)
try:
    import tools.activity_db as activity_db
    DB_IMPORTED = True
except ImportError:
    log_error("task_manager", "import", "activity_db not found. Task management disabled.", None)
    DB_IMPORTED = False
    # Define dummy DB functions if import fails to prevent crashes later
    class activity_db:
        @staticmethod
        def add_or_update_task(*args, **kwargs): return False
        @staticmethod
        def get_task(*args, **kwargs): return None
        @staticmethod
        def delete_task(*args, **kwargs): return False
        @staticmethod
        def list_tasks_for_user(*args, **kwargs): return []
    # TODO: Consider if the application should halt if the DB module can't be imported

# GCal API Import
try:
    from tools.google_calendar_api import GoogleCalendarAPI
    GCAL_API_IMPORTED = True
except ImportError:
    log_warning("task_manager", "import", "GoogleCalendarAPI not found, GCal features disabled.")
    GoogleCalendarAPI = None
    GCAL_API_IMPORTED = False

# Agent State Manager Import (for updating in-memory context)
try:
    from services.agent_state_manager import get_agent_state, add_task_to_context, update_task_in_context, remove_task_from_context
    AGENT_STATE_MANAGER_IMPORTED = True
except ImportError:
    log_error("task_manager", "import", "AgentStateManager not found. In-memory context updates skipped.")
    AGENT_STATE_MANAGER_IMPORTED = False
    # Dummy functions
    def get_agent_state(*a, **k): return None
    def add_task_to_context(*a, **k): pass
    def update_task_in_context(*a, **k): pass
    def remove_task_from_context(*a, **k): pass

# Constants
DEFAULT_REMINDER_DURATION = "15m"

# --- Helper Functions (Keep these as they are useful internally) ---
# Returns GoogleCalendarAPI instance or None
def _get_calendar_api(user_id):
    """Safely retrieves the active calendar API instance from agent state."""
    fn_name = "_get_calendar_api"
    if not AGENT_STATE_MANAGER_IMPORTED or not GCAL_API_IMPORTED or GoogleCalendarAPI is None:
        # log_warning("task_manager", fn_name, f"Cannot get calendar API for {user_id}: Dependencies missing.")
        return None
    agent_state = get_agent_state(user_id)
    if agent_state is not None:
        calendar_api = agent_state.get("calendar")
        if isinstance(calendar_api, GoogleCalendarAPI) and calendar_api.is_active():
            return calendar_api
        # Optionally log if inactive:
        # elif isinstance(calendar_api, GoogleCalendarAPI) and not calendar_api.is_active():
        #      log_info("task_manager", fn_name, f"Calendar API found but inactive for user {user_id}.")
    # else: log_warning("task_manager", fn_name, f"Agent state not found for user {user_id}.")
    return None

# Returns int (minutes) or None
def _parse_duration_to_minutes(duration_str):
    """Parses duration strings like '2h', '90m', '1.5h' into minutes."""
    fn_name = "_parse_duration_to_minutes"
    if not duration_str or not isinstance(duration_str, str): return None
    duration_str = duration_str.lower().replace(' ',''); total_minutes = 0.0
    try:
        hour_match = re.search(r'(\d+(\.\d+)?)\s*h', duration_str)
        minute_match = re.search(r'(\d+)\s*m', duration_str)
        if hour_match: total_minutes += float(hour_match.group(1)) * 60
        if minute_match: total_minutes += int(minute_match.group(1))
        if total_minutes == 0 and hour_match is None and minute_match is None:
             if duration_str.replace('.','',1).isdigit(): total_minutes = float(duration_str)
             else: raise ValueError("Unrecognized duration format")
        return int(round(total_minutes)) if total_minutes > 0 else None
    except (ValueError, TypeError, AttributeError) as e:
        log_warning("task_manager", fn_name, f"Could not parse duration string '{duration_str}': {e}")
        return None

# ==============================================================
# Core Service Functions (Refactored for SQLite via activity_db)
# ==============================================================

# Returns dict (saved item) or None
def create_task(user_id: str, task_params: Dict[str, Any]) -> Dict | None:
    """
    Creates a task or reminder, saves metadata to DB, optionally adds to GCal.
    Returns the dictionary representing the saved task/reminder, or None on failure.
    """
    fn_name = "create_task"
    if not DB_IMPORTED:
        log_error("task_manager", fn_name, "Database module not imported. Cannot create task.")
        return None

    item_type = task_params.get("type")
    if not item_type:
        log_error("task_manager", fn_name, "Missing 'type' in task_params.", user_id=user_id)
        return None
    log_info("task_manager", fn_name, f"Creating item for {user_id}, type: {item_type}")

    calendar_api = _get_calendar_api(user_id)
    google_event_id = None # Track GCal ID for potential rollback

    try:
        # --- Prepare Core Metadata ---
        task_data_to_save = {}
        task_data_to_save["user_id"] = user_id
        task_data_to_save["type"] = item_type
        task_data_to_save["status"] = "pending" # Default status
        task_data_to_save["created_at"] = datetime.now(timezone.utc).isoformat(timespec='seconds')+'Z'
        task_data_to_save["title"] = task_params.get("description", "Untitled Item") # Use description as title
        # Copy other relevant fields from input params if they exist in TASK_FIELDS
        for field in activity_db.TASK_FIELDS:
             if field in task_params and field not in task_data_to_save:
                 task_data_to_save[field] = task_params[field]

        # --- Handle GCal Integration (Reminders with time) ---
        item_time = task_params.get("time")
        has_time = item_time is not None and item_time != ""
        should_create_gcal = item_type == "reminder" and has_time and calendar_api is not None

        if should_create_gcal:
            log_info("task_manager", fn_name, f"Attempting GCal creation for reminder {user_id}...")
            gcal_event_payload = { # Simplified payload for GCalAPI
                "title": task_data_to_save.get("title"),
                "description": f"Reminder: {task_data_to_save.get('description', '')}",
                "date": task_params.get("date"),
                "time": item_time,
                "duration": DEFAULT_REMINDER_DURATION
            }
            try:
                # create_event returns event ID string or None
                created_event_id = calendar_api.create_event(gcal_event_payload)
                if created_event_id is not None:
                    log_info("task_manager", fn_name, f"GCal Reminder created, ID: {created_event_id}")
                    task_data_to_save["event_id"] = created_event_id # Use GCal ID as primary key
                    google_event_id = created_event_id # Track for rollback
                    # Fetch GCal details to store exact times
                    gcal_details = calendar_api._get_single_event(created_event_id)
                    if gcal_details:
                        parsed = calendar_api._parse_google_event(gcal_details)
                        task_data_to_save["gcal_start_datetime"] = parsed.get("gcal_start_datetime")
                        task_data_to_save["gcal_end_datetime"] = parsed.get("gcal_end_datetime")
                    else: log_warning("task_manager", fn_name, f"Failed to fetch details for GCal event {created_event_id}")
                else:
                    log_warning("task_manager", fn_name, f"GCal event creation failed (returned None) for {user_id}. Using local ID.")
                    task_data_to_save["event_id"] = f"local_{uuid.uuid4()}"
            except Exception as gcal_err:
                log_error("task_manager", fn_name, f"Error creating GCal event for {user_id}", gcal_err, user_id=user_id)
                task_data_to_save["event_id"] = f"local_{uuid.uuid4()}" # Fallback to local ID
        else:
            # Assign local ID for tasks or reminders without time/API
            task_data_to_save["event_id"] = f"local_{uuid.uuid4()}"
            if item_type == "reminder" and has_time and calendar_api is None:
                 log_info("task_manager", fn_name, f"Assigning local ID for reminder {user_id}. Reason: GCal API inactive.")

        # --- Save to Database ---
        if not task_data_to_save.get("event_id"): # Should not happen, but safety check
             log_error("task_manager", fn_name, "Failed to assign event_id before DB save.", user_id=user_id)
             return None

        # Fill missing defaults before saving (optional, add_or_update_task handles some)
        task_data_to_save.setdefault('sessions_planned', 0)
        task_data_to_save.setdefault('sessions_completed', 0)
        task_data_to_save.setdefault('progress_percent', 0)
        task_data_to_save.setdefault('session_event_ids', '[]')

        save_success = activity_db.add_or_update_task(task_data_to_save)

        if save_success:
            log_info("task_manager", fn_name, f"Task {task_data_to_save['event_id']} saved to DB for {user_id}")
            # Fetch the potentially updated data from DB to return it
            saved_data = activity_db.get_task(task_data_to_save['event_id'])
            if saved_data and AGENT_STATE_MANAGER_IMPORTED:
                add_task_to_context(user_id, saved_data) # Update memory context
            return saved_data if saved_data else task_data_to_save # Return DB data if possible
        else:
            log_error("task_manager", fn_name, f"Failed to save task {task_data_to_save.get('event_id')} to DB for {user_id}.", user_id=user_id)
            # Attempt GCal rollback if DB save failed *after* GCal success
            if google_event_id is not None and calendar_api is not None:
                log_warning("task_manager", fn_name, f"DB save failed. Rolling back GCal event {google_event_id}")
                try: calendar_api.delete_event(google_event_id)
                except Exception: log_error("task_manager", fn_name, f"GCal rollback failed for {google_event_id}", user_id=user_id)
            return None

    except Exception as e:
        # Catch any unexpected errors during preparation
        log_error("task_manager", fn_name, f"Unexpected error during task creation for {user_id}", e, user_id=user_id)
        # Rollback GCal if it was created before the unexpected error
        if google_event_id is not None and calendar_api is not None:
             log_warning("task_manager", fn_name, f"Unexpected error. Rolling back GCal event {google_event_id}")
             try: calendar_api.delete_event(google_event_id)
             except Exception: log_error("task_manager", fn_name, f"GCal rollback failed for {google_event_id}", user_id=user_id)
        return None

# Returns dict (updated item) or None
def update_task(user_id: str, item_id: str, updates: Dict[str, Any]) -> Dict | None:
    """Updates details of an existing task/reminder in the DB and GCal."""
    fn_name = "update_task"
    if not DB_IMPORTED:
        log_error("task_manager", fn_name, "Database module not imported. Cannot update task.")
        return None

    log_info("task_manager", fn_name, f"Updating item {item_id} for {user_id}, keys: {list(updates.keys())}")

    # 1. Get existing task data from DB
    existing_task = activity_db.get_task(item_id) # Returns dict or None
    if existing_task is None:
        log_error("task_manager", fn_name, f"Task {item_id} not found in DB.", user_id=user_id)
        return None
    if existing_task.get("user_id") != user_id:
        log_error("task_manager", fn_name, f"User mismatch for task {item_id}.", user_id=user_id)
        return None

    calendar_api = _get_calendar_api(user_id)
    item_type = existing_task.get("type")
    gcal_updated = False # Track GCal update success

    # 2. Handle GCal Update (Reminders only)
    if item_type == "reminder" and not item_id.startswith("local_") and calendar_api is not None:
        gcal_payload = {}
        needs_gcal_update = False
        if "description" in updates:
            gcal_payload["title"] = updates["description"]
            gcal_payload["description"] = f"Reminder: {updates['description']}"
            needs_gcal_update = True
        if "date" in updates or "time" in updates:
            gcal_payload["date"] = updates.get("date", existing_task.get("date"))
            gcal_payload["time"] = updates.get("time") if "time" in updates else existing_task.get("time")
            if gcal_payload["date"] is not None: needs_gcal_update = True

        if needs_gcal_update:
            log_info("task_manager", fn_name, f"Attempting GCal update for reminder {item_id}")
            try:
                update_success = calendar_api.update_event(item_id, gcal_payload)
                if update_success:
                    gcal_updated = True
                    log_info("task_manager", fn_name, f"GCal reminder {item_id} updated successfully.")
                else: log_warning("task_manager", fn_name, f"GCal reminder {item_id} update failed (API returned False).")
            except Exception as gcal_err:
                 log_error("task_manager", fn_name, f"Error updating GCal reminder {item_id}", gcal_err, user_id=user_id)
        else: log_info("task_manager", fn_name, f"No relevant fields for GCal reminder {item_id} update.")

    # 3. Prepare DB Updates
    db_update_data = existing_task.copy() # Start with existing data
    # Apply valid updates from the input 'updates' dictionary
    allowed_meta_keys = {"description", "date", "time", "estimated_duration", "project"}
    applied_db_updates = False
    for key, value in updates.items():
        if key in allowed_meta_keys:
            db_update_data[key] = value
            if key == 'description': db_update_data['title'] = value # Keep title synced
            applied_db_updates = True

    # 4. Refresh GCal Timestamps if GCal was updated
    if gcal_updated and calendar_api is not None:
        gcal_details = calendar_api._get_single_event(item_id)
        if gcal_details:
            parsed = calendar_api._parse_google_event(gcal_details)
            db_update_data["gcal_start_datetime"] = parsed.get("gcal_start_datetime")
            db_update_data["gcal_end_datetime"] = parsed.get("gcal_end_datetime")
            applied_db_updates = True # Mark as updated even if only GCal times changed
            log_info("task_manager", fn_name, f"Refreshed GCal times in task data for {item_id}")
        else:
            log_warning("task_manager", fn_name, f"GCal update ok, but failed to re-fetch details for {item_id}")

    # 5. Save to DB if changes were applied or GCal timestamps were refreshed
    if applied_db_updates:
        save_success = activity_db.add_or_update_task(db_update_data)
        if save_success:
            log_info("task_manager", fn_name, f"Task {item_id} updated successfully in DB.")
            # Fetch final state from DB
            updated_task_from_db = activity_db.get_task(item_id)
            if updated_task_from_db and AGENT_STATE_MANAGER_IMPORTED:
                 update_task_in_context(user_id, item_id, updated_task_from_db) # Update memory
            return updated_task_from_db if updated_task_from_db else db_update_data
        else:
            log_error("task_manager", fn_name, f"Failed to save task {item_id} updates to DB.", user_id=user_id)
            return None # DB save failed
    else:
        log_info("task_manager", fn_name, f"No applicable DB updates or GCal timestamp changes for task {item_id}.")
        return existing_task # Return original data if no changes were made

# Returns dict (updated item) or None
def update_task_status(user_id: str, item_id: str, new_status: str) -> Dict | None:
    """Updates only the status and related tracking fields in the DB."""
    fn_name = "update_task_status"
    if not DB_IMPORTED:
        log_error("task_manager", fn_name, "Database module not imported.")
        return None

    log_info("task_manager", fn_name, f"Setting status='{new_status}' for item {item_id}, user {user_id}")
    # Validate and clean status
    new_status_clean = new_status.lower().replace(" ", "")
    allowed_statuses = {"pending", "in_progress", "completed"}
    if new_status_clean == "cancelled":
        log_error("task_manager", fn_name, "Use cancel_item() function for 'cancelled' status.", user_id=user_id)
        return None
    if new_status_clean not in allowed_statuses:
         log_error("task_manager", fn_name, f"Invalid status '{new_status}' provided.", user_id=user_id)
         return None

    # 1. Get existing task data
    existing_task = activity_db.get_task(item_id)
    if existing_task is None:
        log_error("task_manager", fn_name, f"Task {item_id} not found in DB.", user_id=user_id)
        return None
    if existing_task.get("user_id") != user_id:
        log_error("task_manager", fn_name, f"User mismatch for task {item_id}.", user_id=user_id)
        return None

    # 2. Prepare update dictionary
    updates_dict = {"status": new_status_clean}
    now_iso_utc = datetime.now(timezone.utc).isoformat(timespec='seconds')+'Z'

    if new_status_clean == "completed":
        updates_dict["completed_at"] = now_iso_utc
        updates_dict["progress_percent"] = 100
        if existing_task.get("type") == "task":
            updates_dict["sessions_completed"] = existing_task.get("sessions_planned", 0)
    elif new_status_clean == "pending":
         updates_dict["completed_at"] = None # Use None for DB
         updates_dict["progress_percent"] = 0
         updates_dict["sessions_completed"] = 0
    elif new_status_clean == "in_progress":
         updates_dict["completed_at"] = None # Use None for DB

    # 3. Apply updates to a copy and save
    task_data_to_save = existing_task.copy()
    task_data_to_save.update(updates_dict)

    save_success = activity_db.add_or_update_task(task_data_to_save)

    if save_success:
        log_info("task_manager", fn_name, f"Task {item_id} status updated to {new_status_clean} in DB.")
        updated_task_from_db = activity_db.get_task(item_id) # Re-fetch to get final state
        if updated_task_from_db and AGENT_STATE_MANAGER_IMPORTED:
             update_task_in_context(user_id, item_id, updated_task_from_db)
        return updated_task_from_db if updated_task_from_db else task_data_to_save
    else:
        log_error("task_manager", fn_name, f"Failed to save status update for task {item_id} to DB.", user_id=user_id)
        return None

# Returns bool
def cancel_item(user_id: str, item_id: str) -> bool:
    """Sets item status to 'cancelled' in DB and deletes associated GCal events."""
    fn_name = "cancel_item"
    if not DB_IMPORTED:
        log_error("task_manager", fn_name, "Database module not imported.")
        return False

    log_info("task_manager", fn_name, f"Processing cancellation for item {item_id}, user {user_id}")

    # 1. Get task data from DB
    task_data = activity_db.get_task(item_id)
    if task_data is None:
        log_warning("task_manager", fn_name, f"Task {item_id} not found in DB during cancel. Assuming handled.")
        return True # Not found -> already gone? Success.
    if task_data.get("user_id") != user_id:
        log_error("task_manager", fn_name, f"User mismatch for item {item_id}.", user_id=user_id)
        return False
    if task_data.get("status") == "cancelled":
        log_info("task_manager", fn_name, f"Item {item_id} is already cancelled.")
        return True

    # 2. GCal Cleanup
    calendar_api = _get_calendar_api(user_id)
    item_type = task_data.get("type")
    gcal_cleanup_errors = []

    if calendar_api is not None and not item_id.startswith("local_"):
        log_info("task_manager", fn_name, f"Performing GCal cleanup for {item_type} {item_id}")
        # Delete main event if it's a Reminder on GCal
        if item_type == "reminder":
            try:
                deleted = calendar_api.delete_event(item_id)
                if not deleted: log_warning("task_manager", fn_name, f"GCal delete failed/not found for reminder {item_id}")
            except Exception as del_err:
                 log_error("task_manager", fn_name, f"Error deleting GCal reminder {item_id}", del_err, user_id=user_id)
                 gcal_cleanup_errors.append(f"Main event {item_id}")
        # Delete session events if it's a Task
        elif item_type == "task":
            session_ids = task_data.get("session_event_ids", []) # Already decoded list from get_task
            if isinstance(session_ids, list) and session_ids:
                log_info("task_manager", fn_name, f"Deleting {len(session_ids)} GCal sessions for task {item_id}")
                for session_id in session_ids:
                    if not isinstance(session_id, str) or not session_id: continue
                    try:
                        deleted = calendar_api.delete_event(session_id)
                        if not deleted: log_warning("task_manager", fn_name, f"GCal delete failed/not found for session {session_id}")
                    except Exception as sess_del_err:
                         log_error("task_manager", fn_name, f"Error deleting GCal session {session_id}", sess_del_err, user_id=user_id)
                         gcal_cleanup_errors.append(f"Session {session_id}")
            else: log_info("task_manager", fn_name, f"No GCal session IDs to delete for task {item_id}")
    # else: log reason for skipping GCal cleanup

    # 3. Update DB Status
    update_payload = task_data.copy()
    update_payload["status"] = "cancelled"
    # Reset task-specific fields on cancel
    if item_type == "task":
        update_payload["sessions_planned"] = 0
        update_payload["sessions_completed"] = 0
        update_payload["progress_percent"] = 0
        update_payload["session_event_ids"] = [] # Store empty list, will be JSON '[]'

    save_success = activity_db.add_or_update_task(update_payload)

    if save_success:
        log_info("task_manager", fn_name, f"Successfully marked item {item_id} as cancelled in DB.")
        cancelled_task_data = activity_db.get_task(item_id) # Get final state
        if cancelled_task_data and AGENT_STATE_MANAGER_IMPORTED:
             update_task_in_context(user_id, item_id, cancelled_task_data) # Update memory context
        if gcal_cleanup_errors:
             log_warning("task_manager", fn_name, f"Cancel successful for {item_id}, but GCal cleanup errors: {gcal_cleanup_errors}", user_id=user_id)
        return True
    else:
        log_error("task_manager", fn_name, f"Failed to save cancelled status for {item_id} to DB.", user_id=user_id)
        # State is inconsistent: GCal might be cleaned, DB not updated.
        return False

# --- Scheduling Functions ---

# Returns dict with 'success', 'message', 'booked_count', 'session_ids'
def schedule_work_sessions(user_id: str, task_id: str, slots_to_book: List[Dict]) -> Dict:
    """Creates GCal events for proposed work sessions and updates the parent task in DB."""
    fn_name = "schedule_work_sessions"
    default_fail_result = {"success": False, "booked_count": 0, "message": "An unexpected error occurred.", "session_ids": []}
    if not DB_IMPORTED:
        return {**default_fail_result, "message": "Database module not available."}

    log_info("task_manager", fn_name, f"Booking {len(slots_to_book)} sessions for task {task_id}")

    calendar_api = _get_calendar_api(user_id)
    if calendar_api is None:
        return {**default_fail_result, "message": "Calendar is not connected or active."}

    # 1. Get Parent Task Details from DB
    task_metadata = activity_db.get_task(task_id)
    if task_metadata is None:
        log_error("task_manager", fn_name, f"Parent task {task_id} not found in DB.", user_id=user_id)
        return {**default_fail_result, "message": "Original task details not found."}
    if task_metadata.get("user_id") != user_id:
        log_error("task_manager", fn_name, f"User mismatch for task {task_id}.", user_id=user_id)
        return {**default_fail_result, "message": "Task ownership mismatch."}
    if task_metadata.get("type") != "task":
         log_error("task_manager", fn_name, f"Item {task_id} is not a task.", user_id=user_id)
         return {**default_fail_result, "message": "Scheduling only supported for tasks."}

    task_title = task_metadata.get("title", "Task Work")

    # 2. Create GCal Events
    created_session_ids = []
    errors = []
    for i, session_slot in enumerate(slots_to_book):
        session_date = session_slot.get("date")
        session_time = session_slot.get("time")
        session_end_time = session_slot.get("end_time")
        if not all([session_date, session_time, session_end_time]):
             msg = f"Session {i+1} missing date/time/end_time"
             log_warning("task_manager", fn_name, msg + f" for task {task_id}")
             errors.append(msg); continue
        try:
            start_dt = datetime.strptime(f"{session_date} {session_time}", "%Y-%m-%d %H:%M")
            end_dt = datetime.strptime(f"{session_date} {session_end_time}", "%Y-%m-%d %H:%M")
            duration_minutes = int((end_dt - start_dt).total_seconds() / 60)
            if duration_minutes <= 0: raise ValueError("Duration must be positive")

            session_event_data = {
                "title": f"Work: {task_title} [{i+1}/{len(slots_to_book)}]",
                "description": f"Focused work session for task: {task_title}\nParent Task ID: {task_id}",
                "date": session_date, "time": session_time,
                "duration": f"{duration_minutes}m"
            }
            # create_event returns ID string or None
            session_event_id = calendar_api.create_event(session_event_data)
            if session_event_id is not None:
                created_session_ids.append(session_event_id)
            else:
                msg = f"Session {i+1} GCal creation failed (API returned None)"
                log_error("task_manager", fn_name, msg + f" for task {task_id}.", user_id=user_id)
                errors.append(msg)
        except Exception as e:
            msg = f"Session {i+1} creation error: {type(e).__name__}"
            log_error("task_manager", fn_name, f"Error creating GCal session {i+1} for task {task_id}", e, user_id=user_id)
            errors.append(msg)

    if not created_session_ids:
        err_summary = "; ".join(errors) if errors else "Unknown reason"
        log_error("task_manager", fn_name, f"Failed to create any GCal sessions for task {task_id}. Errors: {err_summary}", user_id=user_id)
        return {**default_fail_result, "message": f"Sorry, couldn't add sessions to calendar. Errors: {err_summary}"}

    log_info("task_manager", fn_name, f"Created {len(created_session_ids)} GCal sessions for task {task_id}: {created_session_ids}")

    # 3. Update Parent Task in DB
    # Combine existing and new session IDs
    existing_session_ids = task_metadata.get("session_event_ids", []) # Already decoded list
    if not isinstance(existing_session_ids, list): existing_session_ids = []
    all_session_ids = list(set(existing_session_ids + created_session_ids))

    update_payload = task_metadata.copy()
    update_payload["sessions_planned"] = len(all_session_ids)
    update_payload["session_event_ids"] = all_session_ids # Store list, add_or_update handles JSON
    update_payload["status"] = "in_progress" # Mark task as in progress

    save_success = activity_db.add_or_update_task(update_payload)

    if save_success:
        log_info("task_manager", fn_name, f"Parent task {task_id} updated in DB with session info.")
        updated_task_data = activity_db.get_task(task_id) # Get final state
        if updated_task_data and AGENT_STATE_MANAGER_IMPORTED:
            update_task_in_context(user_id, task_id, updated_task_data) # Update memory

        num_booked = len(created_session_ids)
        plural_s = "s" if num_booked > 1 else ""
        msg = f"Okay, I've scheduled {num_booked} work session{plural_s} for '{task_title}' in your calendar."
        if errors: msg += f" (Issues with {len(errors)} other slots)."
        return {"success": True, "booked_count": num_booked, "message": msg, "session_ids": created_session_ids}
    else:
        log_error("task_manager", fn_name, f"Created GCal sessions for {task_id}, but failed DB update.", user_id=user_id)
        # Rollback GCal changes
        log_warning("task_manager", fn_name, f"Attempting GCal rollback for {len(created_session_ids)} sessions (Task ID: {task_id}).")
        if calendar_api is not None:
            for sid in created_session_ids:
                try: calendar_api.delete_event(sid)
                except Exception: log_error("task_manager", fn_name, f"GCal rollback delete failed for session {sid}", user_id=user_id)
        return {**default_fail_result, "message": "Scheduled sessions, but failed to link to task. Calendar changes rolled back."}

# Returns dict with 'success', 'cancelled_count', 'message'
def cancel_sessions(user_id: str, task_id: str, session_ids_to_cancel: List[str]) -> Dict:
    """Cancels specific GCal work sessions and updates task metadata in DB."""
    fn_name = "cancel_sessions"
    default_fail_result = {"success": False, "cancelled_count": 0, "message": "An unexpected error occurred."}
    if not DB_IMPORTED:
        return {**default_fail_result, "message": "Database module not available."}

    log_info("task_manager", fn_name, f"Cancelling {len(session_ids_to_cancel)} sessions for task {task_id}")

    calendar_api = _get_calendar_api(user_id)
    if calendar_api is None:
        return {**default_fail_result, "message": "Calendar is not connected or active."}

    # 1. Get Parent Task from DB
    task_metadata = activity_db.get_task(task_id)
    if task_metadata is None:
        log_error("task_manager", fn_name, f"Parent task {task_id} not found.", user_id=user_id)
        return {**default_fail_result, "message": "Original task details not found."}
    if task_metadata.get("user_id") != user_id: # Check ownership
         log_error("task_manager", fn_name, f"User mismatch for task {task_id}.", user_id=user_id)
         return {**default_fail_result, "message": "Task ownership mismatch."}
    if task_metadata.get("type") != "task":
         return {**default_fail_result, "message": "Can only cancel sessions for tasks."}

    # 2. Delete GCal Events
    cancelled_count = 0
    errors = []
    valid_gcal_ids_to_cancel = [sid for sid in session_ids_to_cancel if isinstance(sid, str) and not sid.startswith("local_")]

    for session_id in valid_gcal_ids_to_cancel:
        try:
            deleted = calendar_api.delete_event(session_id) # Returns bool
            if deleted: cancelled_count += 1
            # else: delete_event logs warning if not found/failed
        except Exception as e:
            log_error("task_manager", fn_name, f"Error deleting GCal session {session_id} for task {task_id}", e, user_id=user_id)
            errors.append(session_id)
    log_info("task_manager", fn_name, f"GCal delete attempts for task {task_id}: Success/Gone: {cancelled_count}, Errors: {len(errors)}")

    # 3. Update Parent Task in DB
    existing_session_ids = task_metadata.get("session_event_ids", []) # Already list from get_task
    if not isinstance(existing_session_ids, list): existing_session_ids = []

    cancelled_set = set(session_ids_to_cancel) # Use original list (might include local IDs intended for removal)
    remaining_ids = [sid for sid in existing_session_ids if sid not in cancelled_set]

    update_payload = task_metadata.copy()
    update_payload["sessions_planned"] = len(remaining_ids)
    update_payload["session_event_ids"] = remaining_ids # Store list for DB function
    # Only change status to pending if NO sessions remain, otherwise keep current status
    if not remaining_ids:
        update_payload["status"] = "pending"

    save_success = activity_db.add_or_update_task(update_payload)

    if save_success:
        log_info("task_manager", fn_name, f"Parent task {task_id} updated in DB after session cancellation.")
        updated_task_data = activity_db.get_task(task_id) # Get final state
        if updated_task_data and AGENT_STATE_MANAGER_IMPORTED:
            update_task_in_context(user_id, task_id, updated_task_data) # Update memory

        msg = f"Successfully cancelled {cancelled_count} session(s) from your calendar."
        if errors: msg += f" Encountered errors cancelling {len(errors)}."
        return {"success": True, "cancelled_count": cancelled_count, "message": msg}
    else:
        log_error("task_manager", fn_name, f"Deleted GCal sessions for {task_id}, but failed DB update.", user_id=user_id)
        # Inconsistent state: GCal events gone, DB still references them. Hard to roll back GCal deletes.
        return {**default_fail_result, "cancelled_count": cancelled_count, "message": "Cancelled sessions in calendar, but failed to update the task link."}

# --- END OF REFACTORED services/task_manager.py ---

# --- END OF FILE services/task_manager.py ---



================================================================================
üìÑ services/task_query_service.py
================================================================================

# --- START OF FILE services/task_query_service.py ---

# --- START OF REFACTORED services/task_query_service.py ---
"""Service layer for querying and formatting task/reminder data from the database."""
from datetime import datetime, timedelta, timezone # Added timezone
import json
from typing import Dict, List, Any, Set, Tuple # Keep required types
import pytz
import traceback # Keep for detailed error logging

from tools.logger import log_info, log_error, log_warning
# Import the database utility module
try:
    import tools.activity_db as activity_db
    DB_IMPORTED = True
except ImportError:
    log_error("task_query_service", "import", "activity_db not found. Task querying disabled.", None)
    DB_IMPORTED = False
    # Dummy DB functions
    class activity_db:
        @staticmethod
        def list_tasks_for_user(*args, **kwargs): return []
        @staticmethod
        def get_task(*args, **kwargs): return None
    # Consider halting application if DB is critical

# Agent State Manager Import (still needed for preferences/calendar API instance)
try:
    from services.agent_state_manager import get_agent_state
    AGENT_STATE_MANAGER_IMPORTED = True
except ImportError:
    log_error("task_query_service", "import", "AgentStateManager not found.")
    AGENT_STATE_MANAGER_IMPORTED = False
    def get_agent_state(*args, **kwargs): return None # Dummy function

# Google Calendar API Import (needed for checking type and formatting sessions)
try:
    from tools.google_calendar_api import GoogleCalendarAPI
    GCAL_API_IMPORTED = True
except ImportError:
     log_warning("task_query_service", "import", "GoogleCalendarAPI not imported.")
     GoogleCalendarAPI = None
     GCAL_API_IMPORTED = False

# Define active statuses consistently
ACTIVE_STATUSES = ["pending", "in_progress"] # Use list for DB query IN clause

# --- Internal Helper Functions ---

# Keep this function as it retrieves the API instance needed for _format_task_line
def _get_calendar_api_from_state(user_id):
    """Helper to retrieve the active calendar API instance from agent state."""
    fn_name = "_get_calendar_api_from_state"
    if not AGENT_STATE_MANAGER_IMPORTED or not GCAL_API_IMPORTED or GoogleCalendarAPI is None:
        return None
    try:
        agent_state = get_agent_state(user_id) # Returns dict or None
        if agent_state is not None:
            calendar_api_maybe = agent_state.get("calendar")
            if isinstance(calendar_api_maybe, GoogleCalendarAPI) and calendar_api_maybe.is_active():
                return calendar_api_maybe # Return active instance
    except Exception as e:
         log_error("task_query_service", fn_name, f"Error getting calendar API for {user_id}", e, user_id=user_id)
    return None # Return None if not found, not active, or error

# Keep sorting logic, operates on list of dictionaries
def _sort_tasks(task_list: List[Dict]) -> List[Dict]:
    """Sorts task list robustly by date/time."""
    # (Sorting logic remains the same as previous version - operates on dicts)
    fn_name = "_sort_tasks"
    def sort_key(item):
        gcal_start = item.get("gcal_start_datetime")
        if gcal_start and isinstance(gcal_start, str):
             try:
                 if 'T' in gcal_start: # Datetime format
                      dt_aware = datetime.fromisoformat(gcal_start.replace('Z', '+00:00'))
                      return dt_aware.replace(tzinfo=None) # Compare naive UTC equivalent
                 elif len(gcal_start) == 10: # Date format (all-day)
                      dt_date = datetime.strptime(gcal_start, '%Y-%m-%d').date()
                      # Sort all-day events as start of the day
                      return datetime.combine(dt_date, datetime.min.time())
             except ValueError:
                  pass # Fallback to metadata if parse fails

        # Fallback logic using metadata date/time
        meta_date_str = item.get("date")
        meta_time_str = item.get("time")
        sort_dt = datetime.max # Default to max for sorting unknowns last

        if meta_date_str:
            try:
                if meta_time_str: # Timed item
                    time_part = meta_time_str
                    if len(time_part.split(':')) == 2: time_part += ':00' # Add seconds if missing
                    sort_dt = datetime.strptime(f"{meta_date_str} {time_part}", "%Y-%m-%d %H:%M:%S")
                else: # All day item based on metadata date
                    sort_dt = datetime.strptime(meta_date_str, "%Y-%m-%d")
            except (ValueError, TypeError):
                 # Log warning? Maybe too verbose for sorting fallback
                 pass # Use default max time
        return sort_dt

    try:
        # Sort primarily by datetime, secondarily by creation time (if available), finally by title
        return sorted(task_list, key=lambda item: (sort_key(item), item.get("created_at", ""), item.get("title", "").lower()))
    except Exception as e:
        log_error("task_query_service", fn_name, f"Error during task sorting: {e}", e)
        return task_list # Return unsorted list on error

# Keep formatting logic, operates on list of dictionaries
# Ensure it correctly handles data types from DB (e.g., sessions_planned is INT)
# And decodes session_event_ids if needed (activity_db functions handle this now)
def _format_task_line(task_data: Dict, user_timezone_str: str = "UTC", calendar_api = None) -> str:
    """Formats a single task/event dictionary into a display string."""
    fn_name = "_format_task_line"
    try:
        # Determine User Timezone Object
        user_tz = pytz.utc
        try:
            if user_timezone_str: user_tz = pytz.timezone(user_timezone_str)
        except pytz.UnknownTimeZoneError: user_timezone_str = "UTC"

        # --- Assemble Main Line Parts (Largely same logic as before) ---
        parts = []
        item_type_raw = task_data.get("type", "Item")
        item_type = str(item_type_raw).capitalize()
        if item_type_raw == "external_event": item_type = "Event" # Handle external events from sync
        parts.append(f"({item_type})")
        desc = str(task_data.get("title", "")).strip() or "(No Title)"
        parts.append(desc)
        if item_type_raw == "task":
            duration = task_data.get("estimated_duration")
            # Check for various empty-like values
            if duration and not str(duration).strip().lower() in ['', 'none', 'nan', 'null']:
                parts.append(f"[Est: {duration}]")

        # Date/Time Formatting (Prioritize GCal, fallback to metadata)
        gcal_start_str = task_data.get("gcal_start_datetime"); meta_date = task_data.get("date"); meta_time = task_data.get("time")
        dt_str = ""
        if gcal_start_str:
             try: # Format GCal time
                 if 'T' in gcal_start_str: # Datetime
                      dt_aware = datetime.fromisoformat(gcal_start_str.replace('Z', '+00:00'))
                      dt_local = dt_aware.astimezone(user_tz)
                      # Use a clear format like: Tue, Apr 25 @ 10:00 EDT
                      formatted_dt = dt_local.strftime('%a, %b %d @ %H:%M %Z')
                      dt_str = f" on {formatted_dt}"
                 elif len(gcal_start_str) == 10: # Date (All day)
                      dt_local = datetime.strptime(gcal_start_str, '%Y-%m-%d').date()
                      formatted_dt = dt_local.strftime('%a, %b %d (All day)')
                      dt_str = f" on {formatted_dt}"
             except Exception as fmt_err:
                  log_warning("task_query_service", fn_name, f"Could not format gcal_start '{gcal_start_str}': {fmt_err}")
                  dt_str = f" (Time Error: {gcal_start_str})" # Show raw on error
        elif meta_date: # Fallback to metadata date/time
             dt_str = f" on {meta_date}"
             if meta_time: dt_str += f" at {meta_time}"
             else: dt_str += " (All day)"
        if dt_str: parts.append(dt_str)

        project = task_data.get("project"); status = task_data.get("status")
        if project: parts.append(f"{{{project}}}")
        if item_type_raw in ["task", "reminder"] and status:
             parts.append(f"[{str(status).capitalize()}]")

        main_line = " ".join(p for p in parts if p)

        # --- Display Scheduled Session Details ---
        session_details_lines = []
        # session_event_ids should be a list from the DB access layer now
        session_ids = task_data.get("session_event_ids")
        if item_type_raw == "task" and calendar_api is not None and isinstance(session_ids, list) and session_ids:
            session_details_lines.append("    ‚îî‚îÄ‚îÄ Scheduled Sessions:")
            session_num = 0
            for session_id in session_ids:
                if not isinstance(session_id, str) or not session_id.strip(): continue # Skip invalid IDs
                try:
                    session_event_data = calendar_api._get_single_event(session_id) # Returns dict or None
                    if session_event_data:
                        session_num += 1
                        parsed_session = calendar_api._parse_google_event(session_event_data) # Returns dict
                        s_start = parsed_session.get("gcal_start_datetime")
                        s_end = parsed_session.get("gcal_end_datetime")
                        s_info = "(Time Error)" # Default
                        if s_start and s_end:
                            try:
                                s_aware = datetime.fromisoformat(s_start.replace('Z', '+00:00'))
                                e_aware = datetime.fromisoformat(s_end.replace('Z', '+00:00'))
                                s_local, e_local = s_aware.astimezone(user_tz), e_aware.astimezone(user_tz)
                                # Format: YYYY-MM-DD HH:MM-HH:MM TZN
                                s_info = s_local.strftime('%Y-%m-%d %H:%M') + e_local.strftime('-%H:%M %Z')
                            except Exception as parse_err:
                                log_warning("task_query_service", fn_name, f"Could not parse session times {s_start}-{s_end}: {parse_err}")
                                s_info = f"{s_start} to {s_end} (parse error)"
                        session_details_lines.append(f"        {session_num}) {s_info} (ID: {session_id})")
                    # else: log_warning(f"Session ID {session_id} not found in GCal") # Optional: log if GCal event missing
                except Exception as fetch_err:
                    log_error("task_query_service", fn_name, f"Error fetching/processing session {session_id}", fetch_err, user_id=task_data.get("user_id"))
                    session_details_lines.append(f"        - Error fetching session ID {session_id}")
            # Remove header if no actual sessions were listed
            if session_num == 0 and session_details_lines:
                 session_details_lines.pop(0)

        # Combine main line and session details
        return main_line + ("\n" + "\n".join(session_details_lines) if session_details_lines else "")

    except Exception as e:
        # Log error with user context if available
        user_ctx = task_data.get("user_id", "Unknown")
        log_error("task_query_service", fn_name, f"General error formatting item line {task_data.get('event_id')}. Error: {e}\n{traceback.format_exc()}", e, user_id=user_ctx)
        return f"Error displaying item: {task_data.get('event_id', 'Unknown ID')}"


# --- Public Service Functions ---

# Returns Tuple[str, Dict]
def get_formatted_list(
    user_id: str,
    date_range: Tuple[str, str] | None = None,
    status_filter: str = 'active', # Provide default
    project_filter: str | None = None,
    trigger_sync: bool = False # Keep trigger_sync param, though sync isn't fully implemented
) -> Tuple[str, Dict]:
    """
    Gets tasks from DB, filters (optional), sorts, formats into numbered list string & mapping.
    Includes fetching and displaying details for scheduled task sessions if GCal is active.
    """
    fn_name = "get_formatted_list"
    if not DB_IMPORTED:
        log_error("task_query_service", fn_name, "Database module unavailable.", user_id=user_id)
        return "Error: Could not access task data.", {}

    log_info("task_query_service", fn_name, f"Executing for user={user_id}, Status={status_filter}, Range={date_range}, Proj={project_filter}")

    if trigger_sync:
        log_info("task_query_service", fn_name, "Sync triggered (Not Implemented Yet).", user_id=user_id)
        # Future: Call sync_service.perform_full_sync(user_id) here?

    # Determine status list for DB query
    query_status_list = None
    filter_lower = status_filter.lower().replace(" ", "") if status_filter else 'active' # Default if None
    if filter_lower == 'active': query_status_list = ACTIVE_STATUSES
    elif filter_lower == 'completed': query_status_list = ["completed"]
    elif filter_lower == 'pending': query_status_list = ["pending"]
    elif filter_lower == 'in_progress': query_status_list = ["in_progress"]
    elif filter_lower == 'all': query_status_list = None # No status filter for DB
    else:
        log_warning("task_query_service", fn_name, f"Unknown status filter '{status_filter}'. Defaulting 'active'.", user_id=user_id)
        query_status_list = ACTIVE_STATUSES

    log_info("task_query_service", fn_name, f"Querying DB for user={user_id}, Status={query_status_list}, Range={date_range}, Proj={project_filter}")

    # Fetch data from DB, applying filters available in the DB function
    task_list = []
    try:
        task_list = activity_db.list_tasks_for_user(
            user_id=user_id,
            status_filter=query_status_list, # Pass list of statuses or None
            date_range=date_range,         # Pass tuple or None
            project_filter=project_filter  # Pass string or None
        )
        log_info("task_query_service", fn_name, f"Fetched {len(task_list)} tasks from DB for {user_id} with filters.")
    except Exception as e:
        log_error("task_query_service", fn_name, f"Error fetching tasks from DB for {user_id}", e, user_id=user_id)
        return "Error retrieving tasks.", {}

    # No need for python filtering now as DB function handles it

    if not task_list:
        log_info("task_query_service", fn_name, f"No tasks found matching criteria for {user_id} after DB query.")
        return "", {} # Return empty string and dict if no tasks found

    # Get Calendar API and User Timezone for formatting
    calendar_api = _get_calendar_api_from_state(user_id)
    user_tz_str = "UTC"
    if AGENT_STATE_MANAGER_IMPORTED:
        agent_state = get_agent_state(user_id)
        prefs = agent_state.get("preferences", {}) if agent_state else {}
        user_tz_str = prefs.get("TimeZone", "UTC")

    # Sort tasks
    sorted_tasks = _sort_tasks(task_list)

    # Format lines and build mapping
    lines, mapping = [], {}
    item_num = 0
    for task in sorted_tasks:
        item_id = task.get("event_id")
        if not item_id: continue # Skip items missing id

        item_num += 1
        formatted_line = _format_task_line(task, user_tz_str, calendar_api)
        lines.append(f"{item_num}. {formatted_line}")
        mapping[str(item_num)] = item_id # Use string key

    if item_num == 0: # Should only happen if formatting fails for all items
        log_warning("task_query_service", fn_name, f"Formatting resulted in zero list items for {user_id}.", user_id=user_id)
        return "Error formatting the task list.", {}

    list_body = "\n".join(lines)
    log_info("task_query_service", fn_name, f"Generated list body ({len(mapping)} items) for {user_id}")
    return list_body, mapping


# Returns List[Dict]
def get_tasks_for_summary(
    user_id: str,
    date_range: Tuple[str, str], # Date range is typically required for summaries
    status_filter: str = 'active',
    trigger_sync: bool = False
) -> List[Dict]:
    """Gets tasks from DB for summaries, filters, sorts, returns list of dictionaries."""
    fn_name = "get_tasks_for_summary"
    if not DB_IMPORTED:
        log_error("task_query_service", fn_name, "Database module unavailable.", user_id=user_id)
        return []

    log_info("task_query_service", fn_name, f"Executing for user={user_id}, Filter={status_filter}, Range={date_range}")

    if trigger_sync:
        log_info("task_query_service", fn_name, "Sync triggered (Not Implemented Yet).", user_id=user_id)

    # Determine status list for DB query
    query_status_list = None
    filter_lower = status_filter.lower().replace(" ", "") if status_filter else 'active'
    if filter_lower == 'active': query_status_list = ACTIVE_STATUSES
    elif filter_lower == 'completed': query_status_list = ["completed"]
    elif filter_lower == 'pending': query_status_list = ["pending"]
    elif filter_lower == 'in_progress': query_status_list = ["in_progress"]
    # 'all' means query_status_list remains None

    try:
        # Fetch directly using the specific filters required for summaries
        task_list = activity_db.list_tasks_for_user(
            user_id=user_id,
            status_filter=query_status_list, # Pass list of statuses or None
            date_range=date_range
            # No project filter typically needed for summaries
        )
        # Sort results after fetching
        sorted_tasks = _sort_tasks(task_list)
        log_info("task_query_service", fn_name, f"Returning {len(sorted_tasks)} tasks for summary {user_id}")
        return sorted_tasks
    except Exception as db_err:
        log_error("task_query_service", fn_name, f"Database error fetching tasks for summary: {user_id}", db_err, user_id=user_id)
        return []


# Returns Tuple[List[Dict], List[Dict]]
def get_context_snapshot(user_id: str, history_weeks: int = 1, future_weeks: int = 2) -> Tuple[List[Dict], List[Dict]]:
    """Fetches relevant active WT tasks (DB) and GCal events (API) for Orchestrator context."""
    fn_name = "get_context_snapshot"
    if not DB_IMPORTED:
        log_error("task_query_service", fn_name, "Database module unavailable.", user_id=user_id)
        return [], [] # Return empty lists

    log_info("task_query_service", fn_name, f"Get Context Snapshot: User={user_id}")
    task_context, calendar_context = [], []

    try:
        # 1. Calculate date range
        today = datetime.now(timezone.utc).date() # Use UTC date for consistency
        start_date = today - timedelta(weeks=history_weeks)
        end_date = today + timedelta(weeks=future_weeks)
        start_str, end_str = start_date.strftime("%Y-%m-%d"), end_date.strftime("%Y-%m-%d")
        date_range_tuple = (start_str, end_str)

        # 2. Get WT tasks from DB (only active ones within the window)
        try:
            task_context = activity_db.list_tasks_for_user(
                user_id=user_id,
                status_filter=ACTIVE_STATUSES, # Fetch only active tasks
                date_range=date_range_tuple     # Apply date range
            )
            log_info("task_query_service", fn_name, f"Fetched {len(task_context)} active WT tasks from DB for snapshot.")
        except Exception as db_err:
             log_error("task_query_service", fn_name, f"Failed to fetch tasks from DB for snapshot: {db_err}", db_err, user_id=user_id)
             task_context = [] # Continue without tasks if DB fails

        # 3. Get GCal events directly from API
        calendar_api = _get_calendar_api_from_state(user_id)
        if calendar_api:
            try:
                # Fetch GCal events - list_events returns parsed dicts
                calendar_context = calendar_api.list_events(start_str, end_str)
                log_info("task_query_service", fn_name, f"Fetched {len(calendar_context)} GCal events for snapshot.")
            except Exception as cal_e:
                log_error("task_query_service", fn_name, f"Failed fetch calendar events for snapshot: {cal_e}", cal_e, user_id=user_id)
                calendar_context = [] # Continue without calendar events
        else:
            log_info("task_query_service", fn_name, f"Calendar API not active for {user_id}, skipping GCal fetch for snapshot.")

        log_info("task_query_service", fn_name, f"Snapshot created for {user_id}: {len(task_context)} tasks, {len(calendar_context)} external events.")

    except Exception as e:
        log_error("task_query_service", fn_name, f"Error creating context snapshot for {user_id}", e, user_id=user_id)
        return [], [] # Return empty lists on error

    return task_context, calendar_context

# --- END OF REFACTORED services/task_query_service.py ---

# --- END OF FILE services/task_query_service.py ---



================================================================================
üìÑ services/config_manager.py
================================================================================

# --- START OF FILE services/config_manager.py ---

# services/config_manager.py
"""Service layer for managing user configuration and preferences."""
from tools.logger import log_info, log_error, log_warning
# Import registry functions for persistence
from users.user_registry import get_user_preferences as get_prefs_from_registry
from users.user_registry import update_preferences as update_prefs_in_registry # This writes to file
# Import state manager for memory updates
try:
    # This function updates the live agent state dictionary
    from services.agent_state_manager import update_preferences_in_state
    AGENT_STATE_MANAGER_IMPORTED = True
except ImportError:
    log_error("config_manager", "import", "AgentStateManager not found. In-memory preference updates skipped.")
    AGENT_STATE_MANAGER_IMPORTED = False
    def update_preferences_in_state(*args, **kwargs): return False # Dummy

# Import calendar tool auth check
try:
    from tools.calendar_tool import authenticate as check_calendar_auth_status
    CALENDAR_TOOL_IMPORTED = True
except ImportError:
     log_error("config_manager", "import", "calendar_tool not found. Calendar auth initiation fails.")
     CALENDAR_TOOL_IMPORTED = False
     def check_calendar_auth_status(*args, **kwargs): return {"status": "fails", "message": "Calendar tool unavailable."}

from typing import Dict, Any, Optional

def get_preferences(user_id: str) -> Optional[Dict]:
    """Gets user preferences from the persistent registry."""
    try:
        prefs = get_prefs_from_registry(user_id)
        return prefs # Returns None if not found
    except Exception as e:
        log_error("config_manager", "get_preferences", f"Error reading preferences for {user_id}", e)
        return None

def update_preferences(user_id: str, updates: Dict) -> bool:
    """
    Updates preferences in persistent registry AND in-memory agent state.
    Returns True on success (based on registry update), False otherwise.
    """
    log_info("config_manager", "update_preferences", f"Updating preferences for {user_id}: {list(updates.keys())}")
    if not isinstance(updates, dict) or not updates:
        log_warning("config_manager", "update_preferences", "Invalid or empty updates provided.")
        return False

    # 1. Update Persistent Store (Registry File)
    registry_update_success = False
    try:
        update_prefs_in_registry(user_id, updates) # Writes to registry.json
        log_info("config_manager", "update_preferences", f"Registry file update requested for {user_id}")
        registry_update_success = True
    except Exception as e:
        log_error("config_manager", "update_preferences", f"Registry file update failed for {user_id}", e)
        return False # Don't proceed if persistence fails

    # 2. Update In-Memory State via AgentStateManager (If persistence succeeded)
    if registry_update_success and AGENT_STATE_MANAGER_IMPORTED:
        try:
            mem_update_success = update_preferences_in_state(user_id, updates) # Updates live _AGENT_STATE_STORE
            if not mem_update_success:
                log_warning("config_manager", "update_preferences", f"In-memory state update failed or user not found in state for {user_id}.")
                # Should we revert registry? For now, proceed but warn.
        except Exception as mem_e:
             log_error("config_manager", "update_preferences", f"Error updating in-memory state for {user_id}", mem_e)
             # Log error, but persistence succeeded, so arguably return True

    elif registry_update_success: # Log if manager wasn't imported
        log_warning("config_manager", "update_preferences", "AgentStateManager not imported. Skipping in-memory state update.")

    return registry_update_success # Return success based on registry write

def initiate_calendar_auth(user_id: str) -> Dict:
    """Initiates calendar auth flow via calendar_tool."""
    log_info("config_manager", "initiate_calendar_auth", f"Initiating calendar auth for {user_id}")
    if not CALENDAR_TOOL_IMPORTED:
         return {"status": "fails", "message": "Calendar auth component unavailable."}
    current_prefs = get_preferences(user_id) # Use service getter
    if not current_prefs:
        log_error("config_manager", "initiate_calendar_auth", f"Prefs not found for {user_id}")
        return {"status": "fails", "message": "User profile not found."}
    try:
        # Pass current prefs needed by authenticate function
        auth_result = check_calendar_auth_status(user_id, current_prefs)
        return auth_result
    except Exception as e:
        log_error("config_manager", "initiate_calendar_auth", f"Error during calendar auth init: {e}", e)
        return {"status": "fails", "message": "Error starting calendar auth."}

def set_user_status(user_id: str, status: str) -> bool:
    """Helper to specifically update user status in registry and memory."""
    log_info("config_manager", "set_user_status", f"Setting status='{status}' for {user_id}")
    if not status or not isinstance(status, str):
        log_warning("config_manager", "set_user_status", f"Invalid status value: {status}")
        return False
    # Calls the main update function which handles both registry and memory state
    return update_preferences(user_id, {"status": status})

# --- END OF FILE services/config_manager.py ---



================================================================================
üìÑ services/agent_state_manager.py
================================================================================

# --- START OF FILE services/agent_state_manager.py ---

# --- START OF FILE services/agent_state_manager.py ---
# services/agent_state_manager.py
"""
Manages the in-memory state of user agents.
Provides thread-safe functions to access and modify the global agent state dictionary.
Requires initialization via initialize_state_store.
"""
from tools.logger import log_info, log_error, log_warning
from typing import Dict, List, Any, Optional, Set # Added Set
import threading
import copy
from datetime import datetime

# --- Module Level State ---
_AGENT_STATE_STORE: Optional[Dict[str, Dict[str, Any]]] = None
_state_lock = threading.Lock()

def initialize_state_store(agent_dict_ref: Dict):
    """Initializes the state manager with a reference to the global agent state dictionary."""
    global _AGENT_STATE_STORE
    if _AGENT_STATE_STORE is not None:
        log_warning("AgentStateManager", "initialize_state_store", "State store already initialized.")
        return
    if isinstance(agent_dict_ref, dict):
        _AGENT_STATE_STORE = agent_dict_ref
        log_info("AgentStateManager", "initialize_state_store", f"State store initialized with reference (ID: {id(_AGENT_STATE_STORE)}).")
    else:
        log_error("AgentStateManager", "initialize_state_store", "Invalid dictionary reference passed.")
        _AGENT_STATE_STORE = {} # Initialize to empty dict if invalid ref passed

def _is_initialized() -> bool:
    """Checks if the state store has been initialized."""
    if _AGENT_STATE_STORE is None:
        log_error("AgentStateManager", "_is_initialized", "CRITICAL: State store accessed before initialization.")
        return False
    return True

# --- Modifier Functions ---

def register_agent_instance(user_id: str, agent_state: Dict):
    """Adds or replaces the entire state dictionary for a user."""
    if not _is_initialized(): return
    if not isinstance(agent_state, dict):
         log_error("AgentStateManager", "register_agent_instance", f"Invalid agent_state type for {user_id}")
         return
    log_info("AgentStateManager", "register_agent_instance", f"Registering/updating state for user {user_id}")
    with _state_lock:
        _AGENT_STATE_STORE[user_id] = agent_state

def update_preferences_in_state(user_id: str, prefs_updates: Dict) -> bool:
    """Updates the preferences dictionary within the user's in-memory state."""
    if not _is_initialized(): return False
    updated = False
    with _state_lock:
        state = _AGENT_STATE_STORE.get(user_id)
        if state and isinstance(state.get("preferences"), dict):
            state["preferences"].update(prefs_updates)
            log_info("AgentStateManager", "update_preferences_in_state", f"Updated in-memory preferences for {user_id}: {list(prefs_updates.keys())}")
            updated = True
        else:
            log_warning("AgentStateManager", "update_preferences_in_state", f"Cannot update prefs: State or prefs dict missing/invalid for {user_id}")
    return updated

def add_task_to_context(user_id: str, task_data: Dict):
    """Appends or updates a task dictionary in the user's in-memory context list."""
    if not _is_initialized(): return
    with _state_lock:
        state = _AGENT_STATE_STORE.get(user_id)
        if state:
            # Ensure 'active_tasks_context' exists and is a list
            if not isinstance(state.get("active_tasks_context"), list):
                 state["active_tasks_context"] = []

            context = state["active_tasks_context"]
            event_id = task_data.get("event_id") # Use event_id as primary key
            found_idx = -1
            if event_id:
                for i, item in enumerate(context):
                    # Check using event_id which should be unique
                    if item.get("event_id") == event_id:
                        found_idx = i
                        break

            if found_idx != -1:
                 log_info("AgentStateManager", "add_task_to_context", f"Updating task {event_id} in context for {user_id}.")
                 context[found_idx] = task_data # Replace existing entry
            else:
                 context.append(task_data) # Add as new entry
                 log_info("AgentStateManager", "add_task_to_context", f"Added task {event_id} to context for {user_id}. New size: {len(context)}")
        else:
            log_warning("AgentStateManager", "add_task_to_context", f"State missing for {user_id}.")

def update_task_in_context(user_id: str, event_id: str, updated_task_data: Dict):
    """Finds a task by event_id in the context list and replaces it."""
    if not _is_initialized(): return
    with _state_lock:
        state = _AGENT_STATE_STORE.get(user_id)
        if state and isinstance(state.get("active_tasks_context"), list):
            context = state["active_tasks_context"]
            found = False
            for i, item in enumerate(context):
                if item.get("event_id") == event_id:
                    context[i] = updated_task_data # Replace with new data
                    found = True
                    log_info("AgentStateManager", "update_task_in_context", f"Updated task {event_id} in context for {user_id}")
                    break
            if not found:
                 log_warning("AgentStateManager", "update_task_in_context", f"Task {event_id} not found for update. Adding if active.")
                 # Add only if it seems active (optional, depends on desired behavior)
                 if updated_task_data.get("status", "pending").lower() in ["pending", "in_progress", "in progress"]:
                      context.append(updated_task_data)
        else:
             log_warning("AgentStateManager", "update_task_in_context", f"State or active_tasks_context list invalid for {user_id}")

def remove_task_from_context(user_id: str, event_id: str):
    """Removes a task by event_id from the user's in-memory context list."""
    if not _is_initialized(): return
    with _state_lock:
        state = _AGENT_STATE_STORE.get(user_id)
        if state and isinstance(state.get("active_tasks_context"), list):
            original_len = len(state["active_tasks_context"])
            # Use list comprehension for potentially better performance on large lists
            state["active_tasks_context"][:] = [
                item for item in state["active_tasks_context"] if item.get("event_id") != event_id
            ]
            if len(state["active_tasks_context"]) < original_len:
                log_info("AgentStateManager", "remove_task_from_context", f"Removed task {event_id} from context for {user_id}")
            # else: No warning needed if not found, just means it wasn't there
        else:
             log_warning("AgentStateManager", "remove_task_from_context", f"State or active_tasks_context list invalid for {user_id}")

def update_full_context(user_id: str, new_context: List[Dict]):
    """Replaces the entire active_tasks_context list (e.g., after sync)."""
    if not _is_initialized(): return
    with _state_lock:
        state = _AGENT_STATE_STORE.get(user_id)
        if state:
            state["active_tasks_context"] = new_context if isinstance(new_context, list) else []
            log_info("AgentStateManager", "update_full_context", f"Replaced context for {user_id} with {len(state['active_tasks_context'])} items.")
        else:
            log_warning("AgentStateManager", "update_full_context", f"Cannot replace context: State missing for {user_id}")

def add_message_to_user_history(user_id: str, sender: str, message: str):
    """
    Appends a detailed message to the user's conversation history list. Keeps last 50.
    """
    if not _is_initialized(): return
    with _state_lock:
        state = _AGENT_STATE_STORE.get(user_id)
        if not state:
            log_warning("AgentStateManager", "add_message_to_user_history", f"Cannot add message: State missing for {user_id}")
            return

        if not isinstance(state.get("conversation_history"), list):
            log_warning("AgentStateManager", "add_message_to_user_history", f"conversation_history invalid for {user_id}, initializing.")
            state["conversation_history"] = []

        history_list = state["conversation_history"]
        timestamp = datetime.now().isoformat()
        entry = { "sender": sender, "timestamp": timestamp, "content": message }
        history_list.append(entry)
        state["conversation_history"] = history_list[-50:] # Limit size

def update_agent_state_key(user_id: str, key: str, value: Any) -> bool:
    """
    Updates or adds/removes a specific key-value pair in the user's agent state.
    """
    if not _is_initialized(): return False
    updated = False
    with _state_lock:
        state = _AGENT_STATE_STORE.get(user_id)
        if state:
            if value is None:
                if state.pop(key, None) is not None:
                    log_info("AgentStateManager", "update_agent_state_key", f"Removed key '{key}' from state for {user_id}")
            else:
                state[key] = value
                log_info("AgentStateManager", "update_agent_state_key", f"Updated key '{key}' in state for {user_id}")
            updated = True
        else:
            log_warning("AgentStateManager", "update_agent_state_key", f"Cannot update key '{key}': State missing for {user_id}")
    return updated

    # --- Notification Tracking Functions ---
def add_notified_event_id(user_id: str, event_id: str):
    """Adds an event ID to the set of notified events for today."""
    if not _is_initialized(): return
    with _state_lock:
        state = _AGENT_STATE_STORE.get(user_id)
        if state:
            # Ensure the key exists and is a set
            if not isinstance(state.get("notified_event_ids_today"), set):
                state["notified_event_ids_today"] = set()
            state["notified_event_ids_today"].add(event_id)
            # log_info("AgentStateManager", "add_notified_event_id", f"Added {event_id} to notified set for {user_id}") # Maybe too verbose
        else:
            log_warning("AgentStateManager", "add_notified_event_id", f"State missing for {user_id}, cannot add notified event.")

def get_notified_event_ids(user_id: str) -> Set[str]:
    """Gets a copy of the set of notified event IDs for today."""
    if not _is_initialized(): return set()
    with _state_lock:
        state = _AGENT_STATE_STORE.get(user_id)
        if state and isinstance(state.get("notified_event_ids_today"), set):
            return state["notified_event_ids_today"].copy() # Return a copy
    return set() # Return empty set if user or set not found

def clear_notified_event_ids(user_id: str):
    """Clears the set of notified event IDs for the user."""
    if not _is_initialized(): return
    with _state_lock:
        state = _AGENT_STATE_STORE.get(user_id)
        if state:
            # Reset to an empty set, even if key didn't exist before
            state["notified_event_ids_today"] = set()
            log_info("AgentStateManager", "clear_notified_event_ids", f"Cleared notified events set for {user_id}")
        else:
            log_warning("AgentStateManager", "clear_notified_event_ids", f"State missing for {user_id}, cannot clear notified events.")
    # --- End Notification Tracking Functions ---

def get_agent_state(user_id: str) -> Optional[Dict]:
    """
    Safely gets a SHALLOW copy of the full state dictionary for a user.
    """
    if not _is_initialized(): return None
    with _state_lock:
        state = _AGENT_STATE_STORE.get(user_id)
        return state.copy() if state else None

def get_context(user_id: str) -> Optional[List[Dict]]:
    """Gets a deep copy of the active_tasks_context list for a user."""
    if not _is_initialized(): return None
    with _state_lock:
        state = _AGENT_STATE_STORE.get(user_id)
        if state and isinstance(state.get("active_tasks_context"), list):
            return copy.deepcopy(state["active_tasks_context"])
    return [] # Return empty list if user or context list not found/invalid
# --- END OF FILE services/agent_state_manager.py ---

# --- END OF FILE services/agent_state_manager.py ---



================================================================================
üìÑ services/cheats.py
================================================================================

# --- START OF FILE services/cheats.py ---

# --- START OF FULL services/cheats.py ---

"""
Service layer for handling direct 'cheat code' commands, bypassing the LLM orchestrator.
Used primarily for testing, debugging, and direct actions. Interacts with DB via services.
"""
import json
from typing import List, Optional, Dict, Any
from datetime import datetime, timedelta

# Service Imports
from services import task_query_service # For /list
from services import task_manager # For /clear (cancel_item)
from services import agent_state_manager # For /memory
from services import sync_service # For /morning, /evening
from services import routine_service # For /morning, /evening helpers
from users.user_registry import get_user_preferences

# --- Database Import (Needed for /clear) ---
try:
    import tools.activity_db as activity_db
    DB_IMPORTED = True
except ImportError:
    # Use print here as logger might rely on this module indirectly during init issues
    print("[ERROR] [cheats:import] activity_db not found. /clear command may fail.")
    DB_IMPORTED = False
    class activity_db: # Dummy class
        @staticmethod
        def list_tasks_for_user(*args, **kwargs): return []
# --- End DB Import ---

# Utilities
from tools.logger import log_info, log_error, log_warning

# Define constants used by routines (mirroring routine_service)
ROUTINE_CONTEXT_HISTORY_DAYS = 1
ROUTINE_CONTEXT_FUTURE_DAYS = 14 # Consistent with routine_service

# --- Private Handler Functions ---

def _handle_help() -> str:
    """Provides help text for available cheat commands."""
    return """Available Cheat Commands:
/help - Show this help message
/list [status] - List items (status: active*, pending, completed, all)
/memory - Show summary of current agent in-memory state
/clear - !! DANGER !! Mark all user's items as cancelled
/morning - Generate and show today's morning summary
/evening - Generate and show today's evening review"""


def _handle_list(user_id: str, args: List[str]) -> str:
    """Handles the /list command by calling the task_query_service."""
    fn_name = "_handle_list"
    status_filter = args[0].lower() if args else 'active'
    allowed_statuses = ['active', 'pending', 'in_progress', 'completed', 'all']

    if status_filter not in allowed_statuses:
        return f"Invalid status '{status_filter}'. Use one of: {', '.join(allowed_statuses)}"

    try:
        prefs = get_user_preferences(user_id) or {}
        user_tz_str = prefs.get("TimeZone", "UTC")

        list_body, mapping = task_query_service.get_formatted_list(
            user_id=user_id,
            status_filter=status_filter,
        )
        if list_body:
            list_intro = f"Items with status '{status_filter}' (Times relative to {user_tz_str}):\n---\n"
            return list_intro + list_body
        else:
            return f"No items found with status '{status_filter}'."
    except Exception as e:
        log_error("cheats", fn_name, f"Error calling get_formatted_list: {e}", e, user_id=user_id)
        return "Error retrieving list."


def _handle_memory(user_id: str) -> str:
    """Handles the /memory command."""
    fn_name = "_handle_memory"
    try:
        agent_state = agent_state_manager.get_agent_state(user_id)
        if agent_state:
            state_summary = {
                "user_id": agent_state.get("user_id"),
                "preferences_keys": list(agent_state.get("preferences", {}).keys()),
                "history_count": len(agent_state.get("conversation_history", [])),
                "context_item_count": len(agent_state.get("active_tasks_context", [])),
                "calendar_object_present": agent_state.get("calendar") is not None,
                "notified_ids_today_count": len(agent_state.get("notified_event_ids_today", set()))
            }
            return f"Agent Memory Summary:\n```json\n{json.dumps(state_summary, indent=2)}\n```"
        else:
            return "Error: Agent state not found in memory."
    except Exception as e:
        log_error("cheats", fn_name, f"Error retrieving agent state: {e}", e, user_id=user_id)
        return "Error retrieving agent memory state."


def _handle_clear(user_id: str) -> str:
    """Handles the /clear command. Finds non-cancelled items in DB and attempts cancellation."""
    fn_name = "_handle_clear"
    log_warning("cheats", fn_name, f"!! Initiating /clear command for user {user_id} !!")
    cancelled_count = 0
    failed_count = 0
    errors = []

    if not DB_IMPORTED:
        log_error("cheats", fn_name, "Database module not available, cannot perform clear.", user_id=user_id)
        return "Error: Cannot access task database to perform clear."

    try:
        statuses_to_clear = ["pending", "in_progress", "completed"]
        items_to_clear_dicts = activity_db.list_tasks_for_user(user_id=user_id, status_filter=statuses_to_clear)

        if not items_to_clear_dicts:
            return "No items found in a clearable state (pending, in_progress, completed)."

        item_ids_to_clear = [item.get("event_id") for item in items_to_clear_dicts if item.get("event_id")]

        log_info("cheats", fn_name, f"Found {len(item_ids_to_clear)} items in DB to attempt cancellation for user {user_id}.")

        for item_id in item_ids_to_clear:
            try:
                success = task_manager.cancel_item(user_id, item_id)
                if success:
                    cancelled_count += 1
                else:
                    failed_count += 1
                    errors.append(f"Failed cancel: {item_id[:8]}...")
                    log_warning("cheats", fn_name, f"task_manager.cancel_item failed for {item_id}", user_id=user_id)
            except Exception as cancel_e:
                failed_count += 1
                errors.append(f"Error cancel: {item_id[:8]}... ({type(cancel_e).__name__})")
                log_error("cheats", fn_name, f"Exception during cancel_item for {item_id}", cancel_e, user_id=user_id)

        response = f"Clear operation finished.\nSuccessfully cancelled: {cancelled_count}\nFailed/Skipped: {failed_count}"
        if errors:
            response += "\nFailures:\n" + "\n".join(errors[:5]) # Show first 5 errors
            if len(errors) > 5: response += "\n..."

        return response

    except Exception as e:
        log_error("cheats", fn_name, f"Critical error during /clear setup or execution for {user_id}", e, user_id=user_id)
        return "A critical error occurred during the clear operation."


def _handle_morning(user_id: str) -> str:
    """Handles the /morning command by generating the summary."""
    fn_name = "_handle_morning"
    log_info("cheats", fn_name, f"Executing /morning cheat for {user_id}")
    try:
        prefs = get_user_preferences(user_id)
        if not prefs: return "Error: Could not retrieve user preferences."

        user_tz_str = prefs.get("TimeZone", "UTC")
        # --- *** CORRECTED FUNCTION CALL *** ---
        now_local = routine_service._get_local_time(user_tz_str)
        # --- *** END CORRECTION *** ---
        context_start_date = (now_local - timedelta(days=ROUTINE_CONTEXT_HISTORY_DAYS)).strftime("%Y-%m-%d")
        context_end_date = (now_local + timedelta(days=ROUTINE_CONTEXT_FUTURE_DAYS)).strftime("%Y-%m-%d")

        log_info("cheats", fn_name, f"Getting synced context for morning summary (User: {user_id})...")
        aggregated_context = sync_service.get_synced_context_snapshot(user_id, context_start_date, context_end_date)
        summary_msg = routine_service.generate_morning_summary(user_id, aggregated_context)

        return summary_msg if summary_msg else "Could not generate morning summary."

    except Exception as e:
        log_error("cheats", fn_name, f"Error generating morning summary via cheat code for {user_id}", e, user_id=user_id)
        return "An error occurred while generating the morning summary."


def _handle_evening(user_id: str) -> str:
    """Handles the /evening command by generating the review."""
    fn_name = "_handle_evening"
    log_info("cheats", fn_name, f"Executing /evening cheat for {user_id}")
    try:
        prefs = get_user_preferences(user_id)
        if not prefs: return "Error: Could not retrieve user preferences."

        user_tz_str = prefs.get("TimeZone", "UTC")
        # --- *** CORRECTED FUNCTION CALL *** ---
        now_local = routine_service._get_local_time(user_tz_str)
        # --- *** END CORRECTION *** ---
        context_start_date = (now_local - timedelta(days=ROUTINE_CONTEXT_HISTORY_DAYS)).strftime("%Y-%m-%d")
        context_end_date = (now_local + timedelta(days=ROUTINE_CONTEXT_FUTURE_DAYS)).strftime("%Y-%m-%d")

        log_info("cheats", fn_name, f"Getting synced context for evening review (User: {user_id})...")
        aggregated_context = sync_service.get_synced_context_snapshot(user_id, context_start_date, context_end_date)
        review_msg = routine_service.generate_evening_review(user_id, aggregated_context)

        return review_msg if review_msg else "Could not generate evening review."

    except Exception as e:
        log_error("cheats", fn_name, f"Error generating evening review via cheat code for {user_id}", e, user_id=user_id)
        return "An error occurred while generating the evening review."


# --- Main Dispatcher ---

def handle_cheat_command(user_id: str, command: str, args: List[str]) -> str:
    """
    Dispatches cheat commands to the appropriate handler.
    """
    command = command.lower()

    if command == "/help": return _handle_help()
    elif command == "/list": return _handle_list(user_id, args)
    elif command == "/memory": return _handle_memory(user_id)
    elif command == "/clear": return _handle_clear(user_id)
    elif command == "/morning": return _handle_morning(user_id)
    elif command == "/evening": return _handle_evening(user_id)
    else: return f"Unknown command: '{command}'. Try /help."

# --- END OF FULL services/cheats.py ---

# --- END OF FILE services/cheats.py ---



================================================================================
üìÑ services/llm_interface.py
================================================================================

# --- START OF FILE services/llm_interface.py ---

# llm_interface.py
import os
import openai
import instructor
import threading
from tools.logger import log_info, log_error

OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
_client = None
_client_lock = threading.Lock()

def get_instructor_client():
    """Initializes and returns a singleton, instructor-patched OpenAI client."""
    global _client
    if not OPENAI_API_KEY:
        log_error("llm_interface", "get_instructor_client", "OPENAI_API_KEY not found in environment.")
        return None

    with _client_lock:
        if _client is None:
            try:
                log_info("llm_interface", "get_instructor_client", "Initializing instructor-patched OpenAI client...")
                # Initialize the OpenAI client
                base_client = openai.OpenAI(api_key=OPENAI_API_KEY)
                # Patch it with Instructor
                _client = instructor.patch(base_client)
                log_info("llm_interface", "get_instructor_client", "Instructor-patched OpenAI client initialized.")
            except Exception as e:
                log_error("llm_interface", "get_instructor_client", f"Failed to initialize OpenAI client: {e}", e)
                _client = None # Ensure it remains None on failure
    return _client


# --- END OF FILE services/llm_interface.py ---



================================================================================
üìÑ services/scheduler_service.py
================================================================================

# --- START OF FILE services/scheduler_service.py ---

# --- START OF services/scheduler_service.py ---

from typing import Dict, List, Tuple # Added List, Tuple
from apscheduler.schedulers.background import BackgroundScheduler
from apscheduler.executors.pool import ThreadPoolExecutor
from apscheduler.events import EVENT_JOB_ERROR, EVENT_JOB_EXECUTED
import pytz
from tools.logger import log_info, log_error, log_warning
# --- ADD THIS IMPORT ---
from bridge.request_router import send_message
# ----------------------

# ... (Global scheduler instance, constants, _job_listener) ...
scheduler = None
DEFAULT_TIMEZONE = 'UTC'
NOTIFICATION_CHECK_INTERVAL_MINUTES = 60
ROUTINE_CHECK_INTERVAL_MINUTES = 60
DAILY_CLEANUP_HOUR_UTC = 0
DAILY_CLEANUP_MINUTE_UTC = 5

def _job_listener(event):
    # ... (function remains the same) ...
    fn_name = "_job_listener"
    job = scheduler.get_job(event.job_id) if scheduler else None
    job_name = job.name if job else event.job_id
    if event.exception:
        log_error("scheduler_service", fn_name, f"Job '{job_name}' crashed:", event.exception)
        log_error("scheduler_service", fn_name, f"Traceback: {event.traceback}")
    pass

# --- NEW FUNCTION TO WRAP ROUTINE CHECK AND SENDING ---
def _run_routine_check_and_send():
    """Wrapper function called by scheduler to run checks and send messages."""
    fn_name = "_run_routine_check_and_send"
    #log_info("scheduler_service", fn_name, "Scheduler executing routine check job...")
    try:
        # Import the check function here if not already imported globally
        from services.routine_service import check_routine_triggers
        messages_to_send = check_routine_triggers() # This now returns a list

        if messages_to_send:
            log_info("scheduler_service", fn_name, f"Routine check generated {len(messages_to_send)} messages to send.")
            for user_id, message_content in messages_to_send:
                try:
                    send_message(user_id, message_content)
                except Exception as send_err:
                    log_error("scheduler_service", fn_name, f"Error sending routine message to user {user_id}", send_err)
        else:
            log_info("scheduler_service", fn_name, "Routine check completed, no messages to send.")

    except Exception as job_err:
        # Log errors occurring within the job execution itself
        log_error("scheduler_service", fn_name, "Error during scheduled routine check execution", job_err)
# --- END OF NEW WRAPPER FUNCTION ---


def start_scheduler() -> bool:
    global scheduler
    fn_name = "start_scheduler"

    if scheduler and scheduler.running:
        log_warning("scheduler_service", fn_name, "Scheduler is already running.")
        return True

    try:
        log_info("scheduler_service", fn_name, "Initializing APScheduler...")
        executors = {'default': ThreadPoolExecutor(10)}
        job_defaults = {'coalesce': True, 'max_instances': 1}
        scheduler = BackgroundScheduler(
            executors=executors, job_defaults=job_defaults, timezone=pytz.timezone(DEFAULT_TIMEZONE)
        )

        # --- Import Job Functions ---
        check_event_notifications = None
        # We don't import check_routine_triggers here anymore, it's called in the wrapper
        daily_cleanup = None
        try:
            from services.notification_service import check_event_notifications
        except ImportError as e:
            log_error("scheduler_service", fn_name, f"Failed to import 'check_event_notifications': {e}. Notification job NOT scheduled.")
        try:
            # Keep import for daily_cleanup
            from services.routine_service import daily_cleanup
        except ImportError as e:
             log_error("scheduler_service", fn_name, f"Failed to import 'daily_cleanup': {e}. Cleanup job NOT scheduled.")
        # ----------------------------

        # --- Schedule Jobs ---
        jobs_scheduled_count = 0
        if check_event_notifications:
            scheduler.add_job( check_event_notifications, trigger='interval', minutes=NOTIFICATION_CHECK_INTERVAL_MINUTES, id='event_notification_check', name='Check Event Notifications')
            log_info("scheduler_service", fn_name, f"Scheduled 'check_event_notifications' job every {NOTIFICATION_CHECK_INTERVAL_MINUTES} minutes.")
            jobs_scheduled_count += 1
        else:
             log_warning("scheduler_service", fn_name, "'check_event_notifications' job not scheduled.")

        # --- MODIFIED: Schedule the WRAPPER function ---
        scheduler.add_job(
            _run_routine_check_and_send, # Call the wrapper
            trigger='interval',
            minutes=ROUTINE_CHECK_INTERVAL_MINUTES,
            id='routine_trigger_check',
            name='Check Routine Triggers & Send' # Updated name slightly
        )
        log_info("scheduler_service", fn_name, f"Scheduled 'Routine Trigger Check & Send' job every {ROUTINE_CHECK_INTERVAL_MINUTES} minutes.")
        jobs_scheduled_count += 1 # Assuming this job is always added
        # -------------------------------------------------

        if daily_cleanup:
            scheduler.add_job( daily_cleanup, trigger='cron', hour=DAILY_CLEANUP_HOUR_UTC, minute=DAILY_CLEANUP_MINUTE_UTC, timezone=DEFAULT_TIMEZONE, id='daily_cleanup_job', name='Daily Cleanup')
            log_info("scheduler_service", fn_name, f"Scheduled 'daily_cleanup' job daily at {DAILY_CLEANUP_HOUR_UTC:02d}:{DAILY_CLEANUP_MINUTE_UTC:02d} {DEFAULT_TIMEZONE}.")
            jobs_scheduled_count += 1
        else:
            log_warning("scheduler_service", fn_name, "'daily_cleanup' job not scheduled.")

        # ... (rest of the start_scheduler function including listener, start, return True/False) ...
        scheduler.add_listener(_job_listener, EVENT_JOB_ERROR | EVENT_JOB_EXECUTED)
        scheduler.start()
        log_info("scheduler_service", fn_name, "APScheduler started successfully.")
        return True

    except Exception as e:
        log_error("scheduler_service", fn_name, f"Failed to initialize or start APScheduler: {e}", e)
        scheduler = None
        return False

# --- (shutdown_scheduler function remains the same) ---
def shutdown_scheduler():
    # ... (shutdown logic) ...
    global scheduler
    fn_name = "shutdown_scheduler"
    if scheduler and scheduler.running:
        try:
            log_info("scheduler_service", fn_name, "Attempting to shut down scheduler...")
            scheduler.shutdown(wait=False)
            log_info("scheduler_service", fn_name, "Scheduler shut down complete.")
            scheduler = None
        except Exception as e:
            log_error("scheduler_service", fn_name, f"Error during scheduler shutdown: {e}", e)
    elif scheduler:
        log_info("scheduler_service", fn_name, "Scheduler found but was not running.")
        scheduler = None
    else:
        log_info("scheduler_service", fn_name, "No active scheduler instance to shut down.")

# --- END OF services/scheduler_service.py ---

# --- END OF FILE services/scheduler_service.py ---



================================================================================
üìÑ services/sync_service.py
================================================================================

# --- START OF FILE services/sync_service.py ---

# --- START OF REFACTORED services/sync_service.py ---
"""
Provides functionality to get a combined view of WhatsTasker-managed items (DB)
and external Google Calendar events (API) for a specific user and time period.
Does NOT modify the persistent DB for external events found only in GCal.
Updates DB records for WT items if GCal data has changed for that item.
"""
import traceback
from datetime import datetime, timedelta, timezone
from typing import List, Dict, Any

# Central logger
from tools.logger import log_info, log_error, log_warning

# Database access module
try:
    import tools.activity_db as activity_db
    DB_IMPORTED = True
except ImportError:
    log_error("sync_service", "import", "activity_db not found. Sync service disabled.", None)
    DB_IMPORTED = False
    # Dummy DB functions
    class activity_db:
        @staticmethod
        def list_tasks_for_user(*args, **kwargs): return []
        @staticmethod
        def add_or_update_task(*args, **kwargs): return False
# --- End DB Import ---

# User Manager (to get agent state for Calendar API)
try:
    from users.user_manager import get_agent
    USER_MANAGER_IMPORTED = True
except ImportError:
    log_error("sync_service", "import", "Failed to import user_manager.get_agent")
    USER_MANAGER_IMPORTED = False
    def get_agent(*args, **kwargs): return None

# Agent State Manager (to update in-memory context after DB update)
try:
    from services.agent_state_manager import update_task_in_context
    AGENT_STATE_IMPORTED = True
except ImportError:
    log_error("sync_service", "import", "Failed to import agent_state_manager functions.")
    AGENT_STATE_IMPORTED = False
    def update_task_in_context(*args, **kwargs): pass # Dummy

# Google Calendar API (for type checking and fetching events)
try:
    from tools.google_calendar_api import GoogleCalendarAPI
    GCAL_API_IMPORTED = True
except ImportError:
    log_warning("sync_service", "import", "GoogleCalendarAPI not found. Sync will only show DB tasks.")
    GoogleCalendarAPI = None
    GCAL_API_IMPORTED = False


def get_synced_context_snapshot(user_id: str, start_date_str: str, end_date_str: str) -> List[Dict]:
    """
    Fetches WT tasks (DB) and GCal events (API) for a period, merges them,
    identifies external events, and returns a combined list of dictionaries.
    Updates the DB record for a WT item if its corresponding GCal event changed.
    Does not persist external events found only in GCal into the tasks table.
    """
    fn_name = "get_synced_context_snapshot"
    #log_info("sync_service", fn_name, f"Generating synced context for user {user_id}, range: {start_date_str} to {end_date_str}")

    if not DB_IMPORTED:
        log_error("sync_service", fn_name, "Database module not available.", user_id=user_id)
        return []

    # 1. Get Calendar API instance
    calendar_api = None
    if USER_MANAGER_IMPORTED and GCAL_API_IMPORTED and GoogleCalendarAPI is not None:
        agent_state = get_agent(user_id)
        if agent_state:
            calendar_api_maybe = agent_state.get("calendar")
            if isinstance(calendar_api_maybe, GoogleCalendarAPI) and calendar_api_maybe.is_active():
                calendar_api = calendar_api_maybe

    # 2. Fetch GCal Events if API is available
    gcal_events_list = []
    if calendar_api:
        try:
            #log_info("sync_service", fn_name, f"Fetching GCal events for {user_id}...")
            # list_events returns parsed dicts including 'event_id', 'gcal_start_datetime' etc.
            gcal_events_list = calendar_api.list_events(start_date_str, end_date_str)
            log_info("sync_service", fn_name, f"Fetched {len(gcal_events_list)} GCal events for {user_id}.")
        except Exception as e:
            log_error("sync_service", fn_name, f"Error fetching GCal events for {user_id}", e, user_id=user_id)
            # Continue without GCal events
    else:
        log_info("sync_service", fn_name, f"GCal API not available or inactive for {user_id}, skipping GCal fetch.")

    # 3. Fetch WT Tasks from Database within the same date range
    wt_tasks_list = []
    try:
        #log_info("sync_service", fn_name, f"Fetching WT tasks from DB for {user_id}...")
        # Fetch tasks based on the 'date' column matching the range
        # We don't filter by status here; we want all potentially relevant WT items
        wt_tasks_list = activity_db.list_tasks_for_user(
            user_id=user_id,
            date_range=(start_date_str, end_date_str)
            # status_filter=None # Get all statuses within the date range
        )
        log_info("sync_service", fn_name, f"Fetched {len(wt_tasks_list)} WT tasks from DB for {user_id} in range.")
    except Exception as e:
        log_error("sync_service", fn_name, f"Error fetching WT tasks from DB for {user_id}", e, user_id=user_id)
        # If DB fails, should we proceed with only GCal events? Or return empty?
        # Let's return only GCal events if DB fails, but log error clearly.
        # Fall through, wt_tasks_list will be empty.

    # 4. Create Maps for Efficient Lookup
    gcal_events_map = {e['event_id']: e for e in gcal_events_list if e.get('event_id')}
    wt_tasks_map = {t['event_id']: t for t in wt_tasks_list if t.get('event_id')}

    # 5. Merge & Identify Types
    aggregated_context_list: List[Dict[str, Any]] = []
    processed_wt_ids = set() # Keep track of WT items found in GCal map

    # Iterate through GCal events first
    for event_id, gcal_data in gcal_events_map.items():
        if event_id in wt_tasks_map:
            # --- WT Item Found in GCal ---
            processed_wt_ids.add(event_id)
            task_data = wt_tasks_map[event_id] # The task data from our DB
            merged_data = task_data.copy() # Start with DB data
            needs_db_update = False

            # Check for differences that require updating our DB record
            gcal_start = gcal_data.get("gcal_start_datetime")
            gcal_end = gcal_data.get("gcal_end_datetime")
            gcal_title = gcal_data.get("title")
            gcal_desc = gcal_data.get("description")
            # Add GCal status if needed: gcal_status = gcal_data.get("status_gcal")

            # Update stored GCal times if they differ
            if gcal_start != merged_data.get("gcal_start_datetime"):
                merged_data["gcal_start_datetime"] = gcal_start
                needs_db_update = True
            if gcal_end != merged_data.get("gcal_end_datetime"):
                merged_data["gcal_end_datetime"] = gcal_end
                needs_db_update = True

            # Option 1: Always update title/desc from GCal if GCal link exists?
            # Option 2: Only update if DB fields are empty/default? (Safer)
            # Let's go with Option 2 for now to avoid overwriting user edits in WT potentially.
            if gcal_title and not merged_data.get("title", "").strip():
                 merged_data["title"] = gcal_title
                 needs_db_update = True
            if gcal_desc and not merged_data.get("description", "").strip():
                 merged_data["description"] = gcal_desc
                 needs_db_update = True
            # Potentially sync status? If GCal event is 'cancelled', should WT task be? Complex rule. Skip for now.

            # If the merged data differs from original DB data, update DB
            if needs_db_update:
                log_info("sync_service", fn_name, f"GCal data changed for WT item {event_id}. Updating DB.")
                try:
                    # add_or_update_task expects list for session IDs
                    if isinstance(merged_data.get("session_event_ids"), str): # Ensure it's list before saving
                        try: merged_data["session_event_ids"] = json.loads(merged_data["session_event_ids"])
                        except: merged_data["session_event_ids"] = []

                    update_success = activity_db.add_or_update_task(merged_data)
                    if update_success and AGENT_STATE_IMPORTED:
                        # Update in-memory context as well
                        updated_data_from_db = activity_db.get_task(event_id) # Re-fetch to get latest state
                        if updated_data_from_db: update_task_in_context(user_id, event_id, updated_data_from_db)
                    elif not update_success:
                         log_error("sync_service", fn_name, f"Failed DB update for WT item {event_id} after GCal merge.", user_id=user_id)

                except Exception as save_err:
                     log_error("sync_service", fn_name, f"Unexpected error saving updated metadata for WT item {event_id} after GCal merge.", save_err, user_id=user_id)

            # Add the (potentially updated) merged data to the context list
            aggregated_context_list.append(merged_data)

        else:
            # --- External GCal Event (Not in our DB) ---
            external_event_data = gcal_data.copy() # Start with GCal data
            external_event_data["type"] = "external_event" # Mark its type
            external_event_data["user_id"] = user_id # Ensure user_id is present
            # Ensure required fields for formatting have some value?
            external_event_data.setdefault("status", None) # External events don't have WT status
            aggregated_context_list.append(external_event_data)

    # 6. Add WT Tasks Not Found in GCal Fetch Window
    for event_id, task_data in wt_tasks_map.items():
        if event_id not in processed_wt_ids:
            # This is a WT item (task/reminder) that wasn't in the GCal list for this window.
            # Could be a local-only task, or GCal event outside window, or deleted from GCal.
            # We still want it in the context if it's relevant (e.g., active status).
            log_info("sync_service", fn_name, f"Including WT item {event_id} (status: {task_data.get('status')}) which was not found in GCal fetch window.")
            # Make sure session IDs are list (should be from DB layer)
            if isinstance(task_data.get("session_event_ids"), str):
                 try: task_data["session_event_ids"] = json.loads(task_data["session_event_ids"])
                 except: task_data["session_event_ids"] = []
            aggregated_context_list.append(task_data) # Add the DB data as is

    #log_info("sync_service", fn_name, f"Generated aggregated context with {len(aggregated_context_list)} items for {user_id}.")
    # Sort the final aggregated list before returning? Good for routines.
    sorted_aggregated_context = _sort_tasks(aggregated_context_list) # Use the existing sort helper
    return sorted_aggregated_context

# --- Placeholder for future full sync ---
def perform_full_sync(user_id: str):
    """(NOT IMPLEMENTED) Placeholder for a more complex two-way sync."""
    log_warning("sync_service", "perform_full_sync", f"Full two-way sync not implemented. User: {user_id}")
    # This would involve:
    # 1. Fetching *all* relevant GCal events (wider date range? or use sync tokens?)
    # 2. Fetching *all* non-cancelled WT tasks from DB.
    # 3. Complex diffing logic to identify:
    #    - New GCal events -> Create corresponding 'external_event' metadata in DB? (Optional)
    #    - New WT tasks -> Create in GCal? (Maybe only if scheduled?)
    #    - Updated GCal events -> Update corresponding WT task metadata in DB.
    #    - Updated WT tasks -> Update corresponding GCal event? (Be careful!)
    #    - Deleted GCal events -> Update status or delete WT task metadata?
    #    - Deleted WT tasks (marked cancelled) -> Delete GCal event?
    # 4. Handling conflicts gracefully.
    # 5. Updating last_sync timestamp in user preferences.
    pass

# Helper from task_query_service might be needed here if not importing that module
def _sort_tasks(task_list: List[Dict]) -> List[Dict]:
    """Sorts task list robustly by date/time."""
    fn_name = "_sort_tasks_sync" # Different name to avoid potential conflicts if imported elsewhere
    def sort_key(item):
        gcal_start = item.get("gcal_start_datetime")
        if gcal_start and isinstance(gcal_start, str):
             try:
                 if 'T' in gcal_start: dt_aware = datetime.fromisoformat(gcal_start.replace('Z', '+00:00')); return dt_aware.replace(tzinfo=None)
                 elif len(gcal_start) == 10: dt_date = datetime.strptime(gcal_start, '%Y-%m-%d').date(); return datetime.combine(dt_date, datetime.min.time())
             except ValueError: pass
        meta_date_str = item.get("date"); meta_time_str = item.get("time")
        sort_dt = datetime.max
        if meta_date_str:
            try:
                if meta_time_str:
                    time_part = meta_time_str + ':00' if len(meta_time_str.split(':')) == 2 else meta_time_str
                    sort_dt = datetime.strptime(f"{meta_date_str} {time_part}", "%Y-%m-%d %H:%M:%S")
                else: sort_dt = datetime.strptime(meta_date_str, "%Y-%m-%d")
            except (ValueError, TypeError): pass
        return sort_dt
    try:
        return sorted(task_list, key=lambda item: (sort_key(item), item.get("created_at", ""), item.get("title", "").lower()))
    except Exception as e:
        log_error("sync_service", fn_name, f"Error during task sorting: {e}", e)
        return task_list


# --- END OF REFACTORED services/sync_service.py ---

# --- END OF FILE services/sync_service.py ---



================================================================================
üìÑ services/notification_service.py
================================================================================

# --- START OF FILE services/notification_service.py ---

# --- START OF FULL services/notification_service.py ---
"""
Handles checking for upcoming events and sending notifications.
Uses an in-memory set within agent_state to track sent notifications for the day.
"""
import traceback
from datetime import datetime, timedelta, timezone
import pytz

from tools.logger import log_info, log_error, log_warning
from users.user_registry import get_registry, get_user_preferences
from services.sync_service import get_synced_context_snapshot
from services.agent_state_manager import get_notified_event_ids, add_notified_event_id
from bridge.request_router import send_message # Direct import for sending
from services.task_manager import _parse_duration_to_minutes # For parsing lead time

try:
    from tools.activity_db import update_task_fields
    DB_IMPORTED = True
except ImportError:
    DB_IMPORTED = False
    def update_task_fields(*args, **kwargs): return False # Dummy
    log_error("notification_service", "import", "Failed to import activity_db.update_task_fields")
# --- End Imports ---

NOTIFICATION_CHECK_INTERVAL_MINUTES = 60
DEFAULT_NOTIFICATION_LEAD_TIME_MINUTES = 15

def generate_event_notification_message(event_data, user_timezone_str="UTC"):
    """
    Formats a simple notification message for an event, converting the start time
    to the user's local timezone.
    """
    fn_name = "generate_event_notification_message"
    title = event_data.get('title', '(No Title)')
    start_time_str = event_data.get('gcal_start_datetime')
    if not start_time_str:
        log_warning("notification_service", fn_name, f"Cannot generate notification for event '{title}' - missing gcal_start_datetime.")
        return None
    try:
        user_tz = pytz.utc
        try:
            if user_timezone_str: user_tz = pytz.timezone(user_timezone_str)
        except pytz.UnknownTimeZoneError:
            log_warning("notification_service", fn_name, f"Unknown timezone '{user_timezone_str}'. Using UTC for notification message for event '{title}'.")
            user_timezone_str = "UTC"
        dt_aware = datetime.fromisoformat(start_time_str.replace('Z', '+00:00'))
        dt_local = dt_aware.astimezone(user_tz)
        time_str = dt_local.strftime('%H:%M %Z')
        return f"üîî Reminder: '{title}' is starting soon at {time_str}."
    except (ValueError, TypeError) as parse_err:
        log_error("notification_service", fn_name, f"Error parsing/converting start time '{start_time_str}' for event '{title}'. Error: {parse_err}", parse_err)
        return None
    except Exception as e:
        log_error("notification_service", fn_name, f"General error formatting notification for event '{title}': {e}", e)
        return None

def check_event_notifications():
    """
    Scheduled job function. Checks all users for upcoming events needing notification.
    Updates 'internal_reminder_sent' in the database for notified WT items.
    """
    fn_name = "check_event_notifications"
    #log_info("notification_service", fn_name, "Running scheduled check for event notifications...")
    now_utc = datetime.now(timezone.utc)

    # --- Outermost Try ---
    try:
        registry = get_registry()
        if not registry:
            log_warning("notification_service", fn_name, "User registry is empty. Skipping check.")
            return

        user_ids = list(registry.keys())
        #log_info("notification_service", fn_name, f"Checking notifications for {len(user_ids)} users.")

        for user_id in user_ids:
            prefs = None
            # --- Inner Try (User Level) ---
            try:
                prefs = get_user_preferences(user_id)
                if not prefs or prefs.get("status") != "active" or not prefs.get("Calendar_Enabled"):
                    continue

                lead_time_str = prefs.get("Notification_Lead_Time", "15m")
                lead_time_minutes = _parse_duration_to_minutes(lead_time_str)
                if lead_time_minutes is None:
                    log_warning("notification_service", fn_name, f"Invalid Notification_Lead_Time '{lead_time_str}' for user {user_id}. Using default.", user_id=user_id)
                    lead_time_minutes = DEFAULT_NOTIFICATION_LEAD_TIME_MINUTES

                notification_window_end_utc = now_utc + timedelta(minutes=lead_time_minutes)
                today_date_str = now_utc.strftime("%Y-%m-%d")
                aggregated_context = get_synced_context_snapshot(user_id, today_date_str, today_date_str)
                if not aggregated_context: continue

                notified_today_set = get_notified_event_ids(user_id)

                for item in aggregated_context:
                    event_id = item.get("event_id")
                    start_time_iso = item.get("gcal_start_datetime")

                    if not event_id or not start_time_iso or 'T' not in start_time_iso: continue
                    if event_id in notified_today_set: continue

                    # --- Innermost Try (Item Level) ---
                    try:
                        start_dt_aware = datetime.fromisoformat(start_time_iso.replace('Z', '+00:00'))

                        # Corrected check logic
                        if start_dt_aware > now_utc and start_dt_aware <= notification_window_end_utc:
                            log_info("notification_service", fn_name, f"Triggering notification for user {user_id}, event: {event_id} ('{item.get('title')}')")

                            notification_message = generate_event_notification_message(item, prefs.get("TimeZone", "UTC"))

                            if notification_message:
                                send_message(user_id, notification_message)
                                add_notified_event_id(user_id, event_id)
                                log_info("notification_service", fn_name, f"Sent notification and marked as notified (memory) for event {event_id}, user {user_id}")

                                if item.get("type") in ["task", "reminder"] and DB_IMPORTED:
                                    sent_time_utc_str = datetime.now(timezone.utc).isoformat(timespec='seconds')+'Z'
                                    update_payload = {"internal_reminder_sent": sent_time_utc_str}
                                    db_update_success = update_task_fields(event_id, update_payload)
                                    if db_update_success:
                                        log_info("notification_service", fn_name, f"Updated internal_reminder_sent in DB for WT item {event_id}")
                                    else:
                                        log_warning("notification_service", fn_name, f"Failed to update internal_reminder_sent in DB for WT item {event_id}", user_id=user_id)
                            else:
                                log_warning("notification_service", fn_name, f"Failed to generate notification message for event {event_id}.", user_id=user_id)

                    except ValueError:
                        log_warning("notification_service", fn_name, f"Could not parse start time '{start_time_iso}' for event {event_id}. Skipping.", user_id=user_id)
                    except Exception as item_err:
                        log_error("notification_service", fn_name, f"Error processing item {event_id} for user {user_id}", item_err, user_id=user_id)
                    # --- End Innermost Try ---

            except Exception as user_err: # <-- Correctly indented for Inner Try
                user_context = user_id if prefs else f"Unknown User (Error before prefs load)"
                log_error("notification_service", fn_name, f"Error processing notifications for user {user_id}", user_err, user_id=user_context)
            # --- End Inner Try ---

    # --- <<< CORRECTED INDENTATION FOR FINAL EXCEPT >>> ---
    except Exception as main_err: # <-- Correctly indented for Outermost Try
        log_error("notification_service", fn_name, f"General error during notification check run", main_err)
    # --- <<< END CORRECTION >>> ---

    # --- Correctly indented final log ---
    log_info("notification_service", fn_name, "Finished scheduled check for event notifications.")

# --- END OF FILE services/notification_service.py ---

# --- END OF FILE services/notification_service.py ---



================================================================================
üìÑ services/routine_service.py
================================================================================

# --- START OF FILE services/routine_service.py ---

# --- START OF FULL services/routine_service.py ---
"""
Handles scheduled generation of Morning and Evening summaries/reviews.
Includes timezone handling and daily cleanup tasks.
"""

import traceback
from datetime import datetime, timedelta, timezone
from typing import Dict, List, Any, Tuple
import pytz # For timezone handling
import re # Added for title matching

from tools.logger import log_info, log_error, log_warning
from users.user_registry import get_registry, get_user_preferences
from services import sync_service # Import the whole module
from services.config_manager import update_preferences # To update last trigger date
from services.agent_state_manager import clear_notified_event_ids, get_agent_state # For daily cleanup

# Define time window for fetching context for routines
ROUTINE_CONTEXT_HISTORY_DAYS = 1
ROUTINE_CONTEXT_FUTURE_DAYS = 14 # Check tasks/events up to 2 weeks ahead for context
_ROUTINE_AGENT_STATE_IMPORTED = False
_clear_notified_func = None
_get_agent_state_func = None
try:
    from services.agent_state_manager import clear_notified_event_ids, get_agent_state
    _clear_notified_func = clear_notified_event_ids
    _get_agent_state_func = get_agent_state
    _ROUTINE_AGENT_STATE_IMPORTED = True
except ImportError:
    log_error("routine_service", "import", "AgentStateManager not found. Daily cleanup of notified_event_ids will be skipped.")
    # Define dummy functions to prevent crashes if called
    def _dummy_clear_notified(*args, **kwargs): pass
    def _dummy_get_agent_state(*args, **kwargs): return None
    _clear_notified_func = _dummy_clear_notified
    _get_agent_state_func = _dummy_get_agent_state
# --- Time Formatting Helpers ---

def _get_local_time(user_timezone_str: str) -> datetime:
    """Gets the current time in the user's specified timezone."""
    # (No changes needed)
    fn_name = "get_local_time"
    if not user_timezone_str: user_timezone_str = 'UTC'
    try:
        user_tz = pytz.timezone(user_timezone_str)
        return datetime.now(user_tz)
    except pytz.UnknownTimeZoneError:
        log_warning("routine_service", fn_name, f"Unknown timezone '{user_timezone_str}'. Using UTC.")
        return datetime.now(pytz.utc)
    except Exception as e:
        log_error("routine_service", fn_name, f"Error getting local time for tz '{user_timezone_str}'", e)
        return datetime.now(pytz.utc)

def _format_time_local(iso_utc_str: str | None, user_tz: pytz.BaseTzInfo, default_time="??:??") -> str:
    """Formats an ISO UTC datetime string to local HH:MM."""
    # (No changes needed)
    if not iso_utc_str or 'T' not in iso_utc_str:
        return default_time
    try:
        dt_aware = datetime.fromisoformat(iso_utc_str.replace('Z', '+00:00'))
        dt_local = dt_aware.astimezone(user_tz)
        return dt_local.strftime('%H:%M')
    except (ValueError, TypeError):
        log_warning("routine_service", "_format_time_local", f"Could not parse/format time: {iso_utc_str}")
        return default_time

def _format_time_range_local(start_iso: str | None, end_iso: str | None, user_tz: pytz.BaseTzInfo, default_range="??:?? - ??:??") -> str:
    """Formats ISO UTC start/end strings to local HH:MM - HH:MM."""
    # (No changes needed)
    if not start_iso or not end_iso or 'T' not in start_iso or 'T' not in end_iso:
        return default_range
    try:
        start_aware = datetime.fromisoformat(start_iso.replace('Z', '+00:00'))
        end_aware = datetime.fromisoformat(end_iso.replace('Z', '+00:00'))
        start_local = start_aware.astimezone(user_tz)
        end_local = end_aware.astimezone(user_tz)
        return f"{start_local.strftime('%H:%M')} - {end_local.strftime('%H:%M')}"
    except (ValueError, TypeError):
         log_warning("routine_service", "_format_time_range_local", f"Could not parse/format time range: {start_iso} / {end_iso}")
         return default_range

def _get_item_local_date_str(item: Dict, user_tz: pytz.BaseTzInfo) -> str | None:
     """Gets the effective local date string (YYYY-MM-DD) for an item."""
     # (No changes needed)
     start_dt_str = item.get("gcal_start_datetime")
     item_date_local_str = None
     if start_dt_str:
         try:
             if 'T' in start_dt_str: # Datetime
                  dt_aware = datetime.fromisoformat(start_dt_str.replace('Z', '+00:00'))
                  item_date_local_str = dt_aware.astimezone(user_tz).strftime("%Y-%m-%d")
             elif len(start_dt_str) == 10: # Date (All day)
                  item_date_local_str = start_dt_str # Already YYYY-MM-DD
         except (ValueError, TypeError): pass
     elif item.get("date"): # Fallback to WT date field
         item_date_local_str = item.get("date")
     return item_date_local_str

# --- Sorting Helper (Simplified from task_query_service) ---
def _sort_routine_items(items: List[Dict], user_tz: pytz.BaseTzInfo) -> List[Dict]:
    """Sorts items for routine display (by time primarily)."""
    # (No changes needed)
    def sort_key(item):
        start_dt_str = item.get("gcal_start_datetime")
        if start_dt_str and 'T' in start_dt_str:
             try:
                 dt_aware = datetime.fromisoformat(start_dt_str.replace('Z', '+00:00'))
                 return dt_aware.replace(tzinfo=None)
             except ValueError: pass
        meta_date = item.get("date")
        meta_time = item.get("time")
        if meta_date:
             try:
                 day_dt = datetime.strptime(meta_date, "%Y-%m-%d")
                 if meta_time:
                      try:
                           hour, minute = map(int, meta_time.split(':'))
                           return day_dt.replace(hour=hour, minute=minute)
                      except (ValueError, TypeError): pass
                 return day_dt
             except ValueError: pass
        return datetime.max

    return sorted(items, key=sort_key)


# --- Routine Generation Functions (UPDATED FORMATTING) ---

def generate_morning_summary(user_id: str, context: List[Dict]) -> str:
    """Generates the morning summary message including GCal events and WT items."""
    fn_name = "generate_morning_summary"
    prefs = get_user_preferences(user_id)
    user_name = prefs.get("name", "") if prefs else ""
    user_name_greet = f" {user_name}" if user_name else ""
    user_tz_str = prefs.get("TimeZone", "UTC") if prefs else "UTC"
    user_tz = pytz.timezone(user_tz_str)
    now_local = _get_local_time(user_tz_str)
    today_local_str = now_local.strftime("%Y-%m-%d")

    log_info("routine_service", fn_name, f"Generating morning summary for user {user_id}")

    reminders_today = []
    sessions_today = []
    todos_today = []

    for item in context:
        item_local_date = _get_item_local_date_str(item, user_tz)
        item_type = item.get("type")
        item_status = item.get("status")

        if item_local_date == today_local_str:
            if item_type in ["task", "reminder", "todo"] and item_status in ["completed", "cancelled"]:
                continue

            if item_type == "reminder" or item_type == "external_event":
                reminders_today.append(item)
            elif item_type == "task":
                 gcal_start = item.get("gcal_start_datetime")
                 # Only include Tasks if they represent a session with time today
                 if gcal_start and 'T' in gcal_start:
                     sessions_today.append(item)
            elif item_type == "todo":
                todos_today.append(item)

    # Sort items within categories
    reminders_today = _sort_routine_items(reminders_today, user_tz)
    sessions_today = _sort_routine_items(sessions_today, user_tz)
    todos_today.sort(key=lambda x: (x.get("date") is None, x.get("title", "").lower()))

    # Build Message
    message_lines = [f"Good morning{user_name_greet}! ‚òÄÔ∏è Here's your plan for today:"]
    items_found = False

    if reminders_today:
        items_found = True
        message_lines.append("\n**Reminders:**")
        for item in reminders_today:
            title = item.get("title", "(Untitled Reminder)")
            start_time_iso = item.get('gcal_start_datetime')
            is_all_day = item.get('is_all_day', False) or (start_time_iso and 'T' not in start_time_iso) # Check if all-day

            # --- **FORMATTING CHANGE HERE** ---
            if is_all_day:
                message_lines.append(f"- {title} (All Day)")
            else:
                time_str = _format_time_local(start_time_iso, user_tz)
                message_lines.append(f"- {title} @ {time_str}")
            # --- **END FORMATTING CHANGE** ---

    if sessions_today:
        items_found = True
        message_lines.append("\n**Work Sessions:**")
        for item in sessions_today:
             time_range_str = _format_time_range_local(item.get('gcal_start_datetime'), item.get('gcal_end_datetime'), user_tz)
             title = item.get("title", "(Untitled Task)")
             match = re.match(r"Work: (.*?) \[\d+/\d+\]", title)
             display_title = match.group(1).strip() if match else title
             message_lines.append(f"- {display_title}: {time_range_str}")

    if todos_today:
        items_found = True
        message_lines.append("\n**Active ToDos:**")
        for item in todos_today:
             title = item.get("title", "(Untitled ToDo)")
             due_date_str = item.get("date")
             due_suffix = " (Due Today)" if due_date_str == today_local_str else ""
             message_lines.append(f"- {title}{due_suffix}")

    if not items_found:
        return f"Good morning{user_name_greet}! ‚òÄÔ∏è Looks like a clear schedule today. Anything you'd like to add?"

    message_lines.append("\nüëâ Consider tackling one of your ToDos today if you have spare moments!")
    message_lines.append("\nHave a productive day!")
    return "\n".join(message_lines)


def generate_evening_review(user_id: str, context: List[Dict]) -> str:
    """Generates the evening review message, listing scheduled items and active ToDos for the day."""
    fn_name = "generate_evening_review"
    prefs = get_user_preferences(user_id)
    user_name = prefs.get("name", "") if prefs else ""
    user_name_greet = f" {user_name}" if user_name else ""
    user_tz_str = prefs.get("TimeZone", "UTC") if prefs else "UTC"
    user_tz = pytz.timezone(user_tz_str)
    now_local = _get_local_time(user_tz_str)
    today_local_str = now_local.strftime("%Y-%m-%d")

    log_info("routine_service", fn_name, f"Generating evening review for user {user_id}")

    items_for_review = []

    for item in context:
        item_local_date = _get_item_local_date_str(item, user_tz)
        item_type = item.get("type")
        item_status = item.get("status")

        if item_local_date == today_local_str:
            if item_type == "reminder" or item_type == "external_event":
                 if item.get("status_gcal") != "cancelled":
                     items_for_review.append(item)
            elif item_type == "task":
                 gcal_start = item.get("gcal_start_datetime")
                 # Include if it has a specific time scheduled today
                 if gcal_start and 'T' in gcal_start:
                    items_for_review.append(item)
            elif item_type == "todo" and item_status in ["pending", "in_progress"]:
                 items_for_review.append(item)

    if not items_for_review:
        return f"Good evening{user_name_greet}! üëã No active tasks, reminders, or ToDos were scheduled for today. Time to relax or plan for tomorrow?"

    # Sort all items together for the numbered list
    sorted_items = _sort_routine_items(items_for_review, user_tz)

    message_lines = [f"Good evening{user_name_greet}! üëã Let's review today's items:"]
    scheduled_item_indices = []
    todo_item_indices = []

    for i, item in enumerate(sorted_items):
        item_num = i + 1
        item_type = item.get("type")
        title = item.get("title", "(Untitled Item)")
        line = f"{item_num}. "
        start_time_iso = item.get('gcal_start_datetime')
        is_all_day = item.get('is_all_day', False) or (start_time_iso and 'T' not in start_time_iso)

        if item_type == "reminder" or item_type == "external_event":
            # --- **FORMATTING CHANGE HERE** ---
            if is_all_day:
                line += f"(Reminder) {title} (All Day)"
            else:
                time_str = _format_time_local(start_time_iso, user_tz)
                line += f"(Reminder) {title} @ {time_str}"
            # --- **END FORMATTING CHANGE** ---
            scheduled_item_indices.append(str(item_num))
        elif item_type == "task": # Representing a session
            time_range_str = _format_time_range_local(start_time_iso, item.get('gcal_end_datetime'), user_tz)
            match = re.match(r"Work: (.*?) \[\d+/\d+\]", title)
            display_title = match.group(1).strip() if match else title
            line += f"(Session) {display_title}: {time_range_str}"
            scheduled_item_indices.append(str(item_num))
        elif item_type == "todo":
            status_str = str(item.get('status', 'Pending')).capitalize()
            due_date_str = item.get("date")
            due_suffix = " (Due Today)" if due_date_str == today_local_str else ""
            line += f"(ToDo) {title} [{status_str}]{due_suffix}"
            todo_item_indices.append(str(item_num))
        else:
            line += f"(Unknown) {title}"

        message_lines.append(line)

    # Build dynamic footer
    footer_parts = []
    if scheduled_item_indices:
        # Construct range string e.g., "1-3", "1", "1, 3"
        range_str = ""
        if len(scheduled_item_indices) == 1:
            range_str = scheduled_item_indices[0]
        elif len(scheduled_item_indices) > 1:
             # Check if consecutive
             nums = [int(x) for x in scheduled_item_indices]
             is_consecutive = all(nums[j] == nums[0] + j for j in range(len(nums)))
             if is_consecutive:
                 range_str = f"{nums[0]}-{nums[-1]}"
             else:
                 range_str = ", ".join(scheduled_item_indices)

        footer_parts.append(f"Are the scheduled items ({range_str}) complete, or do any need rescheduling?")

    if todo_item_indices:
        range_str = ""
        if len(todo_item_indices) == 1:
             range_str = todo_item_indices[0]
        elif len(todo_item_indices) > 1:
             nums = [int(x) for x in todo_item_indices]
             is_consecutive = all(nums[j] == nums[0] + j for j in range(len(nums)))
             if is_consecutive:
                  range_str = f"{nums[0]}-{nums[-1]}"
             else:
                  range_str = ", ".join(todo_item_indices)

        footer_parts.append(f"You can also update ToDo status (e.g., 'complete {range_str}', 'cancel {range_str}').")

    if footer_parts:
         message_lines.append("\n" + "\n".join(footer_parts))

    return "\n".join(message_lines)


def check_routine_triggers() -> List[Tuple[str, str]]:
    """
    Scheduled job function. Checks all users if morning/evening routines should run.
    Calls sync service before generating summaries.
    Returns a list of (user_id, message_content) tuples for messages to be sent.
    """
    # (No changes needed in this function's logic)
    fn_name = "check_routine_triggers"
    #log_info("routine_service", fn_name, "Running scheduled check for routine triggers...")
    messages_to_send: List[Tuple[str, str]] = []

    try:
        registry = get_registry()
        if not registry:
            log_warning("routine_service", fn_name, "User registry is empty. Skipping check.")
            return []

        user_ids = list(registry.keys())
        log_info("routine_service", fn_name, f"Checking routines for {len(user_ids)} users.")

        for user_id in user_ids:
            prefs = None
            try:
                prefs = get_user_preferences(user_id)
                if not prefs or prefs.get("status") != "active":
                    continue

                user_tz_str = prefs.get("TimeZone")
                if not user_tz_str:
                    continue

                now_local = _get_local_time(user_tz_str)
                today_local_str = now_local.strftime("%Y-%m-%d")
                current_local_hm = now_local.strftime("%H:%M")

                aggregated_context = None
                context_fetched = False

                # --- Check Morning Routine ---
                morning_time_str = prefs.get("Morning_Summary_Time")
                if prefs.get("Enable_Morning") and morning_time_str:
                    last_triggered_morning = prefs.get("last_morning_trigger_date")
                    if current_local_hm >= morning_time_str and last_triggered_morning != today_local_str:
                        log_info("routine_service", fn_name, f"Triggering Morning Summary for user {user_id} at {current_local_hm} {user_tz_str}")
                        if not context_fetched:
                            context_start_date = (now_local - timedelta(days=ROUTINE_CONTEXT_HISTORY_DAYS)).strftime("%Y-%m-%d")
                            context_end_date = (now_local + timedelta(days=ROUTINE_CONTEXT_FUTURE_DAYS)).strftime("%Y-%m-%d")
                            log_info("routine_service", fn_name, f"Getting synced context for routines (User: {user_id})...")
                            aggregated_context = sync_service.get_synced_context_snapshot(user_id, context_start_date, context_end_date)
                            context_fetched = True

                        summary_msg = generate_morning_summary(user_id, aggregated_context or [])
                        if summary_msg:
                            messages_to_send.append((user_id, summary_msg))
                            update_success = update_preferences(user_id, {"last_morning_trigger_date": today_local_str})
                            if not update_success: log_error("routine_service", fn_name, f"Failed to update last_morning_trigger_date for {user_id}")
                        else:
                            log_warning("routine_service", fn_name, f"Morning summary generated empty message for {user_id}")

                # --- Check Evening Routine ---
                evening_time_str = prefs.get("Evening_Summary_Time")
                if prefs.get("Enable_Evening") and evening_time_str:
                    last_triggered_evening = prefs.get("last_evening_trigger_date")
                    if current_local_hm >= evening_time_str and last_triggered_evening != today_local_str:
                        log_info("routine_service", fn_name, f"Triggering Evening Review for user {user_id} at {current_local_hm} {user_tz_str}")
                        if not context_fetched:
                            context_start_date = (now_local - timedelta(days=ROUTINE_CONTEXT_HISTORY_DAYS)).strftime("%Y-%m-%d")
                            context_end_date = (now_local + timedelta(days=ROUTINE_CONTEXT_FUTURE_DAYS)).strftime("%Y-%m-%d")
                            log_info("routine_service", fn_name, f"Getting synced context for routines (User: {user_id})...")
                            aggregated_context = sync_service.get_synced_context_snapshot(user_id, context_start_date, context_end_date)
                            context_fetched = True

                        review_msg = generate_evening_review(user_id, aggregated_context or [])
                        if review_msg:
                            messages_to_send.append((user_id, review_msg))
                            update_success = update_preferences(user_id, {"last_evening_trigger_date": today_local_str})
                            if not update_success: log_error("routine_service", fn_name, f"Failed to update last_evening_trigger_date for {user_id}")
                        else:
                            log_warning("routine_service", fn_name, f"Evening review generated empty message for {user_id}")

            except Exception as user_err:
                 log_error("routine_service", fn_name, f"Error processing routines for user {user_id}. Prefs: {prefs}", user_err)
                 traceback.print_exc()

    except Exception as main_err:
        log_error("routine_service", fn_name, f"General error during routine check run", main_err)
        traceback.print_exc()

    log_info("routine_service", fn_name, f"Finished scheduled check for routine triggers. Found {len(messages_to_send)} messages to send.")
    return messages_to_send


def daily_cleanup():
    """Scheduled job to perform daily cleanup tasks (e.g., reset notification tracker)."""
    fn_name = "daily_cleanup"
    log_info("routine_service", fn_name, "Running daily cleanup job...")

    try:
        registry = get_registry()
        user_ids = list(registry.keys())
        if not user_ids:
            log_info("routine_service", fn_name, "No users found for daily cleanup.")
            return

        log_info("routine_service", fn_name, f"Performing daily cleanup for {len(user_ids)} users...")
        cleared_count = 0
        for user_id in user_ids:
            try:
               
                # Then in daily_cleanup:
                if _ROUTINE_AGENT_STATE_IMPORTED: # Use the distinct name
                    if get_agent_state(user_id):
                       clear_notified_event_ids(user_id)
                       cleared_count += 1
                else:
                    # This branch means the import at the top of routine_service.py failed
                    log_warning("routine_service", fn_name, f"AgentStateManager functions not available for user {user_id} during daily cleanup (import failed).")

            except NameError as ne:
                # This means _ROUTINE_AGENT_STATE_IMPORTED itself wasn't found, which is odd.
                log_error("routine_service", fn_name, f"Critical NameError for _ROUTINE_AGENT_STATE_IMPORTED during daily cleanup for user {user_id}. This indicates a fundamental module loading issue.", ne)
            except Exception as e:
                 log_error("routine_service", fn_name, f"Error during daily cleanup for user {user_id}", e)

        log_info("routine_service", fn_name, f"Daily cleanup finished. Cleared notification sets for {cleared_count} users.")

    except Exception as e:
        log_error("routine_service", fn_name, "Error during daily cleanup main loop", e)

# --- END OF FULL services/routine_service.py ---

# --- END OF FILE services/routine_service.py ---



================================================================================
üìÑ tools/google_calendar_api.py
================================================================================

# --- START OF FILE tools/google_calendar_api.py ---

# --- START OF FULL tools/google_calendar_api.py ---

import os
import re  # <--- ADD THIS IMPORT

from datetime import datetime, timedelta
from typing import Dict, List, Any, TYPE_CHECKING # Removed Optional

# --- Try importing Google libraries ---
Credentials = None
build = None
HttpError = Exception
GoogleAuthRequest = None
RefreshError = Exception
GOOGLE_LIBS_AVAILABLE = False

try:
    from tools.logger import log_info, log_error, log_warning
except ImportError:
    import logging
    logging.basicConfig(level=logging.INFO, format='%(levelname)s:google_calendar_api:%(message)s')
    log_info = logging.info; log_error = logging.error; log_warning = logging.warning
    log_error("google_calendar_api", "import", "Failed to import project logger.")

try:
    from google.oauth2.credentials import Credentials as ImportedCredentials
    from googleapiclient.discovery import build as imported_build
    from googleapiclient.errors import HttpError as ImportedHttpError
    from google.auth.transport.requests import Request as ImportedGoogleAuthRequest
    from google.auth.exceptions import RefreshError as ImportedRefreshError

    Credentials = ImportedCredentials
    build = imported_build
    HttpError = ImportedHttpError
    GoogleAuthRequest = ImportedGoogleAuthRequest
    RefreshError = ImportedRefreshError
    GOOGLE_LIBS_AVAILABLE = True
    log_info("google_calendar_api", "import", "Successfully imported Google API libraries.")
except ImportError as import_error_exception:
    log_error("google_calendar_api", "import", f"Failed to import one or more Google API libraries: {import_error_exception}. GoogleCalendarAPI will be non-functional.", import_error_exception)

# --- Other Local Project Imports ---
try:
    from tools.token_store import get_user_token, save_user_token_encrypted
except ImportError as e:
     log_error("google_calendar_api", "import", f"Failed to import from token_store: {e}", e)
     # Define dummy functions if import fails
     def get_user_token(*args, **kwargs): return None
     def save_user_token_encrypted(*args, **kwargs): return False

if TYPE_CHECKING:
    from googleapiclient.discovery import Resource
    if Credentials:
         from google.oauth2.credentials import Credentials

# --- Configuration ---
GOOGLE_CLIENT_ID = os.getenv("GOOGLE_CLIENT_ID")
GOOGLE_CLIENT_SECRET = os.getenv("GOOGLE_CLIENT_SECRET")
if not GOOGLE_CLIENT_ID: log_error("google_calendar_api", "config", "CRITICAL: GOOGLE_CLIENT_ID not set.")
if not GOOGLE_CLIENT_SECRET: log_error("google_calendar_api", "config", "CRITICAL: GOOGLE_CLIENT_SECRET not set.")
DEFAULT_TIMEZONE = "Asia/Jerusalem" # TODO: Make user-specific via preferences
GOOGLE_TOKEN_URI = "https://oauth2.googleapis.com/token"


class GoogleCalendarAPI:
    """Handles interactions with the Google Calendar API for a specific user."""
    def __init__(self, user_id: str):
        fn_name = "__init__"
        self.user_id = user_id
        self.service = None # Initialize service to None
        self.user_timezone = DEFAULT_TIMEZONE # TODO: Load from user prefs eventually

        log_info("GoogleCalendarAPI", fn_name, f"Initializing for user {self.user_id}")
        if not GOOGLE_LIBS_AVAILABLE:
            log_error("GoogleCalendarAPI", fn_name, "Google API libraries not available. Initialization skipped.")
            return

        credentials = self._load_credentials() # This now returns Credentials or None

        if credentials is not None: # Explicit check for None
            try:
                if build is None:
                    raise ImportError("Build function ('googleapiclient.discovery.build') not available.")
                # Assign the built service to self.service
                self.service = build("calendar", "v3", credentials=credentials, cache_discovery=False)
                log_info("GoogleCalendarAPI", fn_name, f"GCal service built successfully for {self.user_id}")
            except ImportError as e:
                 log_error("GoogleCalendarAPI", fn_name, f"Import error during service build: {e}", e)
                 self.service = None # Ensure service is None on error
            except Exception as e:
                log_error("GoogleCalendarAPI", fn_name, f"Failed to build GCal service: {e}", e)
                self.service = None # Ensure service is None on error
        else:
            log_warning("GoogleCalendarAPI", fn_name, f"Initialization incomplete for {self.user_id} due to credential failure.")
            self.service = None # Ensure service is None if creds fail


    # Return type is now 'Credentials | None', but we remove the hint as requested
    # The function still returns None on failure.
    def _load_credentials(self):
        fn_name = "_load_credentials"
        log_info("GoogleCalendarAPI", fn_name, f"Attempting credentials load for {self.user_id}")

        if not GOOGLE_LIBS_AVAILABLE or Credentials is None:
            log_error("GoogleCalendarAPI", fn_name, "Google libraries or Credentials class not available.")
            return None

        if not GOOGLE_CLIENT_ID or not GOOGLE_CLIENT_SECRET:
             log_error("GoogleCalendarAPI", fn_name, "Client ID or Secret missing in environment config.")
             return None

        token_data = get_user_token(self.user_id)
        if token_data is None: # Explicit check
             log_info("GoogleCalendarAPI", fn_name, f"No token data found for user {self.user_id}.")
             return None

        if "refresh_token" not in token_data:
             log_error("GoogleCalendarAPI", fn_name, f"FATAL: refresh_token missing in stored data for {self.user_id}. Re-auth needed.")
             # Consider deleting the bad token file here?
             # delete_token_file(self.user_id) # Hypothetical function
             return None

        credential_info_for_lib = {
            'token': token_data.get('access_token'),
            'refresh_token': token_data.get('refresh_token'),
            'token_uri': GOOGLE_TOKEN_URI,
            'client_id': GOOGLE_CLIENT_ID,
            'client_secret': GOOGLE_CLIENT_SECRET,
            'scopes': token_data.get('scopes', [])
        }
        # Ensure scopes is a list
        if isinstance(credential_info_for_lib['scopes'], str):
            credential_info_for_lib['scopes'] = credential_info_for_lib['scopes'].split()

        creds = None
        try:
            creds = Credentials.from_authorized_user_info(credential_info_for_lib)

            if not creds.valid:
                log_warning("GoogleCalendarAPI", fn_name, f"Credentials invalid/expired for {self.user_id}. Checking refresh token.")
                if creds.refresh_token:
                    log_info("GoogleCalendarAPI", fn_name, f"Attempting explicit token refresh for {self.user_id}...")
                    try:
                        if GoogleAuthRequest is None: raise ImportError("GoogleAuthRequest class not available for refresh.")
                        creds.refresh(GoogleAuthRequest())
                        log_info("GoogleCalendarAPI", fn_name, f"Token refresh successful for {self.user_id}.")
                        # Prepare data for saving (including expiry)
                        refreshed_token_data_to_save = {
                            'access_token': creds.token,
                            'refresh_token': creds.refresh_token,
                            'token_uri': creds.token_uri,
                            'client_id': creds.client_id,
                            'client_secret': creds.client_secret,
                            'scopes': creds.scopes,
                            'expiry_iso': creds.expiry.isoformat() if creds.expiry else None
                        }
                        # Attempt to save the refreshed token
                        if save_user_token_encrypted is None:
                            log_error("GoogleCalendarAPI", fn_name, "save_user_token_encrypted function not available.")
                        elif not save_user_token_encrypted(self.user_id, refreshed_token_data_to_save):
                            log_warning("GoogleCalendarAPI", fn_name, f"Failed to save refreshed token for {self.user_id}.")
                        # else: Saved successfully (no log needed unless verbose)

                    except RefreshError as refresh_err:
                        log_error("GoogleCalendarAPI", fn_name, f"Token refresh FAILED for {self.user_id} (RefreshError): {refresh_err}. Re-authentication required.", refresh_err)
                        token_file_path = os.path.join("data", f"tokens_{self.user_id}.json.enc")
                        if os.path.exists(token_file_path):
                            log_warning("GoogleCalendarAPI", fn_name, f"Deleting invalid token file due to refresh failure: {token_file_path}")
                            try: os.remove(token_file_path)
                            except OSError as rm_err: log_error("GoogleCalendarAPI", fn_name, f"Failed to remove token file: {rm_err}")
                        return None # Return None as refresh failed
                    except ImportError as imp_err:
                         log_error("GoogleCalendarAPI", fn_name, f"Import error during refresh for {self.user_id}: {imp_err}")
                         return None
                    except Exception as refresh_err:
                        log_error("GoogleCalendarAPI", fn_name, f"Unexpected error during token refresh for {self.user_id}: {refresh_err}", refresh_err)
                        return None # Return None on unexpected error
                else:
                     log_error("GoogleCalendarAPI", fn_name, f"Credentials invalid for {self.user_id}, and no refresh token available. Re-authentication needed.")
                     return None # Return None as creds invalid and no refresh possible

            # Final check after potential refresh attempt
            if creds and creds.valid:
                 log_info("GoogleCalendarAPI", fn_name, f"Credentials loaded and valid for {self.user_id}.")
                 return creds # Return the valid credentials object
            else:
                 log_error("GoogleCalendarAPI", fn_name, f"Failed to obtain valid credentials for {self.user_id} after potential refresh.")
                 return None # Return None as still not valid
        except Exception as e:
            log_error("GoogleCalendarAPI", fn_name, f"Error creating/validating credentials object for {self.user_id}: {e}", e)
            return None # Return None on error


    def is_active(self):
        """Checks if the Google Calendar service object was successfully initialized."""
        return self.service is not None

    # Returns event ID string or None
    def create_event(self, event_data: Dict):
        fn_name = "create_event"
        if not self.is_active():
            log_error("GoogleCalendarAPI", fn_name, f"Service not active for user {self.user_id}.")
            return None
        assert self.service is not None # Should be active if is_active() passed

        try:
            event_date = event_data.get('date')
            event_time = event_data.get('time')
            duration_minutes = None
            if event_data.get('duration'):
                try:
                    # Improved duration parsing logic
                    duration_str = str(event_data['duration']).lower().replace(' ','')
                    total_minutes = 0.0
                    hour_match = re.search(r'(\d+(\.\d+)?)\s*h', duration_str)
                    minute_match = re.search(r'(\d+)\s*m', duration_str)
                    if hour_match: total_minutes += float(hour_match.group(1)) * 60
                    if minute_match: total_minutes += int(minute_match.group(1))
                    # Handle plain numbers (assume minutes) only if no h/m found
                    if total_minutes == 0 and hour_match is None and minute_match is None:
                         if duration_str.replace('.','',1).isdigit():
                              total_minutes = float(duration_str)
                         else: raise ValueError("Unrecognized duration format")
                    duration_minutes = int(round(total_minutes)) if total_minutes > 0 else None
                except (ValueError, TypeError, AttributeError):
                    log_warning("GoogleCalendarAPI", fn_name, f"Could not parse duration: {event_data.get('duration')}, using default.")
                    duration_minutes = 30 # Default to 30 mins if parse fails

            if not event_date:
                 log_error("GoogleCalendarAPI", fn_name, f"Missing mandatory 'date' field.")
                 return None

            start_obj = {}
            end_obj = {}
            time_zone = self.user_timezone # Use the instance timezone

            if event_time:
                 # Handle timed event
                 try:
                     start_dt = datetime.strptime(f"{event_date} {event_time}", "%Y-%m-%d %H:%M")
                     # Use parsed duration or default
                     delta = timedelta(minutes=duration_minutes if duration_minutes is not None else 30)
                     end_dt = start_dt + delta
                     start_obj = {"dateTime": start_dt.isoformat(), "timeZone": time_zone}
                     end_obj = {"dateTime": end_dt.isoformat(), "timeZone": time_zone}
                 except ValueError as time_err:
                      log_error("GoogleCalendarAPI", fn_name, f"Invalid date/time format: {event_date} {event_time}", time_err)
                      return None
            else: # Handle all-day event
                try:
                    start_dt_date = datetime.strptime(event_date, "%Y-%m-%d").date()
                    # All-day events end on the *next* day according to GCal API
                    end_date_dt = start_dt_date + timedelta(days=1)
                    start_obj = {"date": start_dt_date.strftime("%Y-%m-%d")}
                    end_obj = {"date": end_date_dt.strftime("%Y-%m-%d")}
                except ValueError as date_err:
                     log_error("GoogleCalendarAPI", fn_name, f"Invalid date format '{event_date}' for all-day event", date_err)
                     return None

            # Construct the event body
            google_event_body = {
                "summary": event_data.get("title", event_data.get("description", "New Item")),
                "description": event_data.get("description", ""),
                "start": start_obj,
                "end": end_obj
            }

            log_info("GoogleCalendarAPI", fn_name, f"Creating GCal event for user {self.user_id}: {google_event_body.get('summary')}")
            created_event = self.service.events().insert(calendarId='primary', body=google_event_body).execute()
            google_event_id = created_event.get("id")

            if google_event_id:
                log_info("GoogleCalendarAPI", fn_name, f"Successfully created GCal event ID: {google_event_id}")
                return google_event_id # Return the ID string
            else:
                log_error("GoogleCalendarAPI", fn_name, f"GCal API response missing 'id'. Response: {created_event}")
                return None # Return None on failure
        except HttpError as http_err:
             log_error("GoogleCalendarAPI", fn_name, f"HTTP error creating event for user {self.user_id}: Status {http_err.resp.status}", http_err)
             return None
        except Exception as e:
            log_error("GoogleCalendarAPI", fn_name, f"Unexpected error creating event for user {self.user_id}", e)
            return None

    # Returns bool
    def update_event(self, event_id: str, updates: Dict):
        fn_name = "update_event"
        if not self.is_active():
            log_error("GoogleCalendarAPI", fn_name, f"Service not active for user {self.user_id}, cannot update event {event_id}.")
            return False
        assert self.service is not None

        try:
            # Get the existing event first to determine current times/duration if needed
            try:
                  existing_event = self.service.events().get(calendarId='primary', eventId=event_id).execute()
            except HttpError as get_err:
                 # If event not found, cannot update
                 if get_err.resp.status == 404:
                     log_warning("GoogleCalendarAPI", fn_name, f"Cannot update event {event_id}: Not found.")
                     return False
                 else:
                      log_error("GoogleCalendarAPI", fn_name, f"HTTP error getting event {event_id} before update: {get_err}", get_err)
                      return False

            update_payload = {}
            needs_update = False
            time_zone = self.user_timezone

            # Update simple fields
            if "title" in updates: update_payload["summary"] = updates["title"]; needs_update = True
            if "description" in updates: update_payload["description"] = updates["description"]; needs_update = True

            # Handle date/time updates carefully
            new_date_str = updates.get("date")
            new_time_str = updates.get("time") # Can be None if time is cleared

            # Check if date or time is being explicitly modified
            if new_date_str is not None or "time" in updates:
                 current_start_info = existing_event.get('start', {})
                 current_end_info = existing_event.get('end', {})
                 is_currently_all_day = 'date' in current_start_info and 'dateTime' not in current_start_info

                 # Determine the target date
                 target_date_str = new_date_str
                 if target_date_str is None: # Date not provided in update, use existing
                      if is_currently_all_day:
                           target_date_str = current_start_info.get('date')
                      elif current_start_info.get('dateTime'):
                           try: target_date_str = datetime.fromisoformat(current_start_info['dateTime']).strftime('%Y-%m-%d')
                           except ValueError: target_date_str = None # Fallback if parse fails
                      else: target_date_str = None # Cannot determine existing date

                 # Determine the target time (can be None)
                 target_time_str = new_time_str if "time" in updates else (datetime.fromisoformat(current_start_info['dateTime']).strftime('%H:%M') if current_start_info.get('dateTime') and not is_currently_all_day else None)

                 if target_date_str:
                      if target_time_str: # Update to a timed event
                           try:
                               start_dt = datetime.strptime(f"{target_date_str} {target_time_str}", "%Y-%m-%d %H:%M")
                               # Preserve duration if possible
                               duration = timedelta(minutes=30) # Default fallback
                               if current_start_info.get('dateTime') and current_end_info.get('dateTime'):
                                    try: duration = datetime.fromisoformat(current_end_info['dateTime']) - datetime.fromisoformat(current_start_info['dateTime'])
                                    except ValueError: pass # Use default if parse fails
                               end_dt = start_dt + duration
                               update_payload["start"] = {"dateTime": start_dt.isoformat(), "timeZone": time_zone}
                               update_payload["end"] = {"dateTime": end_dt.isoformat(), "timeZone": time_zone}
                               needs_update = True
                           except ValueError as e:
                                log_error("GoogleCalendarAPI", fn_name, f"Invalid date/time '{target_date_str} {target_time_str}' on update: {e}")
                                # Don't proceed with this part of the update if format is bad
                      else: # Update to an all-day event
                           try:
                               start_dt_date = datetime.strptime(target_date_str, "%Y-%m-%d").date()
                               end_date_dt = start_dt_date + timedelta(days=1)
                               update_payload["start"] = {"date": start_dt_date.strftime("%Y-%m-%d")}
                               update_payload["end"] = {"date": end_date_dt.strftime("%Y-%m-%d")}
                               # Clear any existing dateTime fields if changing to all-day
                               if 'dateTime' in update_payload.get("start",{}): del update_payload["start"]["dateTime"]
                               if 'dateTime' in update_payload.get("end",{}): del update_payload["end"]["dateTime"]
                               needs_update = True
                           except ValueError as e:
                                log_error("GoogleCalendarAPI", fn_name, f"Invalid date '{target_date_str}' for all-day update: {e}")

            if not needs_update:
                 log_info("GoogleCalendarAPI", fn_name, f"No fields require patching for GCal event {event_id}")
                 return True # No change needed, considered success

            log_info("GoogleCalendarAPI", fn_name, f"Patching GCal event {event_id} for user {self.user_id}. Fields: {list(update_payload.keys())}")
            self.service.events().patch(calendarId='primary', eventId=event_id, body=update_payload).execute()
            log_info("GoogleCalendarAPI", fn_name, f"Successfully updated GCal event {event_id}")
            return True # Return True on success
        except HttpError as http_err:
             log_error("GoogleCalendarAPI", fn_name, f"HTTP error updating event {event_id}: Status {http_err.resp.status}", http_err)
             return False
        except Exception as e:
            log_error("GoogleCalendarAPI", fn_name, f"Unexpected error updating event {event_id}", e)
            return False

    # Returns bool
    def delete_event(self, event_id: str):
        fn_name = "delete_event"
        if not self.is_active():
            log_error("GoogleCalendarAPI", fn_name, f"Service not active for user {self.user_id}, cannot delete event {event_id}.")
            return False
        assert self.service is not None
        try:
            log_info("GoogleCalendarAPI", fn_name, f"Attempting delete GCal event {event_id} for user {self.user_id}")
            # sendNotifications=False might be useful if you handle reminders internally
            self.service.events().delete(calendarId='primary', eventId=event_id, sendNotifications=False).execute()
            log_info("GoogleCalendarAPI", fn_name, f"Successfully deleted GCal event {event_id}.")
            return True # Return True on success
        except HttpError as http_err:
            if http_err.resp.status in [404, 410]: # Not Found or Gone
                log_warning("GoogleCalendarAPI", fn_name, f"GCal event {event_id} not found or already gone (Status {http_err.resp.status}). Assuming deleted.")
                return True # Consider deletion successful if it's already gone
            else:
                 log_error("GoogleCalendarAPI", fn_name, f"HTTP error deleting event {event_id}: Status {http_err.resp.status}", http_err)
                 return False # Return False on other HTTP errors
        except Exception as e:
            log_error("GoogleCalendarAPI", fn_name, f"Unexpected error deleting event {event_id}", e)
            return False

    # Returns list of dicts or empty list
    def list_events(self, start_date: str, end_date: str):
        fn_name = "list_events"
        if not self.is_active():
            log_error("GoogleCalendarAPI", fn_name, f"Service not active for user {self.user_id}, cannot list events.")
            return []
        assert self.service is not None

        try:
            # Format dates for API (inclusive start, exclusive end)
            start_dt = datetime.strptime(start_date, "%Y-%m-%d").replace(hour=0, minute=0, second=0, microsecond=0)
            end_dt_exclusive = datetime.strptime(end_date, "%Y-%m-%d").replace(hour=0, minute=0, second=0, microsecond=0) + timedelta(days=1)
            # Use UTC 'Z' for timeMin/timeMax as recommended by Google API
            time_min = start_dt.isoformat() + "Z"
            time_max = end_dt_exclusive.isoformat() + "Z"
            #log_info("GoogleCalendarAPI", fn_name, f"Listing GCal events for user {self.user_id} from {time_min} to {time_max}")
        except ValueError as date_err:
            log_error("GoogleCalendarAPI", fn_name, f"Invalid date format for listing events: {start_date} / {end_date}", date_err)
            return [] # Return empty list on bad date format

        try:
            all_items = []
            page_token = None
            while True:
                events_result = self.service.events().list(
                    calendarId='primary',
                    timeMin=time_min,
                    timeMax=time_max,
                    maxResults=250, # Max allowed by API per page
                    singleEvents=True, # Expand recurring events
                    orderBy='startTime',
                    pageToken=page_token
                ).execute()
                items = events_result.get("items", [])
                all_items.extend(items)
                page_token = events_result.get('nextPageToken')
                if not page_token: break # Exit loop when no more pages
            log_info("GoogleCalendarAPI", fn_name, f"Found {len(all_items)} GCal events for user {self.user_id} in range.")
            # Parse *after* collecting all items
            return [self._parse_google_event(e) for e in all_items]
        except HttpError as http_err:
             log_error("GoogleCalendarAPI", fn_name, f"HTTP error listing events for user {self.user_id}: Status {http_err.resp.status}", http_err)
             return []
        except Exception as e:
            log_error("GoogleCalendarAPI", fn_name, f"Unexpected error listing events for user {self.user_id}", e)
            return []

    # Returns dict or None
    def _get_single_event(self, event_id: str):
        fn_name = "_get_single_event"
        if not self.is_active():
            log_error("GoogleCalendarAPI", fn_name, f"Service not active for user {self.user_id}.")
            return None
        assert self.service is not None
        try:
            event = self.service.events().get(calendarId='primary', eventId=event_id).execute()
            return event # Return the raw GCal event dictionary
        except HttpError as http_err:
             if http_err.resp.status in [404, 410]: # Not Found or Gone
                  log_warning("GoogleCalendarAPI", fn_name, f"Event {event_id} not found (Status {http_err.resp.status}).")
                  return None # Return None if not found
             else:
                  log_error("GoogleCalendarAPI", fn_name, f"HTTP error getting event {event_id}: Status {http_err.resp.status}", http_err)
                  return None # Return None on other errors
        except Exception as e:
            log_error("GoogleCalendarAPI", fn_name, f"Unexpected error getting event {event_id}", e)
            return None

    # Returns dict
    def _parse_google_event(self, event: Dict):
        # This function parses the raw GCal event into our standard format
        start_info = event.get("start", {})
        end_info = event.get("end", {})
        # Get dateTime first, fallback to date for all-day events
        start_datetime_str = start_info.get("dateTime", start_info.get("date"))
        end_datetime_str = end_info.get("dateTime", end_info.get("date"))
        # Determine if it's an all-day event
        is_all_day = "date" in start_info and "dateTime" not in start_info

        # Basic parsing
        parsed = {
            "event_id": event.get("id"),
            "title": event.get("summary", ""),
            "description": event.get("description", ""),
            "gcal_start_datetime": start_datetime_str, # Store the full string
            "gcal_end_datetime": end_datetime_str,     # Store the full string
            "is_all_day": is_all_day,
            "gcal_link": event.get("htmlLink", ""),
            "status_gcal": event.get("status", ""), # e.g., 'confirmed', 'tentative', 'cancelled'
            "created_gcal": event.get("created"), # ISO timestamp
            "updated_gcal": event.get("updated"), # ISO timestamp
            # Add any other relevant fields we might want later
        }
        return parsed

# --- END OF CLASS GoogleCalendarAPI ---

# --- END OF FULL tools/google_calendar_api.py ---

# --- END OF FILE tools/google_calendar_api.py ---



================================================================================
üìÑ tools/calendar_tool.py
================================================================================

# --- START OF FILE tools/calendar_tool.py ---

# --- START OF FULL tools/calendar_tool.py ---

import os
import requests
import json # For logging potentially
from fastapi import APIRouter, Request, Response
from fastapi.responses import HTMLResponse
from tools.logger import log_info, log_error, log_warning
from tools.encryption import encrypt_data # Only need encrypt_data from here
import jwt
import requests.compat # Needed for urlencode in authenticate
from datetime import datetime # Keep if used elsewhere

# --- REMOVE service layer import attempt from module level ---
# --- Keep ONLY direct registry update as fallback ---
from users.user_registry import update_preferences as update_prefs_direct
log_warning("calendar_tool", "import", "Using direct registry update for preferences in callback.")
# ----------------------------------------------

router = APIRouter()

# --- Configuration ---
GOOGLE_CLIENT_SECRET = os.getenv("GOOGLE_CLIENT_SECRET")
CLIENT_ID = os.getenv("GOOGLE_CLIENT_ID")
REDIRECT_URI = os.getenv("GOOGLE_REDIRECT_URI", "http://localhost:8000/oauth2callback")
SCOPE = "https://www.googleapis.com/auth/calendar"
TOKEN_URL = "https://oauth2.googleapis.com/token"
AUTH_URL_BASE = "https://accounts.google.com/o/oauth2/auth"

if not GOOGLE_CLIENT_SECRET or not CLIENT_ID:
    log_error("calendar_tool", "config", "CRITICAL: GOOGLE_CLIENT_ID or GOOGLE_CLIENT_SECRET env var not set.")

log_info("calendar_tool", "config", f"Calendar Tool loaded. Client ID starts with: {str(CLIENT_ID)[:10]}")

# --- Core Authentication Check Function ---
def authenticate(user_id: str, prefs: dict) -> dict:
    """
    Checks user's calendar auth status based on provided preferences.
    Returns status and auth URL if needed, or status/message if token exists.
    """
    log_info("calendar_tool", "authenticate", f"Checking auth status for user {user_id}")
    from tools.token_store import get_user_token # Import here
    token_data = get_user_token(user_id)
    calendar_enabled = prefs.get("Calendar_Enabled", False)

    if token_data is not None and calendar_enabled:
        log_info("calendar_tool", "authenticate", f"Token data exists and enabled flag is True for {user_id}.")
        return {"status": "token_exists", "message": "Stored calendar credentials found. Attempting to use..."}
    else:
        # ... (rest of auth URL generation logic remains the same) ...
        if token_data is not None and not calendar_enabled:
             log_info("calendar_tool", "authenticate", f"Token data exists but Calendar_Enabled=False for {user_id}. Initiating re-auth/enable.")
        else: # token_data is None
             log_info("calendar_tool", "authenticate", f"No valid token data found for {user_id}. Initiating auth.")

        if not CLIENT_ID or not REDIRECT_URI:
             log_error("calendar_tool", "authenticate", f"Client ID or Redirect URI missing for auth URL generation.")
             return {"status": "fails", "message": "Server configuration error prevents authentication."}

        normalized_state = user_id.replace("@c.us", "").replace("+","")
        params = {
            "client_id": CLIENT_ID, "redirect_uri": REDIRECT_URI, "scope": SCOPE,
            "response_type": "code", "access_type": "offline", "state": normalized_state, "prompt": "consent"
        }
        try:
             encoded_params = requests.compat.urlencode(params)
             auth_url = f"{AUTH_URL_BASE}?{encoded_params}"
             log_info("calendar_tool", "authenticate", f"Generated auth URL for {user_id}")
             return {"status": "pending", "message": f"Please authenticate your calendar by visiting this URL: {auth_url}"}
        except Exception as url_e:
             log_error("calendar_tool", "authenticate", f"Failed to build auth URL for {user_id}", url_e)
             return {"status": "fails", "message": "Failed to generate authentication URL."}


# --- OAuth Callback Endpoint (Corrected Scope) ---
@router.get("/oauth2callback", response_class=HTMLResponse)
async def oauth2callback(request: Request, code: str | None = None, state: str | None = None, error: str | None = None):
    """
    Handles the OAuth2 callback from Google.
    Exchanges code, saves token, updates preferences.
    """
    # --- Define flag and function placeholder INSIDE the function scope ---
    config_manager_imported_locally = False
    update_prefs_service = None
    # --- Attempt import locally ---
    try:
        from services.config_manager import update_preferences
        update_prefs_service = update_preferences
        config_manager_imported_locally = True
    except ImportError:
         # Error/warning already logged at module level, no need to repeat
         pass # Keep flag as False, function as None
    # ---------------------------------------------------------------------

    html_error_template = "<html><body><h1>Authentication Error</h1><p>Details: {details}</p><p>Please try authenticating again or contact support if the issue persists.</p></body></html>"
    html_success_template = "<html><body><h1>Authentication Successful!</h1><p>Your credentials have been saved. The connection will be fully tested when first used. You can close this window and return to the chat.</p></body></html>"

    if error:
        log_error("calendar_tool", "oauth2callback", f"OAuth error received from Google: {error}")
        return HTMLResponse(content=html_error_template.format(details=f"Google reported an error: {error}"), status_code=400)
    if not code or not state:
        log_error("calendar_tool", "oauth2callback", "Callback missing code or state.")
        return HTMLResponse(content=html_error_template.format(details="Invalid response received from Google (missing code or state)."), status_code=400)

    user_id = state
    log_info("calendar_tool", "oauth2callback", f"Callback received for user {user_id}.")

    if not GOOGLE_CLIENT_SECRET or not CLIENT_ID:
        log_error("calendar_tool", "oauth2callback", "Server configuration error: Client ID/Secret not set.")
        return HTMLResponse(content=html_error_template.format(details="Server configuration error."), status_code=500)

    try:
        # --- 1. Exchange Code for Tokens ---
        payload = {
            "code": code, "client_id": CLIENT_ID, "client_secret": GOOGLE_CLIENT_SECRET,
            "redirect_uri": REDIRECT_URI, "grant_type": "authorization_code"
        }
        log_info("calendar_tool", "oauth2callback", f"Exchanging authorization code for tokens for user {user_id}.")
        token_response = requests.post(TOKEN_URL, data=payload)
        token_response.raise_for_status()
        tokens = token_response.json()
        log_info("calendar_tool", "oauth2callback", f"Tokens received successfully. Keys: {list(tokens.keys())}")

        if 'access_token' not in tokens:
            log_error("calendar_tool", "oauth2callback", f"Access token *NOT* received for user {user_id}.")
            return HTMLResponse(content=html_error_template.format(details="Failed to obtain access token from Google."), status_code=500)
        if 'refresh_token' not in tokens:
            log_warning("calendar_tool", "oauth2callback", f"Refresh token *NOT* received for user {user_id}. Offline access might fail later.")

        # --- 2. Extract Email ---
        email = ""
        id_token = tokens.get("id_token")
        if id_token:
            try:
                decoded = jwt.decode(id_token, options={"verify_signature": False, "verify_aud": False})
                email = decoded.get("email", "")
                log_info("calendar_tool", "oauth2callback", f"Extracted email '{email}' for user {user_id}.")
            except jwt.exceptions.DecodeError as jwt_e:
                log_warning("calendar_tool", "oauth2callback", f"Failed decode id_token for {user_id}, proceeding without email.", jwt_e)

        # --- 3. Save Tokens Encrypted ---
        from tools.token_store import save_user_token_encrypted # Import locally
        if not save_user_token_encrypted(user_id, tokens):
             log_error("calendar_tool", "oauth2callback", f"Failed to save token via token_store for {user_id}.")
             return HTMLResponse(content=html_error_template.format(details="Failed to save credentials securely."), status_code=500)
        log_info("calendar_tool", "oauth2callback", f"Tokens stored successfully via token_store for {user_id}.")

        # --- 4. Update Preferences ---
        prefs_update = { "email": email, "Calendar_Enabled": True }
        pref_update_success = False
        # --- Use locally checked flag and function ---
        if config_manager_imported_locally and update_prefs_service:
            pref_update_success = update_prefs_service(user_id, prefs_update)
            if pref_update_success:
                 log_info("calendar_tool", "oauth2callback", f"Preferences updated via ConfigManager for {user_id}: {prefs_update}")
            else:
                 log_error("calendar_tool", "oauth2callback", f"ConfigManager failed update preferences for {user_id} after token save.")
        else:
             # Fallback to direct update
             # Warning already logged at module level
             pref_update_success = update_prefs_direct(user_id, prefs_update) # Use direct update
             if pref_update_success:
                  log_info("calendar_tool", "oauth2callback", f"Preferences updated DIRECTLY for {user_id}: {prefs_update}")
             else:
                  log_error("calendar_tool", "oauth2callback", f"Direct registry update failed for {user_id} after token save.")
        # -----------------------------------------

        if pref_update_success:
             return HTMLResponse(content=html_success_template, status_code=200)
        else:
             # Tokens saved, but profile update failed
             return HTMLResponse(content=html_error_template.format(details="Credentials saved, but failed to update user profile. Contact support."), status_code=500)

    except requests.exceptions.HTTPError as http_e:
        response_text = http_e.response.text; status_code = http_e.response.status_code
        error_details = f"Error {status_code} during token exchange.";
        try: error_json = http_e.response.json(); error_details = error_json.get('error_description', error_json.get('error', f"HTTP {status_code}"))
        except ValueError: pass
        log_error("calendar_tool", "oauth2callback", f"HTTP error {status_code} during token exchange for {user_id}. Details: {error_details}", http_e)
        return HTMLResponse(content=html_error_template.format(details=f"Could not get authorization from Google: {error_details}."), status_code=status_code)
    except Exception as e:
        log_error("calendar_tool", "oauth2callback", f"Generic unexpected error during callback for {user_id}", e)
        return HTMLResponse(content=html_error_template.format(details=f"An unexpected server error occurred: {e}."), status_code=500)

# --- END OF FULL tools/calendar_tool.py ---

# --- END OF FILE tools/calendar_tool.py ---



================================================================================
üìÑ tools/token_store.py
================================================================================

# --- START OF FILE tools/token_store.py ---

# --- START OF FULL tools/token_store.py ---

import os
import json
from dotenv import load_dotenv # Added for .env loading
from tools.encryption import decrypt_data, encrypt_data # Ensure both are imported
from tools.logger import log_info, log_error, log_warning

# --- Load Environment Variables ---
# Load variables from .env file in the current directory or parent directories
load_dotenv()

# --- Configuration ---
# Get the data suffix (_cli or empty) from environment variables
DATA_SUFFIX = os.getenv("DATA_SUFFIX", "") # Default to empty string if not set

# Define the specific subdirectory for tokens
TOKEN_BASE_DIR = "data"
TOKEN_SUB_DIR = "tokens" # New subdirectory name
TOKEN_DIR_PATH = os.path.join(TOKEN_BASE_DIR, TOKEN_SUB_DIR)

# --- Helper Function to Construct Full Path ---
def _get_token_path(user_id: str) -> str:
    """Constructs the absolute path for a user's token file including the suffix."""
    # Construct filename with suffix: e.g., tokens_1234_cli.json.enc or tokens_1234.json.enc
    filename = f"tokens_{user_id}{DATA_SUFFIX}.json.enc"
    # Construct the relative path including the subdirectory
    relative_path = os.path.join(TOKEN_DIR_PATH, filename)
    # Return the absolute path
    return os.path.abspath(relative_path)

# --- Core Functions ---

def get_user_token(user_id: str) -> dict | None:
    """Loads and decrypts the user's token from the encrypted file in data/tokens."""
    fn_name = "get_user_token"
    absolute_path = _get_token_path(user_id) # Get the full path

    # Log the path being checked
    log_info("token_store", fn_name, f"Attempting to load token from: '{absolute_path}'")

    # Check if the file exists
    if not os.path.exists(absolute_path):
        log_info("token_store", fn_name, f"Token file not found for user {user_id} at '{absolute_path}'.")
        return None

    # Proceed if file exists
    log_info("token_store", fn_name, f"Token file found at '{absolute_path}'. Attempting to read and decrypt.")
    try:
        with open(absolute_path, "rb") as f:
            encrypted_data = f.read()

        # Decrypt the data
        token_data = decrypt_data(encrypted_data) # Assumes decrypt_data returns dict or None

        if token_data:
            log_info("token_store", fn_name, f"Successfully decrypted token data for user {user_id}. Keys: {list(token_data.keys())}")
            return token_data
        else:
            # decrypt_data should log its own errors
            log_error("token_store", fn_name, f"Decryption failed for token file: '{absolute_path}'. Check encryption logs.")
            return None

    except FileNotFoundError:
         # Should ideally not happen after os.path.exists, but handles race conditions
         log_info("token_store", fn_name, f"Token file disappeared before reading for user {user_id} at '{absolute_path}'.")
         return None
    except PermissionError as pe:
         log_error("token_store", fn_name, f"Permission denied reading token file '{absolute_path}'", pe)
         return None
    except Exception as e:
        log_error("token_store", fn_name, f"Unexpected error loading token from '{absolute_path}'", e)
        return None


def save_user_token_encrypted(user_id: str, token_data: dict) -> bool:
    """
    Encrypts and saves the user's token data to a file in data/tokens using atomic write.
    """
    fn_name = "save_user_token_encrypted"
    path = _get_token_path(user_id) # Get the full target path
    temp_path = path + ".tmp"      # Temporary file for atomic write

    log_info("token_store", fn_name, f"Attempting to save encrypted token for user {user_id} to '{path}'")

    try:
        # --- Data Validation ---
        # Ensure necessary tokens are present
        data_to_save = token_data.copy()
        # Handle potential 'token' key from oauth callback -> 'access_token'
        if 'token' in data_to_save and 'access_token' not in data_to_save:
            data_to_save['access_token'] = data_to_save.pop('token')

        if not data_to_save.get('access_token'):
            log_error("token_store", fn_name, f"Cannot save token: Missing 'access_token' for user {user_id}.")
            return False
        # It's critical for Google OAuth to have a refresh token for offline access
        if not data_to_save.get('refresh_token'):
            log_warning("token_store", fn_name, f"Saving token data missing 'refresh_token' for user {user_id}. Offline access/refresh will fail.")
        # -----------------------

        # Encrypt the validated data
        encrypted_tokens = encrypt_data(data_to_save)
        if not encrypted_tokens:
            # encrypt_data should log its own errors
            log_error("token_store", fn_name, f"Encryption failed for user {user_id}'s tokens during save.")
            return False

        # --- Atomic Write ---
        # Ensure the target directory exists (data/tokens/)
        os.makedirs(os.path.dirname(path), exist_ok=True)

        # Write to temporary file first
        with open(temp_path, "wb") as f:
            f.write(encrypted_tokens)

        # Atomically replace the old file with the new one
        os.replace(temp_path, path)
        # --------------------

        log_info("token_store", fn_name, f"Token stored successfully for {user_id} at '{path}'.")
        return True

    except PermissionError as pe:
        log_error("token_store", fn_name, f"Permission denied writing token file to '{path}' or temp file '{temp_path}'.", pe)
        return False
    except Exception as e:
        log_error("token_store", fn_name, f"Unexpected error saving token file to '{path}'", e)
        # Clean up temporary file if it exists after an error
        if os.path.exists(temp_path):
            try:
                os.remove(temp_path)
                log_info("token_store", fn_name, f"Removed temporary token file '{temp_path}' after error.")
            except OSError as rm_err:
                log_error("token_store", fn_name, f"Failed to remove temporary token file '{temp_path}' after error: {rm_err}")
        return False

# --- END OF FULL tools/token_store.py ---

# --- END OF FILE tools/token_store.py ---



================================================================================
üìÑ tools/encryption.py
================================================================================

# --- START OF FILE tools/encryption.py ---

# tools/encryption.py

import os
import json
from cryptography.fernet import Fernet, InvalidToken
from tools.logger import log_error, log_info

# --- Key Management ---
# Load the encryption key from environment variables
# CRITICAL: This key MUST be kept secret and secure.
# Generate one using generate_key() below and set it as an environment variable.
ENCRYPTION_KEY_ENV_VAR = "ENCRYPTION_KEY"
_encryption_key = os.getenv(ENCRYPTION_KEY_ENV_VAR)

if not _encryption_key:
    log_error("encryption", "__init__",
              f"CRITICAL ERROR: Environment variable '{ENCRYPTION_KEY_ENV_VAR}' not set. Encryption disabled.")
    # You might want to raise an exception here to halt execution
    # raise ValueError(f"Environment variable '{ENCRYPTION_KEY_ENV_VAR}' is required for encryption.")
    _fernet = None
else:
    try:
        # Ensure the key is bytes
        _key_bytes = _encryption_key.encode('utf-8')
        _fernet = Fernet(_key_bytes)
        log_info("encryption", "__init__", "Fernet encryption service initialized successfully.")
    except Exception as e:
        log_error("encryption", "__init__", f"Failed to initialize Fernet. Invalid key format? Error: {e}", e)
        _fernet = None
        # raise ValueError(f"Invalid encryption key format: {e}") # Optional: Halt execution

# --- Encryption/Decryption Functions ---

def encrypt_data(data: dict) -> bytes | None:
    """
    Encrypts a dictionary using Fernet.

    Args:
        data: The dictionary to encrypt.

    Returns:
        Encrypted bytes if successful, None otherwise.
    """
    if not _fernet:
        log_error("encryption", "encrypt_data", "Encryption service not available (key missing or invalid).")
        return None
    try:
        # Serialize the dictionary to a JSON string, then encode to bytes
        data_bytes = json.dumps(data).encode('utf-8')
        encrypted_data = _fernet.encrypt(data_bytes)
        return encrypted_data
    except Exception as e:
        log_error("encryption", "encrypt_data", f"Encryption failed: {e}", e)
        return None

def decrypt_data(encrypted_data: bytes) -> dict | None:
    """
    Decrypts data encrypted with Fernet back into a dictionary.

    Args:
        encrypted_data: The encrypted bytes to decrypt.

    Returns:
        The original dictionary if successful, None otherwise.
    """
    if not _fernet:
        log_error("encryption", "decrypt_data", "Encryption service not available (key missing or invalid).")
        return None
    try:
        decrypted_bytes = _fernet.decrypt(encrypted_data)
        # Decode bytes back to JSON string, then parse into dictionary
        decrypted_json = decrypted_bytes.decode('utf-8')
        original_data = json.loads(decrypted_json)
        return original_data
    except InvalidToken:
        log_error("encryption", "decrypt_data", "Decryption failed: Invalid token (key mismatch or data corrupted).")
        return None
    except Exception as e:
        log_error("encryption", "decrypt_data", f"Decryption failed: {e}", e)
        return None

# --- Key Generation Utility ---

def generate_key() -> str:
    """Generates a new Fernet key (URL-safe base64 encoded)."""
    return Fernet.generate_key().decode('utf-8')

# Example usage for generating a key (run this file directly: python -m tools.encryption)
if __name__ == "__main__":
    new_key = generate_key()
    print("Generated Fernet Key (set this as your ENCRYPTION_KEY environment variable):")
    print(new_key)
    print("\nWARNING: Keep this key secure and secret!")

# --- END OF FILE tools/encryption.py ---



================================================================================
üìÑ tools/logger.py
================================================================================

# --- START OF FILE tools/logger.py ---

# --- START OF FULL tools/logger.py ---
import os
import pytz # Make sure pytz is imported
from datetime import datetime, timezone # timezone from datetime is for UTC specifically
import traceback
import json

# --- Database Logging Import ---
ACTIVITY_DB_IMPORTED = False
_activity_db_log_func = None

try:
    pass
except ImportError:
    # Use direct print as logger isn't ready
    print(f"[{datetime.now(timezone.utc).isoformat(timespec='seconds')+'Z'}] [ERROR] [logger:import_check] Failed initial check for activity_db module. DB logging disabled.")


# === Config ===
DEBUG_MODE = os.getenv("DEBUG_MODE", "True").lower() in ('true', '1', 't', 'yes', 'y')
LOG_DIR = "logs"
LOG_FILE = os.path.join(LOG_DIR, "whats_tasker.log") # For file logging if DEBUG_MODE is False

# --- *** CHANGE THIS LINE *** ---
LOG_TIMEZONE_STR = "Asia/Jerusalem" # Timezone for general log timestamps
try:
    LOG_TIMEZONE_PYTZ = pytz.timezone(LOG_TIMEZONE_STR)
except pytz.UnknownTimeZoneError:
    print(f"[{datetime.now(timezone.utc).isoformat(timespec='seconds')+'Z'}] [ERROR] [logger:init] Unknown LOG_TIMEZONE_STR '{LOG_TIMEZONE_STR}'. Defaulting to UTC.")
    LOG_TIMEZONE_PYTZ = pytz.utc
# --- *** END CHANGE *** ---


try:
    os.makedirs(LOG_DIR, exist_ok=True)
except OSError as e:
    print(f"[{datetime.now(timezone.utc).isoformat(timespec='seconds')+'Z'}] [ERROR] [logger:init] Failed to create log directory '{LOG_DIR}': {e}")

# === Helpers ===
def _timestamp_utc_iso(): # Renamed to clarify its UTC and ISO format
    """Returns current time in UTC ISO format for DB logging or specific needs."""
    return datetime.now(timezone.utc).isoformat(timespec='seconds').replace('+00:00', 'Z')

def _format_log_entry(level: str, module: str, func: str, message: str):
    """Formats a log entry with the configured LOG_TIMEZONE_PYTZ."""
    # --- *** USES THE NEW LOG_TIMEZONE_PYTZ *** ---
    ts_aware = datetime.now(LOG_TIMEZONE_PYTZ)
    # Format: YYYY-MM-DD HH:MM:SS TZN (e.g., 2025-05-07 10:30:00 IDT)
    ts_formatted = ts_aware.strftime("%Y-%m-%d %H:%M:%S %Z")
    # --- *** END CHANGE *** ---
    return f"[{ts_formatted}] [{level.upper()}] [{module}:{func}] {message}"

# === Log functions ===
def log_info(module: str, func: str, message: str):
    """Logs informational messages. Prints only if DEBUG_MODE is True."""
    entry = _format_log_entry("INFO", module, func, message)
    if DEBUG_MODE:
        print(entry)
    # Optionally write INFO to file log even in production
    # else:
    #     try:
    #         with open(LOG_FILE, "a", encoding="utf-8") as f:
    #             f.write(entry + "\n")
    #     except Exception: pass


def _try_log_to_db(level: str, module: str, function: str, message: str, traceback_str: str | None = None, user_id_context: str | None = None, timestamp_utc_iso: str | None = None): # Ensure timestamp is UTC for DB
    """Internal helper to attempt DB logging dynamically. Timestamp for DB should be UTC."""
    global _activity_db_log_func, ACTIVITY_DB_IMPORTED

    if not ACTIVITY_DB_IMPORTED and _activity_db_log_func is None:
        try:
            from tools.activity_db import log_system_event
            _activity_db_log_func = log_system_event
            ACTIVITY_DB_IMPORTED = True
            print(f"[{_timestamp_utc_iso()}] [INFO] [logger:_try_log_to_db] Successfully linked activity_db.log_system_event.")
        except ImportError:
            _activity_db_log_func = None
            ACTIVITY_DB_IMPORTED = False
        except Exception as e:
             print(f"[{_timestamp_utc_iso()}] [ERROR] [logger:_try_log_to_db] Unexpected error linking activity_db.log_system_event: {e}")
             _activity_db_log_func = None
             ACTIVITY_DB_IMPORTED = False

    if _activity_db_log_func:
        try:
            # For DB logging, always use UTC ISO timestamp
            db_ts = timestamp_utc_iso or _timestamp_utc_iso()
            _activity_db_log_func(
                level=level.upper(),
                module=module,
                function=function,
                message=message,
                traceback_str=traceback_str,
                user_id_context=user_id_context,
                timestamp=db_ts
            )
        except Exception as db_log_err:
             ts_iso_fallback = timestamp_utc_iso or _timestamp_utc_iso()
             print(f"[{ts_iso_fallback}] [CRITICAL DB LOG FAIL] [{level.upper()}] [{module}:{function}] DB log failed: {db_log_err} | Original Msg: {message}")
    # else:
        # If DB logging is not available, we don't print a "DB_LOG_SKIP" here for every console log,
        # as console logs (INFO/ERROR/WARNING) will still print to console/file anyway.
        # The DB log attempt is silent if _activity_db_log_func is None.


def log_error(module: str, func: str, message: str, exception: Exception = None, user_id: str | None = None):
    """Logs error messages. Prints/logs to file AND attempts to log to database."""
    level = "ERROR"
    traceback_str = None
    if exception:
        traceback_str = traceback.format_exc()

    # Log to Console/File (uses LOG_TIMEZONE_PYTZ via _format_log_entry)
    entry = _format_log_entry(level, module, func, message)
    if DEBUG_MODE:
        print(entry)
        if traceback_str: print(traceback_str)
    else:
        try:
            with open(LOG_FILE, "a", encoding="utf-8") as f:
                f.write(entry + "\n")
                if traceback_str: f.write(traceback_str + "\n")
        except Exception as file_log_e:
            # Use direct print for critical file log failure
            print(f"[{datetime.now(LOG_TIMEZONE_PYTZ).strftime('%Y-%m-%d %H:%M:%S %Z')}] [CRITICAL] [logger:log_error] Failed to write ERROR to log file {LOG_FILE}: {file_log_e}")

    # Attempt to log to Database (with UTC ISO timestamp)
    _try_log_to_db(level, module, func, message, traceback_str, user_id, _timestamp_utc_iso())


def log_warning(module: str, func: str, message: str, exception: Exception = None, user_id: str | None = None):
    """Logs warning messages. Prints/logs to file AND attempts to log to database."""
    level = "WARNING"
    traceback_str = None
    if exception:
        traceback_str = traceback.format_exc()

    # Log to Console/File (uses LOG_TIMEZONE_PYTZ via _format_log_entry)
    entry = _format_log_entry(level, module, func, message)
    if DEBUG_MODE:
        print(entry)
        if traceback_str and exception: print(f"Warning Exception Info:\n{traceback_str}")
    else:
        try:
            with open(LOG_FILE, "a", encoding="utf-8") as f:
                f.write(entry + "\n")
                if traceback_str and exception: f.write(f"Warning Exception Info:\n{traceback_str}\n")
        except Exception as file_log_e:
            print(f"[{datetime.now(LOG_TIMEZONE_PYTZ).strftime('%Y-%m-%d %H:%M:%S %Z')}] [CRITICAL] [logger:log_warning] Failed to write WARNING to log file {LOG_FILE}: {file_log_e}")

    # Attempt to log to Database (with UTC ISO timestamp)
    _try_log_to_db(level, module, func, message, traceback_str, user_id, _timestamp_utc_iso())

# --- END OF FULL tools/logger.py ---

# --- END OF FILE tools/logger.py ---



================================================================================
üìÑ tools/activity_db.py
================================================================================

# --- START OF FILE tools/activity_db.py ---

# --- START OF FULL tools/activity_db.py ---

import sqlite3
import os
import json
import threading
from datetime import datetime, timezone
from typing import Dict, List, Any, Tuple # <--- ADD Dict, List, Any, Tuple HERE
from tools.logger import log_info, log_error, log_warning

# --- Configuration ---
DATA_SUFFIX = os.getenv("DATA_SUFFIX", "")
DB_DIR = "data"
DB_FILE = os.path.join(DB_DIR, f"whatstasker_activity{DATA_SUFFIX}.db") # Dynamic DB file name
DB_LOCK = threading.Lock() # Lock for thread-safe writes if needed later

# --- Initialization ---
def init_db():
    """Initializes the database: creates directory, file, tables, and indexes if they don't exist."""
    fn_name = "init_db"
    try:
        os.makedirs(DB_DIR, exist_ok=True)
        log_info("activity_db", fn_name, f"Connecting to database: {DB_FILE}")
        # Use 'with' statement for automatic connection management
        with sqlite3.connect(DB_FILE, check_same_thread=False) as conn: # Allow access from different threads if needed by scheduler/web server
            cursor = conn.cursor()
            log_info("activity_db", fn_name, "Ensuring tables and indexes exist...")

            # === Create users_tasks Table ===
            cursor.execute("""
            CREATE TABLE IF NOT EXISTS users_tasks (
                event_id TEXT PRIMARY KEY NOT NULL,
                user_id TEXT NOT NULL,
                type TEXT NOT NULL,
                status TEXT NOT NULL,
                title TEXT NOT NULL,
                description TEXT,
                date TEXT,
                time TEXT,
                estimated_duration TEXT,
                sessions_planned INTEGER DEFAULT 0,
                sessions_completed INTEGER DEFAULT 0,
                progress_percent INTEGER DEFAULT 0,
                session_event_ids TEXT DEFAULT '[]',
                project TEXT,
                series_id TEXT,
                gcal_start_datetime TEXT,
                gcal_end_datetime TEXT,
                duration TEXT,
                created_at TEXT NOT NULL,
                completed_at TEXT,
                internal_reminder_sent TEXT,
                original_date TEXT,
                progress TEXT
            )
            """)
            # === Create Indexes for users_tasks ===
            cursor.execute("CREATE INDEX IF NOT EXISTS idx_tasks_user_id ON users_tasks (user_id)")
            cursor.execute("CREATE INDEX IF NOT EXISTS idx_tasks_user_id_status ON users_tasks (user_id, status)")
            cursor.execute("CREATE INDEX IF NOT EXISTS idx_tasks_user_id_date ON users_tasks (user_id, date)")
            cursor.execute("CREATE INDEX IF NOT EXISTS idx_tasks_project ON users_tasks (project)")
            cursor.execute("CREATE INDEX IF NOT EXISTS idx_tasks_status ON users_tasks (status)")

            # === Create messages Table ===
            cursor.execute("""
            CREATE TABLE IF NOT EXISTS messages (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                timestamp TEXT NOT NULL,
                user_id TEXT NOT NULL,
                direction TEXT NOT NULL,
                content TEXT NOT NULL,
                raw_user_id TEXT,
                bridge_message_id TEXT
            )
            """)
            cursor.execute("CREATE INDEX IF NOT EXISTS idx_messages_timestamp ON messages (timestamp)")
            cursor.execute("CREATE INDEX IF NOT EXISTS idx_messages_user_id ON messages (user_id)")

            # === Create llm_activity Table ===
            cursor.execute("""
            CREATE TABLE IF NOT EXISTS llm_activity (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                timestamp TEXT NOT NULL,
                user_id TEXT NOT NULL,
                agent_type TEXT NOT NULL,
                activity_type TEXT NOT NULL,
                tool_name TEXT,
                tool_call_id TEXT,
                content_summary TEXT,
                details_json TEXT
            )
            """)
            cursor.execute("CREATE INDEX IF NOT EXISTS idx_llm_activity_timestamp ON llm_activity (timestamp)")
            cursor.execute("CREATE INDEX IF NOT EXISTS idx_llm_activity_user_id ON llm_activity (user_id)")
            cursor.execute("CREATE INDEX IF NOT EXISTS idx_llm_activity_type ON llm_activity (activity_type)")

            # === Create system_logs Table ===
            cursor.execute("""
            CREATE TABLE IF NOT EXISTS system_logs (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                timestamp TEXT NOT NULL,
                level TEXT NOT NULL,
                module TEXT NOT NULL,
                function TEXT NOT NULL,
                message TEXT NOT NULL,
                traceback TEXT,
                user_id_context TEXT
            )
            """)
            cursor.execute("CREATE INDEX IF NOT EXISTS idx_system_logs_timestamp ON system_logs (timestamp)")
            cursor.execute("CREATE INDEX IF NOT EXISTS idx_system_logs_level ON system_logs (level)")

            conn.commit() # Commit table/index creations
            log_info("activity_db", fn_name, "Database initialization check complete.")

    except sqlite3.Error as e:
        log_error("activity_db", fn_name, f"Database initialization failed: {e}", e)
        raise # Re-raise critical error

# --- users_tasks Table Functions ---

# Define the expected fields for consistency (matches FIELDNAMES from metadata_store)
TASK_FIELDS = [
    "event_id", "user_id", "type", "status", "title", "description", "date", "time",
    "estimated_duration", "sessions_planned", "sessions_completed", "progress_percent",
    "session_event_ids", "project", "series_id", "gcal_start_datetime",
    "gcal_end_datetime", "duration", "created_at", "completed_at",
    "internal_reminder_sent", "original_date", "progress"
]

def _dict_factory(cursor, row):
    """Converts DB rows into dictionaries."""
    fields = [column[0] for column in cursor.description]
    return {key: value for key, value in zip(fields, row)}

def add_or_update_task(task_data: dict) -> bool:
    """Adds a new task or updates an existing one based on event_id."""
    fn_name = "add_or_update_task"
    event_id = task_data.get("event_id")
    if not event_id:
        log_error("activity_db", fn_name, "Cannot save task: 'event_id' is missing.")
        return False

    # Prepare data: ensure only valid fields, handle JSON, default numerics
    db_params = []
    placeholders = []
    columns = []
    for field in TASK_FIELDS:
        columns.append(field)
        value = task_data.get(field)
        if field == "session_event_ids":
            # Ensure it's a JSON string, default to '[]'
            if isinstance(value, (list, tuple)):
                value = json.dumps(value)
            elif not isinstance(value, str) or not value.strip():
                value = '[]'
        elif field in ["sessions_planned", "sessions_completed", "progress_percent"]:
            # Ensure it's an integer, default to 0
            try:
                value = int(value) if value is not None else 0
            except (ValueError, TypeError):
                value = 0
        # Convert None to NULL for DB, handle other types simply
        db_params.append(value)
        placeholders.append("?")

    sql = f"""
    INSERT INTO users_tasks ({', '.join(columns)})
    VALUES ({', '.join(placeholders)})
    ON CONFLICT(event_id) DO UPDATE SET
    {', '.join([f'{col}=excluded.{col}' for col in columns if col != 'event_id'])}
    """
    # Note: UPSERT syntax `ON CONFLICT...` requires SQLite 3.24.0+

    try:
        with DB_LOCK: # Use lock for write operations if needed later
            with sqlite3.connect(DB_FILE, check_same_thread=False) as conn:
                cursor = conn.cursor()
                cursor.execute(sql, db_params)
                conn.commit()
        log_info("activity_db", fn_name, f"Successfully added/updated task {event_id}")
        return True
    except sqlite3.Error as e:
        log_error("activity_db", fn_name, f"Database error saving task {event_id}: {e}", e)
        return False
    except Exception as e: # Catch other potential errors like JSON issues
        log_error("activity_db", fn_name, f"Unexpected error saving task {event_id}: {e}", e)
        return False

def get_task(event_id: str) -> dict | None:
    """Retrieves a single task by event_id."""
    fn_name = "get_task"
    sql = "SELECT * FROM users_tasks WHERE event_id = ?"
    try:
        with sqlite3.connect(DB_FILE, check_same_thread=False) as conn:
            conn.row_factory = _dict_factory # Return rows as dicts
            cursor = conn.cursor()
            cursor.execute(sql, (event_id,))
            row = cursor.fetchone()
            if row:
                # Decode JSON field
                try:
                    row['session_event_ids'] = json.loads(row.get('session_event_ids', '[]') or '[]')
                except (json.JSONDecodeError, TypeError):
                    log_warning("activity_db", fn_name, f"Failed to decode session_event_ids JSON for task {event_id}. Using empty list.")
                    row['session_event_ids'] = []
            return row # Returns dict or None if not found
    except sqlite3.Error as e:
        log_error("activity_db", fn_name, f"Database error getting task {event_id}: {e}", e)
        return None

def delete_task(event_id: str) -> bool:
    """Deletes a task by event_id."""
    fn_name = "delete_task"
    sql = "DELETE FROM users_tasks WHERE event_id = ?"
    try:
        with DB_LOCK:
            with sqlite3.connect(DB_FILE, check_same_thread=False) as conn:
                cursor = conn.cursor()
                cursor.execute(sql, (event_id,))
                conn.commit()
                if cursor.rowcount > 0:
                    log_info("activity_db", fn_name, f"Successfully deleted task {event_id}")
                    return True
                else:
                    log_warning("activity_db", fn_name, f"Task {event_id} not found for deletion.")
                    return False # Return False if not found
    except sqlite3.Error as e:
        log_error("activity_db", fn_name, f"Database error deleting task {event_id}: {e}", e)
        return False

def list_tasks_for_user(
    user_id: str,
    status_filter: list[str] | None = None, # Allows filtering by multiple statuses
    date_range: tuple[str, str] | None = None,
    project_filter: str | None = None
) -> list[dict]:
    """Lists tasks for a user, optionally filtering by status, date range, and project."""
    fn_name = "list_tasks_for_user"
    sql = "SELECT * FROM users_tasks WHERE user_id = ?"
    params = [user_id]
    conditions = []

    if status_filter:
        # Ensure status_filter is a list or tuple
        if isinstance(status_filter, str):
            status_filter = [status_filter] # Convert single string to list
        if isinstance(status_filter, (list, tuple)) and status_filter:
            # Sanitize statuses just in case
            clean_statuses = [s.lower().strip() for s in status_filter if isinstance(s, str)]
            if clean_statuses:
                 placeholders = ','.join('?' * len(clean_statuses))
                 conditions.append(f"status IN ({placeholders})")
                 params.extend(clean_statuses)
            else:
                 log_warning("activity_db", fn_name, f"Received empty or invalid status_filter list for user {user_id}.")
                 # Decide behavior: fetch all? fetch none? Let's fetch none to be safe.
                 return []

    # If NO status filter is provided, we fetch ALL statuses by default currently.
    # <<<--- This is where the change would happen if we altered the default

    if date_range and len(date_range) == 2:
        conditions.append("date BETWEEN ? AND ?")
        params.extend(date_range)

    if project_filter:
        conditions.append("LOWER(project) = LOWER(?)")
        params.append(project_filter)

    if conditions:
        sql += " AND " + " AND ".join(conditions)

    sql += " ORDER BY date, time"

    results = []
    try:
        with sqlite3.connect(DB_FILE, check_same_thread=False) as conn:
            conn.row_factory = _dict_factory
            cursor = conn.cursor()
            cursor.execute(sql, params)
            rows = cursor.fetchall()
            for row in rows:
                try:
                    row['session_event_ids'] = json.loads(row.get('session_event_ids', '[]') or '[]')
                except (json.JSONDecodeError, TypeError):
                    log_warning("activity_db", fn_name, f"Failed decode session_event_ids JSON for task {row.get('event_id')}. Using empty list.")
                    row['session_event_ids'] = []
                results.append(row)
        return results
    except sqlite3.Error as e:
        log_error("activity_db", fn_name, f"Database error listing tasks for user {user_id}: {e}", e)
        return []


# --- Logging Table Functions ---

def log_system_event(level: str, module: str, function: str, message: str, traceback_str: str | None = None, user_id_context: str | None = None, timestamp: str | None = None): # Added timestamp param
    """Logs errors and warnings to the system_logs table."""
    fn_name = "log_system_event"
    sql = """
    INSERT INTO system_logs (timestamp, level, module, function, message, traceback, user_id_context)
    VALUES (?, ?, ?, ?, ?, ?, ?)
    """
    ts = timestamp or datetime.now(timezone.utc).isoformat(timespec='seconds')+'Z' # Use provided or generate new
    params = (ts, level, module, function, message, traceback_str, user_id_context)
    try:
        with DB_LOCK:
            with sqlite3.connect(DB_FILE, check_same_thread=False) as conn:
                cursor = conn.cursor()
                cursor.execute(sql, params)
                conn.commit()
        # Avoid logging the log success itself to prevent loops
    except sqlite3.Error as e:
        # Fallback to print if DB logging fails
        print(f"CRITICAL DB LOG ERROR: {e} while logging: {params}")

def log_message_db(direction: str, user_id: str, content: str, raw_user_id: str | None = None, bridge_message_id: str | None = None):
    """Logs incoming/outgoing messages to the messages table."""
    fn_name = "log_message_db"
    sql = """
    INSERT INTO messages (timestamp, user_id, direction, content, raw_user_id, bridge_message_id)
    VALUES (?, ?, ?, ?, ?, ?)
    """
    ts = datetime.now(timezone.utc).isoformat(timespec='seconds')+'Z'
    params = (ts, user_id, direction, content, raw_user_id, bridge_message_id)
    try:
        with DB_LOCK:
            with sqlite3.connect(DB_FILE, check_same_thread=False) as conn:
                cursor = conn.cursor()
                cursor.execute(sql, params)
                conn.commit()
    except sqlite3.Error as e:
        print(f"CRITICAL DB LOG ERROR: {e} while logging message: {params}")

def log_llm_activity_db(user_id: str, agent_type: str, activity_type: str, tool_name: str | None = None, tool_call_id: str | None = None, content_summary: str | None = None, details: dict | list | None = None):
    """Logs LLM interactions to the llm_activity table."""
    fn_name = "log_llm_activity_db"
    sql = """
    INSERT INTO llm_activity
    (timestamp, user_id, agent_type, activity_type, tool_name, tool_call_id, content_summary, details_json)
    VALUES (?, ?, ?, ?, ?, ?, ?, ?)
    """
    ts = datetime.now(timezone.utc).isoformat(timespec='seconds')+'Z'
    details_str = None
    if isinstance(details, (dict, list)):
        try:
            details_str = json.dumps(details)
        except TypeError as json_err:
            log_warning("activity_db", fn_name, f"Could not serialize details to JSON for LLM activity: {json_err}. Details: {details}")
            details_str = json.dumps({"error": "Serialization failed", "original_type": str(type(details))})
    elif isinstance(details, str): # Allow passing pre-serialized JSON
        details_str = details

    params = (ts, user_id, agent_type, activity_type, tool_name, tool_call_id, content_summary, details_str)
    try:
        with DB_LOCK:
             with sqlite3.connect(DB_FILE, check_same_thread=False) as conn:
                 cursor = conn.cursor()
                 cursor.execute(sql, params)
                 conn.commit()
    except sqlite3.Error as e:
        print(f"CRITICAL DB LOG ERROR: {e} while logging LLM activity: {params}")

# --- Add this function ---
def update_task_fields(event_id: str, updates: Dict[str, Any]) -> bool:
    """Updates specific fields for a task given its event_id."""
    fn_name = "update_task_fields"
    if not event_id or not updates:
        log_warning("activity_db", fn_name, "Missing event_id or updates dictionary.")
        return False

    # Ensure we only try to update valid columns
    allowed_fields = {f for f in TASK_FIELDS if f != 'event_id'} # Cannot update primary key
    update_cols = []
    update_params = []
    for key, value in updates.items():
        if key in allowed_fields:
            update_cols.append(f"{key} = ?")
            # Handle potential JSON encoding for session_event_ids if needed
            if key == "session_event_ids" and isinstance(value, list):
                 update_params.append(json.dumps(value))
            else:
                 update_params.append(value)
        else:
            log_warning("activity_db", fn_name, f"Ignoring invalid field '{key}' in update for {event_id}.")

    if not update_cols:
        log_warning("activity_db", fn_name, f"No valid fields to update for {event_id}.")
        return False # Nothing to update

    update_params.append(event_id) # Add event_id for the WHERE clause
    sql = f"UPDATE users_tasks SET {', '.join(update_cols)} WHERE event_id = ?"

    try:
        with DB_LOCK:
            with sqlite3.connect(DB_FILE, check_same_thread=False) as conn:
                cursor = conn.cursor()
                cursor.execute(sql, update_params)
                conn.commit()
                if cursor.rowcount > 0:
                    log_info("activity_db", fn_name, f"Successfully updated fields {list(updates.keys())} for task {event_id}")
                    return True
                else:
                    # This could happen if the event_id doesn't exist
                    log_warning("activity_db", fn_name, f"Task {event_id} not found for update or no changes needed.")
                    return False # Return False if no row was updated
    except sqlite3.Error as e:
        log_error("activity_db", fn_name, f"Database error updating task {event_id}: {e}", e)
        return False
    except Exception as e:
        log_error("activity_db", fn_name, f"Unexpected error updating task {event_id}: {e}", e)
        return False

# --- End of added function ---


# --- Initialize DB on module load ---
init_db()

# --- END OF tools/activity_db.py ---

# --- END OF FILE tools/activity_db.py ---



================================================================================
üìÑ users/user_manager.py
================================================================================

# --- START OF FILE users/user_manager.py ---

# --- START OF FILE users/user_manager.py ---

import os
import re
from datetime import datetime, timedelta
from typing import Dict, Any, Optional, List
import traceback
from tools.logger import log_info, log_error, log_warning
from users.user_registry import get_registry, register_user, get_user_preferences

# --- Database Import ---
try:
    import tools.activity_db as activity_db
    DB_IMPORTED = True
    log_info("user_manager", "import", "Successfully imported activity_db.")
except ImportError:
    DB_IMPORTED = False
    class activity_db:
        @staticmethod
        def list_tasks_for_user(*args, **kwargs): return []
    log_error("user_manager", "import", "activity_db not found. Task preloading disabled.")

# --- State Manager Import ---
try:
    from services.agent_state_manager import (
        register_agent_instance,
        get_agent_state,
        initialize_state_store
    )
    AGENT_STATE_MANAGER_IMPORTED = True
except ImportError:
     log_error("user_manager", "import", "AgentStateManager not found.")
     AGENT_STATE_MANAGER_IMPORTED = False
     _user_agents_in_memory: Dict[str, Dict[str, Any]] = {}
     def register_agent_instance(uid, state): _user_agents_in_memory[uid] = state
     def get_agent_state(uid): return _user_agents_in_memory.get(uid)
     def initialize_state_store(ref): global _user_agents_in_memory; _user_agents_in_memory = ref

# --- Service/Tool Imports ---
try:
    from tools.google_calendar_api import GoogleCalendarAPI
    GCAL_API_IMPORTED = True
except ImportError:
    GCAL_API_IMPORTED = False
    GoogleCalendarAPI = None
    log_warning("user_manager","import", "GoogleCalendarAPI not found.")

# --- Token Store Import (NEEDED FOR CHECK) ---
try:
    from tools.token_store import get_user_token
    TOKEN_STORE_IMPORTED = True
except ImportError:
     TOKEN_STORE_IMPORTED = False
     log_error("user_manager", "import", "Failed to import token_store.get_user_token. GCalAPI check will fail.")
     def get_user_token(*args, **kwargs): return None
# ---------------------------------------------

# --- In-Memory State Dictionary Reference ---
_user_agents_in_memory: Dict[str, Dict[str, Any]] = {}
if AGENT_STATE_MANAGER_IMPORTED:
    initialize_state_store(_user_agents_in_memory)


# --- Preload Context ---
def _preload_initial_context(user_id: str) -> list[dict]:
    """Loads initial context (all tasks) for a user from the SQLite database."""
    fn_name = "_preload_initial_context"
    log_info("user_manager", fn_name, f"Preloading initial context for {user_id} from activity_db.")

    if not DB_IMPORTED:
        log_error("user_manager", fn_name, "Database module not imported. Cannot preload context.")
        return []

    try:
        task_list = activity_db.list_tasks_for_user(user_id=user_id)
        log_info("user_manager", fn_name, f"Preloaded {len(task_list)} tasks from DB for {user_id}.")
        return task_list
    except Exception as e:
        log_error("user_manager", fn_name, f"Error preloading context for {user_id} from DB", e, user_id=user_id)
        return []


# --- Agent State Creation ---
def create_and_register_agent_state(user_id: str): # REMOVED TYPE HINT
    """Creates the full agent state dictionary and registers it."""
    fn_name = "create_and_register_agent_state"
    log_info("user_manager", fn_name, f"Creating FULL agent state for {user_id}")
    norm_user_id = re.sub(r'\D', '', user_id)
    if not norm_user_id:
        log_error("user_manager", fn_name, f"Invalid user_id after normalization: '{user_id}'")
        return None

    register_user(norm_user_id)
    preferences = get_user_preferences(norm_user_id)
    if not preferences:
        log_error("user_manager", fn_name, f"Failed to get/create prefs for {norm_user_id} after registration attempt.")
        return None

    # --- Refined GCal Initialization ---
    calendar_api_instance = None
    # Check if GCal enabled in prefs AND necessary libraries/functions are loaded
    if preferences.get("Calendar_Enabled") and GCAL_API_IMPORTED and GoogleCalendarAPI is not None and TOKEN_STORE_IMPORTED:
        # --- MODIFIED CHECK: Try loading token data ---
        token_data = get_user_token(norm_user_id)
        if token_data is not None:
            # --- END MODIFIED CHECK ---
            log_info("user_manager", fn_name, f"Valid token data found for {norm_user_id}. Attempting GCalAPI init.")
            temp_cal_api = None
            try:
                temp_cal_api = GoogleCalendarAPI(norm_user_id)
                if temp_cal_api.is_active():
                    calendar_api_instance = temp_cal_api
                    log_info("user_manager", fn_name, f"GCalAPI initialized and active for {norm_user_id}")
                else:
                    log_warning("user_manager", fn_name, f"GCalAPI initialized but NOT active for {norm_user_id}. Calendar features disabled.")
                    calendar_api_instance = None
            except Exception as cal_e:
                 tb_str = traceback.format_exc()
                 log_error("user_manager", fn_name, f"Exception during GCalAPI initialization or is_active() check for {norm_user_id}. Traceback:\n{tb_str}", cal_e)
                 calendar_api_instance = None
        else:
            # Token data not found (logged by get_user_token)
            log_warning("user_manager", fn_name, f"GCal enabled for {norm_user_id} but no valid token data found via token_store.")
    elif not preferences.get("Calendar_Enabled"):
         log_info("user_manager", fn_name, f"Calendar not enabled for {norm_user_id}, skipping GCal init.")
    # Log if libs were the issue
    elif not GCAL_API_IMPORTED:
         log_warning("user_manager", fn_name, f"GoogleCalendarAPI library not imported, skipping calendar init for {norm_user_id}.")
    elif not TOKEN_STORE_IMPORTED:
         log_warning("user_manager", fn_name, f"token_store not imported, skipping calendar init for {norm_user_id}.")
    # --- End Refined GCal Initialization ---

    initial_context = _preload_initial_context(norm_user_id)

    agent_state = {
        "user_id": norm_user_id,
        "preferences": preferences,
        "active_tasks_context": initial_context,
        "calendar": calendar_api_instance,
        "conversation_history": [],
        "notified_event_ids_today": set()
    }

    try:
        register_agent_instance(norm_user_id, agent_state)
        log_info("user_manager", fn_name, f"Successfully registered agent state for {norm_user_id}")
        return agent_state
    except Exception as e:
        log_error("user_manager", fn_name, f"Failed state registration for {norm_user_id}", e)
        return None

# --- Initialize All Agents ---
def init_all_agents():
    """Initializes states for all users found in the registry."""
    fn_name = "init_all_agents"
    log_info("user_manager", fn_name, "Initializing states for all registered users...")
    registry_data = get_registry()
    registered_users = list(registry_data.keys())
    initialized_count = 0
    failed_count = 0

    if not registered_users:
        log_info("user_manager", fn_name, "No users found in registry.")
        return

    log_info("user_manager", fn_name, f"Found {len(registered_users)} users. Initializing...")
    for user_id in registered_users:
        norm_user_id = re.sub(r'\D', '', user_id)
        if not norm_user_id:
             log_warning("user_manager", fn_name, f"Skipping invalid user_id found in registry: '{user_id}'")
             failed_count += 1
             continue
        try:
            created_state = create_and_register_agent_state(norm_user_id)
            if created_state:
                initialized_count += 1
            else:
                failed_count += 1
        except Exception as e:
            log_error("user_manager", fn_name, f"Unexpected error initializing agent state for user {norm_user_id}", e)
            failed_count += 1

    log_info("user_manager", fn_name, f"Agent state initialization complete. Success: {initialized_count}, Failed: {failed_count}")


# --- Get Agent State ---
def get_agent(user_id: str) -> Optional[Dict]:
    """Retrieves or creates and registers the agent state for a user."""
    fn_name = "get_agent"
    norm_user_id = re.sub(r'\D', '', user_id)
    if not norm_user_id:
         log_error("user_manager", fn_name, f"Cannot get agent state for invalid normalized user_id from '{user_id}'")
         return None

    agent_state = None
    try:
        agent_state = get_agent_state(norm_user_id)
        if not agent_state:
            log_warning("user_manager", fn_name, f"State for {norm_user_id} not in memory. Creating now.")
            agent_state = create_and_register_agent_state(norm_user_id)
    except Exception as e:
         log_error("user_manager", fn_name, f"Error retrieving/creating agent state for {norm_user_id}", e)
         agent_state = None

    return agent_state

# --- END OF FILE users/user_manager.py ---

# --- END OF FILE users/user_manager.py ---



================================================================================
üìÑ users/user_registry.py
================================================================================

# --- START OF FILE users/user_registry.py ---

# --- START OF FILE users/user_registry.py ---

import json
import os
from datetime import datetime
from tools.logger import log_info, log_error, log_warning

DATA_SUFFIX = os.getenv("DATA_SUFFIX", "") # Default to empty for whatsapp mode
USER_REGISTRY_PATH = f"data/users/registry{DATA_SUFFIX}.json" # Dynamic path

# --- UPDATED Default Preferences ---
DEFAULT_PREFERENCES = {
    "status": "new", # 'new', 'onboarding', 'active'
    # Time & Scheduling Preferences
    "TimeZone": None, # REQUIRED during onboarding (e.g., "Asia/Jerusalem", "America/New_York")
    "Work_Start_Time": None, # REQUIRED during onboarding (HH:MM)
    "Work_End_Time": None,   # REQUIRED during onboarding (HH:MM)
    "Work_Days": ["Sunday", "Monday", "Tuesday", "Wednesday", "Thursday"], # Default, modifiable
    "Working_Session_Length": "60", # REQUIRED during onboarding (e.g., "60m", "1.5h")
    # Routine Preferences
    "Morning_Summary_Time": "09:00" , # User local time (HH:MM), default None
    "Evening_Summary_Time": "18:00" , # User local time (HH:MM), default None
    "Enable_Morning": True, # Default enabled if time is set
    "Enable_Evening": True, # Default enabled if time is set
    "Enable_Weekly_Reflection": False, # Future use
    # Notification Preferences (NEW)
    "Notification_Lead_Time": "15m", # Default lead time for event notifications
    # Calendar Integration
    "Calendar_Enabled": False, # Flag if GCal connected
    "Calendar_Type": "", # "Google" or potentially others later
    "email": "", # User's Google email (extracted during auth)
    "token_file": None, # Path to encrypted token file
    # Internal Tracking
    "Last_Sync": "", # ISO 8601 UTC timestamp (e.g., "2025-04-22T15:30:00Z")
    "last_morning_trigger_date": "", # YYYY-MM-DD string
    "last_evening_trigger_date": "", # YYYY-MM-DD string
    # Misc/Future Use
    "Holiday_Dates": [], # List of YYYY-MM-DD strings
}
# --- END OF UPDATED DEFAULT PREFERENCES ---


# Global in-memory registry variable.
_registry = {}

def load_registry():
    """Loads the registry from disk into memory."""
    global _registry
    if os.path.exists(USER_REGISTRY_PATH):
        try:
            with open(USER_REGISTRY_PATH, "r", encoding="utf-8") as f:
                # Handle empty file case
                content = f.read()
                if not content.strip():
                    _registry = {}
                else:
                    f.seek(0) # Go back to start if not empty
                    _registry = json.load(f)
            # Ensure existing users have all default keys
            updated_registry = False
            for user_id, user_data in _registry.items():
                 if "preferences" not in user_data:
                      user_data["preferences"] = DEFAULT_PREFERENCES.copy()
                      updated_registry = True
                 else:
                      for key, default_value in DEFAULT_PREFERENCES.items():
                           if key not in user_data["preferences"]:
                                user_data["preferences"][key] = default_value
                                updated_registry = True
            if updated_registry:
                 log_info("user_registry", "load_registry", "Added missing default preference keys to existing users.")
                 save_registry() # Save immediately if defaults were added

        except (json.JSONDecodeError, IOError) as e:
            log_error("user_registry", "load_registry", f"Failed to load or parse registry file {USER_REGISTRY_PATH}", e)
            _registry = {} # Fallback to empty registry on error
    else:
        _registry = {}
    log_info("user_registry", "load_registry", f"Registry loaded with {len(_registry)} users.")
    return _registry

def get_registry():
    """Returns the in-memory registry. Loads it if not already loaded."""
    global _registry
    # Check if registry is empty dictionary, load only if file exists
    if not _registry and os.path.exists(USER_REGISTRY_PATH):
         load_registry()
    # If still empty after attempt, it's genuinely empty or failed load
    return _registry

# Compatibility alias
def load_registered_users():
    return get_registry()

def save_registry():
    """Saves the current in-memory registry to disk."""
    global _registry
    try:
        # Ensure directory exists
        os.makedirs(os.path.dirname(USER_REGISTRY_PATH), exist_ok=True)
        # Use atomic write pattern
        temp_path = USER_REGISTRY_PATH + ".tmp"
        with open(temp_path, "w", encoding="utf-8") as f:
            json.dump(_registry, f, indent=2, ensure_ascii=False)
        os.replace(temp_path, USER_REGISTRY_PATH)
        # log_info("user_registry", "save_registry", "Registry saved to disk.") # Can be noisy
    except IOError as e:
        log_error("user_registry", "save_registry", f"Failed to write registry file {USER_REGISTRY_PATH}", e)
        if os.path.exists(temp_path):
            try: os.remove(temp_path)
            except OSError: pass
    except Exception as e:
        log_error("user_registry", "save_registry", f"Unexpected error saving registry", e)
        if os.path.exists(temp_path):
             try: os.remove(temp_path)
             except OSError: pass


def register_user(user_id):
    """Registers a new user with default preferences if not already present."""
    reg = get_registry() # Ensures registry is loaded
    if user_id not in reg:
        log_info("user_registry", "register_user", f"Registering new user {user_id}...")
        # Use deep copy to avoid modifying the original DEFAULT_PREFERENCES
        reg[user_id] = {"preferences": DEFAULT_PREFERENCES.copy()}
        save_registry() # Save after adding the new user
        log_info("user_registry", "register_user", f"Registered new user {user_id} with default preferences.")
    # else: User already exists, do nothing silently


def update_preferences(user_id, new_preferences):
    """Updates preferences for a given user and saves the registry."""
    reg = get_registry()
    if user_id in reg:
        # Ensure the preferences key exists and is a dict
        if not isinstance(reg[user_id].get("preferences"), dict):
             reg[user_id]["preferences"] = DEFAULT_PREFERENCES.copy()

        # Validate keys before updating? Optional, but good practice.
        valid_updates = {k: v for k, v in new_preferences.items() if k in DEFAULT_PREFERENCES}
        invalid_keys = set(new_preferences.keys()) - set(valid_updates.keys())
        if invalid_keys:
            log_warning("user_registry", "update_preferences", f"Ignoring invalid preference keys for user {user_id}: {invalid_keys}")

        if not valid_updates:
             log_warning("user_registry", "update_preferences", f"No valid preference keys provided for update for user {user_id}.")
             return False # Or True if ignoring invalid keys is considered success? Let's say False.

        reg[user_id]["preferences"].update(valid_updates)
        save_registry() # Save after updating
        log_info("user_registry", "update_preferences", f"Updated preferences for user {user_id}: {list(valid_updates.keys())}")
        return True
    else:
        log_error("user_registry", "update_preferences", f"User {user_id} not registered, cannot update preferences.")
        return False

def get_user_preferences(user_id):
    """Gets preferences for a user, returns None if user not found."""
    reg = get_registry()
    user_data = reg.get(user_id)
    if user_data:
        # Ensure preferences key exists and return a copy with all defaults ensured
        prefs = user_data.get("preferences", {})
        if not isinstance(prefs, dict):
             prefs = {} # Reset if not a dict

        # Create a copy of defaults, update with user's saved prefs
        # This ensures all keys exist in the returned dict
        full_prefs = DEFAULT_PREFERENCES.copy()
        full_prefs.update(prefs)
        return full_prefs
    else:
        return None

# Load registry into memory on module import.
load_registry()

# (Keep __main__ block for testing if desired)

# --- END OF FILE users/user_registry.py ---

# --- END OF FILE users/user_registry.py ---



================================================================================
üìÑ tests/mock_browser_chat.py
================================================================================

# --- START OF FILE tests/mock_browser_chat.py ---

# --- START OF FULL tests/mock_browser_chat.py ---
# Simplified version using only print() for errors/info

import os
import requests
import json
import time
import threading
from flask import Flask, render_template, request, jsonify
from collections import deque
from datetime import datetime
from dotenv import load_dotenv
import logging # Keep logging import

# --- Load Environment Variables ---
load_dotenv()

# --- Configuration ---
VIEWER_PORT = int(os.getenv("VIEWER_PORT", "5001"))
MAX_MESSAGES = 100

# --- Main Backend Configuration ---
MAIN_BACKEND_PORT = os.getenv("PORT", "8001") # Read from PORT env var like main.py
MAIN_BACKEND_BASE_URL = f"http://localhost:{MAIN_BACKEND_PORT}"
MAIN_BACKEND_INCOMING_URL = f"{MAIN_BACKEND_BASE_URL}/incoming"
MAIN_BACKEND_OUTGOING_URL = f"{MAIN_BACKEND_BASE_URL}/outgoing"
MAIN_BACKEND_ACK_URL = f"{MAIN_BACKEND_BASE_URL}/ack"

# --- Mock User ID (Will be updated by user input) ---
MOCK_USER_ID = "1234" # Default value

# --- In-memory message store (bot messages only) ---
message_store_bot = deque(maxlen=MAX_MESSAGES)
message_lock = threading.Lock()
_stop_polling_event = threading.Event()

# --- Flask App Setup ---
script_dir = os.path.dirname(os.path.abspath(__file__))
template_dir = os.path.join(script_dir, 'templates')
app = Flask(__name__, template_folder=template_dir)
app.secret_key = os.getenv("FLASK_SECRET_KEY", os.urandom(24))

# --- Background Polling Function (Simplified Output) ---
def poll_main_backend():
    """Polls the main backend for outgoing messages and sends ACKs. Prints only errors."""
    print(f"[Polling Thread] Started. Target: {MAIN_BACKEND_OUTGOING_URL}")
    session = requests.Session()
    connection_lost = False
    last_successful_poll = time.time()

    while not _stop_polling_event.is_set():
        try:
            res = session.get(MAIN_BACKEND_OUTGOING_URL, timeout=10)
            res.raise_for_status()
            if connection_lost: print("[Polling Thread] Connection restored."); connection_lost = False
            last_successful_poll = time.time()
            data = res.json()
            messages = data.get("messages", [])

            if messages:
                timestamp = datetime.now().strftime("%H:%M:%S")
                with message_lock:
                    # Filter messages for the MOCK_USER_ID before processing
                    user_messages = [
                        msg for msg in messages
                        if msg.get('user_id') == MOCK_USER_ID # Check if message is for our mock user
                    ]

                    if user_messages:
                        # Add only the relevant messages to the display store
                        for msg in reversed(user_messages):
                            message_content = msg.get('message', '[No message content]')
                            message_id = msg.get('message_id', f'nomockid-{time.time()}')
                            message_store_bot.append({
                                "sender": "bot", "timestamp": timestamp,
                                "content": message_content, "id": message_id
                            })
                            # Send ACK for the processed message
                            try:
                                ack_payload = {"message_id": message_id, "user_id": msg.get("user_id")}
                                ack_res = session.post(MAIN_BACKEND_ACK_URL, json=ack_payload, timeout=3)
                                if ack_res.status_code != 200:
                                    print(f"[Polling Thread WARNING] Failed ACK for {message_id}. Status: {ack_res.status_code}")
                            except Exception as ack_e:
                                print(f"[Polling Thread ERROR] Error sending ACK for {message_id}: {ack_e}")
                    # else: No messages specifically for this MOCK_USER_ID in this poll

            # Reset connection_lost flag if request was successful (even if no messages)
            if not connection_lost and res.status_code == 200:
                 pass # Already reset above if connection_lost was true

        except requests.exceptions.Timeout:
            if not connection_lost and (time.time() - last_successful_poll > 30):
                print(f"[Polling Thread WARNING] Connection lost? Repeated timeouts polling {MAIN_BACKEND_OUTGOING_URL}.")
                connection_lost = True
        except requests.exceptions.RequestException as e:
             error_msg = f"Connection error polling {MAIN_BACKEND_OUTGOING_URL}: {e}"
             if isinstance(e, requests.exceptions.ConnectionError) and "actively refused it" in str(e).lower():
                  error_msg = f"Connection error polling {MAIN_BACKEND_OUTGOING_URL}: Target refused connection. Is main backend running?"
             if not connection_lost: print(f"[Polling Thread ERROR] {error_msg}"); connection_lost = True
        except json.JSONDecodeError as e:
             response_text_snippet = res.text[:100] if 'res' in locals() else 'N/A'
             print(f"[Polling Thread ERROR] Failed JSON decode from {MAIN_BACKEND_OUTGOING_URL}. Response: {response_text_snippet}. Error: {e}")
        except Exception as e:
            print(f"[Polling Thread ERROR] Unexpected error: {e}")
            connection_lost = True
        finally:
            sleep_time = 0.5 if not connection_lost else 2.0
            # Use max to prevent negative sleep times if system clock changes
            time.sleep(max(0, sleep_time))


    print("[Polling Thread] Stopped.")


# --- Flask Routes (Simplified Output) ---
@app.route('/')
def index():
    # Use the globally set MOCK_USER_ID for the title
    return render_template('browser_chat.html', title=f"WhatsTasker Chat (User: {MOCK_USER_ID})")

@app.route('/send_message', methods=['POST'])
def send_message():
    """Receives message from browser, forwards to main backend using the current MOCK_USER_ID."""
    try:
        data = request.get_json()
        message_text = data.get('message')
        if not message_text: return jsonify({"status": "error", "message": "No message content"}), 400

        # Use the globally set MOCK_USER_ID when sending
        user_id_to_send = MOCK_USER_ID
        # print(f"Forwarding message from {user_id_to_send}: {message_text[:50]}...") # Removed info print

        backend_payload = {"user_id": user_id_to_send, "message": message_text}
        backend_timeout = 60

        try:
            response = requests.post(MAIN_BACKEND_INCOMING_URL, json=backend_payload, timeout=backend_timeout)
            response.raise_for_status()
            try:
                ack_data = response.json();
                if not ack_data.get("ack"): print(f"[Flask WARNING] Main backend response ({response.status_code}) missing ACK.")
            except ValueError: print(f"[Flask WARNING] Main backend response ({response.status_code}) not JSON.")
            return jsonify({"status": "ok", "message": "Forwarded to backend"}), 200

        except requests.exceptions.Timeout: print(f"[Flask ERROR] Timeout sending to {MAIN_BACKEND_INCOMING_URL}"); return jsonify({"status": "error", "message": f"Timeout sending to backend"}), 503
        except requests.exceptions.ConnectionError: print(f"[Flask ERROR] Conn refused by {MAIN_BACKEND_INCOMING_URL}"); return jsonify({"status": "error", "message": f"Connection refused by backend"}), 503
        except requests.exceptions.RequestException as e: print(f"[Flask ERROR] Failed forward to backend: {e}"); return jsonify({"status": "error", "message": f"Failed to forward: {e}"}), 500

    except Exception as e: print(f"[Flask ERROR] Error in /send_message: {e}"); return jsonify({"status": "error", "message": str(e)}), 500

@app.route('/get_messages')
def get_messages():
    """Provides ONLY BOT messages to the frontend."""
    with message_lock:
        bot_messages = list(message_store_bot)
    sorted_bot_messages = sorted(bot_messages, key=lambda x: x.get('timestamp', ''))
    return jsonify({"messages": sorted_bot_messages})

@app.route('/clear_messages', methods=['POST'])
def clear_messages():
    """Clears the server's BOT message store."""
    with message_lock:
        message_store_bot.clear()
    print("[Flask INFO] Browser chat BOT messages cleared on server.")
    return jsonify({"status": "ok"}), 200

# --- Main Execution ---
if __name__ == '__main__':
    # --- *** ADDED User ID Prompt *** ---
    print("--- Starting Mock Browser Chat Interface ---")
    try:
        user_input_id = input(f"Enter User ID to simulate (leave blank for default '{MOCK_USER_ID}'): ")
        if user_input_id.strip():
            MOCK_USER_ID = user_input_id.strip() # Use user input if provided
        else:
            # Default is already set, no action needed, but log it
            print(f"Using default User ID: {MOCK_USER_ID}")
            pass
    except Exception as e:
        print(f"[ERROR] Failed to get user input for ID, using default '{MOCK_USER_ID}'. Error: {e}")
    # --- *** END User ID Prompt *** ---

    print(f"Serving chat UI on: http://localhost:{VIEWER_PORT}")
    print(f"Acting as User ID:  {MOCK_USER_ID}") # Display the chosen ID
    print(f"Talking to Backend: {MAIN_BACKEND_INCOMING_URL}")
    print(f"Polling Backend at: {MAIN_BACKEND_OUTGOING_URL}")
    print(f"--------------------------------------------")

    polling_thread = threading.Thread(target=poll_main_backend, daemon=True)
    polling_thread.start()

    try:
        log = logging.getLogger('werkzeug')
        log.setLevel(logging.WARNING)
        app.run(host='0.0.0.0', port=VIEWER_PORT, debug=False, use_reloader=False)
    except KeyboardInterrupt:
        print("\nCtrl+C received, shutting down...")
    except Exception as e:
        print(f"[ERROR] Flask server crashed: {e}")
    finally:
        _stop_polling_event.set()
        print("Waiting for polling thread to stop...")
        polling_thread.join(timeout=2)
        print("Mock browser chat server stopped.")

# --- END OF FULL tests/mock_browser_chat.py ---

# --- END OF FILE tests/mock_browser_chat.py ---



================================================================================
üìÑ tests/templates/browser_chat.html
================================================================================

# --- START OF FILE tests/templates/browser_chat.html ---

<!-- tests/templates/browser_chat.html -->
<!doctype html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>{{ title }}</title>
    <style>
        /* Styles remain the same */
        body { font-family: sans-serif; margin: 0; padding: 0; display: flex; flex-direction: column; height: 100vh; background-color: #f4f4f4; }
        h1 { text-align: center; color: #333; margin: 10px 0; }
        #chat-container { flex-grow: 1; border: 1px solid #ccc; background-color: #fff; margin: 0 10px 10px 10px; overflow-y: auto; padding: 10px; }
        #messages { list-style-type: none; padding: 0; margin: 0; }
        #messages li { margin-bottom: 10px; padding: 8px; border-radius: 5px; word-wrap: break-word; max-width: 80%; clear: both; }
        #messages li.user { background-color: #dcf8c6; margin-left: auto; float: right; text-align: right; }
        #messages li.bot { background-color: #e0e0e0; margin-right: auto; float: left; text-align: left; }
        #messages li.system { background-color: #f0e68c; margin-left: auto; margin-right: auto; text-align: center; font-style: italic; color: #555; max-width: 90%; float: none; font-size: 0.9em;}
        #messages li[dir="rtl"] { text-align: right; }
        #messages li[dir="ltr"] { text-align: left; }
        .msg-meta { font-size: 0.8em; color: #888; display: block; margin-top: 4px; }
        .msg-content { white-space: pre-wrap; }
        #input-area { display: flex; padding: 10px; border-top: 1px solid #ccc; background-color: #eee; }
        #messageInput { flex-grow: 1; padding: 10px; border: 1px solid #ccc; border-radius: 3px; margin-right: 5px;}
        #sendButton { padding: 10px 15px; cursor: pointer; }
        #controls { text-align: right; padding: 0 10px 5px 0; font-size: 0.8em; }
    </style>
</head>
<body>

    <h1>{{ title }}</h1>
    <div id="controls">
        <button id="clearButton" title="Clear messages displayed in this browser window">Clear Display</button>
    </div>

    <div id="chat-container">
        <ul id="messages">
            <!-- Messages will be added dynamically -->
        </ul>
    </div>

    <div id="input-area">
        <input type="text" id="messageInput" placeholder="Type your message..." autocomplete="off">
        <button id="sendButton">Send</button>
    </div>

    <script>
        const messagesContainer = document.getElementById('messages');
        const messageInput = document.getElementById('messageInput');
        const sendButton = document.getElementById('sendButton');
        const clearButton = document.getElementById('clearButton');

        let displayedMessageIds = new Set(); // Track IDs shown in browser
        let isSending = false;
        let isFetching = false;

        function containsHebrew(text) {
            if (!text) return false;
            return /[\u0590-\u05FF]/.test(text);
        }

        // Function to add a single message object to the display UL
        function addMessageToDisplay(msg) {
             if (!msg || !msg.id || displayedMessageIds.has(msg.id)) {
                 return false; // Don't add if no message, no ID, or already displayed
             }

             const li = document.createElement('li');
             const senderClass = msg.sender || 'system';
             li.classList.add(senderClass);

             const isRtl = containsHebrew(msg.content);
             li.setAttribute('dir', isRtl ? 'rtl' : 'ltr');

             const contentSpan = document.createElement('span');
             contentSpan.className = 'msg-content';
             contentSpan.textContent = msg.content;

             const metaSpan = document.createElement('span');
             metaSpan.className = 'msg-meta';
             // Use sender from message object now
             metaSpan.textContent = `[${msg.timestamp}] ${senderClass.toUpperCase()}`;

             li.appendChild(contentSpan);
             li.appendChild(metaSpan);

             messagesContainer.appendChild(li);
             displayedMessageIds.add(msg.id); // Mark as displayed
             return true;
        }

        // Fetches ONLY BOT messages and adds them if not already displayed
        async function fetchAndUpdateMessages() {
            if (isFetching) return;
            isFetching = true;
            let addedNew = false;
             try {
                const response = await fetch('/get_messages'); // Fetches BOT messages from server store
                if (!response.ok) throw new Error(`HTTP error! status: ${response.status}`);
                const result = await response.json();
                const botMessages = result.messages || [];

                botMessages.forEach(msg => {
                    // addMessageToDisplay checks displayedMessageIds
                    if(addMessageToDisplay(msg)) {
                        addedNew = true;
                    }
                });

            } catch (error) {
                console.error('Error fetching messages:', error);
            } finally {
                 isFetching = false;
                 if (addedNew) {
                     messagesContainer.scrollTop = messagesContainer.scrollHeight;
                 }
             }
        }

       async function sendMessage() {
            const messageText = messageInput.value.trim();
            if (!messageText || isSending) return;
            isSending = true;
            sendButton.disabled = true;
            messageInput.disabled = true;

            // 1. Create and display user message OBJECT immediately
             const userTimestamp = new Date().toLocaleTimeString([], { hour: '2-digit', minute: '2-digit', second: '2-digit' });
             const localUserId = `user-${Date.now()}`;
             const userMsg = {
                 sender: 'user',
                 timestamp: userTimestamp,
                 content: messageText,
                 id: localUserId
             };
             if(addMessageToDisplay(userMsg)){ // Add user message to display
                 messagesContainer.scrollTop = messagesContainer.scrollHeight;
             }
             messageInput.value = '';

            // 2. Send message to viewer backend to forward to main backend
            try {
                const response = await fetch('/send_message', { // Send to viewer backend
                    method: 'POST',
                    headers: { 'Content-Type': 'application/json' },
                    body: JSON.stringify({ message: messageText })
                });
                if (!response.ok) {
                    const errorData = await response.json().catch(() => ({ message: response.statusText }));
                    console.error('Error sending message via viewer:', errorData.message);
                    // Add error message to display
                    addMessageToDisplay({ sender: 'system', timestamp: new Date().toLocaleTimeString(), content: `Error sending: ${errorData.message}`, id:`err-${Date.now()}`});
                    messagesContainer.scrollTop = messagesContainer.scrollHeight;
                }
                 // Bot response will arrive via the fetchAndUpdateMessages polling
            } catch (error) {
                console.error('Network error sending message via viewer:', error);
                 addMessageToDisplay({ sender: 'system', timestamp: new Date().toLocaleTimeString(), content: `Network Error: ${error}`, id:`neterr-${Date.now()}`});
                 messagesContainer.scrollTop = messagesContainer.scrollHeight;
            } finally {
                 isSending = false;
                 sendButton.disabled = false;
                 messageInput.disabled = false;
                 messageInput.focus();
            }
        }

       async function clearMessages() {
             displayedMessageIds.clear(); // Clear JS tracking
             messagesContainer.innerHTML = '<li>Clearing...</li>'; // Update display
            try {
                await fetch('/clear_messages', { method: 'POST' }); // Tell server to clear its bot store
                 messagesContainer.innerHTML = '<li>Messages cleared.</li>';
            } catch (error) {
                console.error('Error signaling viewer to clear messages:', error);
                messagesContainer.innerHTML = '<li>Error clearing messages.</li>';
            }
        }

        // Event Listeners
        sendButton.addEventListener('click', sendMessage);
        messageInput.addEventListener('keypress', (e) => {
            if (e.key === 'Enter') { sendMessage(); }
        });
        clearButton.addEventListener('click', clearMessages);

        // Fetch messages periodically
        setInterval(fetchAndUpdateMessages, 1500);

        // Initial fetch
        // No initial fetch needed, or fetch then clear display?
        // Let's start clean
        messagesContainer.innerHTML = '<li>Connecting...</li>'; // Initial message

    </script>

</body>
</html>

# --- END OF FILE tests/templates/browser_chat.html ---



================================================================================
üì¶ Node.js Dependencies Note
================================================================================

# The 'package.json' file lists Node.js dependencies.
# The 'package-lock.json' file (not included) locks specific versions.
# Run 'npm install' in the project root to install these dependencies (including whatsapp-web.js, axios, qrcode-terminal, dotenv, nodemailer).
# The 'node_modules/' directory containing the installed packages is NOT included in this dump.

