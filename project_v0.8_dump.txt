# WhatsTasker Project Code Dump (v0.8 Architecture Target + Scheduler)
# Generated: 2025-04-24 14:09:36

================================================================================
üìÑ README.md
================================================================================

# --- START OF FILE README.md ---

# --- END OF FILE README.md ---


================================================================================
üìÑ WhatsTasker_PRD_08.txt
================================================================================

# --- START OF FILE WhatsTasker_PRD_08.txt ---
ÔªøWhatsTasker: Product Requirements (v0.8 - Orchestrator Focused)
üß≠ Overview
WhatsTasker is a personal productivity assistant designed for individuals seeking to manage their tasks, improve focus, and reduce procrastination through a conversational interface. Accessed via WhatsApp, it functions as an intelligent Time Management Expert, integrating directly with the user's primary calendar (initially Google Calendar) and utilizing an internal metadata store for enhanced tracking. WhatsTasker aims to streamline daily planning, facilitate frictionless task capture, and support helpful routines.
üåü Core System Goal
The system shall act as a proactive and intelligent Time Management Expert. It must understand user context (history, preferences, schedule, tasks), interpret requests accurately, facilitate effective task and schedule management, and anticipate user needs where feasible within the defined scope, moving beyond simple command execution.
üîë Core Functional Requirements
1. Multi-User Support & Identification:
   * Uniquely identify users via their WhatsApp phone number (user_id).
   * Maintain consistent user_id linkage across all system components.
2. Calendar-Based Task & Reminder Management:
   * Direct Calendar Integration: Synchronize confirmed tasks and scheduled events with the user's designated primary calendar (initially Google Calendar).
   * Metadata Augmentation: Maintain an internal metadata store linked to calendar events/internal IDs to track additional details crucial for system operation (e.g., item status [pending, in progress, completed, cancelled], type [task, reminder], estimated effort, project tags, timestamps).
3. Natural Language Interaction (via WhatsApp):
   * Conversational Interface: Enable users to interact using free-form, natural language commands and requests.
   * Contextual Understanding: Maintain and utilize conversational context (recent history, user state) to understand follow-up questions, resolve ambiguities, and provide relevant, coherent responses.
4. User Capabilities & Outcomes:
   * Onboarding: Guide new users through an initial setup process to configure essential preferences (e.g., work parameters, calendar connection authorization) and activate their account.
   * Task/Reminder Capture & Classification:
      * Allow users to efficiently capture intentions (tasks, reminders) via natural language.
      * Intelligently distinguish between simple Reminders (typically requiring only description and time) and effortful Tasks (implying duration/effort and eligibility for scheduling work sessions).
      * Reliably extract key details: description, due date/time, type (task or reminder), and estimated effort (for tasks).
   *    * Task Modification: Enable users to accurately modify details of existing tasks or reminders (e.g., description, due date/time, status, estimated effort) based on their requests.
   * Task & Reminder Status Updates: Allow users to easily update the status of items (e.g., mark as completed, pending), including handling interactive replies to system-generated lists.
   * Task & Reminder Viewing & Filtering: Enable users to view their items (tasks and reminders), with options to filter by relevant criteria such as date range, status (active, completed, etc.), or an associated project tag/label.
   * Clarification Handling: Intelligently request clarification from the user when input is ambiguous, incomplete, or conflicts with existing information, guiding the user towards providing necessary details.
   * Task Scheduling Assistance (for Type: task only):
      * Proactively offer to find and schedule dedicated work time in the calendar for newly created Tasks (unless scheduling was part of the initial request).
      * Upon user request, propose suitable, potentially distributed, time slots for Tasks requiring work sessions, considering task requirements, user preferences, and calendar availability.
      * Allow users to easily confirm and book proposed work session slots into their calendar.
5. Automated Routines & Summaries:
   * Morning Check-in: Provide a concise overview of the user's upcoming day, including scheduled calendar events, due tasks, and reminders. Goal: Enhance daily awareness and readiness.
   * Evening Review: Facilitate a quick end-of-day review:
      * Present Tasks and Reminders scheduled for or due that day.
      * Allow users to easily update the completion status of these items.
      * For Tasks identified as incomplete, prompt the user (optionally asking for a reason) and proactively offer to reschedule the task or its remaining work, potentially suggesting new time slots. Goal: Prevent tasks from being dropped and adapt planning.
   * 6. Fallback Operation:
   * Support basic task and reminder management (capture, status updates, viewing lists) even without calendar integration. Smart scheduling proposals, calendar-based summaries, and rescheduling offers will be unavailable in this mode.
üí° Future Capabilities (Beyond Initial Scope)
* Advanced Scheduling Intelligence (Optimal times, breaks, conflict handling)
* Proactive Time Management Advice & Goal Tracking
* Weekly Reflection Module (Structured prompts, insight storage)
* Recurring Tasks/Reminders Support
* Advanced Daily Planning Assistance ("Just One Thing", backlog suggestions)
* Timed Pre-Event Notifications (Requires external scheduler)
* Shared/Team Task Collaboration Features
* Expanded Calendar Integrations (Outlook, etc.)
* User Subscription Tiers & Monetization
‚ö†Ô∏è MVP Scope & Limitations (Initial Release)
* Calendar: Integration only with Google Calendar; one active calendar per user.
* Task Types: No built-in support for recurring tasks/reminders.
* Collaboration: No shared tasks or team features.
* Proactivity: Limited to offering scheduling for new tasks and rescheduling incomplete tasks in the evening review. No broader unsolicited advice.
* Notifications: Timed pre-event notifications are out of scope.
* Reflection: Weekly reflection module is out of scope.
* Monetization: No user plans or payments.
‚úÖ Acceptance Criteria (High-Level)
* Users can successfully onboard, configure preferences, and connect their Google Calendar.
* The system correctly distinguishes between Tasks and Reminders based on user input.
* Users can reliably create, view (with date/status/project filters), update, and mark tasks/reminders as complete via natural language.
* The system proactively offers scheduling for newly created Tasks.
* The system can propose schedule slots for Tasks based on calendar availability and preferences, and book confirmed slots.
* The system provides functional Morning Check-in and Evening Review summaries.
* The Evening Review correctly identifies incomplete Tasks and offers rescheduling options.
* The system handles clarifications when user input is ambiguous.
* Basic reminder functionality works without calendar integration.
# --- END OF FILE WhatsTasker_PRD_08.txt ---


================================================================================
üìÑ WhatsTasker_SRS_08.txt
================================================================================

# --- START OF FILE WhatsTasker_SRS_08.txt ---
Ôªø# --- START OF FILE WhatsTasker_SRS_08.txt ---
ÔªøWhatsTasker: SRS (v0.8h - Pure LLM Flow & Onboarding)

**1. Goals & Principles**

*   **Primary Goal:** Create a WhatsApp-based personal productivity assistant that acts as an intelligent Time Management Expert. It should understand user context, facilitate task/schedule management via natural language, and integrate seamlessly with the user's primary calendar (Google Calendar initially).
*   **Core Architecture:**
    *   An **OrchestratorAgent** (for active users) and an **OnboardingAgent** (for new/onboarding users) serve as the central reasoning hubs and primary user interfaces, processing user input within its full context. Routing is determined by user status (`new`, `onboarding`, `active`).
    *   Agents leverage Structured Tool Use (via Instructor/Pydantic and OpenAI Function Calling/Tool Use) to delegate specific actions to well-defined Tools.
    *   **Pure LLM Control Flow:** Agents rely **entirely on the LLM** (guided by specific system prompts) to manage conversational state, ask clarifying questions, interpret user replies, decide which tool to call (if any), and formulate responses based on history and tool results. Python code primarily executes validated tool calls requested by the LLM.
    *   **Two-Step LLM Interaction (with Tools):**
        1.  Agent sends context + history + user message + tool definitions to LLM.
        2.  LLM responds either with direct text OR a tool call request.
        3.  If a tool call is requested, the Agent executes the tool via the Tool Layer/Service Layer.
        4.  Agent sends the original context + history + LLM's tool call request + the tool's execution result back to the LLM.
        5.  LLM analyzes the tool result and generates the final text response for the user.
    *   A **Service Layer** (TaskManager, TaskQueryService, ConfigManager, Cheats) encapsulates business logic and data manipulation using plain Python, interacting with persistent stores and external APIs based on Tool invocations or direct cheat code calls.
*   **Modularity & Reusability:** Components (Agents, Tools, Services, Data Stores) are clearly separated. Services contain reusable business logic. Tools provide reliable interfaces for specific actions callable by the LLM.
*   **Reliability & Maintainability:** Prioritize reliable execution via structured Tool Use and Pydantic validation. Code includes type hinting and clear documentation. Conversational logic resides primarily within the LLM prompts.
*   **LLM Interaction:** Utilize OpenAI's Tool Use capabilities, facilitated by Instructor/Pydantic, for Agents to reliably select and invoke Tools with validated parameters. The `propose_task_slots` tool uses a focused LLM sub-call internally for scheduling logic. Input interpretation (e.g., flexible time formats) relies on LLM interpretation guided by prompt instructions before tool calls.
*   **Data Handling & State:**
    *   Persistence: Metadata Store (CSV/DB), User Registry (JSON/DB), and Google Calendar are sources of truth. Encrypted Token Store for credentials. MVP accepts CSV/JSON.
    *   User Status: User state is tracked via a `status` field in preferences (`new`, `onboarding`, `active`), managed by `user_registry.py` and `config_manager.py`. This status dictates routing in `request_router.py`.
    *   Runtime State: A central, thread-safe AgentStateManager manages the in-memory state for each active user (preferences, history, task context snapshot, API clients). State loaded/created on demand via `user_manager.py`.
    *   Context Provision: Relevant context (history, prefs, task/calendar snapshot) loaded via AgentStateManager and provided to the appropriate Agent (Onboarding or Orchestrator) on each interaction.
    *   State Updates: Services invoked by Tools update persistent stores and signal necessary updates to in-memory state via AgentStateManager.
    *   Synchronization: (Deferred) SyncService needed for long-term reconciliation. MVP relies on action-driven state updates.
*   **Error Handling:**
    *   Tools/Services handle expected errors and return clear status/messages via their result dictionaries.
    *   Pydantic validation handles malformed parameters before tool execution.
    *   Agents interpret tool failures (passed back via the second LLM call) and rely on the LLM (guided by prompts) to communicate appropriately with the user.

**2. Architecture Overview**

1.  **Interface Layer:** User message arrives (CLI/WhatsApp) -> `bridge.cli_interface` (FastAPI).
2.  **Routing Layer (`request_router.py`):**
    *   Receives message, normalizes `user_id`.
    *   Ensures user state exists via `user_manager.get_agent` (creates default state with `status="new"` if first time).
    *   Retrieves current `agent_state` (including `preferences` with `status`).
    *   **Status Check & Routing:**
        *   If `status == "new"`: Sends welcome message, calls `config_manager.set_user_status` to change status to `"onboarding"`, returns welcome message.
        *   If message starts with `/` and `CHEATS_IMPORTED`: Calls `services.cheats.handle_cheat_command`, sends result, returns.
        *   If `status == "onboarding"` and `ONBOARDING_AGENT_IMPORTED`: Adds user message to history, loads basic context (prefs, history), invokes `agents.onboarding_agent.handle_onboarding_request`.
        *   If `status == "active"` and `ORCHESTRATOR_IMPORTED`: Adds user message to history, loads full context (prefs, history, tasks, calendar) via `task_query_service.get_context_snapshot`, invokes `agents.orchestrator_agent.handle_user_request`.
        *   Otherwise: Handles error/unknown status.
3.  **Agent Layer (`onboarding_agent.py` or `orchestrator_agent.py`):**
    *   Receives user message and relevant context snapshot.
    *   Loads appropriate system prompt (`onboarding_agent_system_prompt` or `orchestrator_agent_system_prompt`).
    *   Defines the specific set of Tools available for its context (onboarding tools vs. full toolset).
    *   **LLM Call 1 (Planner):** Interacts with LLM via Instructor, providing context, history, user message, and available tool definitions.
    *   LLM decides next step: Respond directly or call a tool.
4.  **Execution Layer:**
    *   **If LLM Responds Directly:** Agent receives text.
    *   **If LLM Calls a Tool:**
        *   Agent receives structured tool call request (Pydantic object).
        *   Agent validates parameters (redundant check, Pydantic/Instructor handle primary).
        *   Agent calls the corresponding Tool function in `agents.tool_definitions.py`.
        *   Tool function interacts with **Service Layer** (`TaskManager`, `ConfigManager`, `TaskQueryService`).
        *   Services perform logic, interact with **Data Layer** (`MetadataStore`, `UserRegistry`, `TokenStore`) and external APIs (`GoogleCalendarAPI`).
        *   Services update persistent data and signal updates to **State Layer** (`AgentStateManager`).
        *   Tool function returns result dictionary (`{"success": bool, "message": str, ...}`) back to the Agent.
5.  **Agent Layer (Response Generation):**
    *   **If LLM Responded Directly (Step 3):** Agent uses that text as the final response.
    *   **If Tool Was Called (Step 4):**
        *   Agent prepares messages for **LLM Call 2 (Responder)**, including the original messages plus the assistant's tool call request and the `role: "tool"` message containing the tool's result (JSON string).
        *   Agent sends these messages to the LLM.
        *   LLM analyzes the tool result and history, then generates the final text response based on its system prompt instructions.
        *   Agent uses the LLM's generated text as the final response.
6.  **Response Flow:** Agent returns the final response string -> Router calls `send_message` (which adds agent response to history and sends via Bridge) -> User.

*Diagram:* (The existing diagram is still largely representative at a high level, showing the main components. The key change is the conditional routing in the Router and the two-step LLM call pattern within the Agents when tools are used.)

**3. Module & Function Breakdown**

*   **Core Infrastructure:** (Largely Unchanged - main.py, cli_interface.py, logger.py, encryption.py, token_store.py, metadata_store.py, google_calendar_api.py, calendar_tool.py, agent_state_manager.py, user_manager.py, user_registry.py)
    *   `bridge/request_router.py`: **Updated** - Handles status checking (`new`, `onboarding`, `active`), welcome message for new users, routing to appropriate agent (onboarding or orchestrator), cheat code detection and routing to `services/cheats.py`.
*   **Service Layer:**
    *   `services/task_manager.py`: **Updated** - Core task/reminder logic. Contains `create_task`, `update_task` (details only), `update_task_status` (excludes cancel), `cancel_item` (handles GCal cleanup + sets status), `schedule_work_sessions`, `cancel_sessions`. Accepts structured input from tools.
    *   `services/config_manager.py`: **Updated** - User configuration logic. Includes `set_user_status`.
    *   `services/task_query_service.py`: **Updated** - Data retrieval. `get_formatted_list` accepts `project_filter`. `get_context_snapshot` provides context for Orchestrator.
    *   `services/cheats.py`: **NEW** - Contains logic for handling cheat code commands (`/help`, `/list`, `/memory`, `/clear`, placeholders for `/morning`, `/evening`).
    *   `services/sync_service.py`: (Deferred)
    *   `services/llm_interface.py`: Initializes Instructor-patched OpenAI client.
*   **Agent Layer:**
    *   `agents/orchestrator_agent.py`: **Updated** - Central reasoning hub for **active** users. Implements pure LLM control flow with two-step LLM calls for tool execution. Uses full toolset. Relies on prompt for conversational logic.
    *   `agents/onboarding_agent.py`: **NEW** - Central reasoning hub for **onboarding** users. Implements pure LLM control flow. Uses limited toolset (`update_user_preferences`, `initiate_calendar_connection`). Relies on specific onboarding prompt to guide preference collection and status update via `update_user_preferences`.
    *   `agents/tool_definitions.py`: **Updated** - Defines Pydantic models and Python functions for the **current, refined toolset** (v0.8f): `create_item`, `update_item_details`, `update_item_status`, `propose_task_slots`, `book_task_slots`, `cancel_task_sessions`, `update_user_preferences`, `initiate_calendar_connection`, `get_formatted_task_list`, `interpret_list_reply`. Includes helpers for scheduler parsing/API access.
    *   **(Obsolete Files):** `agents/intention_agent.py`, `agents/task_agent.py`, `agents/config_agent.py`, `agents/scheduler_agent.py`, `agents/scheduling_logic.py`, `agents/list_reply_logic.py`.
*   **LLM Interaction Layer:** (Primarily `llm_interface.py` and Instructor library)
    *   **(Obsolete Files):** `langchain_chains/chain_builders.py`.

**4. Configuration Files**

*   `config/prompts.yaml`:
    *   **Updated:** Contains `orchestrator_agent_system_prompt` (detailed instructions for pure LLM flow, tool usage, response generation based on tool results, multi-step scheduling flow).
    *   **Updated:** Contains `onboarding_agent_system_prompt` (specific instructions for guiding preference collection, using limited tools, interpreting inputs, calling `update_user_preferences` to set status `active`).
    *   **Updated:** Contains `session_scheduler_system_prompt` (instructions for the sub-LLM called by `propose_task_slots_tool`).
*   `config/messages.yaml`: **Updated** - Includes refined `welcome_confirmation_message` and other standard messages.
*   `config/settings.yaml`: (If used) General settings.
*   `.env`: Secrets.

**5. Key Considerations / Future Work**

*   **Prompt Engineering:** Success heavily relies on the clarity, detail, and robustness of the prompts (especially `orchestrator_agent_system_prompt` and `onboarding_agent_system_prompt`) to guide the LLM's reasoning, interpretation, tool use, and conversational state management. Continuous refinement will be needed.
*   **LLM Reliability for State:** The pure LLM flow depends on the LLM maintaining context and correctly interpreting history for multi-turn interactions (like scheduling confirmations). This might be less reliable than explicit state management for very complex flows.
*   **Error Handling & Presentation:** The LLM needs to effectively interpret `success: false` results from tools and present user-friendly error messages based on the provided `message` field, guided by the prompt.
*   **Input Flexibility:** Relies on LLM's ability (guided by prompt examples/instructions) to parse flexible user inputs (like time formats) before calling tools. May require prompt adjustments or fallback to clarification if LLM fails interpretation.
*   **Synchronization (`SyncService`):** Still deferred but crucial for long-term data consistency.
*   **Testing:** Requires testing the agents' ability to follow prompts, select/invoke tools correctly, handle tool results, and manage conversation flow, including edge cases and clarifications. Mocking LLM responses is essential.
*   **(Future):** Implement remaining features (Summaries, Reflection, Recurring Tasks, etc.) likely as new Tools callable by the Orchestrator, potentially requiring prompt updates.

# --- END OF FILE WhatsTasker_SRS_08.txt ---
# --- END OF FILE WhatsTasker_SRS_08.txt ---


================================================================================
üìÑ requirements.txt
================================================================================

# --- START OF FILE requirements.txt ---
# --- START OF FILE requirements.txt ---
# Web Framework & Server
fastapi>=0.110.0,<0.112.0
uvicorn[standard]>=0.29.0,<0.30.0

# Langchain Core & OpenAI Integration
langchain>=0.1.16,<0.2.0
langchain-core>=0.1.40,<0.2.0
langchain-openai>=0.1.3,<0.2.0

# Google API Libraries
google-api-python-client>=2.120.0,<3.0.0
google-auth-oauthlib>=1.2.0,<2.0.0
google-auth>=2.29.0,<3.0.0

# Configuration & Environment
python-dotenv>=1.0.1,<2.0.0
PyYAML>=6.0.1,<7.0.0

# Utilities
requests>=2.31.0,<3.0.0
pytz>=2024.1
cryptography>=42.0.0,<43.0.0
PyJWT>=2.8.0,<3.0.0

# Pydantic (Core dependency for FastAPI & Langchain)
pydantic>=2.7.0,<3.0.0

# --- ADDED FOR SCHEDULING ---
APScheduler>=3.10.0,<4.0.0
# --------------------------

# Optional: If using pandas checks (e.g., pd.isna) - uncomment if needed
# pandas>=2.0.0,<3.0.0

# --- END OF FILE requirements.txt ---
# --- END OF FILE requirements.txt ---


================================================================================
üìÑ config\prompts.yaml
================================================================================

# --- START OF FILE config\prompts.yaml ---
# --- START OF FILE config/prompts.yaml ---

# =====================================================
# Prompt(s) for OrchestratorAgent (v0.8 Flow Option 1: Find Time First)
# =====================================================
orchestrator_agent_system_prompt: |
  You are WhatsTasker, an expert Time Management Assistant communicating via WhatsApp. Your goal is to help the user manage their tasks, reminders, and schedule effectively and conversationally by understanding their requests and utilizing the appropriate tools. **You control the entire conversation flow.**

  **Core Principles:**
    - Be helpful, concise, and friendly.
    - Use the provided Conversation History extensively.
    - Differentiate between simple 'reminders' (alerts) and effortful 'tasks' (work items).
    - **If essential information is missing** for the *intended action*, **ask a clear clarifying question**.
    - **Decide which tool to call** based on the user's intent and the current conversation state. Use the correct tool for the job (`create_reminder`, `create_task`, `propose_task_slots`, `finalize_task_and_book_sessions`, etc.).
    - **After a tool executes**, analyze its result ('success', 'message', 'proposed_slots', 'search_context', 'item_id', etc.).
    - **Formulate the final response** based on the tool result and history. Confirm success, relay failure messages, present data (like proposed slots), or ask the *next* logical question.
    - Rely on the LLM (you!) for interaction decisions and message generation.

  **Flow for Creating Reminders:**
  1. User asks to create a reminder (e.g., "remind me X at Y", "add reminder Z for tomorrow").
  2. Gather required info: `description`, `date`, optional `time`. Ask if missing.
  3. **Call `create_reminder` Tool:** Provide the gathered parameters.
  4. Confirm outcome based on tool result.

  **Flow for Creating Tasks (WITHOUT Scheduling):**
  1. User asks to create a task WITHOUT explicitly asking to schedule time (e.g., "add task X due Friday", "new task Y takes 2 hours").
  2. Gather required info: `description`, `date`, optional `estimated_duration`. Ask if missing.
  3. **Call `create_task` Tool:** Provide the gathered parameters. This tool only creates the metadata record.
  4. Confirm task creation based on tool result. **Optionally, ask if the user wants to schedule work sessions for the task now** (if it has a duration). If yes, initiate the 'Finding Time & Scheduling' flow in the next turn using `propose_task_slots`.

  **Flow for Finding Time & Scheduling Tasks (NEW):**
  1.  **Identify Request:** User explicitly asks to find time or schedule a task (e.g., "find time for X", "schedule Y for 3 hours next week").
  2.  **Gather Search Info:** Ensure you have `duration` and `timeframe`. Ask if missing. Use the task description for context if provided. Ask about `split_preference` if ambiguous.
  3.  **Call `propose_task_slots` Tool:** Provide `duration`, `timeframe`, optional `description`, optional `split_preference`.
  4.  **Present Slots:** Receive `proposed_slots` and `search_context` from the tool. Present the slots clearly.
  5.  **Get Confirmation:** Wait for user confirmation (e.g., "yes", "book slot 1"). Handle rejections/requests for more options by potentially calling `propose_task_slots` again.
  6.  **Call `finalize_task_and_book_sessions` Tool (IMPORTANT):** Once slots are approved, call this specific tool. You MUST provide:
      *   `approved_slots`: The list of slot dictionaries the user confirmed.
      *   `search_context`: The *exact* context dictionary returned by the `propose_task_slots` tool.
      *   Optional `project` tag if provided by user.
  7.  **Confirm Outcome:** Analyze the result of `finalize_task_and_book_sessions`. Confirm task creation AND session booking.

  **Critical Rules for Tool Usage:**
    - **Rule 1: Use the Correct Creation/Scheduling Tool:** Use `create_reminder` for reminders. Use `create_task` for task metadata *only*. Use the `propose_task_slots` -> `finalize_task_and_book_sessions` sequence ONLY when the user explicitly asks to schedule or find time for a task (or confirms they want to schedule after a task was created).
    - **Rule 2: Pass Context & Slots:** When calling `finalize_task_and_book_sessions`, you **MUST** include the `search_context` (from `propose_task_slots`) and the user-`approved_slots`.
    - **Rule 3: One Primary Action per Turn:** Decide the *single next step*: ask clarification, call ONE tool, or respond directly.

  **Context Provided:**
    - Current Reference Time ({timezone}): Date: YYYY-MM-DD, Day: Weekday, Time: HH:MM.
    - Current User Preferences (JSON Object): User settings.
    - Recent Conversation History: Use this. **Pay close attention to `tool` role messages for results like `search_context`.**
    - Relevant Active Items (List of JSON Objects): Snapshot.
    - Upcoming Calendar Events (List of JSON Objects): Snapshot.

  **Example Flow (Finding Time First - Refined Tools):**

  1.  User: "Need to schedule 'Update Report', estimate 2 hours, sometime next Thursday."
  2.  You have description, duration, timeframe. **Call `propose_task_slots`** with `description="Update Report"`, `duration="2h"`, `timeframe="next Thursday"`.
  3.  Tool returns `success: true`, `proposed_slots: [...]`, `search_context: {"description": "Update Report", ...}`.
  4.  You present slots: "...Shall I book these?"
  5.  User: "Yes book them"
  6.  Retrieve `proposed_slots` and `search_context` from history. **Call `finalize_task_and_book_sessions`** with `approved_slots=[...]` and `search_context={...}`.
  7.  Tool runs (creates metadata AND books sessions). Returns `success: true`, `item_id: "local_xyz"`, `booked_count: 2`, `message: "..."`.
  8.  You respond: "Great! Task 'Update Report' created and I've booked the 2 work sessions..."

  **Available Tools & Required Parameter Structures (SEPARATED):**

  1.  **`create_reminder`**: Creates a simple reminder, potentially adding to GCal if time is given. Use only for reminders.
      *   `description` (string, required), `date` (string, required, YYYY-MM-DD), `time` (string, optional, HH:MM), `project` (string, optional)
  2.  **`create_task`**: Creates task metadata ONLY (no GCal interaction). Use when user adds a task but doesn't request scheduling.
      *   `description` (string, required), `date` (string, required, YYYY-MM-DD), `estimated_duration` (string, optional), `project` (string, optional)
  3.  **`propose_task_slots`**: Finds available work session slots BEFORE task creation. Use when user asks to schedule/find time for a task. Requires `duration` and `timeframe`. Returns proposed slots and search context.
      *   `duration` (string, required), `timeframe` (string, required), `description` (string, optional), `split_preference` (string, optional, "continuous"|"separate"), `num_options_to_propose` (integer, optional, default 3)
  4.  **`finalize_task_and_book_sessions`**: Creates task metadata AND books approved GCal sessions. Use **only** after `propose_task_slots` succeeds and user approves slots. Requires `approved_slots` and `search_context`.
      *   `search_context` (object, required), `approved_slots` (list of objects, required), `project` (string, optional)
  5.  **`update_item_details`**: Modifies core details ONLY (desc, date, time, estimate, project). Not status. Requires existing `item_id`.
      *   `item_id` (string, required), `updates` (object, required, non-empty, allowed keys: description, date, time, estimated_duration, project)
  6.  **`update_item_status`**: Changes status OR cancels/deletes item. Requires existing `item_id`.
      *   `item_id` (string, required), `new_status` (string, required, "pending"|"in_progress"|"completed"|"cancelled")
  7.  **`update_user_preferences`**: Changes user settings.
      *   `updates` (object, required, non-empty)
  8.  **`initiate_calendar_connection`**: Starts GCal OAuth flow. No parameters needed.
  9.  **`cancel_task_sessions`**: Deletes specific GCal work sessions for an **existing** Task. Requires `item_id`.
      *   `task_id` (string, required), `session_ids_to_cancel` (list of strings, required, non-empty)
  10. **`interpret_list_reply`**: (Placeholder) Parses replies to numbered lists. Requires `list_mapping`.
      *   `user_reply` (string, required), `list_mapping` (object, required)
  11. **`get_formatted_task_list`**: Gets a filtered/formatted list.
      *   `date_range` (list of strings, optional), `status_filter` (string, optional), `project_filter` (string, optional)

  **Your Task Summary:**
    Read history/context -> Understand user message -> Decide next step (clarify, call tool, respond directly) -> **Adhere to Critical Rules (Use correct tool, Pass Context)** -> If calling tool: ensure params valid -> If tool called: wait for result -> Analyze tool result -> Formulate final response/next step based *only* on history and tool results.

# =====================================================
# Prompt(s) for ConfigAgent (Keep as is for now)
# =====================================================
config_agent_system_prompt: |
  You are the Configuration Agent for WhatsTasker...
  # ... (rest of config agent prompt remains unchanged) ...

config_agent_human_prompt: |
  **Current Situation Analysis:**
  # ... (rest of config agent prompt remains unchanged) ...

# =====================================================
# OBSOLETE PROMPTS
# =====================================================
# task_agent_system_prompt: | ...
# task_agent_human_prompt: | ...
# list_reply_agent_system_prompt: | ...
# list_reply_agent_human_prompt: | ...

# =====================================================
# Prompt(s) for Session Scheduling LLM (Used BY propose_task_slots TOOL)
# =====================================================
session_scheduler_system_prompt: |
  You are an expert Scheduler assistant used by the propose_task_slots tool for WhatsTasker.
  Your goal is to propose a schedule of work sessions for a specific task, distributing them reasonably over the available time, based on user preferences, task details, and existing calendar events.

  **Core Task:** Given the task details, user preferences, existing calendar events, the number of slots requested (`num_slots_requested`), and the desired duration for *each* slot (`user_session_length` - which might be the user's preference OR the task's total duration if only 1 slot is requested), generate a list of proposed work session slots.

  **Input Variables Provided:**
  - Task Description: {task_description}
  - Task Due Date: {task_due_date} (YYYY-MM-DD) - Use for context and buffer.
  - Task Estimated Duration: {task_estimated_duration} (e.g., "3h", "90m") - Total work estimate.
  - User Working Days: {user_working_days} (List of strings)
  - User Work Start Time: {user_work_start_time} (HH:MM)
  - User Work End Time: {user_work_end_time} (HH:MM)
  - User Session Length: {user_session_length} (String, e.g., "60m", "3h") - **This is the DURATION of EACH slot you need to find.** It might be the user's preferred length OR the total task duration if num_slots_requested=1.
  - Existing Calendar Events (JSON list): {existing_events_json} (Format: [{"start_datetime": "...", "end_datetime": "...", "summary": "..."}, ...])
  - Current Date: {current_date} (YYYY-MM-DD)
  - Number of Slots to Propose: {num_slots_requested} (Integer) - Find exactly this many slots.

  **Processing Logic:**
  1.  **Calculate Slot Duration:** Determine the required slot duration in minutes from the provided `user_session_length` input variable.
  2.  **Identify Available Time Slots:**
      - Consider dates from the day after `current_date` up to **1-2 days BEFORE** `task_due_date`.
      - Filter these dates based on `user_working_days`.
      - Within each valid date, consider the time window between `user_work_start_time` and `user_work_end_time`.
      - Check the `existing_events_json` for conflicts. An existing event conflicts if its time range overlaps *at all* with a potential session slot of the **required duration** calculated in step 1.
      - Find time slots within the working hours that are free and match the **required duration**.
  3.  **Select & Distribute Sessions:** From the available slots, select exactly `num_slots_requested` sessions. **Attempt to distribute these sessions reasonably** across the available period (between tomorrow and the buffer before the due date). Don't bunch them all up unless necessary. Prioritize earlier dates if distribution allows. Ensure sufficient buffer before the due date.
  4.  **Calculate End Times:** For each selected session start time (`date`, `time`), calculate the corresponding `end_time` by adding the **required duration** (from step 1). Ensure the `end_time` does not exceed `user_work_end_time`.
  5.  **Format Output:** Create the JSON output with unique `slot_ref` (starting from 1) for each proposal. Ensure the response is ONLY the JSON object.

  **Output Format Requirements:**
  Respond ONLY with a single, valid JSON object containing exactly two keys:
  1.  `"proposed_sessions"`: A JSON list of proposed sessions. Each element MUST be a dictionary with "slot_ref" (integer, starting from 1), "date" (YYYY-MM-DD), "time" (HH:MM - start time), and "end_time" (HH:MM) keys. Return an empty list `[]` if no suitable slots found.
  2.  `"response_message"`: A user-facing message summarizing the proposal or explaining failure.
      - Success Example: "Okay, I found {N} potential slots for '{Task Description}'."
      - Failure Example: "I looked at your calendar but couldn't find {N} free slots for '{Task Description}' (each needing {duration} duration) before the due date based on your preferences. You might need to adjust the task details or your work settings."

session_scheduler_human_prompt: |
  **Task Details:**
  - Description: {task_description}
  - Due Date: {task_due_date}
  - Estimated Duration: {task_estimated_duration}

  **User Preferences & Slot Request:**
  - Working Days: {user_working_days}
  - Work Start Time: {user_work_start_time}
  - Work End Time: {user_work_end_time}
  - **Duration of EACH slot to find**: {user_session_length}
  - **Number of Slots to Find**: {num_slots_requested}

  **Calendar Context:**
  - Today's Date: {current_date}
  - Existing Events (JSON): {existing_events_json}

  **Your Task:** Propose exactly {num_slots_requested} schedule slots, each of duration {user_session_length}. Distribute slots reasonably, leaving buffer before due date. Respond ONLY in the specified JSON format (`proposed_sessions` with slot_ref, date, time, end_time; `response_message`). Ensure JSON validity
session_scheduler_human_prompt: |
  **Task Details:**
  - Description: {task_description}
  - Due Date: {task_due_date}
  - Estimated Duration: {task_estimated_duration}

  **User Preferences & Slot Request:**
  - Working Days: {user_working_days}
  - Work Start Time: {user_work_start_time}
  - Work End Time: {user_work_end_time}
  - **Duration of EACH slot to find**: {user_session_length}
  - **Number of Slots to Find**: {num_slots_requested}

  **Calendar Context:**
  - Today's Date: {current_date}
  - Existing Events (JSON): {existing_events_json}

  **Your Task:** Propose exactly {num_slots_requested} schedule slots, each of duration {user_session_length}. Distribute slots reasonably, leaving buffer before due date. Respond ONLY in the specified JSON format (`proposed_sessions` with slot_ref, date, time, end_time; `response_message`). Ensure JSON validity.


# =====================================================
# Prompt(s) for OnboardingAgent (Keep as is)
# =====================================================
onboarding_agent_system_prompt: |
  You are the Onboarding Assistant for WhatsTasker. Your goal is to guide a new user through the initial setup process conversationally by collecting essential preferences and ensuring data is in the correct format BEFORE calling any tools.

  **Core Task & Rules:**
  1. Examine the `Current User Preferences` provided in the context.
  2. Identify the *first* essential preference that is missing (`null`). The required preferences and their **STRICT required formats** are:
     *   `TimeZone`: Must be a valid Olson Timezone Name (e.g., `America/New_York`, `Europe/London`, `Asia/Jerusalem`).
     *   `Work_Start_Time`: Must be in **HH:MM format (24-hour clock)** (e.g., `09:00`, `17:30`).
     *   `Work_End_Time`: Must be in **HH:MM format (24-hour clock)** (e.g., `18:00`).
     *   `Preferred_Session_Length`: Must be a string indicating duration (e.g., `60m`, `90m`, `1.5h`, `2h`).
  3. Ask the user a clear, friendly question for **only that specific missing preference**. Clearly state the required format in your question.
  4. **Interpret the user's reply.** Try to understand common variations:
     *   For **Time**: Convert inputs like `6pm` to `18:00`, `9am` to `09:00`, `noon` to `12:00`, `midnight` to `00:00`. Handle `0900` as `09:00`, `1730` as `17:30`.
     *   For **TimeZone**: If the user gives a city like `london` or `new york` or `tel aviv`, try to infer the correct Olson name (`Europe/London`, `America/New_York`, `Asia/Jerusalem`). If they give an abbreviation like `EST`, try to map it to a common Olson name (but be cautious, ask if unsure, e.g., "Do you mean America/New_York for EST?").
     *   For **Duration**: Convert `1 hour` to `60m`, `1.5 hours` to `90m`, `2 hours` to `2h`.
  5. **If you can confidently convert the user's reply to the EXACT required format:** Call the `update_user_preferences` tool. The parameters MUST be `{{"updates": {{KEY: "FORMATTED_VALUE"}}}}`. Example: `{{"updates": {{"Work_End_Time": "18:00"}}}}`.
  6. **If you CANNOT confidently interpret or convert the user's reply to the required format:** DO NOT call the tool. Instead, **ask the user for clarification**, reminding them of the specific format needed. Example: "Sorry, I need the time in HH:MM format like 09:00 or 17:30. Could you please provide your work start time again?"
  7. After the tool runs successfully (you'll see `success: true` in the tool result), repeat from step 1: check the preferences for the *next* missing item and ask for it.
  8. Once all four required preferences (`TimeZone`, `Work_Start_Time`, `Work_End_Time`, `Preferred_Session_Length`) have been collected and successfully saved via the tool:
     - Ask the user if they want to connect Google Calendar: "I've got your basic preferences setup. Would you like to connect your Google Calendar now...? (yes/no)"
     - If 'yes': Call `initiate_calendar_connection`. Relay the message/URL from the tool result. End interaction for now.
     - If 'no': Acknowledge their choice. Proceed to the final step.
  9. **Final Step (After prefs collected AND calendar handled):** Call `update_user_preferences` with `{{"updates": {{"status": "active"}}}}`.
  10. After the final status update succeeds, respond with a concluding message like: "Great, setup is complete! ..."

  **Important:** Be precise. Ask for one thing. Interpret carefully. Format correctly *before* calling the tool. Ask again if unsure or if the format is wrong.

  **Context Provided:**
  - Current User Preferences (JSON Object): Check for `null` values for required keys.
  - Conversation History: See what you last asked for and how the user replied.

  **Tools Available During Onboarding:**
  - `update_user_preferences`: `{{"updates": {{KEY: "VALUE"}}}}` (VALUE must be correctly formatted by you).
  - `initiate_calendar_connection`: No parameters.

onboarding_agent_human_prompt: |
  Current Preferences:
  ```json
  {current_preferences_json}
    History:
    {conversation_history}
    User message: {message}
    Your Task: Based on the system instructions, determine the next step: Ask for the next missing REQUIRED preference (TimeZone, Work_Start_Time, Work_End_Time, Preferred_Session_Length), ask about calendar connection, call a tool (update_user_preferences or initiate_calendar_connection), or finalize onboarding (by calling update_user_preferences with status: active). Formulate your response or tool call.
# --- END OF FILE config/prompts.yaml ---
# --- END OF FILE config\prompts.yaml ---


================================================================================
üìÑ config\messages.yaml
================================================================================

# --- START OF FILE config\messages.yaml ---
# config/messages.yaml

# --- Add these ---
welcome_confirmation_message: |
  Hello! üëã Ready to take control of your time and focus? Welcome to WhatsTasker!
  Imagine having a personal Time Management Expert right here in WhatsApp, helping you:
  üß† **Capture Everything Instantly:** Just text me your tasks and reminders like you normally would ‚Äì "remind me about the team sync at 10", "add task finish the report by Friday, takes about 3 hours". No more switching apps!
  üìÖ **Stay Aligned with Your Calendar:** I connect directly to your Google Calendar, ensuring your tasks and scheduled work time fit seamlessly into your day.
  ‚è±Ô∏è **Beat Procrastination:** Need focused time for important tasks? I can find empty slots in your calendar and block out dedicated work sessions for you.
  ‚òÄÔ∏è **Start Strong, End Clear:** Get ahead with helpful morning check-ins and wrap up your day effectively with quick evening reviews to ensure nothing falls through the cracks (coming soon!).
  Think of me as your partner in productivity, helping you plan better, focus deeply, and get more done with less stress.
  Ready to unlock WhatsTasker? 

setup_starting_message: "Great! Let's set things up. I'll ask a few questions to configure your preferences."
setup_declined_message: "Okay, no problem. Just message me again whenever you're ready to set things up!"
ask_confirmation_again_message: "Sorry, I didn't quite understand that. Are you ready to start the setup process? (yes/no)"
user_registered_already_message: "Welcome back! How can I help you today?" # Optional: For returning users found by get_agent

# --- Keep existing messages ---
generic_error_message: "Sorry, an unexpected error occurred. Please try again."
intent_parse_error_message: "Sorry, I had trouble understanding the structure of that request."
intent_unknown_message: "Sorry, I'm not sure how to help with that. You can ask me to add tasks, list tasks, or change settings."
intent_clarify_message: "Sorry, I didn't quite understand that. Could you please rephrase?"
# ... add any other messages you have ...
# --- END OF FILE config\messages.yaml ---


================================================================================
üìÑ config\settings.yaml
================================================================================

# --- START OF FILE config\settings.yaml ---

# --- END OF FILE config\settings.yaml ---


================================================================================
üìÑ main.py
================================================================================

# --- START OF FILE main.py ---
# --- START OF FILE main.py ---

import os
import sys
import asyncio
import signal # Import signal for shutdown
from dotenv import load_dotenv
load_dotenv()

from tools.logger import log_info, log_error, log_warning
import uvicorn
from users.user_manager import init_all_agents
import traceback

# --- Import Scheduler Service ---
try:
    from services.scheduler_service import start_scheduler, shutdown_scheduler
    SCHEDULER_IMPORTED = True
except ImportError:
    log_error("main", "import", "Scheduler service not found. Background tasks disabled.")
    SCHEDULER_IMPORTED = False
    # Define dummy functions if import fails
    def start_scheduler(): pass
    def shutdown_scheduler(): pass
# ----------------------------

def main():
    log_info("main", "main", "Initializing agent states...")
    try:
        init_all_agents()
        log_info("main", "main", "Agent state initialization complete.")
    except Exception as init_e:
        log_error("main", "main", "CRITICAL error during init_all_agents.", init_e)
        sys.exit(1) # Exit if agent init fails

    # --- Start Scheduler ---
    if SCHEDULER_IMPORTED:
        try:
            log_info("main", "main", "Starting scheduler service...")
            # Assume start_scheduler now returns True on success, False on failure
            scheduler_started = start_scheduler()
            if scheduler_started:
                log_info("main", "main", "Scheduler service started successfully.")
            else:
                # Log the failure clearly, decide if the app should exit
                log_error("main", "main", "Scheduler service FAILED to start. Background tasks disabled.")
                # sys.exit(1) # Optional: Exit if scheduler is critical
        except Exception as sched_e:
            log_error("main", "main", "CRITICAL error starting scheduler service.", sched_e)
            # Decide if you want to exit if scheduler fails to start
            # sys.exit(1)
    # -----------------------

    reload_enabled = os.getenv("APP_ENV", "production").lower() == "development"
    log_level = "debug" if reload_enabled else "info"

    log_info("main", "main", f"Starting FastAPI server (Reload: {reload_enabled}, Log Level: {log_level})...")

    config = uvicorn.Config(
        "bridge.cli_interface:app",
        host="0.0.0.0",
        port=8000,
        reload=reload_enabled,
        access_log=False,
        log_level=log_level
    )
    server = uvicorn.Server(config)

    # Graceful shutdown handling
    loop = asyncio.get_event_loop()
    stop_event = asyncio.Event()

    def handle_signal(sig, frame):
        log_warning("main", "handle_signal", f"Received signal {sig}. Initiating shutdown...")
        # Signal the main loop to stop
        loop.call_soon_threadsafe(stop_event.set)

    # Register signal handlers for graceful shutdown
    for sig in (signal.SIGINT, signal.SIGTERM):
        try:
             # Use add_signal_handler for asyncio loops
             loop.add_signal_handler(sig, lambda s=sig: handle_signal(s, None))
        except NotImplementedError:
             # Fallback for environments where add_signal_handler isn't available (like Windows sometimes)
             signal.signal(sig, handle_signal)


    async def main_server_task():
        try:
            await server.serve()
        finally:
            # Ensure stop_event is set if server.serve() exits unexpectedly
            if not stop_event.is_set():
                 stop_event.set()

    async def shutdown_manager():
         # Wait until the stop event is set (by signal or server exit)
         await stop_event.wait()
         log_info("main", "shutdown_manager", "Stop event received, initiating graceful shutdown...")

         # --- Shutdown Scheduler ---
         if SCHEDULER_IMPORTED:
             try:
                 log_info("main", "shutdown_manager", "Shutting down scheduler service...")
                 shutdown_scheduler()
                 log_info("main", "shutdown_manager", "Scheduler service shut down.")
             except Exception as sched_down_e:
                 log_error("main", "shutdown_manager", "Error shutting down scheduler.", sched_down_e)
         # -------------------------

         # Give server a chance to stop cleanly (Uvicorn handles its own shutdown on signal)
         # If server.serve() hasn't finished, signal it if needed (complex, Uvicorn usually handles)
         # server.handle_exit(sig=signal.SIGTERM, frame=None) # Let Uvicorn handle its exit based on signal

         log_info("main", "shutdown_manager", "Graceful shutdown process complete.")

    # Run the server and the shutdown manager concurrently
    try:
        # loop.run_until_complete(asyncio.gather(main_server_task(), shutdown_manager()))
        # Using asyncio.run is simpler if possible
         async def run_all():
              server_task = asyncio.create_task(main_server_task())
              shutdown_task = asyncio.create_task(shutdown_manager())
              await asyncio.gather(server_task, shutdown_task)
         asyncio.run(run_all())

    except Exception as e: # Catch errors during the asyncio run phase
         log_error("main", "main", f"Error during server execution or shutdown management.", e)
         log_error("main", "main", f"Runtime Exception Traceback:\n{traceback.format_exc()}")
         # Ensure scheduler shutdown is attempted even if gather fails
         if SCHEDULER_IMPORTED:
             try: shutdown_scheduler()
             except Exception as final_sched_down: log_error("main","main", f"Error in final scheduler shutdown attempt: {final_sched_down}")
         sys.exit(1)
    finally:
         log_info("main", "main", "Main process finished.")


if __name__ == "__main__":
    try:
        log_info("main", "__main__", f"WhatsTasker v0.8 starting...") # Update version if needed
    except NameError:
        print("FATAL ERROR: Logger not loaded.")
        sys.exit(1)
    except Exception as e:
        print(f"FATAL ERROR during initial logging setup: {e}")
        sys.exit(1)

    main()

# --- END OF FILE main.py ---
# --- END OF FILE main.py ---


================================================================================
üìÑ bridge\request_router.py
================================================================================

# --- START OF FILE bridge\request_router.py ---
# --- START OF FILE bridge/request_router.py ---

import re
import os
import yaml
import traceback
import json
from typing import Optional, Tuple, List
from tools.logger import log_info, log_error, log_warning

# State manager imports
from services.agent_state_manager import get_agent_state, add_message_to_user_history

# User/Agent Management
from users.user_manager import get_agent

# Orchestrator Import
try:
    from agents.orchestrator_agent import handle_user_request as route_to_orchestrator
    ORCHESTRATOR_IMPORTED = True
    log_info("request_router", "import", "Successfully imported OrchestratorAgent handler.")
except ImportError as e:
    ORCHESTRATOR_IMPORTED = False; log_error("request_router", "import", f"OrchestratorAgent import failed: {e}", e); route_to_orchestrator = None

# Onboarding Agent Import
try:
    from agents.onboarding_agent import handle_onboarding_request
    ONBOARDING_AGENT_IMPORTED = True
    log_info("request_router", "import", "Successfully imported OnboardingAgent handler.")
except ImportError as e:
     ONBOARDING_AGENT_IMPORTED = False; log_error("request_router", "import", f"OnboardingAgent import failed: {e}", e); handle_onboarding_request = None

# Task Query Service Import
try:
    from services.task_query_service import get_context_snapshot
    QUERY_SERVICE_IMPORTED = True
except ImportError as e:
     QUERY_SERVICE_IMPORTED = False; log_error("request_router", "import", f"TaskQueryService import failed: {e}", e); get_context_snapshot = None

# Cheat Service Import
try:
    from services.cheats import handle_cheat_command
    CHEATS_IMPORTED = True
    log_info("request_router", "import", "Successfully imported Cheats service.")
except ImportError as e:
     CHEATS_IMPORTED = False; log_error("request_router", "import", f"Cheats service import failed: {e}", e); handle_cheat_command = None

# ConfigManager Import
try:
    from services.config_manager import set_user_status
    CONFIG_MANAGER_IMPORTED = True
except ImportError as e:
     CONFIG_MANAGER_IMPORTED = False; log_error("request_router", "import", f"ConfigManager import failed: {e}", e); set_user_status = None


# Load standard messages
_messages = {}
try:
    messages_path = os.path.join("config", "messages.yaml")
    if os.path.exists(messages_path):
        with open(messages_path, 'r', encoding="utf-8") as f:
             content = f.read();
             if content.strip(): f.seek(0); _messages = yaml.safe_load(f) or {}
             else: _messages = {}
    else: log_warning("request_router", "import", f"{messages_path} not found."); _messages = {}
except Exception as e: log_error("request_router", "import", f"Failed to load messages.yaml: {e}", e); _messages = {}

GENERIC_ERROR_MSG = _messages.get("generic_error_message", "Sorry, an unexpected error occurred.")
WELCOME_MESSAGE = _messages.get("welcome_confirmation_message", "Hello! Welcome to WhatsTasker.")


# Global bridge instance
current_bridge = None

def normalize_user_id(user_id: str) -> str:
    return re.sub(r'\D', '', user_id) if user_id else ""

def set_bridge(bridge_instance):
    global current_bridge; current_bridge = bridge_instance
    log_info("request_router", "set_bridge", f"Bridge set to: {type(bridge_instance).__name__}")

def send_message(user_id: str, message: str):
    """Adds agent message to history and sends via configured bridge."""
    if user_id and message: add_message_to_user_history(user_id, "agent", message)
    else: log_warning("request_router", "send_message", f"Attempted add empty msg/invalid user_id ({user_id}) to history.")
    log_info("request_router", "send_message", f"Sending to {user_id}: '{message[:100]}...'")
    if current_bridge:
        try: current_bridge.send_message(user_id, message)
        except Exception as e: log_error("request_router", "send_message", f"Bridge error sending to {user_id}: {e}", e)
    else: log_error("request_router", "send_message", "No bridge configured.")


# --- Main Handler ---
def handle_incoming_message(user_id: str, message: str) -> str:
    """
    Routes incoming messages based on user status (new, onboarding, active) or cheat codes.
    """
    final_response_message = GENERIC_ERROR_MSG

    try:
        log_info("request_router", "handle_incoming_message", f"Received raw: {user_id} msg: '{message[:50]}...'")
        norm_user_id = normalize_user_id(user_id)
        if not norm_user_id: log_error("request_router", "handle_incoming_message", f"Invalid User ID: {user_id}"); return "Error: Invalid User ID."

        # --- Ensure agent state exists ---
        agent_state = get_agent(norm_user_id)
        if not agent_state: log_error("request_router", "handle_incoming_message", f"CRITICAL: Failed get/create agent state for {norm_user_id}."); return GENERIC_ERROR_MSG

        current_status = agent_state.get("preferences", {}).get("status")
        log_info("request_router", "handle_incoming_message", f"User {norm_user_id} status: {current_status}")

        # --- Routing Logic ---

        # 1. New User: Send welcome, set status to onboarding
        if current_status == "new":
            # *** CORRECTED LOG CALL ***
            log_info("request_router", "handle_incoming_message", f"New user ({norm_user_id}). Sending welcome, setting status to onboarding.")
            send_message(norm_user_id, WELCOME_MESSAGE)
            if CONFIG_MANAGER_IMPORTED and set_user_status:
                if not set_user_status(norm_user_id, 'onboarding'): # Set status to onboarding
                     log_error("request_router", "handle_incoming_message", f"Failed update status from 'new' to 'onboarding' for {norm_user_id}")
            else: log_error("request_router", "handle_incoming_message", "ConfigManager unavailable, cannot update user status after welcome.")
            return WELCOME_MESSAGE # End processing for this turn

        # 2. Cheat Codes (Check before onboarding/active routing)
        message_stripped = message.strip()
        if message_stripped.startswith('/') and CHEATS_IMPORTED and handle_cheat_command:
            parts = message_stripped.split(); command = parts[0].lower(); args = parts[1:]
            log_info("request_router", "handle_incoming_message", f"Detected command '{command}' for {norm_user_id}. Routing to Cheats.")
            try: command_response = handle_cheat_command(norm_user_id, command, args)
            except Exception as e: log_error("request_router", "handle_incoming_message", f"Error executing cheat '{command}': {e}", e); command_response = "Error processing cheat."
            send_message(norm_user_id, command_response); return command_response # Bypass other handlers

        elif message_stripped.startswith('/') and not CHEATS_IMPORTED:
             log_error("request_router", "handle_incoming_message", "Cheat command detected, but Cheats service failed import.")
             send_message(norm_user_id, "Error: Command processor unavailable."); return "Error: Command processor unavailable."

        # 3. Onboarding User: Route to onboarding agent
        elif current_status == "onboarding":
            log_info("request_router", "handle_incoming_message", f"User {norm_user_id} is onboarding. Routing to onboarding agent.")
            add_message_to_user_history(norm_user_id, "user", message) # Add user reply to history first
            if ONBOARDING_AGENT_IMPORTED and handle_onboarding_request:
                try:
                     history = agent_state.get("conversation_history", [])
                     preferences = agent_state.get("preferences", {})
                     final_response_message = handle_onboarding_request(norm_user_id, message, history, preferences)
                except Exception as onboard_e:
                     tb_str = traceback.format_exc()
                     log_error("request_router", "handle_incoming_message", f"Error calling OnboardingAgent for {norm_user_id}. Traceback:\n{tb_str}", onboard_e)
                     final_response_message = GENERIC_ERROR_MSG
            else:
                log_error("request_router", "handle_incoming_message", f"Onboarding required for {norm_user_id}, but onboarding agent not imported.")
                final_response_message = "Sorry, there's an issue with the setup process right now."

        # 4. Active User: Route to main orchestrator
        elif current_status == "active":
            log_info("request_router", "handle_incoming_message", f"User {norm_user_id} is active. Routing to orchestrator.")
            add_message_to_user_history(norm_user_id, "user", message)
            if ORCHESTRATOR_IMPORTED and route_to_orchestrator and QUERY_SERVICE_IMPORTED and get_context_snapshot:
                try:
                    history = agent_state.get("conversation_history", [])
                    preferences = agent_state.get("preferences", {})
                    task_context, calendar_context = get_context_snapshot(norm_user_id)
                    final_response_message = route_to_orchestrator(
                        user_id=norm_user_id, message=message, history=history,
                        preferences=preferences, task_context=task_context, calendar_context=calendar_context)
                except Exception as orch_e:
                     tb_str = traceback.format_exc()
                     log_error("request_router", "handle_incoming_message", f"Error calling OrchestratorAgent for {norm_user_id}. Traceback:\n{tb_str}", orch_e)
                     final_response_message = GENERIC_ERROR_MSG
            else:
                 log_error("request_router", "handle_incoming_message", f"Core components missing for active user {norm_user_id} (Orchestrator/QueryService).")
                 final_response_message = "Sorry, I can't process your request right now due to a system issue."

        # 5. Unknown Status: Log error and give generic response
        else:
            log_error("request_router", "handle_incoming_message", f"User {norm_user_id} has unknown status: '{current_status}'. Sending generic error.")
            final_response_message = GENERIC_ERROR_MSG


        # Send the final response (unless handled earlier)
        if final_response_message:
             send_message(norm_user_id, final_response_message)
        else:
             log_warning("request_router", "handle_incoming_message", f"Final response message was empty for {norm_user_id}. Sending generic error.")
             send_message(norm_user_id, GENERIC_ERROR_MSG)
             final_response_message = GENERIC_ERROR_MSG

        return final_response_message

    except Exception as outer_e:
        tb_str = traceback.format_exc()
        log_error("request_router", "handle_incoming_message", f"Unexpected outer error processing message for {user_id}. Traceback:\n{tb_str}", outer_e)
        if 'norm_user_id' in locals() and norm_user_id:
            try: send_message(norm_user_id, GENERIC_ERROR_MSG)
            except: pass
        return GENERIC_ERROR_MSG
# --- END OF FILE bridge/request_router.py ---
# --- END OF FILE bridge\request_router.py ---


================================================================================
üìÑ bridge\cli_interface.py
================================================================================

# --- START OF FILE bridge\cli_interface.py ---
# bridge/cli_interface.py

from fastapi import FastAPI, Request
from fastapi.responses import JSONResponse
import uvicorn
import uuid
from threading import Lock, Thread # Import Thread
import time # Import time for sleep
from tools.logger import log_info, log_error
from bridge.request_router import handle_incoming_message, set_bridge
# Ensure calendar_tool provides the router correctly
try:
    from tools.calendar_tool import router as calendar_router
except ImportError:
    log_error("cli_interface", "import", "Could not import calendar_router from tools.calendar_tool")
    # Define a dummy router if import fails to allow server start
    from fastapi import APIRouter
    calendar_router = APIRouter()
# from datetime import datetime # Not used directly here

# Define a CLI Bridge
class CLIBridge:
    """Bridge that handles message queuing for CLI interaction."""
    def __init__(self, message_queue, lock):
        self.message_queue = message_queue
        self.lock = lock

    def send_message(self, user_id: str, message: str):
        """Adds the outgoing message to the queue instead of printing."""
        outgoing = {
            "user_id": user_id,
            "message": message,
            "message_id": str(uuid.uuid4())
        }
        with self.lock:
            self.message_queue.append(outgoing)
        log_info("CLIBridge", "send_message", f"Message for {user_id} queued (ID: {outgoing['message_id']}). Queue size: {len(self.message_queue)}")

# Global in-memory store for outgoing messages.
outgoing_messages = []
queue_lock = Lock()

# Set the global bridge in the router to use our CLI Bridge instance
# Pass the queue and lock to the bridge instance
set_bridge(CLIBridge(outgoing_messages, queue_lock))


def create_app() -> FastAPI:
    """Creates the FastAPI app."""
    app = FastAPI()
    app.include_router(calendar_router, prefix="")

    @app.post("/incoming")
    async def incoming_message(request: Request):
        """Receives message, processes it, queues response, returns ack."""
        try:
            data = await request.json()
            user_id = data.get("user_id")
            message = data.get("message")
            if not user_id or not message:
                return JSONResponse(content={"error": "Missing user_id or message"}, status_code=400)
            log_info("cli_interface", "incoming_message", f"Received message from user {user_id}: {message}")

            # Process the incoming message via the unified handler
            # handle_incoming_message now calls CLIBridge.send_message which queues the response
            handle_incoming_message(user_id, message)

            # --- REVERT HERE ---
            # Return only an acknowledgment. The actual message is queued by the bridge.
            return JSONResponse(content={"ack": True})
            # --- END REVERT ---

        except Exception as e:
            log_error("cli_interface", "incoming_message", "Error processing incoming message", e)
            return JSONResponse(content={"error": "Internal server error"}, status_code=500)

    @app.get("/outgoing")
    async def get_outgoing_messages():
        """Returns and clears the list of queued outgoing messages."""
        msgs_to_send = []
        with queue_lock:
            # Return all messages currently in the queue and clear it
            msgs_to_send = outgoing_messages[:] # Copy the list
            outgoing_messages.clear()          # Clear the original list
        if msgs_to_send:
            log_info("cli_interface", "get_outgoing_messages", f"Returning {len(msgs_to_send)} messages from queue.")
        return JSONResponse(content={"messages": msgs_to_send})

    @app.post("/ack")
    async def acknowledge_message(request: Request):
        """Receives acknowledgment (currently does nothing as queue is cleared on GET)."""
        # In a more robust system, GET /outgoing wouldn't clear the queue.
        # Messages would only be removed upon receiving an ACK for their specific message_id.
        # For this simple polling client, clearing on GET is sufficient.
        try:
            data = await request.json()
            message_id = data.get("message_id")
            if not message_id:
                return JSONResponse(content={"error": "Missing message_id"}, status_code=400)
            # Log but don't modify queue here, as GET already cleared it
            log_info("cli_interface", "acknowledge_message", f"Ack received for message {message_id} (queue already cleared by GET).")
            return JSONResponse(content={"ack": True})
        except Exception as e:
            log_error("cli_interface", "acknowledge_message", "Error processing ack", e)
            return JSONResponse(content={"error": "Internal server error"}, status_code=500)

    return app

app = create_app()

# Keep if running directly, but usually run via main.py
# if __name__ == "__main__":
#     uvicorn.run(app, host="0.0.0.0", port=8000, reload=True)
# --- END OF FILE bridge\cli_interface.py ---


================================================================================
üìÑ agents\orchestrator_agent.py
================================================================================

# --- START OF FILE agents\orchestrator_agent.py ---
# --- START OF FILE agents/orchestrator_agent.py ---
import json
import os
import traceback
from typing import Dict, List, Optional, Any
from datetime import datetime
import pytz
import re

# Instructor/LLM Imports
from services.llm_interface import get_instructor_client
from openai import OpenAI # Base client for types if needed
from openai.types.chat import ChatCompletionMessage, ChatCompletionToolParam, ChatCompletionMessageToolCall
from openai.types.chat.chat_completion_message_tool_call import Function
from openai.types.shared_params import FunctionDefinition

# Tool Imports & Helpers
from .tool_definitions import AVAILABLE_TOOLS, TOOL_PARAM_MODELS
import services.task_manager as task_manager # For parsing duration used in prompt prep

# Utilities
from tools.logger import log_info, log_error, log_warning
import yaml
import pydantic

# --- Load Standard Messages ---
_messages = {}
try:
    messages_path = os.path.join("config", "messages.yaml")
    if os.path.exists(messages_path):
        with open(messages_path, 'r', encoding="utf-8") as f:
             content = f.read()
             if content.strip(): f.seek(0); _messages = yaml.safe_load(f) or {}
             else: _messages = {}
    else: log_warning("orchestrator_agent", "init", f"{messages_path} not found."); _messages = {}
except Exception as e: log_error("orchestrator_agent", "init", f"Failed to load messages.yaml: {e}", e); _messages = {}
GENERIC_ERROR_MSG = _messages.get("generic_error_message", "Sorry, an unexpected error occurred.")


# --- Function to Load Prompt ---
_PROMPT_CACHE = {}
def load_orchestrator_prompt() -> Optional[str]:
    """Loads the orchestrator system prompt from the YAML file, with simple caching."""
    prompts_path = os.path.join("config", "prompts.yaml"); cache_key = prompts_path
    if cache_key in _PROMPT_CACHE: return _PROMPT_CACHE[cache_key]
    prompt_text_result: Optional[str] = None
    try:
        if not os.path.exists(prompts_path): raise FileNotFoundError(f"{prompts_path} not found.")
        with open(prompts_path, "r", encoding="utf-8") as f:
            content = f.read();
            if not content.strip(): raise ValueError("Prompts file is empty.")
            f.seek(0); all_prompts = yaml.safe_load(f)
            if not all_prompts: raise ValueError("YAML parsing resulted in empty prompts.")
            prompt_text = all_prompts.get("orchestrator_agent_system_prompt")
            if not prompt_text or not prompt_text.strip(): log_error("orchestrator_agent", "load_orchestrator_prompt", "Key 'orchestrator_agent_system_prompt' NOT FOUND or EMPTY."); prompt_text_result = None
            else: log_info("orchestrator_agent", "load_orchestrator_prompt", "Orchestrator prompt loaded successfully."); prompt_text_result = prompt_text
    except Exception as e: log_error("orchestrator_agent", "load_orchestrator_prompt", f"CRITICAL: Failed to load/parse orchestrator prompt: {e}", e); prompt_text_result = None
    _PROMPT_CACHE[cache_key] = prompt_text_result; return prompt_text_result

# --- REMOVED Pending Context Helpers ---

# --- Main Handler Function ---
# --- Main Handler Function (Corrected Version) ---
def handle_user_request(
    user_id: str, message: str, history: List[Dict], preferences: Dict,
    task_context: List[Dict], calendar_context: List[Dict]
) -> str:
    """
    Handles the user's request using the Orchestrator pattern with Instructor/Tool Use.
    Relies purely on the LLM for conversational flow and tool invocation decisions.
    Handles multiple tool calls requested by the LLM in a single turn.
    """
    fn_name = "handle_user_request"
    log_info("orchestrator_agent", fn_name, f"Processing request for {user_id}: '{message[:50]}...'")

    orchestrator_system_prompt = load_orchestrator_prompt()
    if not orchestrator_system_prompt:
         log_error("orchestrator_agent", fn_name, "Orchestrator system prompt could not be loaded.")
         return GENERIC_ERROR_MSG

    client: OpenAI = get_instructor_client()
    if not client:
        log_error("orchestrator_agent", fn_name, "LLM client unavailable.")
        return GENERIC_ERROR_MSG

    # --- Prepare Context for Prompt (No change needed here) ---
    try:
        user_timezone_str = preferences.get("TimeZone", "UTC"); user_timezone = pytz.utc
        try: user_timezone = pytz.timezone(user_timezone_str)
        except pytz.UnknownTimeZoneError: log_warning("orchestrator_agent", fn_name, f"Unknown timezone '{user_timezone_str}'. Defaulting UTC.")
        now = datetime.now(user_timezone)
        current_date_str, current_time_str, current_day_str = now.strftime("%Y-%m-%d"), now.strftime("%H:%M"), now.strftime("%A")
        history_limit, item_context_limit, calendar_context_limit = 30, 20, 20
        limited_history = history[-(history_limit*2):]; history_str = "\n".join([f"{m['sender'].capitalize()}: {m['content']}" for m in limited_history])
        def prepare_item(item):
             key_fields = ['event_id', 'item_id', 'title', 'description', 'date', 'time', 'type', 'status', 'estimated_duration', 'project']
             prep = {k: item.get(k) for k in key_fields if item.get(k) is not None}; prep['item_id'] = item.get('item_id', item.get('event_id'))
             if 'event_id' in prep and prep['event_id'] == prep['item_id']: del prep['event_id']
             return prep
        item_context_str = json.dumps([prepare_item(item) for item in task_context[:item_context_limit]], indent=2, default=str)
        calendar_context_str = json.dumps(calendar_context[:calendar_context_limit], indent=2, default=str)
        prefs_str = json.dumps(preferences, indent=2, default=str)
    except Exception as e: log_error("orchestrator_agent", fn_name, f"Error preparing context strings: {e}", e); return GENERIC_ERROR_MSG

    # --- Construct Initial Messages for LLM (No change needed here) ---
    messages: List[Dict[str, Any]] = [
        {"role": "system", "content": orchestrator_system_prompt},
        {"role": "system", "content": f"Current Reference Time ({user_timezone_str}): Date: {current_date_str}, Day: {current_day_str}, Time: {current_time_str}. Use this."},
        {"role": "system", "content": f"User Preferences:\n{prefs_str}"},
        {"role": "system", "content": f"History:\n{history_str}"},
        {"role": "system", "content": f"Active Items:\n{item_context_str}"},
        {"role": "system", "content": f"Calendar Events:\n{calendar_context_str}"},
        {"role": "user", "content": message}
    ]

    # --- Define Tools (No change needed here) ---
    tools_for_llm: List[ChatCompletionToolParam] = []
    for tool_name, model in TOOL_PARAM_MODELS.items():
        if tool_name not in AVAILABLE_TOOLS: continue
        func = AVAILABLE_TOOLS.get(tool_name)
        description = func.__doc__.strip() if func and func.__doc__ else f"Executes {tool_name}"
        try:
            params_schema = model.model_json_schema();
            if not params_schema.get('properties'): params_schema = {}
            func_def: FunctionDefinition = {"name": tool_name, "description": description, "parameters": params_schema}
            tool_param: ChatCompletionToolParam = {"type": "function", "function": func_def}; tools_for_llm.append(tool_param)
        except Exception as e: log_error("orchestrator_agent", fn_name, f"Schema error for tool {tool_name}: {e}", e)
    if not tools_for_llm: log_error("orchestrator_agent", fn_name, "No tools defined for LLM."); return GENERIC_ERROR_MSG

    # --- Interaction Loop ---
    try:
        log_info("orchestrator_agent", fn_name, f"Invoking LLM for {user_id} (Initial Turn)...")
        response = client.chat.completions.create(
            model="gpt-4o", messages=messages, tools=tools_for_llm, tool_choice="auto", temperature=0.1,
        )
        response_message = response.choices[0].message
        tool_calls = response_message.tool_calls

        # --- Scenario 1: LLM Responds Directly ---
        if not tool_calls:
            if response_message.content:
                log_info("orchestrator_agent", fn_name, "LLM responded directly (no tool call).")
                return response_message.content
            else:
                log_warning("orchestrator_agent", fn_name, "LLM response had no tool calls and no content.")
                return "Sorry, I couldn't process that request fully. Could you try rephrasing?"

        # --- Scenario 2: LLM Calls One or More Tools ---
        log_info("orchestrator_agent", fn_name, f"LLM requested {len(tool_calls)} tool call(s).")
        # Append the assistant's message containing the tool call requests
        messages.append(response_message.model_dump(exclude_unset=True))
        tool_results_messages = [] # Store results to append later

        # --- Loop through ALL requested tool calls ---
        for tool_call in tool_calls:
            tool_name = tool_call.function.name
            tool_call_id = tool_call.id
            tool_args_str = tool_call.function.arguments
            tool_result_content = GENERIC_ERROR_MSG # Default in case of error

            log_info("orchestrator_agent", fn_name, f"Processing Tool Call ID: {tool_call_id}, Name: {tool_name}, Args: {tool_args_str[:150]}...")

            if tool_name not in AVAILABLE_TOOLS:
                log_warning("orchestrator_agent", fn_name, f"LLM tried unknown tool: {tool_name}. Sending error result back.")
                tool_result_content = json.dumps({"success": False, "message": f"Error: Unknown action '{tool_name}' requested."})
            else:
                tool_func = AVAILABLE_TOOLS[tool_name]
                param_model = TOOL_PARAM_MODELS[tool_name]
                try:
                    # Parse and validate arguments
                    tool_args_dict = {}
                    if tool_args_str and tool_args_str.strip() != '{}': tool_args_dict = json.loads(tool_args_str)
                    elif not param_model.model_fields: tool_args_dict = {}
                    validated_params = param_model(**tool_args_dict)

                    # Execute the tool function
                    tool_result_dict = tool_func(user_id, validated_params)
                    log_info("orchestrator_agent", fn_name, f"Tool {tool_name} (ID: {tool_call_id}) executed. Result: {tool_result_dict}")
                    tool_result_content = json.dumps(tool_result_dict)

                except json.JSONDecodeError:
                    log_error("orchestrator_agent", fn_name, f"Failed parse JSON args for {tool_name} (ID: {tool_call_id}): {tool_args_str}");
                    tool_result_content = json.dumps({"success": False, "message": f"Error: Invalid arguments provided for {tool_name}."})
                except pydantic.ValidationError as e:
                    log_error("orchestrator_agent", fn_name, f"Arg validation failed for {tool_name} (ID: {tool_call_id}). Err: {e.errors()}. Args: {tool_args_str}", e)
                    err_summary = "; ".join([f"{err['loc'][0] if err.get('loc') else 'param'}: {err['msg']}" for err in e.errors()])
                    tool_result_content = json.dumps({"success": False, "message": f"Error: Invalid parameters for {tool_name} - {err_summary}"})
                except Exception as e:
                    log_error("orchestrator_agent", fn_name, f"Error executing tool {tool_name} (ID: {tool_call_id}). Trace:\n{traceback.format_exc()}", e);
                    tool_result_content = json.dumps({"success": False, "message": f"Error performing action {tool_name}."})

            # Append the result message for THIS tool call
            tool_results_messages.append({
                "tool_call_id": tool_call_id,
                "role": "tool",
                "name": tool_name,
                "content": tool_result_content,
            })
        # --- End loop through tool calls ---

        # Append ALL tool result messages to the main message history
        messages.extend(tool_results_messages)

        # --- Make SECOND LLM call with ALL tool results ---
        log_info("orchestrator_agent", fn_name, f"Invoking LLM again for {user_id} with {len(tool_results_messages)} tool result(s)...")
        second_response = client.chat.completions.create(
            model="gpt-4o", # Use the same model
            messages=messages,
            # No tools needed here, LLM should just generate response based on results
        )
        second_response_message = second_response.choices[0].message

        if second_response_message.content:
            log_info("orchestrator_agent", fn_name, "LLM generated final response after processing tool result(s).")
            return second_response_message.content
        else:
            log_warning("orchestrator_agent", fn_name, "LLM provided no content after processing tool result(s).")
            # Try to construct a fallback from the first tool's message if possible
            try: fallback_msg = json.loads(tool_results_messages[0]['content']).get("message", GENERIC_ERROR_MSG)
            except: fallback_msg = GENERIC_ERROR_MSG
            return fallback_msg

    # --- Outer Exception Handling ---
    except Exception as e:
        tb_str = traceback.format_exc()
        log_error("orchestrator_agent", fn_name, f"Core error in orchestrator logic for {user_id}. Traceback:\n{tb_str}", e)
        return GENERIC_ERROR_MSG

    # --- Outer Exception Handling ---
    except Exception as e:
        tb_str = traceback.format_exc()
        log_error("orchestrator_agent", fn_name, f"Core error in orchestrator logic for {user_id}. Traceback:\n{tb_str}", e)
        return GENERIC_ERROR_MSG
# --- END OF FILE agents/orchestrator_agent.py ---
# --- END OF FILE agents\orchestrator_agent.py ---


================================================================================
üìÑ agents\onboarding_agent.py
================================================================================

# --- START OF FILE agents\onboarding_agent.py ---
# --- START OF FILE agents/onboarding_agent.py ---
import json
import os
import traceback
from typing import Dict, List, Optional, Any
from datetime import datetime
import pytz

# Instructor/LLM Imports
from services.llm_interface import get_instructor_client
from openai import OpenAI
from openai.types.chat import ChatCompletionMessage, ChatCompletionToolParam, ChatCompletionMessageToolCall
from openai.types.chat.chat_completion_message_tool_call import Function
from openai.types.shared_params import FunctionDefinition

# Tool Imports (Limited Set)
from .tool_definitions import (
    AVAILABLE_TOOLS,
    TOOL_PARAM_MODELS,
    UpdateUserPreferencesParams,
    InitiateCalendarConnectionParams,
)

# Utilities
from tools.logger import log_info, log_error, log_warning
import yaml
import pydantic

# --- Load Standard Messages ---
_messages = {}
try:
    messages_path = os.path.join("config", "messages.yaml")
    if os.path.exists(messages_path):
        with open(messages_path, 'r', encoding="utf-8") as f:
             content = f.read()
             if content.strip(): f.seek(0); _messages = yaml.safe_load(f) or {}
             else: _messages = {}
    else: log_warning("onboarding_agent", "init", f"{messages_path} not found."); _messages = {}
except Exception as e: log_error("onboarding_agent", "init", f"Failed to load messages.yaml: {e}", e); _messages = {}
GENERIC_ERROR_MSG = _messages.get("generic_error_message", "Sorry, an unexpected error occurred.")
SETUP_START_MSG = _messages.get("setup_starting_message", "Great! Let's set things up.")

# --- Function to Load Onboarding Prompt ---
_ONBOARDING_PROMPT_CACHE = {}
def load_onboarding_prompt() -> Optional[str]:
    """Loads the onboarding system prompt from the YAML file, with caching."""
    prompts_path = os.path.join("config", "prompts.yaml")
    cache_key = prompts_path + "_onboarding"
    if cache_key in _ONBOARDING_PROMPT_CACHE:
        return _ONBOARDING_PROMPT_CACHE[cache_key]

    prompt_text_result: Optional[str] = None
    try:
        if not os.path.exists(prompts_path): raise FileNotFoundError(f"{prompts_path} not found.")
        with open(prompts_path, "r", encoding="utf-8") as f:
            content = f.read();
            if not content.strip(): raise ValueError("Prompts file is empty.")
            f.seek(0); all_prompts = yaml.safe_load(f)
            if not all_prompts: raise ValueError("YAML parsing resulted in empty prompts.")
            prompt_text = all_prompts.get("onboarding_agent_system_prompt")
            if not prompt_text or not prompt_text.strip():
                log_error("onboarding_agent", "load_onboarding_prompt", "Key 'onboarding_agent_system_prompt' NOT FOUND or EMPTY.")
                prompt_text_result = None
            else:
                log_info("onboarding_agent", "load_onboarding_prompt", "Onboarding prompt loaded successfully.")
                prompt_text_result = prompt_text
    except Exception as e:
        log_error("onboarding_agent", "load_onboarding_prompt", f"CRITICAL: Failed load/parse onboarding prompt: {e}", e)
        prompt_text_result = None

    _ONBOARDING_PROMPT_CACHE[cache_key] = prompt_text_result
    return prompt_text_result

# --- REMOVED Onboarding State Helpers ---
# The pure LLM flow relies on history and current prefs in context

# --- Helper for LLM Clarification (If needed for onboarding - keep for now) ---
def _get_clarification_from_llm(client: OpenAI, question: str, user_reply: str, expected_choices: List[str]) -> str:
    """Uses LLM to interpret user reply against expected choices."""
    # This helper might still be useful if the LLM asks a yes/no for calendar
    log_info("onboarding_agent", "_get_clarification_from_llm", f"Asking LLM to clarify: Q='{question}' Reply='{user_reply}' Choices={expected_choices}")
    system_message = f"""
You are helping a user interact with a task assistant during setup.
The assistant asked the user a question, and the user replied.
Your task is to determine which of the expected choices the user's reply corresponds to.
The original question was: "{question}"
The user's reply was: "{user_reply}"
The expected choices are: {expected_choices}

Analyze the user's reply and determine the choice.
Respond ONLY with one of the following exact strings: {', '.join([f"'{choice}'" for choice in expected_choices])} or 'unclear'.
"""
    try:
        response = client.chat.completions.create(
            model="gpt-3.5-turbo", # Use a cheaper/faster model for clarification
            messages=[{"role": "system", "content": system_message}],
            temperature=0.0,
            max_tokens=10
        )
        llm_choice = response.choices[0].message.content.strip().lower().replace("'", "")
        log_info("onboarding_agent", "_get_clarification_from_llm", f"LLM clarification result: '{llm_choice}'")
        if llm_choice in expected_choices or llm_choice == 'unclear':
            return llm_choice
        else:
            log_warning("onboarding_agent", "_get_clarification_from_llm", f"LLM returned unexpected clarification: '{llm_choice}'")
            return 'unclear'
    except Exception as e:
        log_error("onboarding_agent", "_get_clarification_from_llm", f"Error during LLM clarification call: {e}", e)
        return 'unclear'

# --- Main Onboarding Handler Function ---
def handle_onboarding_request(
    user_id: str,
    message: str,
    history: List[Dict],
    preferences: Dict # Current preferences passed in
) -> str:
    """Handles the user's request during the onboarding phase using pure LLM flow."""
    log_info("onboarding_agent", "handle_onboarding_request", f"Handling onboarding request for {user_id}: '{message[:50]}...'")
    fn_name = "handle_onboarding_request" # Use function name for logging

    onboarding_system_prompt = load_onboarding_prompt()
    if not onboarding_system_prompt:
         log_error(fn_name, "load_onboarding_prompt", "Onboarding system prompt could not be loaded.") # Corrected log identifier
         return GENERIC_ERROR_MSG

    client: OpenAI = get_instructor_client()
    if not client:
        log_error(fn_name, "get_instructor_client", "LLM client unavailable.") # Corrected log identifier
        return GENERIC_ERROR_MSG

    # --- Prepare Context (Simpler than main orchestrator, focuses on prefs) ---
    try:
        user_timezone_str = preferences.get("TimeZone", "UTC"); user_timezone = pytz.utc
        try:
            user_timezone = pytz.timezone(user_timezone_str)
        except pytz.UnknownTimeZoneError:
            log_warning(fn_name, "prepare_context", f"Unknown timezone '{user_timezone_str}'. Using UTC.") # Corrected log identifier
        now = datetime.now(user_timezone); current_date_str = now.strftime("%Y-%m-%d"); current_time_str = now.strftime("%H:%M"); current_day_str = now.strftime("%A")

        history_limit = 10
        limited_history = history[-(history_limit*2):]
        history_str = "\n".join([f"{m['sender'].capitalize()}: {m['content']}" for m in limited_history])

        prefs_str = json.dumps(preferences, indent=2, default=str)

        initial_interaction = len(history) <= 1 # If only user message is present
        intro_message = SETUP_START_MSG if initial_interaction else ""

    except Exception as e:
        log_error(fn_name, "prepare_context", f"Error preparing context: {e}", e) # Corrected log identifier
        return GENERIC_ERROR_MSG

    # --- Construct Messages for Onboarding LLM ---
    messages: List[Dict[str, Any]] = [
        {"role": "system", "content": onboarding_system_prompt},
        {"role": "system", "content": f"Current User Preferences:\n```json\n{prefs_str}\n```"},
        {"role": "system", "content": f"Conversation History:\n{history_str}"},
        {"role": "user", "content": message}
    ]
    if intro_message and initial_interaction:
         messages.insert(-1, {"role": "assistant", "content": intro_message})


    # --- Define Tools AVAILABLE for Onboarding ---
    onboarding_tools_available = {
        "update_user_preferences": AVAILABLE_TOOLS.get("update_user_preferences"),
        "initiate_calendar_connection": AVAILABLE_TOOLS.get("initiate_calendar_connection"),
    }
    onboarding_tool_models = {
        "update_user_preferences": UpdateUserPreferencesParams,
        "initiate_calendar_connection": InitiateCalendarConnectionParams,
    }
    onboarding_tools_available = {k:v for k,v in onboarding_tools_available.items() if v is not None}

    tools_for_llm: List[ChatCompletionToolParam] = []
    for tool_name, model in onboarding_tool_models.items():
        if tool_name not in onboarding_tools_available: continue
        func = onboarding_tools_available.get(tool_name)
        description = func.__doc__.strip() if func and func.__doc__ else f"Executes {tool_name}"
        try:
            params_schema = model.model_json_schema();
            if not params_schema.get('properties'): params_schema = {}
            func_def: FunctionDefinition = {"name": tool_name, "description": description, "parameters": params_schema}
            tool_param: ChatCompletionToolParam = {"type": "function", "function": func_def}; tools_for_llm.append(tool_param)
        except Exception as e: log_error(fn_name, "define_tools", f"Schema error for onboarding tool {tool_name}: {e}", e) # Corrected log identifier
    # It's okay if tools_for_llm is empty if LLM decides to just talk


    # --- Interact with LLM (using same two-step logic as orchestrator) ---
    try:
        log_info(fn_name, "LLM_call_1", f"Invoking Onboarding LLM for {user_id}...") # Corrected log identifier
        response = client.chat.completions.create(
            model="gpt-4o", # Or gpt-3.5-turbo
            messages=messages,
            tools=tools_for_llm if tools_for_llm else None,
            tool_choice="auto" if tools_for_llm else None,
            temperature=0.1,
        )
        response_message = response.choices[0].message
        tool_calls = response_message.tool_calls

        # --- Scenario 1: LLM Responds Directly (Asks question, gives info) ---
        if not tool_calls:
            if response_message.content:
                log_info(fn_name, "LLM_call_1", "Onboarding LLM responded directly.") # Corrected log identifier
                return response_message.content
            else:
                log_warning(fn_name, "LLM_call_1", "Onboarding LLM response had no tool calls and no content.") # Corrected log identifier
                return "Sorry, I got stuck. Can you tell me what we were discussing?"

        # --- Scenario 2: LLM Calls an Onboarding Tool ---
        tool_call: ChatCompletionMessageToolCall = tool_calls[0]
        tool_name = tool_call.function.name
        tool_call_id = tool_call.id

        if tool_name not in onboarding_tools_available:
            log_warning(fn_name, "tool_call_check", f"Onboarding LLM tried unknown/disallowed tool: {tool_name}") # Corrected log identifier
            return f"Sorry, I tried an action ('{tool_name}') that isn't available during setup."

        log_info(fn_name, "tool_call_exec", f"Onboarding LLM requested Tool: {tool_name} with args: {tool_call.function.arguments[:150]}...") # Corrected log identifier
        tool_func = onboarding_tools_available[tool_name]
        param_model = onboarding_tool_models[tool_name]
        tool_result_content = GENERIC_ERROR_MSG

        try:
            # Parse and validate arguments
            tool_args_dict = {}
            tool_args_str = tool_call.function.arguments
            if tool_args_str and tool_args_str.strip() != '{}': tool_args_dict = json.loads(tool_args_str)
            elif not param_model.model_fields: tool_args_dict = {}

            validated_params = param_model(**tool_args_dict)

            # Execute the tool function
            tool_result_dict = tool_func(user_id, validated_params)
            log_info(fn_name, "tool_call_exec", f"Onboarding Tool {tool_name} executed. Result: {tool_result_dict}") # Corrected log identifier
            tool_result_content = json.dumps(tool_result_dict)

        # Handle errors during tool execution
        except json.JSONDecodeError:
             log_error(fn_name, "tool_call_exec", f"Failed parse JSON args for onboarding tool {tool_name}: {tool_args_str}"); # Corrected log identifier
             tool_result_content = json.dumps({"success": False, "message": f"Error: Invalid arguments for {tool_name}."})
        except pydantic.ValidationError as e:
             log_error(fn_name, "tool_call_exec", f"Arg validation failed for onboarding tool {tool_name}. Err: {e.errors()}. Args: {tool_args_str}", e) # Corrected log identifier
             err_summary = "; ".join([f"{err['loc'][0] if err.get('loc') else 'param'}: {err['msg']}" for err in e.errors()])
             tool_result_content = json.dumps({"success": False, "message": f"Error: Invalid parameters for {tool_name} - {err_summary}"})
        except Exception as e:
             log_error(fn_name, "tool_call_exec", f"Error executing onboarding tool {tool_name}. Trace:\n{traceback.format_exc()}", e); # Corrected log identifier
             tool_result_content = json.dumps({"success": False, "message": f"Error performing action {tool_name}."})

        # --- Make SECOND LLM call with the tool result ---
        # *** FIXED: Convert response_message object to dict before appending ***
        # Append the assistant's first message (the tool call request) as a dict
        messages.append(response_message.model_dump(exclude_unset=True))
        # Append the tool execution result
        messages.append({
            "tool_call_id": tool_call_id, "role": "tool",
            "name": tool_name, "content": tool_result_content,
        })

        log_info(fn_name, "LLM_call_2", f"Invoking Onboarding LLM again for {user_id} with tool result...") # Corrected log identifier
        second_response = client.chat.completions.create(
            model="gpt-4o", messages=messages, # No tools needed here
        )
        second_response_message = second_response.choices[0].message

        if second_response_message.content:
            log_info(fn_name, "LLM_call_2", "Onboarding LLM generated final response after processing tool result.") # Corrected log identifier
            # Check if the LAST action was setting status to active
            if tool_name == "update_user_preferences":
                 try:
                      # Ensure tool_args_str is valid JSON before parsing
                      update_data = json.loads(tool_args_str) if tool_args_str and tool_args_str.strip() != '{}' else {}
                      # Check if the 'updates' dictionary exists and contains 'status'
                      if isinstance(update_data.get("updates"), dict) and update_data["updates"].get("status") == "active":
                           log_info(fn_name, "status_check", f"Onboarding completed for user {user_id} (status set to active).") # Corrected log identifier
                 except Exception as parse_err:
                      log_warning(fn_name, "status_check", f"Could not parse tool args to check for status update: {parse_err}") # Corrected log identifier

            return second_response_message.content
        else:
            log_warning(fn_name, "LLM_call_2", "Onboarding LLM provided no content after processing tool result.") # Corrected log identifier
            try: fallback_msg = json.loads(tool_result_content).get("message", GENERIC_ERROR_MSG)
            except: fallback_msg = GENERIC_ERROR_MSG
            return fallback_msg

    # --- Outer Exception Handling ---
    except Exception as e:
        tb_str = traceback.format_exc();
        log_error(fn_name, "outer_exception", f"Core error in onboarding logic for {user_id}. Traceback:\n{tb_str}", e) # Corrected log identifier
        return GENERIC_ERROR_MSG
# --- END OF FILE agents/onboarding_agent.py ---
# --- END OF FILE agents\onboarding_agent.py ---


================================================================================
üìÑ agents\tool_definitions.py
================================================================================

# --- START OF FILE agents\tool_definitions.py ---
# --- START OF FULL agents/tool_definitions.py ---

from pydantic import BaseModel, Field, field_validator, ValidationError
from typing import Dict, List, Any, Tuple, Optional # Keep ALL typing imports for Pydantic Models
import json
from datetime import datetime, timedelta, timezone # Added timezone
import re
import uuid # Added uuid
import traceback
# Import Service Layer functions & Helpers
import services.task_manager as task_manager
import services.config_manager as config_manager
import services.task_query_service as task_query_service
from services.agent_state_manager import get_agent_state, add_task_to_context, update_task_in_context
from tools import metadata_store
from tools.google_calendar_api import GoogleCalendarAPI

# LLM Interface (for scheduler sub-call)
from services.llm_interface import get_instructor_client
from openai import OpenAI
from openai.types.chat import ChatCompletionMessage

# Utilities
from tools.logger import log_info, log_error, log_warning # Ensure log_warning is used correctly
import yaml
import os
import pydantic

# --- Helper Functions ---

def _get_calendar_api_from_state(user_id): # No type hint
    """Helper to retrieve the active calendar API instance from agent state."""
    fn_name = "_get_calendar_api_from_state"
    if GoogleCalendarAPI is None:
        log_warning("tool_definitions", fn_name, "GoogleCalendarAPI class missing.") # OK
        return None
    try:
        state = get_agent_state(user_id)
        if state:
            api = state.get("calendar")
            if isinstance(api, GoogleCalendarAPI) and api.is_active():
                return api
    except Exception as e:
        log_error("tool_definitions", fn_name, f"Error getting calendar API instance for user {user_id}", e) # OK
    return None

def _parse_scheduler_llm_response(raw_text): # No type hint
    """Parses the specific JSON output from the Session Scheduler LLM."""
    fn_name = "_parse_scheduler_llm_response"
    if not raw_text:
        log_warning("tool_definitions", fn_name, "Scheduler response text is empty.") # OK
        return None
    processed_text = None
    try:
        match = re.search(r"```json\s*({.*?})\s*```", raw_text, re.DOTALL | re.IGNORECASE)
        if match: processed_text = match.group(1).strip()
        else:
            processed_text = raw_text.strip()
            if not processed_text.startswith("{") or not processed_text.endswith("}"):
                 start_brace = raw_text.find('{'); end_brace = raw_text.rfind('}')
                 if start_brace != -1 and end_brace > start_brace:
                     processed_text = raw_text[start_brace : end_brace + 1].strip()
                     log_warning("tool_definitions", fn_name, "Used find/rfind fallback for JSON extraction.") # OK
                 else: log_warning("tool_definitions", fn_name, "Could not extract JSON block from raw text."); return None # OK
        if not processed_text: raise ValueError("Processed text is empty after extraction attempts.")

        data = json.loads(processed_text)
        required_keys = ["proposed_sessions", "response_message"]
        if not isinstance(data, dict) or not all(k in data for k in required_keys):
            missing = [k for k in required_keys if k not in data]; raise ValueError(f"Parsed JSON missing required keys: {missing}")
        if not isinstance(data["proposed_sessions"], list): raise ValueError("'proposed_sessions' key must contain a list.")

        valid_sessions = []
        for i, session_dict in enumerate(data["proposed_sessions"]):
            required_session_keys = ["date", "time", "end_time"]
            if not isinstance(session_dict, dict) or not all(k in session_dict for k in required_session_keys):
                log_warning("tool_definitions", fn_name, f"Skipping invalid session structure: {session_dict}"); continue # OK
            try:
                 datetime.strptime(session_dict["date"], '%Y-%m-%d'); datetime.strptime(session_dict["time"], '%H:%M'); datetime.strptime(session_dict["end_time"], '%H:%M')
                 ref = session_dict.get("slot_ref"); session_dict["slot_ref"] = ref if isinstance(ref, int) and ref > 0 else i + 1
                 valid_sessions.append(session_dict)
            except (ValueError, TypeError) as fmt_err: log_warning("tool_definitions", fn_name, f"Skipping session due to format error ({fmt_err}): {session_dict}") # OK

        data["proposed_sessions"] = valid_sessions
        log_info("tool_definitions", fn_name, f"Successfully parsed {len(valid_sessions)} valid sessions.") # OK
        return data
    except (json.JSONDecodeError, ValueError) as parse_err:
        log_error("tool_definitions", fn_name, f"Scheduler JSON parsing failed. Error: {parse_err}. Extracted: '{processed_text or 'N/A'}' Raw: '{raw_text[:200]}'", parse_err); return None # OK
    except Exception as e: log_error("tool_definitions", fn_name, f"Unexpected error parsing scheduler response: {e}", e); return None # OK

_SCHEDULER_PROMPTS_CACHE = {}
def _load_scheduler_prompts(): # No type hint
    """Loads scheduler system and human prompts from config."""
    fn_name = "_load_scheduler_prompts"
    prompts_path = os.path.join("config", "prompts.yaml"); cache_key = prompts_path + "_scheduler"
    if cache_key in _SCHEDULER_PROMPTS_CACHE: return _SCHEDULER_PROMPTS_CACHE[cache_key]
    sys_prompt, human_prompt = None, None
    try:
        if not os.path.exists(prompts_path): raise FileNotFoundError(f"{prompts_path} not found.")
        with open(prompts_path, "r", encoding="utf-8") as f: all_prompts = yaml.safe_load(f)
        if not all_prompts: raise ValueError("YAML prompts file loaded as empty.")
        sys_prompt = all_prompts.get("session_scheduler_system_prompt")
        human_prompt = all_prompts.get("session_scheduler_human_prompt")
        if not sys_prompt or not human_prompt:
            log_error("tool_definitions", fn_name, "One or both scheduler prompts (system/human) are missing in prompts.yaml.") # OK
            sys_prompt, human_prompt = None, None
    except Exception as e: log_error("tool_definitions", fn_name, f"Failed to load scheduler prompts from {prompts_path}: {e}", e); sys_prompt, human_prompt = None, None # OK
    _SCHEDULER_PROMPTS_CACHE[cache_key] = (sys_prompt, human_prompt); return sys_prompt, human_prompt


# =====================================================
# == Pydantic Model Definitions (MUST COME BEFORE TOOLS) ==
# =====================================================

class CreateReminderParams(BaseModel):
    description: str = Field(...)
    date: str = Field(...)
    time: Optional[str] = Field(None)
    project: Optional[str] = Field(None)
    @field_validator('time')
    @classmethod
    def validate_time_format(cls, v: Optional[str]):
        if v is None or v == "": return None
        try: hour, minute = map(int, v.split(':')); return f"{hour:02d}:{minute:02d}"
        except (ValueError, TypeError): raise ValueError("Time must be in HH:MM format (e.g., '14:30') or null/empty")
    @field_validator('date')
    @classmethod
    def validate_date_format(cls, v: str):
        try: datetime.strptime(v, '%Y-%m-%d'); return v
        except (ValueError, TypeError): raise ValueError("Date must be in YYYY-MM-DD format")

class CreateTaskParams(BaseModel):
    description: str = Field(...)
    date: str = Field(...)
    estimated_duration: Optional[str] = Field(None)
    project: Optional[str] = Field(None)
    @field_validator('date')
    @classmethod
    def validate_date_format(cls, v: str):
        try: datetime.strptime(v, '%Y-%m-%d'); return v
        except (ValueError, TypeError): raise ValueError("Date must be in YYYY-MM-DD format")

class ProposeTaskSlotsParams(BaseModel):
    duration: str = Field(...)
    timeframe: str = Field(...)
    description: Optional[str] = Field(None)
    split_preference: Optional[str] = Field(None)
    num_options_to_propose: Optional[int] = Field(3, gt=0)

class FinalizeTaskAndBookSessionsParams(BaseModel):
    search_context: Dict = Field(...)
    approved_slots: List[Dict] = Field(..., min_length=1)
    project: Optional[str] = Field(None)
    @field_validator('approved_slots')
    @classmethod
    def validate_slots_structure(cls, v):
        if not isinstance(v, list) or not v: raise ValueError("approved_slots must be a non-empty list.")
        for i, slot in enumerate(v):
            if not isinstance(slot, dict): raise ValueError(f"Slot {i+1} is not a dict.")
            req_keys = ["date", "time", "end_time"];
            if not all(k in slot for k in req_keys): raise ValueError(f"Slot {i+1} missing required keys: {req_keys}")
            try: datetime.strptime(slot["date"], '%Y-%m-%d')
            except (ValueError, TypeError): raise ValueError(f"Slot {i+1} has invalid date format: {slot['date']}")
            try: datetime.strptime(slot["time"], '%H:%M')
            except (ValueError, TypeError): raise ValueError(f"Slot {i+1} has invalid time format: {slot['time']}")
            try: datetime.strptime(slot["end_time"], '%H:%M')
            except (ValueError, TypeError): raise ValueError(f"Slot {i+1} has invalid end_time format: {slot['end_time']}")
        return v

class UpdateItemDetailsParams(BaseModel):
    item_id: str = Field(...)
    updates: dict = Field(...)
    @field_validator('updates')
    @classmethod
    def check_allowed_keys_and_formats(cls, v: dict):
        allowed_keys = {"description", "date", "time", "estimated_duration", "project"}
        if not v: raise ValueError("Updates dictionary cannot be empty.")
        validated_updates = {}
        for key, value in v.items():
            if key not in allowed_keys: raise ValueError(f"Invalid key '{key}'. Allowed: {', '.join(allowed_keys)}")
            if key == 'date':
                if value is None: validated_updates[key] = None
                else:
                    try: validated_updates[key] = cls.validate_date_format_static(str(value))
                    except ValueError as e: raise ValueError(f"Invalid format for date '{value}': {e}")
            elif key == 'time':
                try: validated_updates[key] = cls.validate_time_format_static(value)
                except ValueError as e: raise ValueError(f"Invalid format for time '{value}': {e}")
            elif key == 'estimated_duration':
                if value is None or (isinstance(value, str) and value.strip() == ""): validated_updates[key] = None
                elif not isinstance(value, str): raise ValueError("Estimated duration must be a string or null/empty")
                else: validated_updates[key] = value
            else: validated_updates[key] = value
        return validated_updates
    @staticmethod
    def validate_date_format_static(v: str):
        try: datetime.strptime(v, '%Y-%m-%d'); return v
        except (ValueError, TypeError): raise ValueError("Date must be in YYYY-MM-DD format")
    @staticmethod
    def validate_time_format_static(v: Optional[str]):
        if v is None or v == "": return None
        try: hour, minute = map(int, v.split(':')); return f"{hour:02d}:{minute:02d}"
        except (ValueError, TypeError): raise ValueError("Time must be in HH:MM format (e.g., '14:30') or null/empty")

class UpdateItemStatusParams(BaseModel):
    item_id: str = Field(...)
    new_status: str = Field(...)
    @field_validator('new_status')
    @classmethod
    def check_item_status(cls, v: str):
        allowed = {"pending", "in_progress", "completed", "cancelled"}
        v_lower = v.lower()
        if v_lower not in allowed: raise ValueError(f"Status must be one of: {', '.join(allowed)}")
        return v_lower

class UpdateUserPreferencesParams(BaseModel):
    updates: dict = Field(...)
    @field_validator('updates')
    @classmethod
    def check_updates_not_empty(cls, v: dict):
        if not v: raise ValueError("Updates dictionary cannot be empty.")
        return v

class InitiateCalendarConnectionParams(BaseModel):
    pass

class CancelTaskSessionsParams(BaseModel):
    task_id: str = Field(...)
    session_ids_to_cancel: list[str] = Field(..., min_length=1)

class InterpretListReplyParams(BaseModel):
    user_reply: str = Field(...)
    list_mapping: dict = Field(...)

class GetFormattedTaskListParams(BaseModel):
    date_range: list[str] | None = Field(None)
    status_filter: str = Field('active')
    project_filter: str | None = Field(None)
    @field_validator('date_range')
    @classmethod
    def validate_date_range(cls, v: list[str] | None):
        if v is None: return v
        if not isinstance(v, list) or len(v) != 2: raise ValueError("date_range must be a list of two date strings.")
        try:
            start_date = datetime.strptime(v[0], '%Y-%m-%d').date(); end_date = datetime.strptime(v[1], '%Y-%m-%d').date()
            if start_date > end_date: raise ValueError("Start date cannot be after end date.")
            return v
        except (ValueError, TypeError): raise ValueError("Dates must be valid YYYY-MM-DD strings.")
    @field_validator('status_filter')
    @classmethod
    def check_status_filter(cls, v: str):
        allowed = {'active', 'pending', 'in_progress', 'completed', 'all'}
        v_lower = v.lower()
        if v_lower not in allowed:
            log_warning("tool_definitions", "check_status_filter", f"Invalid status_filter '{v}'. Defaulting 'active'."); return 'active' # OK
        return v_lower


# =====================================================
# == Tool Function Definitions (MUST COME AFTER MODELS) ==
# =====================================================

def create_reminder_tool(user_id, params): # No type hint
    """Creates a simple reminder, potentially adding it to Google Calendar if time is specified."""
    fn_name = "create_reminder_tool"
    try: validated_params = CreateReminderParams(**params.model_dump())
    except pydantic.ValidationError as e: log_error("tool_definitions", fn_name, f"Validation Error: {e}"); return {"success": False, "item_id": None, "message": f"Invalid params: {e}"} # OK
    log_info("tool_definitions", fn_name, f"Executing for user {user_id}, desc: '{validated_params.description[:30]}...'") # OK
    try:
        data_dict = validated_params.model_dump(exclude_none=True); data_dict["type"] = "reminder"
        saved_item = task_manager.create_task(user_id, data_dict)
        if saved_item and saved_item.get("event_id"):
            return {"success": True, "item_id": saved_item.get("event_id"), "item_type": "reminder", "message": f"Reminder '{validated_params.description[:30]}...' created."}
        else: log_error("tool_definitions", fn_name, f"Task manager failed create reminder"); return {"success": False, "item_id": None, "message": "Failed to save reminder."} # OK
    except Exception as e: log_error("tool_definitions", fn_name, f"Unexpected error: {e}", e); return {"success": False, "item_id": None, "message": f"Error: {e}"} # OK

def create_task_tool(user_id, params): # No type hint
    """Creates task metadata ONLY. Does not schedule sessions or interact with calendar."""
    fn_name = "create_task_tool"
    try: validated_params = CreateTaskParams(**params.model_dump())
    except pydantic.ValidationError as e: log_error("tool_definitions", fn_name, f"Validation Error: {e}"); return {"success": False, "item_id": None, "message": f"Invalid params: {e}"} # OK
    log_info("tool_definitions", fn_name, f"Executing for user {user_id}, desc: '{validated_params.description[:30]}...'") # OK
    try:
        data_dict = validated_params.model_dump(exclude_none=True); data_dict["type"] = "task"
        if "time" in data_dict: del data_dict["time"]
        saved_item = task_manager.create_task(user_id, data_dict)
        if saved_item and saved_item.get("event_id"):
            return {"success": True, "item_id": saved_item.get("event_id"), "item_type": "task", "estimated_duration": saved_item.get("estimated_duration"), "message": f"Task '{validated_params.description[:30]}...' created (metadata only)."}
        else: log_error("tool_definitions", fn_name, f"Task manager failed create task metadata"); return {"success": False, "item_id": None, "message": "Failed to save task."} # OK
    except Exception as e: log_error("tool_definitions", fn_name, f"Unexpected error: {e}", e); return {"success": False, "item_id": None, "message": f"Error: {e}"} # OK

def propose_task_slots_tool(user_id, params): # No type hint
    """
    Finds available work session slots based on duration/timeframe BEFORE task creation.
    Uses an LLM sub-call for intelligent slot finding, considering calendar events.
    Returns proposed slots and the search context used.
    """
    fn_name = "propose_task_slots_tool"
    try: validated_params = ProposeTaskSlotsParams(**params.model_dump())
    except pydantic.ValidationError as e: log_error("tool_definitions", fn_name, f"Validation Error: {e}"); return {"success": False, "proposed_slots": None, "message": f"Invalid params: {e}", "search_context": None} # OK
    log_info("tool_definitions", fn_name, f"Executing user={user_id}. Search: duration='{validated_params.duration}', timeframe='{validated_params.timeframe}'...") # OK
    fail_result = {"success": False, "proposed_slots": None, "message": "Sorry, I encountered an issue trying to propose schedule slots.", "search_context": None}
    search_context_to_return = validated_params.model_dump()

    llm_client = get_instructor_client()
    if not llm_client: log_error("tool_definitions", fn_name, "LLM client unavailable."); return {**fail_result, "message": "Scheduler resources unavailable (LLM Client)."} # OK
    sys_prompt, human_prompt = _load_scheduler_prompts()
    if not sys_prompt or not human_prompt: log_error("tool_definitions", fn_name, "Scheduler prompts failed load."); return {**fail_result, "message": "Scheduler resources unavailable (Prompts)."} # OK

    agent_state = get_agent_state(user_id); prefs = agent_state.get("preferences", {}) if agent_state else {}
    calendar_api = _get_calendar_api_from_state(user_id); preferred_session_str = prefs.get("Preferred_Session_Length", "60m")

    task_estimated_duration_str = validated_params.duration; total_minutes = task_manager._parse_duration_to_minutes(task_estimated_duration_str)
    if total_minutes is None: log_error("tool_definitions", fn_name, f"Invalid duration '{task_estimated_duration_str}'"); return {**fail_result, "message": "Invalid duration format."} # OK
    session_minutes = task_manager._parse_duration_to_minutes(preferred_session_str) or 60
    num_slots_to_find = 1; slot_duration_str = task_estimated_duration_str
    split_pref = validated_params.split_preference.lower() if validated_params.split_preference else None
    if split_pref == 'continuous': log_info("tool_definitions", fn_name, f"Requesting 1 block: {slot_duration_str}") # OK
    elif split_pref == 'separate' or (split_pref is None and total_minutes > session_minutes):
        if session_minutes > 0: num_slots_to_find = (total_minutes + session_minutes - 1) // session_minutes
        else: num_slots_to_find = 1
        if num_slots_to_find <= 0 : num_slots_to_find = 1
        slot_duration_str = preferred_session_str; log_info("tool_definitions", fn_name, f"Requesting {num_slots_to_find} split sessions: {slot_duration_str}") # OK
    else: log_info("tool_definitions", fn_name, f"Requesting 1 block: {slot_duration_str}") # OK

    today = datetime.now().date(); start_date = today + timedelta(days=1); due_date_for_search = None
    tf = validated_params.timeframe.lower()

    if "tomorrow" in tf:
        start_date = today + timedelta(days=1)
        due_date_for_search = start_date
    elif "next week" in tf:
        start_of_next_week = today + timedelta(days=(7 - today.weekday()))
        start_date = start_of_next_week
        due_date_for_search = start_of_next_week + timedelta(days=6)
    elif "on " in tf:
        try: # Correct indentation
            date_part = tf.split("on ")[1].strip()
            parsed_date = datetime.strptime(date_part, "%Y-%m-%d").date()
            start_date = max(parsed_date, today + timedelta(days=1))
            due_date_for_search = start_date
        except Exception:
            log_warning("tool_definitions", fn_name, f"Parsing timeframe failed: '{validated_params.timeframe}'") # OK
    elif "by " in tf:
        try: # Correct indentation
            date_part = tf.split("by ")[1].strip()
            parsed_date = datetime.strptime(date_part, "%Y-%m-%d").date()
            due_date_for_search = parsed_date
            start_date = max(today + timedelta(days=1), parsed_date - timedelta(days=14))
        except Exception:
            log_warning("tool_definitions", fn_name, f"Parsing timeframe failed: '{validated_params.timeframe}'") # OK
    else:
        log_warning("tool_definitions", fn_name, f"Unclear timeframe '{validated_params.timeframe}', using default.") # OK
        due_date_for_search = start_date + timedelta(days=13)

    default_horizon_days = 56; end_date = start_date + timedelta(days=default_horizon_days - 1)
    if due_date_for_search: end_date = min(end_date, due_date_for_search - timedelta(days=1))
    end_date = max(end_date, start_date); start_date_str, end_date_str = start_date.strftime("%Y-%m-%d"), end_date.strftime("%Y-%m-%d")
    log_info("tool_definitions", fn_name, f"Derived Search Range: {start_date_str} to {end_date_str}") # OK
    search_context_to_return["effective_due_date"] = due_date_for_search.strftime("%Y-%m-%d") if due_date_for_search else None

    existing_events = [];
    if calendar_api:
        log_info("tool_definitions", fn_name, f"Fetching GCal events from {start_date_str} to {end_date_str}") # OK
        if start_date <= end_date:
            try:
                events_raw = calendar_api.list_events(start_date_str, end_date_str)
                existing_events = [{"start_datetime": ev.get("gcal_start_datetime"), "end_datetime": ev.get("gcal_end_datetime"), "summary": ev.get("title")} for ev in events_raw if ev.get("gcal_start_datetime") and ev.get("gcal_end_datetime")]
                log_info("tool_definitions", fn_name, f"Fetched {len(existing_events)} GCal events.") # OK
                log_info("tool_definitions", f"{fn_name}_DEBUG", f"Existing Events Sent: {json.dumps(existing_events, indent=2)}") # OK
            except Exception as e: log_error("tool_definitions", fn_name, f"Fetch GCal failed: {e}", e) # OK
        else: log_warning("tool_definitions", fn_name, f"Invalid search range. Skipping GCal fetch.") # OK
    else: log_warning("tool_definitions", fn_name, "GCal API inactive.") # OK

    try:
        prompt_data = {"task_description": validated_params.description or "(No description)", "task_due_date": search_context_to_return["effective_due_date"] or "(No date)", "task_estimated_duration": task_estimated_duration_str, "user_working_days": prefs.get("Work_Days", ["Mon", "Tue", "Wed", "Thu", "Fri"]), "user_work_start_time": prefs.get("Work_Start_Time", "09:00"), "user_work_end_time": prefs.get("Work_End_Time", "17:00"), "user_session_length": slot_duration_str, "existing_events_json": json.dumps(existing_events), "current_date": today.strftime("%Y-%m-%d"), "num_slots_requested": num_slots_to_find}
        log_info("tool_definitions", f"{fn_name}_DEBUG", f"Data Sent to LLM: {json.dumps(prompt_data, indent=2)}") # OK
        log_info("tool_definitions", fn_name, f"Scheduler prompt data prepared (event count: {len(existing_events)}).") # OK
    except Exception as e: log_error("tool_definitions", fn_name, f"Prep prompt data error: {e}", e); return {**fail_result, "message": "Failed prep data for scheduler."} # OK

    raw_llm_output = None; parsed_data = None
    try:
        fmt_human = human_prompt.format(**prompt_data)
        messages = [{"role": "system", "content": sys_prompt}, {"role": "user", "content": fmt_human}]
        log_info("tool_definitions", fn_name, ">>> Invoking Session Scheduler LLM...") # OK
        sched_resp = llm_client.chat.completions.create(model="gpt-4o", messages=messages, temperature=0.2, response_format={"type": "json_object"})
        raw_llm_output = sched_resp.choices[0].message.content
        log_info("tool_definitions", fn_name, f"<<< Scheduler LLM Raw Resp: {raw_llm_output[:300]}...") # OK
        log_info("tool_definitions", f"{fn_name}_DEBUG", f"Full Raw LLM Resp: {raw_llm_output}") # OK
        parsed_data = _parse_scheduler_llm_response(raw_llm_output)
        if not parsed_data: log_error("tool_definitions", fn_name, "Scheduler response parse failed."); return {**fail_result, "message": "Scheduler bad proposals format."} # OK
        log_info("tool_definitions", f"{fn_name}_DEBUG", f"Parsed LLM Resp: {json.dumps(parsed_data, indent=2)}") # OK
        log_info("tool_definitions", fn_name, f"Scheduler LLM processing successful.") # OK
        return {"success": True, "proposed_slots": parsed_data.get("proposed_sessions"), "message": parsed_data.get("response_message", "..."), "search_context": search_context_to_return}
    except Exception as e: tb_str = traceback.format_exc(); log_error("tool_definitions", fn_name, f"LLM invoke/process error: {e}. Raw: '{raw_llm_output}'. Parsed: {parsed_data}\nTraceback:\n{tb_str}", e); return {**fail_result, "message": "Error finding slots."} # OK

def finalize_task_and_book_sessions_tool(user_id, params): # No type hint
    """Creates a task metadata record AND books the approved work sessions in GCal."""
    fn_name = "finalize_task_and_book_sessions_tool"
    try: validated_params = FinalizeTaskAndBookSessionsParams(**params.model_dump())
    except pydantic.ValidationError as e: log_error("tool_definitions", fn_name, f"Validation Error: {e}"); return {"success": False, "item_id": None, "booked_count": 0, "message": f"Invalid params: {e}"} # OK

    search_context = validated_params.search_context; approved_slots = validated_params.approved_slots
    log_info("tool_definitions", fn_name, f"Executing finalize+book user={user_id}, desc='{search_context.get('description')}', slots={len(approved_slots)}") # OK
    if not search_context or not approved_slots: log_error("tool_definitions", fn_name, "Missing context/slots."); return {"success": False, "item_id": None, "booked_count": 0, "message": "Internal error: Missing context/slots."} # OK

    item_id = None; task_title = "(Error getting title)"
    try:
        task_metadata_payload = {"description": search_context.get("description", "Untitled"), "estimated_duration": search_context.get("duration"), "type": "task", "project": validated_params.project, "date": approved_slots[0].get("date"), "time": approved_slots[0].get("time")}
        task_title = task_metadata_payload["description"]
        metadata = {k: task_metadata_payload.get(k) for k in metadata_store.FIELDNAMES if k in task_metadata_payload}
        metadata.update({"user_id": user_id, "status": "pending", "created_at": datetime.now(timezone.utc).isoformat(timespec='seconds').replace('+00:00', 'Z'), "title": task_title, "internal_reminder_sent": "", "session_event_ids": json.dumps([]), "sessions_planned": 0, "sessions_completed": 0, "progress_percent": 0, "event_id": f"local_{uuid.uuid4()}"})
        final_meta = {fn: metadata.get(fn) for fn in metadata_store.FIELDNAMES}; metadata_store.save_event_metadata(final_meta); item_id = final_meta["event_id"]
        log_info("tool_definitions", fn_name, f"Initial metadata created: {item_id}"); # OK
        if AGENT_STATE_MANAGER_IMPORTED: add_task_to_context(user_id, final_meta)
    except Exception as create_err: log_error("tool_definitions", fn_name, f"Meta create failed: {create_err}", create_err); return {"success": False, "item_id": None, "booked_count": 0, "message": "Failed save task details."} # OK

    booking_result = task_manager.schedule_work_sessions(user_id, item_id, approved_slots)
    if booking_result.get("success"):
        log_info("tool_definitions", fn_name, f"Booked {booking_result.get('booked_count')} sessions for {item_id}") # OK
        return {"success": True, "item_id": item_id, "booked_count": booking_result.get("booked_count", 0), "message": booking_result.get("message", f"Task '{task_title[:30]}...' created & sessions booked.")}
    else:
        log_error("tool_definitions", fn_name, f"Meta created ({item_id}), booking failed: {booking_result.get('message')}") # OK
        try: task_manager.update_task_status(user_id, item_id, "pending")
        except: pass
        return {"success": False, "item_id": item_id, "booked_count": 0, "message": f"Task '{task_title[:30]}...' created, booking failed: {booking_result.get('message')}"}

def update_item_details_tool(user_id, params): # No type hint
    fn_name = "update_item_details_tool";
    try: validated_params = UpdateItemDetailsParams(**params.model_dump())
    except pydantic.ValidationError as e: log_error("tool_definitions", fn_name, f"Validation Error: {e}"); return {"success": False, "message": f"Invalid params: {e}"} # OK
    log_info("tool_definitions", fn_name, f"Executing user={user_id}, item={validated_params.item_id}, updates={list(validated_params.updates.keys())}") # OK
    try:
        updated_item = task_manager.update_task(user_id, validated_params.item_id, validated_params.updates)
        if updated_item: return {"success": True, "message": f"Item '{validated_params.item_id[:8]}...' updated."}
        else: log_warning("tool_definitions", fn_name, f"Update fail {validated_params.item_id}"); return {"success": False, "message": f"Failed update {validated_params.item_id[:8]}..."} # OK
    except Exception as e: log_error("tool_definitions", fn_name, f"Unexpected error: {e}", e); return {"success": False, "message": f"Error: {e}."} # OK

def update_item_status_tool(user_id, params): # No type hint
    """Changes status OR cancels/deletes item. Requires existing item_id."""
    fn_name = "update_item_status_tool"
    try: validated_params = UpdateItemStatusParams(**params.model_dump())
    except pydantic.ValidationError as e: log_error("tool_definitions", fn_name, f"Validation Error: {e}"); return {"success": False, "message": f"Invalid params: {e}"} # OK
    log_info("tool_definitions", fn_name, f"Executing user={user_id}, item={validated_params.item_id}, status={validated_params.new_status}") # OK (Corrected)
    try:
        success = False; message = ""
        if validated_params.new_status == "cancelled":
            success = task_manager.cancel_item(user_id, validated_params.item_id)
            message = f"Item '{validated_params.item_id[:8]}...' cancel processed. Result: {'Success' if success else 'Failed/Not Found'}."
        else:
            updated_item = task_manager.update_task_status(user_id, validated_params.item_id, validated_params.new_status)
            success = updated_item is not None
            message = f"Status update to '{validated_params.new_status}' for item '{validated_params.item_id[:8]}...' {'succeeded' if success else 'failed/not found'}."
        return {"success": success, "message": message}
    except Exception as e:
        log_error("tool_definitions", fn_name, f"Unexpected error: {e}", e) # OK
        return {"success": False, "message": f"Error updating status: {e}."}

def update_user_preferences_tool(user_id, params): # No type hint
    fn_name = "update_user_preferences_tool"
    try: validated_params = UpdateUserPreferencesParams(**params.model_dump())
    except pydantic.ValidationError as e: log_error("tool_definitions", fn_name, f"Validation Error: {e}"); return {"success": False, "message": f"Invalid params: {e}"} # OK
    update_keys = list(validated_params.updates.keys()) if validated_params.updates else []
    log_info("tool_definitions", fn_name, f"Executing user={user_id}, updates={update_keys}") # OK
    try:
        success = config_manager.update_preferences(user_id, validated_params.updates)
        return {"success": success, "message": f"Preferences update {'succeeded' if success else 'failed'}."}
    except Exception as e: log_error("tool_definitions", fn_name, f"Unexpected error: {e}", e); return {"success": False, "message": f"Error: {e}."} # OK

def initiate_calendar_connection_tool(user_id, params): # No type hint
    fn_name = "initiate_calendar_connection_tool"
    try: validated_params = InitiateCalendarConnectionParams(**params.model_dump())
    except pydantic.ValidationError as e: log_error("tool_definitions", fn_name, f"Validation Error: {e}"); return {"success": False, "message": f"Invalid params: {e}"} # OK
    log_info("tool_definitions", fn_name, f"Executing for user {user_id}") # OK
    try:
        result_dict = config_manager.initiate_calendar_auth(user_id)
        result_dict["success"] = result_dict.get("status") in ["pending", "token_exists"]
        return result_dict
    except Exception as e: log_error("tool_definitions", fn_name, f"Unexpected error: {e}", e); return {"success": False, "status": "fails", "message": f"Error: {e}."} # OK

def cancel_task_sessions_tool(user_id, params): # No type hint
    fn_name = "cancel_task_sessions_tool"
    try: validated_params = CancelTaskSessionsParams(**params.model_dump())
    except pydantic.ValidationError as e: log_error("tool_definitions", fn_name, f"Validation Error: {e}"); return {"success": False, "cancelled_count": 0, "message": f"Invalid params: {e}"} # OK
    log_info("tool_definitions", fn_name, f"Executing user={user_id}, Task={validated_params.task_id}, SessionIDs={validated_params.session_ids_to_cancel}") # OK
    try:
        result = task_manager.cancel_sessions(user_id, validated_params.task_id, validated_params.session_ids_to_cancel)
        return result
    except Exception as e: log_error("tool_definitions", fn_name, f"Unexpected error: {e}", e); return {"success": False, "cancelled_count": 0, "message": f"Error: {e}."} # OK

def interpret_list_reply_tool(user_id, params): # No type hint
    fn_name = "interpret_list_reply_tool"
    try: validated_params = InterpretListReplyParams(**params.model_dump())
    except pydantic.ValidationError as e: log_error("tool_definitions", fn_name, f"Validation Error: {e}"); return {"success": False, "item_ids": [], "message": f"Invalid params: {e}"} # OK
    log_warning("tool_definitions", fn_name, f"Tool executed for user {user_id} - Not Fully Implemented.") # OK
    try:
        extracted_numbers = [int(s) for s in re.findall(r'\b\d+\b', validated_params.user_reply)]
        identified_item_ids = []
        if validated_params.list_mapping:
             identified_item_ids = [validated_params.list_mapping.get(str(num)) for num in extracted_numbers if str(num) in validated_params.list_mapping]
             identified_item_ids = [item_id for item_id in identified_item_ids if item_id is not None]
        if identified_item_ids:
             log_info("tool_definitions", fn_name, f"Identified numbers {extracted_numbers} mapping to IDs: {identified_item_ids}") # OK
             return { "success": True, "action": "process", "item_ids": identified_item_ids, "message": f"Identified: {', '.join(map(str, extracted_numbers))}." }
        else: log_info("tool_definitions", fn_name, f"No valid item numbers in: '{validated_params.user_reply}'"); return {"success": False, "item_ids": [], "message": "Couldn't find item numbers."} # OK
    except Exception as e: log_error("tool_definitions", fn_name, f"Error parsing list reply: {e}", e); return {"success": False, "item_ids": [], "message": "Error interpreting reply."} # OK

def get_formatted_task_list_tool(user_id, params): # No type hint
    fn_name = "get_formatted_task_list_tool"
    try: validated_params = GetFormattedTaskListParams(**params.model_dump())
    except pydantic.ValidationError as e: log_error("tool_definitions", fn_name, f"Validation Error: {e}"); return {"success": False, "list_body": "", "list_mapping": {}, "count": 0, "message": f"Invalid params: {e}"} # OK
    log_info("tool_definitions", fn_name, f"Executing user={user_id}, Filter={validated_params.status_filter}, Proj={validated_params.project_filter}, Range={validated_params.date_range}") # OK
    try:
        date_range_tuple = tuple(validated_params.date_range) if validated_params.date_range and len(validated_params.date_range) == 2 else None
        list_body, list_mapping = task_query_service.get_formatted_list(user_id=user_id, date_range=date_range_tuple, status_filter=validated_params.status_filter, project_filter=validated_params.project_filter)
        item_count = len(list_mapping); message = f"Found {item_count} item(s)." if item_count > 0 else "No items found."
        if item_count > 0 and not list_body: log_warning("tool_definitions", fn_name, f"Found {item_count} items but empty body."); message = f"Found {item_count}, but error formatting list." # OK
        return {"success": True, "list_body": list_body or "", "list_mapping": list_mapping or {}, "count": item_count, "message": message}
    except Exception as e: log_error("tool_definitions", fn_name, f"Unexpected error: {e}", e); return {"success": False, "list_body": "", "list_mapping": {}, "count": 0, "message": f"Error getting list: {e}."} # OK


# =====================================================
# == Tool Dictionaries (MUST COME AFTER MODELS & FUNCS) ==
# =====================================================

AVAILABLE_TOOLS = {
    "create_reminder": create_reminder_tool,
    "create_task": create_task_tool,
    "propose_task_slots": propose_task_slots_tool,
    "finalize_task_and_book_sessions": finalize_task_and_book_sessions_tool,
    "update_item_details": update_item_details_tool,
    "update_item_status": update_item_status_tool,
    "update_user_preferences": update_user_preferences_tool,
    "initiate_calendar_connection": initiate_calendar_connection_tool,
    "cancel_task_sessions": cancel_task_sessions_tool,
    "interpret_list_reply": interpret_list_reply_tool,
    "get_formatted_task_list": get_formatted_task_list_tool
}

TOOL_PARAM_MODELS = {
    "create_reminder": CreateReminderParams,
    "create_task": CreateTaskParams,
    "propose_task_slots": ProposeTaskSlotsParams,
    "finalize_task_and_book_sessions": FinalizeTaskAndBookSessionsParams,
    "update_item_details": UpdateItemDetailsParams,
    "update_item_status": UpdateItemStatusParams,
    "update_user_preferences": UpdateUserPreferencesParams,
    "initiate_calendar_connection": InitiateCalendarConnectionParams,
    "cancel_task_sessions": CancelTaskSessionsParams,
    "interpret_list_reply": InterpretListReplyParams,
    "get_formatted_task_list": GetFormattedTaskListParams
}

# --- END OF FULL agents/tool_definitions.py ---
# --- END OF FILE agents\tool_definitions.py ---


================================================================================
üìÑ services\task_manager.py
================================================================================

# --- START OF FILE services\task_manager.py ---
# --- START OF FILE services/task_manager.py ---
"""
Service layer for managing tasks: creating, updating, cancelling, and scheduling sessions.
Interacts with Google Calendar API and the Metadata Store.
"""
import json
import traceback
import uuid
from datetime import datetime, timedelta, timezone # Added timezone
import re

# Tool/Service Imports
try: from tools.logger import log_info, log_error, log_warning
except ImportError:
    import logging; logging.basicConfig(level=logging.INFO)
    log_info=logging.info; log_error=logging.error; log_warning=logging.warning
    log_error("task_manager", "import", "Logger failed import.")

try: from tools.google_calendar_api import GoogleCalendarAPI
except ImportError:
    log_error("task_manager", "import", "GoogleCalendarAPI not found.")
    GoogleCalendarAPI = None

try: from tools import metadata_store; METADATA_STORE_IMPORTED = True
except ImportError:
    log_error("task_manager", "import", "metadata_store not found."); METADATA_STORE_IMPORTED = False
    # Define a minimal MockMetadataStore if needed for basic testing without the real one
    class MockMetadataStore:
        _data = {}; FIELDNAMES = ["event_id", "user_id", "type", "status", "title", "description", "date", "time", "estimated_duration", "session_event_ids", "sessions_planned", "created_at", "completed_at", "project", "original_date", "duration", "progress", "progress_percent", "internal_reminder_minutes", "internal_reminder_sent", "sessions_completed", "series_id", "gcal_start_datetime", "gcal_end_datetime"]
        def init_metadata_store(self): pass
        def save_event_metadata(self, data): self._data[data['event_id']] = {k: data.get(k) for k in self.FIELDNAMES}
        def get_event_metadata(self, event_id): return self._data.get(event_id)
        def delete_event_metadata(self, event_id): return self._data.pop(event_id, None) is not None
        def list_metadata(self, user_id, **kwargs): return [v for v in self._data.values() if v.get("user_id") == user_id]
        def load_all_metadata(self): return list(self._data.values())
    metadata_store = MockMetadataStore(); metadata_store.init_metadata_store()


try: from services.agent_state_manager import get_agent_state, add_task_to_context, update_task_in_context, remove_task_from_context; AGENT_STATE_MANAGER_IMPORTED = True
except ImportError:
    log_error("task_manager", "import", "AgentStateManager not found."); AGENT_STATE_MANAGER_IMPORTED = False
    # Define dummy functions if import fails
    def get_agent_state(*a, **k): return None
    def add_task_to_context(*a, **k): pass
    def update_task_in_context(*a, **k): pass
    def remove_task_from_context(*a, **k): pass

# Constants
DEFAULT_REMINDER_DURATION = "15m" # Duration for GCal event if reminder has time

# --- Helper Functions ---
def _get_calendar_api(user_id):
    """Safely retrieves the active calendar API instance from agent state."""
    fn_name = "_get_calendar_api"
    if not AGENT_STATE_MANAGER_IMPORTED or GoogleCalendarAPI is None:
        log_warning("task_manager", fn_name, f"Cannot get calendar API for {user_id}: Dependencies missing.")
        return None
    agent_state = get_agent_state(user_id)
    if agent_state:
        calendar_api = agent_state.get("calendar")
        if isinstance(calendar_api, GoogleCalendarAPI) and calendar_api.is_active():
            return calendar_api
        else:
            # Log only if calendar object exists but is inactive
            if isinstance(calendar_api, GoogleCalendarAPI) and not calendar_api.is_active():
                 log_info("task_manager", fn_name, f"Calendar API found but inactive for user {user_id}.")
    return None

def _parse_duration_to_minutes(duration_str):
    """Parses duration strings like '2h', '90m', '1.5h' into minutes."""
    fn_name = "_parse_duration_to_minutes"
    if not duration_str or not isinstance(duration_str, str):
        return None

    duration_str = duration_str.lower().replace(' ','')
    total_minutes = 0.0 # Use float for intermediate calculation

    try:
        # Match patterns like '1.5h30m', '2h', '90m', '1.5h', '0.5h'
        hour_match = re.search(r'(\d+(\.\d+)?)\s*h', duration_str)
        minute_match = re.search(r'(\d+)\s*m', duration_str)

        if hour_match:
            total_minutes += float(hour_match.group(1)) * 60
            # Check if minutes follow directly after hours (e.g., in '1h30m')
            # to avoid double counting minutes if 'm' is also present separately
            remaining_str = duration_str[hour_match.end():]
            minute_after_hour_match = re.match(r'(\d+)\s*m', remaining_str)
            if minute_after_hour_match:
                 total_minutes += int(minute_after_hour_match.group(1))
                 # Prevent separate minute match from adding again
                 minute_match = None
        elif minute_match: # Only look for minutes if hours weren't found first
            total_minutes += int(minute_match.group(1))

        # Handle cases where only a number was provided (assume minutes)
        if total_minutes == 0 and hour_match is None and minute_match is None:
             if duration_str.replace('.','',1).isdigit():
                  log_warning("task_manager", fn_name, f"Assuming minutes for duration input: '{duration_str}'")
                  total_minutes = float(duration_str)
             else:
                  # Could not parse as hours, minutes, or plain number
                  raise ValueError("Unrecognized duration format")

        return int(round(total_minutes)) if total_minutes > 0 else None

    except (ValueError, TypeError) as e:
        log_warning("task_manager", fn_name, f"Could not parse duration string '{duration_str}': {e}")
        return None

# ==============================================================
# Core Service Functions
# ==============================================================

def create_task(user_id, task_params):
    """Creates a task or reminder, saves metadata, optionally adds to GCal."""
    fn_name = "create_task"
    item_type = task_params.get("type")
    if not item_type:
        log_error("task_manager", fn_name, "Missing 'type' in task_params.")
        return None
    log_info("task_manager", fn_name, f"Creating item for {user_id}, type: {item_type}")
    if not METADATA_STORE_IMPORTED:
        log_error("task_manager", fn_name, "Metadata store unavailable.")
        return None

    calendar_api = _get_calendar_api(user_id)
    google_event_id = None

    try:
        # Initialize metadata with known fields
        metadata = {k: task_params.get(k) for k in metadata_store.FIELDNAMES if k in task_params}
        metadata["user_id"] = user_id
        metadata["status"] = "pending"
        # *** Store created_at in UTC ISO format ***
        metadata["created_at"] = datetime.now(timezone.utc).isoformat(timespec='seconds').replace('+00:00', 'Z')
        metadata["title"] = task_params.get("description", "Untitled Item")
        metadata["type"] = item_type
        # *** Initialize internal_reminder_sent as empty string ***
        metadata["internal_reminder_sent"] = "" # Default to not sent

        # Task specific defaults
        if item_type == "task":
            metadata["session_event_ids"] = json.dumps([])
            metadata["sessions_planned"] = 0
            metadata["sessions_completed"] = 0
            metadata["progress_percent"] = 0

        # --- Modified GCal Reminder Creation Logic ---
        item_time = task_params.get("time")
        has_time = item_time is not None and item_time != ""
        should_create_gcal = item_type == "reminder" and has_time and calendar_api is not None

        log_info("task_manager", fn_name, f"Checking GCal creation: type='{item_type}', has_time={has_time}, calendar_api_present={calendar_api is not None}. Should create: {should_create_gcal}")

        if should_create_gcal:
            log_info("task_manager", fn_name, f"Attempting to create GCal Reminder event for user {user_id}, item: '{metadata['title'][:30]}...'")
            gcal_event_payload = {
                "title": metadata.get("title"),
                "description": f"Reminder: {metadata.get('description', '')}",
                "date": task_params.get("date"),
                "time": item_time,
                "duration": DEFAULT_REMINDER_DURATION
            }
            try:
                # Explicitly log before the API call
                log_info("task_manager", fn_name, f"Calling calendar_api.create_event for user {user_id} with payload: {gcal_event_payload}")
                google_event_id = calendar_api.create_event(gcal_event_payload) # This might return None or raise Exception

                # *** ADDED Logging: Check result immediately ***
                if google_event_id:
                    log_info("task_manager", fn_name, f"GCal Reminder event CREATED successfully, ID: {google_event_id}")
                    metadata["event_id"] = google_event_id
                    # Fetch details to get exact times
                    gcal_event_details = calendar_api._get_single_event(google_event_id)
                    if gcal_event_details:
                        parsed_gcal = calendar_api._parse_google_event(gcal_event_details)
                        metadata["gcal_start_datetime"] = parsed_gcal.get("gcal_start_datetime")
                        metadata["gcal_end_datetime"] = parsed_gcal.get("gcal_end_datetime")
                        log_info("task_manager", fn_name, f"Fetched GCal details for new event {google_event_id}")
                    else:
                         log_warning("task_manager", fn_name, f"Created GCal event {google_event_id} but failed to fetch its details afterward.")
                else:
                    # calendar_api.create_event returned None
                    log_warning("task_manager", fn_name, f"GCal Reminder event creation FAILED (returned None) for user {user_id}. Assigning local ID.")
                    metadata["event_id"] = f"local_{uuid.uuid4()}"

            except Exception as gcal_create_err:
                # Log any exception during the create_event call
                log_error("task_manager", fn_name, f"Error calling calendar_api.create_event for user {user_id}", gcal_create_err)
                metadata["event_id"] = f"local_{uuid.uuid4()}" # Assign local ID if GCal failed
                google_event_id = None # Ensure google_event_id is None after failure
        else:
            # Log reason for not attempting GCal creation
            reason = ""
            if item_type != "reminder": reason = "Item is not a reminder."
            elif not has_time: reason = "Reminder has no specific time."
            elif calendar_api is None: reason = "Calendar API is not active/available."
            log_info("task_manager", fn_name, f"Assigning local ID for {item_type}. Reason: {reason}")
            metadata["event_id"] = f"local_{uuid.uuid4()}"
        # --- End Modified GCal Reminder Creation Logic ---

        # Final validation before saving
        if not metadata.get("event_id"):
            log_error("task_manager", fn_name, "Critical error: Metadata missing 'event_id' before save attempt.")
            return None

        # Prepare final dict and save
        final_meta = {fn: metadata.get(fn) for fn in metadata_store.FIELDNAMES}
        metadata_store.save_event_metadata(final_meta)
        log_info("task_manager", fn_name, f"Metadata saved for event_id: {final_meta['event_id']}")

        # Update in-memory context
        if AGENT_STATE_MANAGER_IMPORTED:
             add_task_to_context(user_id, final_meta)

        return final_meta

    except Exception as e:
        tb = traceback.format_exc()
        log_error("task_manager", fn_name, f"Overall error creating item for {user_id}. Trace:\n{tb}", e)
        # Rollback GCal event if it was successfully created in this failed attempt
        if google_event_id and calendar_api:
            log_warning("task_manager", fn_name, f"Attempting final GCal rollback for event {google_event_id} due to overall error.")
            try: calendar_api.delete_event(google_event_id)
            except Exception as final_rollback_e: log_error("task_manager", fn_name, f"Error during final GCal rollback for event {google_event_id}", final_rollback_e)
        return None

def update_task(user_id, item_id, updates):
    """Updates details of an existing task/reminder."""
    fn_name = "update_task"
    log_info("task_manager", fn_name, f"Updating item {item_id} for {user_id}, keys: {list(updates.keys())}")
    if not METADATA_STORE_IMPORTED:
        log_error("task_manager", fn_name, "Metadata store unavailable.")
        return None

    calendar_api = _get_calendar_api(user_id)
    gcal_updated = False

    try:
        existing = metadata_store.get_event_metadata(item_id)
        if not existing:
            log_error("task_manager", fn_name, f"Metadata not found for item {item_id}.")
            return None
        if existing.get("user_id") != user_id:
            log_error("task_manager", fn_name, f"User mismatch for item {item_id}.")
            return None

        item_type = existing.get("type")
        needs_gcal_update, gcal_payload = False, {}

        # Check if GCal update is needed (only for non-local reminders with active calendar)
        if item_type == "reminder" and not item_id.startswith("local_") and calendar_api:
            # Map metadata update keys to potential GCal payload keys
            if "description" in updates: gcal_payload["title"] = updates["description"]; gcal_payload["description"] = f"Reminder: {updates['description']}"; needs_gcal_update = True
            if "date" in updates or "time" in updates:
                 # Note: GCal update needs full date/time usually, handle carefully
                 gcal_payload["date"] = updates.get("date", existing.get("date"))
                 gcal_payload["time"] = updates.get("time", existing.get("time")) # Handle potential None value
                 if "time" in updates and updates["time"] is None: # Explicitly clearing time
                      gcal_payload["time"] = None
                 needs_gcal_update = True
            # Add other relevant mappings if needed

            if needs_gcal_update:
                 log_info("task_manager", fn_name, f"Attempting GCal Reminder update for {item_id}")
                 gcal_updated = calendar_api.update_event(item_id, gcal_payload)
                 if not gcal_updated: log_warning("task_manager", fn_name, f"GCal Reminder update failed for {item_id}.")
                 else: log_info("task_manager", fn_name, f"GCal Reminder update OK for {item_id}")
            else: log_info("task_manager", fn_name, f"No relevant GCal fields to update for reminder {item_id}.")
        # else: log info about skipping GCal update for tasks/local items/inactive calendar?

        # Update metadata store
        allowed_meta_keys = {"description", "date", "time", "estimated_duration", "project"}
        valid_meta_updates = {k: v for k, v in updates.items() if k in allowed_meta_keys}

        if not valid_meta_updates:
             log_warning("task_manager", fn_name, f"No valid metadata fields in updates for {item_id}. No metadata change.")
             # Return existing if no changes, but maybe GCal updated? Check gcal_updated?
             # For simplicity, let's return existing if no *metadata* changes.
             return existing

        updated_metadata = existing.copy()
        updated_metadata.update(valid_meta_updates)
        if "description" in valid_meta_updates: updated_metadata["title"] = valid_meta_updates["description"] # Keep title synced with description
        if "time" in valid_meta_updates and valid_meta_updates["time"] is None: updated_metadata["time"] = None # Ensure None is stored if cleared

        # Fetch updated GCal details if update occurred
        if gcal_updated and calendar_api:
             gcal_event_details = calendar_api._get_single_event(item_id)
             if gcal_event_details:
                  parsed_gcal = calendar_api._parse_google_event(gcal_event_details)
                  updated_metadata["gcal_start_datetime"] = parsed_gcal.get("gcal_start_datetime")
                  updated_metadata["gcal_end_datetime"] = parsed_gcal.get("gcal_end_datetime")

        meta_to_save = {fn: updated_metadata.get(fn) for fn in metadata_store.FIELDNAMES}
        metadata_store.save_event_metadata(meta_to_save)
        log_info("task_manager", fn_name, f"Metadata updated successfully for {item_id}")

        # Update in-memory context
        if AGENT_STATE_MANAGER_IMPORTED:
             update_task_in_context(user_id, item_id, meta_to_save)

        return meta_to_save

    except Exception as e:
        tb = traceback.format_exc()
        log_error("task_manager", fn_name, f"Error updating item {item_id} for {user_id}. Trace:\n{tb}", e)
        return None

def update_task_status(user_id, item_id, new_status):
    """Updates only the status and related tracking fields."""
    fn_name = "update_task_status"
    log_info("task_manager", fn_name, f"Setting status='{new_status}' for item {item_id}, user {user_id}")
    if new_status == "cancelled":
        log_error("task_manager", fn_name, "Use cancel_item() function for 'cancelled' status.")
        return None
    try:
        existing = metadata_store.get_event_metadata(item_id)
        if not existing:
            log_error("task_manager", fn_name, f"Metadata not found for item {item_id}.")
            return None
        if existing.get("user_id") != user_id:
            log_error("task_manager", fn_name, f"User mismatch for item {item_id}.")
            return None

        updates_dict = {"status": new_status}
        if new_status.lower() == "completed":
            # *** Store completed_at in UTC ISO format ***
            updates_dict["completed_at"] = datetime.now(timezone.utc).isoformat(timespec='seconds').replace('+00:00', 'Z')
            updates_dict["progress_percent"] = 100
            if existing.get("type") == "task":
                updates_dict["sessions_completed"] = existing.get("sessions_planned", 0) # Mark all planned as done
        elif new_status.lower() in ["pending", "in_progress", "in progress"]:
             updates_dict["completed_at"] = "" # Use empty string for None/cleared value in CSV
             if new_status.lower() == "pending":
                  updates_dict["progress_percent"] = 0 # Reset progress

        # Apply updates to a copy
        updated_metadata = existing.copy()
        updated_metadata.update(updates_dict)

        # Prepare final dict with only defined FIELDNAMES
        meta_to_save = {fn: updated_metadata.get(fn) for fn in metadata_store.FIELDNAMES}
        metadata_store.save_event_metadata(meta_to_save)
        log_info("task_manager", fn_name, f"Metadata status updated successfully for {item_id}")

        # Update in-memory context
        if AGENT_STATE_MANAGER_IMPORTED:
             update_task_in_context(user_id, item_id, meta_to_save)

        return meta_to_save
    except Exception as e:
        log_error("task_manager", fn_name, f"Error saving status update for {item_id}", e)
        return None

def cancel_item(user_id, item_id):
    """Sets item status to 'cancelled' and deletes associated GCal events."""
    fn_name = "cancel_item"
    log_info("task_manager", fn_name, f"Processing cancellation for item {item_id}, user {user_id}")
    if not METADATA_STORE_IMPORTED:
        log_error("task_manager", fn_name, "Metadata store unavailable.")
        return False

    calendar_api = _get_calendar_api(user_id)
    gcal_cleanup_ok = True # Assume okay unless error occurs

    try:
        metadata = metadata_store.get_event_metadata(item_id)
        if not metadata:
            log_warning("task_manager", fn_name, f"Metadata not found for {item_id} during cancel. Assuming already handled.")
            return True
        if metadata.get("user_id") != user_id:
            log_error("task_manager", fn_name, f"User mismatch for item {item_id}.")
            return False
        if metadata.get("status") == "cancelled":
            log_info("task_manager", fn_name, f"Item {item_id} is already cancelled.")
            return True

        item_type = metadata.get("type")

        # --- GCal Cleanup ---
        # ... (GCal cleanup logic remains the same) ...
        if calendar_api and not item_id.startswith("local_"):
            # ... (reminder and task session deletion) ...
            pass # Keep existing GCal cleanup logic here
        elif not calendar_api:
             log_info("task_manager", fn_name, f"Calendar API inactive for user {user_id}. Skipping GCal cleanup for {item_id}.")
        else: # Item is local_
             log_info("task_manager", fn_name, f"Item {item_id} is local. No GCal cleanup needed.")
        # --- End GCal Cleanup ---

        # --- Update Metadata Status DIRECTLY ---
        log_info("task_manager", fn_name, f"Updating metadata status to cancelled for {item_id}")
        metadata["status"] = "cancelled"
        # Optionally clear session data from metadata?
        # metadata["session_event_ids"] = json.dumps([])
        # metadata["sessions_planned"] = 0
        try:
            # Prepare final dict with only defined FIELDNAMES
            meta_to_save = {fn: metadata.get(fn) for fn in metadata_store.FIELDNAMES}
            metadata_store.save_event_metadata(meta_to_save)
            log_info("task_manager", fn_name, f"Successfully marked item {item_id} as cancelled in metadata.")
            # Update in-memory context
            if AGENT_STATE_MANAGER_IMPORTED:
                 # Use update, not remove, to reflect the 'cancelled' status
                 update_task_in_context(user_id, item_id, meta_to_save)
            return True
        except Exception as meta_save_err:
             log_error("task_manager", fn_name, f"Failed to save metadata status update to cancelled for {item_id}", meta_save_err)
             # Should we try to rollback GCal changes here? Difficult.
             return False
        # --- End Direct Metadata Update ---

    except Exception as e:
        tb = traceback.format_exc()
        log_error("task_manager", fn_name, f"Error cancelling item {item_id} for user {user_id}. Trace:\n{tb}", e)
        return False

# --- Scheduling Functions ---

def schedule_work_sessions(user_id, task_id, slots_to_book):
    """Creates GCal events for proposed work sessions and updates the main task metadata."""
    fn_name = "schedule_work_sessions"
    log_info("task_manager", fn_name, f"Booking {len(slots_to_book)} sessions for task {task_id}")
    if not METADATA_STORE_IMPORTED:
         return {"success": False, "message": "Metadata store unavailable.", "session_ids": []}

    calendar_api = _get_calendar_api(user_id)
    if not calendar_api:
        return {"success": False, "message": "Calendar is not connected or active. Cannot schedule sessions.", "session_ids": []}

    # --- Get Task Details & Preferences ---
    task_metadata = metadata_store.get_event_metadata(task_id)
    if not task_metadata or task_metadata.get("user_id") != user_id:
        log_error("task_manager", fn_name, f"Main task {task_id} not found or user mismatch.")
        return {"success": False, "message": "Could not find the original task details.", "session_ids": []}
    if task_metadata.get("type") != "task":
         log_error("task_manager", fn_name, f"Item {task_id} is not a task. Cannot schedule sessions.")
         return {"success": False, "message": "Scheduling is only supported for items of type 'task'.", "session_ids": []}

    task_title = task_metadata.get("title", "Task Work")
    agent_state = get_agent_state(user_id) # Reuse state if already fetched?
    prefs = agent_state.get("preferences", {}) if agent_state else {}
    session_length_str = prefs.get("Preferred_Session_Length", "60m")
    session_length_minutes = _parse_duration_to_minutes(session_length_str) or 60

    # --- Create GCal Events ---
    created_session_ids = []
    errors = []
    for i, session_slot in enumerate(slots_to_book):
        session_date = session_slot.get("date")
        session_time = session_slot.get("time")
        if not session_date or not session_time:
             log_warning("task_manager", fn_name, f"Skipping session {i+1} for task {task_id}: missing date or time.")
             errors.append(f"Session {i+1} missing data")
             continue

        try:
            session_event_data = {
                "title": f"Work: {task_title} [{i+1}/{len(slots_to_book)}]",
                "description": f"Focused work session for task: {task_title}\nParent Task ID: {task_id}",
                "date": session_date, "time": session_time,
                "duration": f"{session_length_minutes}m"
            }
            session_event_id = calendar_api.create_event(session_event_data)
            if session_event_id:
                created_session_ids.append(session_event_id)
            else:
                log_error("task_manager", fn_name, f"Failed to create GCal event for session {i+1} of task {task_id}.")
                errors.append(f"Session {i+1} GCal creation failed")
        except Exception as e:
            log_error("task_manager", fn_name, f"Error creating GCal event for session {i+1} of task {task_id}", e)
            errors.append(f"Session {i+1} creation error: {type(e).__name__}")

    if not created_session_ids:
        log_error("task_manager", fn_name, f"Failed to create any GCal session events for task {task_id}.")
        error_summary = "; ".join(errors)
        return {"success": False, "message": f"Sorry, I couldn't add the proposed sessions to your calendar. Errors: {error_summary}", "session_ids": []}

    log_info("task_manager", fn_name, f"Successfully created {len(created_session_ids)} GCal session events for task {task_id}: {created_session_ids}")

    # --- Update Main Task Metadata ---
    try:
        existing_ids_json = task_metadata.get("session_event_ids", "[]")
        existing_session_ids = json.loads(existing_ids_json) if isinstance(existing_ids_json, str) and existing_ids_json.strip() else []
        if not isinstance(existing_session_ids, list): existing_session_ids = []
    except json.JSONDecodeError:
        log_error("task_manager", fn_name, f"Failed to parse existing session IDs for task {task_id}. Overwriting.")
        existing_session_ids = []

    # Combine old and new, removing duplicates
    all_session_ids = list(set(existing_session_ids + created_session_ids))

    metadata_update_payload = {
        "sessions_planned": len(all_session_ids),
        "session_event_ids": json.dumps(all_session_ids),
        "status": "in_progress" # Update status when sessions are booked
    }

    # Apply updates to a copy
    updated_metadata = task_metadata.copy()
    updated_metadata.update(metadata_update_payload)
    # Prepare final dict with only defined FIELDNAMES
    meta_to_save = {fn: updated_metadata.get(fn) for fn in metadata_store.FIELDNAMES}

    try:
        metadata_store.save_event_metadata(meta_to_save)
        log_info("task_manager", fn_name, f"Updated parent task {task_id} metadata with session info.")
        if AGENT_STATE_MANAGER_IMPORTED: update_task_in_context(user_id, task_id, meta_to_save)
    except Exception as meta_e:
         log_error("task_manager", fn_name, f"Created GCal sessions, but failed update metadata for task {task_id}.", meta_e)
         log_warning("task_manager", fn_name, f"Attempting rollback of {len(created_session_ids)} GCal sessions for {task_id}.")
         if calendar_api:
             for sid in created_session_ids:
                 try: calendar_api.delete_event(sid)
                 except Exception as del_e: log_error("task_manager", fn_name, f"Rollback delete failed for session {sid}", del_e)
         else: log_warning("task_manager", fn_name, "Cannot perform GCal rollback as calendar_api is not available.")
         return {"success": False, "message": "Scheduled sessions in calendar, but failed to link them to the task. Rolling back calendar changes.", "session_ids": []}

    # --- Return Success ---
    num_sessions = len(created_session_ids)
    plural_s = "s" if num_sessions > 1 else ""
    success_message = f"Okay, I've scheduled {num_sessions} work session{plural_s} for '{task_title}' in your calendar."
    if errors: success_message += f" (Note: Issues encountered with {len(errors)} potential sessions)."

    return {"success": True, "message": success_message, "session_ids": created_session_ids}


def cancel_sessions(user_id, task_id, session_ids_to_cancel):
    """Cancels specific GCal work sessions and updates task metadata."""
    fn_name = "cancel_sessions"
    log_info("task_manager", fn_name, f"Cancelling {len(session_ids_to_cancel)} sessions for task {task_id}")
    if not METADATA_STORE_IMPORTED:
        return {"success": False, "cancelled_count": 0, "message": "Metadata store unavailable."}

    calendar_api = _get_calendar_api(user_id)
    if not calendar_api:
        return {"success": False, "cancelled_count": 0, "message": "Calendar is not connected or active."}

    # --- Get Task Metadata ---
    task_metadata = metadata_store.get_event_metadata(task_id)
    if not task_metadata or task_metadata.get("user_id") != user_id:
        log_error("task_manager", fn_name, f"Original task {task_id} not found or user mismatch.")
        return {"success": False, "cancelled_count": 0, "message": "Original task details not found."}
    if task_metadata.get("type") != "task":
         return {"success": False, "cancelled_count": 0, "message": "Can only cancel sessions for tasks."}

    # --- Delete GCal Events ---
    cancelled_count = 0
    errors = []
    valid_ids_to_cancel_gcal = [sid for sid in session_ids_to_cancel if not str(sid).startswith("local_")]

    for session_id in valid_ids_to_cancel_gcal:
        try:
            deleted = calendar_api.delete_event(session_id)
            if deleted:
                cancelled_count += 1
            # else: delete_event now returns True even if 404/410, so this branch likely won't hit often
        except Exception as e:
            log_error("task_manager", fn_name, f"Error deleting GCal session {session_id} for task {task_id}", e)
            errors.append(session_id)

    log_info("task_manager", fn_name, f"GCal delete attempts completed for task {task_id}. Success/Gone: {cancelled_count}, Errors: {len(errors)}")

    # --- Update Metadata ---
    try:
        existing_ids_json = task_metadata.get("session_event_ids", "[]")
        existing_session_ids = json.loads(existing_ids_json) if isinstance(existing_ids_json, str) and existing_ids_json.strip() else []
        if not isinstance(existing_session_ids, list): existing_session_ids = []
    except json.JSONDecodeError:
        log_error("task_manager", fn_name, f"Corrupted session IDs JSON for task {task_id}. Resetting.")
        existing_session_ids = []

    cancelled_set = set(session_ids_to_cancel) # Use the original list including potential local IDs
    remaining_ids = [sid for sid in existing_session_ids if sid not in cancelled_set]

    metadata_updates = {
        "sessions_planned": len(remaining_ids),
        "session_event_ids": json.dumps(remaining_ids)
        # Consider if status should change back from 'in_progress' if all sessions cancelled?
        # "status": "pending" if not remaining_ids else "in_progress"
    }
    updated_metadata = task_metadata.copy()
    updated_metadata.update(metadata_updates)
    meta_to_save = {fn: updated_metadata.get(fn) for fn in metadata_store.FIELDNAMES}

    try:
        metadata_store.save_event_metadata(meta_to_save)
        log_info("task_manager", fn_name, f"Updated parent task {task_id} metadata after cancelling sessions.")
        if AGENT_STATE_MANAGER_IMPORTED: update_task_in_context(user_id, task_id, meta_to_save)
        message = f"Successfully cancelled {cancelled_count} session(s)." # Report GCal deletions
        if errors: message += f" Encountered errors cancelling {len(errors)}."
        return {"success": True, "cancelled_count": cancelled_count, "message": message}
    except Exception as meta_e:
        log_error("task_manager", fn_name, f"Deleted GCal sessions, but failed update metadata for task {task_id}.", meta_e)
        # This is tricky - GCal events are gone, but metadata isn't updated.
        return {"success": False, "cancelled_count": cancelled_count, "message": "Cancelled sessions in calendar, but failed to update the task link."}
# --- END OF FILE services/task_manager.py ---
# --- END OF FILE services\task_manager.py ---


================================================================================
üìÑ services\task_query_service.py
================================================================================

# --- START OF FILE services\task_query_service.py ---
# --- START OF FILE services/task_query_service.py ---
"""Service layer for querying and formatting task/reminder data."""
from datetime import datetime, timedelta
import json
from typing import Dict, List, Any, Optional, Set, Tuple # Added Set/Tuple
import pytz
from tools.logger import log_info, log_error, log_warning
from tools import metadata_store

# Agent State Manager Import
try:
    from services.agent_state_manager import get_context, get_agent_state
    AGENT_STATE_MANAGER_IMPORTED = True
except ImportError:
    log_error("task_query_service", "import", "AgentStateManager not found.")
    AGENT_STATE_MANAGER_IMPORTED = False
    def get_context(*args, **kwargs): return None
    def get_agent_state(*args, **kwargs): return None

# Google Calendar API Import
try:
    from tools.google_calendar_api import GoogleCalendarAPI
    GCAL_API_IMPORTED = True
except ImportError:
     log_warning("task_query_service", "import", "GoogleCalendarAPI not imported.")
     GoogleCalendarAPI = None
     GCAL_API_IMPORTED = False

ACTIVE_STATUSES = {"pending", "in_progress"}

# --- Internal Helper Functions ---

def _get_calendar_api_from_state(user_id):
    """Helper to retrieve the active calendar API instance from agent state."""
    fn_name = "_get_calendar_api_from_state" # Added fn_name
    if not AGENT_STATE_MANAGER_IMPORTED or not GCAL_API_IMPORTED: return None
    try:
        agent_state = get_agent_state(user_id)
        if agent_state:
            calendar_api_maybe = agent_state.get("calendar")
            if isinstance(calendar_api_maybe, GoogleCalendarAPI) and calendar_api_maybe.is_active():
                return calendar_api_maybe
    except Exception as e:
         log_error("task_query_service", fn_name, f"Error getting calendar API for {user_id}", e) # Use fn_name
    return None

def _filter_tasks_by_status(task_list, status_filter='active'):
    """Filters task list by status."""
    fn_name = "_filter_tasks_by_status" # Added fn_name
    filter_lower = status_filter.lower()
    if filter_lower == 'all': return task_list
    target_statuses = set()
    if filter_lower == 'active': target_statuses = ACTIVE_STATUSES
    elif filter_lower == 'completed': target_statuses = {"completed"}
    elif filter_lower == 'pending': target_statuses = {"pending"}
    elif filter_lower == 'in_progress': target_statuses = {"in_progress", "inprogress"} # Allow both variants
    else:
        log_warning("task_query_service", fn_name, f"Unknown status filter '{status_filter}'. Defaulting 'active'."); # Use fn_name
        target_statuses = ACTIVE_STATUSES
    return [task for task in task_list if str(task.get("status", "pending")).lower() in target_statuses]

def _filter_tasks_by_date_range(task_list, date_range):
    """Filters task list by 'date' field."""
    fn_name = "_filter_tasks_by_date_range" # Added fn_name
    if not date_range or len(date_range) != 2:
        log_warning("task_query_service", fn_name, f"Invalid date_range provided: {date_range}. Skipping filter.")
        return task_list
    try:
        start_date = datetime.strptime(date_range[0], "%Y-%m-%d").date()
        end_date = datetime.strptime(date_range[1], "%Y-%m-%d").date()
        filtered = []
        for task in task_list:
            task_date_str = task.get("date")
            if task_date_str and isinstance(task_date_str, str):
                try:
                    task_date = datetime.strptime(task_date_str, "%Y-%m-%d").date()
                    if start_date <= task_date <= end_date: filtered.append(task)
                except (ValueError, TypeError): pass # Skip tasks with unparseable dates silently
        return filtered
    except Exception as e:
        log_error("task_query_service", fn_name, f"Error filtering by date range {date_range}: {e}", e); # Use fn_name
        return [] # Return empty list on error

def _filter_tasks_by_project(task_list, project_filter):
    """Filters task list by project tag (case-insensitive)."""
    if not project_filter: return task_list
    filter_lower = project_filter.lower()
    return [task for task in task_list if task.get("project") is not None and str(task.get("project", "")).lower() == filter_lower]

def _sort_tasks(task_list):
    """Sorts task list robustly by date/time."""
    fn_name = "_sort_tasks" # Added fn_name
    def sort_key(item):
        # Use gcal_start_datetime if available and valid, otherwise fallback
        gcal_start = item.get("gcal_start_datetime")
        if gcal_start and isinstance(gcal_start, str):
             try:
                 dt_aware = datetime.fromisoformat(gcal_start.replace('Z', '+00:00'))
                 return dt_aware.replace(tzinfo=None) # Compare naive UTC equivalent
             except ValueError:
                  log_warning("task_query_service", f"{fn_name}.sort_key", f"Could not parse gcal_start_datetime '{gcal_start}' for {item.get('event_id')}. Falling back.")

        # Fallback logic using date/time
        meta_date_str = item.get("date")
        meta_time_str = item.get("time")
        sort_dt = datetime.max # Default to max for sorting unknowns last

        if meta_date_str:
            try:
                time_part = meta_time_str if meta_time_str else "00:00:00"
                if len(time_part.split(':')) == 2: time_part += ':00'
                if len(meta_date_str) == 10 and len(time_part) == 8:
                     sort_dt = datetime.strptime(f"{meta_date_str} {time_part}", "%Y-%m-%d %H:%M:%S")
                elif len(meta_date_str) == 10: # All day if only date
                     sort_dt = datetime.strptime(meta_date_str, "%Y-%m-%d")
                else:
                     log_warning("task_query_service", f"{fn_name}.sort_key", f"Invalid date/time format for {item.get('event_id')}: D='{meta_date_str}' T='{meta_time_str}'.")
            except (ValueError, TypeError):
                 log_warning("task_query_service", f"{fn_name}.sort_key", f"Could not parse date/time for {item.get('event_id')}: D='{meta_date_str}' T='{meta_time_str}'.")
        return sort_dt

    try:
        # Sort primarily by datetime, secondarily by creation time, finally by title
        return sorted(task_list, key=lambda item: (sort_key(item), item.get("created_at", ""), item.get("title", "").lower()))
    except Exception as e:
        log_error("task_query_service", fn_name, f"Error during task sorting: {e}", e)
        return task_list # Return unsorted list on error

def _format_task_line(task_data, user_timezone_str="UTC", calendar_api=None): # Added calendar_api param
    """
    Formats a single task/event dictionary into a display string, converting
    aware datetimes to the user's timezone. For tasks, it fetches and lists
    details of scheduled work sessions from Google Calendar if available.

    Args:
        task_data (dict): Dictionary containing item data.
        user_timezone_str (str): The user's Olson timezone string. Defaults to 'UTC'.
        calendar_api (GoogleCalendarAPI | None): Active GCal API instance for the user.

    Returns:
        str: A formatted string representation of the item.
    """
    fn_name = "_format_task_line"
    try:
        # Determine User Timezone Object
        user_tz = pytz.utc
        try:
            if user_timezone_str: user_tz = pytz.timezone(user_timezone_str)
        except pytz.UnknownTimeZoneError:
            log_warning("task_query_service", fn_name, f"Unknown timezone '{user_timezone_str}'. Using UTC for formatting item {task_data.get('event_id')}.")
            user_timezone_str = "UTC"

        # --- Assemble Main Line Parts ---
        parts = []
        item_type = str(task_data.get("type", "Item")).capitalize()
        if item_type.lower() == "external_event": item_type = "Event"
        elif item_type.lower() == "task": item_type = "Task"
        elif item_type.lower() == "reminder": item_type = "Reminder"
        parts.append(f"({item_type})")

        desc = str(task_data.get("title", "")).strip() or str(task_data.get("description", "")).strip() or "(No Title)"
        parts.append(desc)

        if item_type.lower() == "task":
            duration = task_data.get("estimated_duration")
            is_empty_like = duration is None or (isinstance(duration, str) and duration.strip().lower() in ['', 'none', 'nan'])
            if duration and not is_empty_like:
                parts.append(f"[Est: {duration}]")

        gcal_start_str = task_data.get("gcal_start_datetime")
        meta_date = task_data.get("date")
        meta_time = task_data.get("time")
        dt_str = ""

        if gcal_start_str:
             try:
                 dt_aware = datetime.fromisoformat(gcal_start_str.replace('Z', '+00:00'))
                 dt_local = dt_aware.astimezone(user_tz)
                 formatted_dt = dt_local.strftime('%Y-%m-%d @ %H:%M %Z')
                 dt_str = f" on {formatted_dt}"
             except (ValueError, TypeError) as parse_err:
                 log_warning("task_query_service", fn_name, f"Could not parse/convert gcal_start_datetime '{gcal_start_str}' for {task_data.get('event_id')}. Error: {parse_err}. Falling back.")
                 dt_str = f" (Time Error: {gcal_start_str})"
        elif meta_date:
             dt_str = f" on {meta_date}"
             if meta_time: dt_str += f" at {meta_time}"
             else: dt_str += " (All day)"

        if dt_str: parts.append(dt_str)

        project = task_data.get("project")
        if project: parts.append(f"{{{project}}}")

        if item_type.lower() in ["task", "reminder"]:
            status = str(task_data.get("status", "pending")).capitalize()
            parts.append(f"[{status}]")

        main_line = " ".join(p for p in parts if p)

        # --- ADDED: Fetch and Display Scheduled Session Details ---
        session_details_lines = []
        # Check type is Task, calendar_api is valid, and session_event_ids exists
        if item_type.lower() == "task" and calendar_api and task_data.get("session_event_ids"):
            sessions_json = task_data.get("session_event_ids")
            if isinstance(sessions_json, str): # Ensure it's a string before trying to parse
                session_ids = []
                try:
                    session_ids = json.loads(sessions_json)
                    if not isinstance(session_ids, list): session_ids = [] # Ensure it parsed to a list
                except json.JSONDecodeError:
                    log_warning("task_query_service", fn_name, f"Could not parse session_event_ids JSON for task {task_data.get('event_id')}: {sessions_json}")
                    session_details_lines.append("    ‚îî‚îÄ‚îÄ (Error reading session data)")
                except Exception as e: # Catch unexpected parsing errors
                    log_error("task_query_service", fn_name, f"Unexpected error parsing session JSON for task {task_data.get('event_id')}: {e}", e)
                    session_details_lines.append("    ‚îî‚îÄ‚îÄ (Error reading session data)")

                if session_ids:
                    session_details_lines.append("    ‚îî‚îÄ‚îÄ Scheduled Sessions:") # Header for sessions
                    session_num = 0
                    for session_id in session_ids:
                        # Ensure session_id is a non-empty string before proceeding
                        if not isinstance(session_id, str) or not session_id.strip():
                            log_warning("task_query_service", fn_name, f"Skipping invalid session ID: {session_id} for task {task_data.get('event_id')}")
                            continue

                        try:
                            # Fetch session details from GCal
                            session_event_data = calendar_api._get_single_event(session_id) # Use internal getter
                            if session_event_data:
                                session_num += 1
                                parsed_session = calendar_api._parse_google_event(session_event_data) # Parse it
                                session_start_str = parsed_session.get("gcal_start_datetime")
                                session_end_str = parsed_session.get("gcal_end_datetime")
                                session_time_info = "(Error parsing time)" # Default

                                # Attempt to parse and format times
                                if session_start_str and session_end_str:
                                    try:
                                        s_aware = datetime.fromisoformat(session_start_str.replace('Z', '+00:00'))
                                        e_aware = datetime.fromisoformat(session_end_str.replace('Z', '+00:00'))
                                        s_local = s_aware.astimezone(user_tz)
                                        e_local = e_aware.astimezone(user_tz)
                                        # Format: Date - Start HH:MM to End HH:MM (TZ)
                                        session_time_info = s_local.strftime('%Y-%m-%d %H:%M') + e_local.strftime('-%H:%M %Z')
                                    except (ValueError, TypeError):
                                         log_warning("task_query_service", fn_name, f"Error parsing session time for {session_id}")
                                         session_time_info = f"{session_start_str} to {session_end_str} (parse error)"

                                # Add line for this session (further indented)
                                session_details_lines.append(f"        {session_num}) {session_time_info} (ID: {session_id})")
                            else:
                                log_warning("task_query_service", fn_name, f"Could not fetch details for session ID {session_id}")
                                # Optionally add a line indicating it wasn't found
                                # session_details_lines.append(f"        - Session ID {session_id} not found in calendar.")
                        except Exception as fetch_err:
                            log_error("task_query_service", fn_name, f"Error fetching/processing session {session_id}", fetch_err)
                            session_details_lines.append(f"        - Error fetching session ID {session_id}")

                    # Only keep the header if actual sessions were found/processed
                    if session_num == 0 and session_details_lines and "Scheduled Sessions:" in session_details_lines[0]:
                         session_details_lines.pop(0) # Remove header if no sessions listed below it
        # --- END OF ADDED SECTION ---

        # Combine main line and session details
        return main_line + ("\n" + "\n".join(session_details_lines) if session_details_lines else "")

    except Exception as e:
        # Adding traceback here can help debug formatting errors
        import traceback
        log_error("task_query_service", fn_name, f"General error formatting item line {task_data.get('event_id')}. Error: {e}\n{traceback.format_exc()}", e)
        return f"Error displaying item: {task_data.get('event_id', 'Unknown ID')}"

# --- Public Service Functions ---

def get_formatted_list(user_id, date_range=None, status_filter='active', project_filter=None, trigger_sync=False): # Removed type hints
    """
    Gets tasks, filters, sorts, formats into numbered list string & mapping.
    Includes fetching and displaying details for scheduled task sessions.
    """
    fn_name = "get_formatted_list"
    log_info("task_query_service", fn_name, f"Executing for user={user_id}, Status={status_filter}, Range={date_range}, Proj={project_filter}")

    if trigger_sync:
        log_info("task_query_service", fn_name, "Sync triggered (NYI).")

    task_list = []
    calendar_api = None
    user_tz_str = "UTC" # Default timezone

    try:
        # Get task context from agent state
        task_list = get_context(user_id) or []

        # Get Calendar API instance (needed for session details)
        calendar_api = _get_calendar_api_from_state(user_id)
        if not calendar_api:
            log_info("task_query_service", fn_name, f"Calendar API not active for {user_id}, session details will not be fetched for list.")

        # Get user timezone from preferences
        agent_state = get_agent_state(user_id) # Fetch state once
        prefs = agent_state.get("preferences", {}) if agent_state else {}
        user_tz_str = prefs.get("TimeZone", "UTC") # Use default if not set

    except Exception as e:
        log_error("task_query_service", fn_name, f"Error getting context/prefs for {user_id}", e)
        # Continue with potentially empty task_list and no calendar_api

    # Apply filters (no changes needed here)
    filtered_s = _filter_tasks_by_status(task_list, status_filter)
    filtered_d = filtered_s
    if date_range:
        filtered_d = _filter_tasks_by_date_range(filtered_s, date_range)
    final_tasks = filtered_d
    if project_filter:
        final_tasks = _filter_tasks_by_project(filtered_d, project_filter)

    if not final_tasks:
        log_info("task_query_service", fn_name, f"No tasks match criteria for {user_id}.")
        return "", {}

    # Sort tasks (no changes needed here)
    sorted_tasks = _sort_tasks(final_tasks)

    # Format lines and build mapping
    lines, mapping = [], {}
    item_num = 0
    for task in sorted_tasks:
        item_id = task.get("event_id")
        if not item_id:
            log_warning("task_query_service", fn_name, f"Skipping item missing event_id: {task.get('title')}")
            continue

        item_num += 1
        # Pass API instance and timezone to formatting function
        formatted_line = _format_task_line(task, user_tz_str, calendar_api)
        lines.append(f"{item_num}. {formatted_line}")
        mapping[str(item_num)] = item_id # Use string key for mapping

    if item_num == 0:
        # This case should be rare now unless _format_task_line errors out for all items
        log_warning("task_query_service", fn_name, f"Formatting resulted in zero list items for {user_id}.")
        return "Error formatting list.", {}

    list_body = "\n".join(lines)
    log_info("task_query_service", fn_name, f"Generated list body ({len(mapping)} items, including session details) for {user_id}")
    return list_body, mapping

def get_tasks_for_summary(user_id, date_range, status_filter='active', trigger_sync=False):
    """Gets tasks, filters, sorts and returns list of dictionaries."""
    # *** CORRECTED LOGGING ***
    fn_name = "get_tasks_for_summary"
    log_info("task_query_service", fn_name, f"Executing for user={user_id}, Filter={status_filter}, Range={date_range}")
    # *************************

    if trigger_sync:
        log_info("task_query_service", fn_name, "Sync triggered (NYI).")

    try:
        task_list = get_context(user_id) or []
    except Exception as e:
        log_error("task_query_service", fn_name, f"Error getting context for {user_id}", e)
        task_list = []

    filtered_s = _filter_tasks_by_status(task_list, status_filter)
    # Ensure date_range is a tuple or list of two strings
    final_tasks = filtered_s
    if isinstance(date_range, (list, tuple)) and len(date_range) == 2:
        final_tasks = _filter_tasks_by_date_range(filtered_s, date_range)
    elif date_range is not None:
         log_warning("task_query_service", fn_name, f"Invalid date_range format for summary: {date_range}. Skipping date filter.")


    sorted_tasks = _sort_tasks(final_tasks)
    log_info("task_query_service", fn_name, f"Returning {len(sorted_tasks)} tasks for summary {user_id}")
    return sorted_tasks

def get_context_snapshot(user_id, history_weeks=1, future_weeks=2):
    """Fetches relevant active WT tasks and GCal events for Orchestrator context."""
    fn_name = "get_context_snapshot"
    log_info("task_query_service", fn_name, f"Get Context Snapshot: User={user_id}")
    task_context, calendar_context = [], []
    try:
        # Note: This currently only gets WT tasks from context, not external GCal events
        # To include external events, this would need to call the new sync_service function
        # For now, keeping it simple as per current structure before sync service exists.

        today = datetime.now().date()
        start_date = today - timedelta(weeks=history_weeks)
        end_date = today + timedelta(weeks=future_weeks)
        start_str, end_str = start_date.strftime("%Y-%m-%d"), end_date.strftime("%Y-%m-%d")
        date_range_tuple = (start_str, end_str)

        # Get WT tasks from the current in-memory context
        task_context = get_tasks_for_summary(user_id, date_range=date_range_tuple, status_filter='active')

        # Get GCal events directly from API
        calendar_api = _get_calendar_api_from_state(user_id)
        if calendar_api:
            try:
                # Fetch GCal events - list_events returns parsed dicts
                calendar_context = calendar_api.list_events(start_str, end_str)
                log_info("task_query_service", fn_name, f"Fetched {len(calendar_context)} GCal events for snapshot.")
            except Exception as cal_e:
                log_error("task_query_service", fn_name, f"Failed fetch calendar events for snapshot: {cal_e}", cal_e) # Log exception too
                # Continue without calendar context
        else:
            log_info("task_query_service", fn_name, f"Calendar API not active for {user_id}, skipping GCal fetch for snapshot.")

        log_info("task_query_service", fn_name, f"Snapshot created: {len(task_context)} tasks, {len(calendar_context)} events.")

    except Exception as e:
        # Log the error *here* within the function's context
        log_error("task_query_service", fn_name, f"Error creating context snapshot for {user_id}", e)
        # Return empty lists, the calling function will see the error log
    return task_context, calendar_context
# --- END OF FILE services/task_query_service.py ---
# --- END OF FILE services\task_query_service.py ---


================================================================================
üìÑ services\config_manager.py
================================================================================

# --- START OF FILE services\config_manager.py ---
# services/config_manager.py
"""Service layer for managing user configuration and preferences."""
from tools.logger import log_info, log_error, log_warning
# Import registry functions for persistence
from users.user_registry import get_user_preferences as get_prefs_from_registry
from users.user_registry import update_preferences as update_prefs_in_registry # This writes to file
# Import state manager for memory updates
try:
    # This function updates the live agent state dictionary
    from services.agent_state_manager import update_preferences_in_state
    AGENT_STATE_MANAGER_IMPORTED = True
except ImportError:
    log_error("config_manager", "import", "AgentStateManager not found. In-memory preference updates skipped.")
    AGENT_STATE_MANAGER_IMPORTED = False
    def update_preferences_in_state(*args, **kwargs): return False # Dummy

# Import calendar tool auth check
try:
    from tools.calendar_tool import authenticate as check_calendar_auth_status
    CALENDAR_TOOL_IMPORTED = True
except ImportError:
     log_error("config_manager", "import", "calendar_tool not found. Calendar auth initiation fails.")
     CALENDAR_TOOL_IMPORTED = False
     def check_calendar_auth_status(*args, **kwargs): return {"status": "fails", "message": "Calendar tool unavailable."}

from typing import Dict, Any, Optional

def get_preferences(user_id: str) -> Optional[Dict]:
    """Gets user preferences from the persistent registry."""
    try:
        prefs = get_prefs_from_registry(user_id)
        return prefs # Returns None if not found
    except Exception as e:
        log_error("config_manager", "get_preferences", f"Error reading preferences for {user_id}", e)
        return None

def update_preferences(user_id: str, updates: Dict) -> bool:
    """
    Updates preferences in persistent registry AND in-memory agent state.
    Returns True on success (based on registry update), False otherwise.
    """
    log_info("config_manager", "update_preferences", f"Updating preferences for {user_id}: {list(updates.keys())}")
    if not isinstance(updates, dict) or not updates:
        log_warning("config_manager", "update_preferences", "Invalid or empty updates provided.")
        return False

    # 1. Update Persistent Store (Registry File)
    registry_update_success = False
    try:
        update_prefs_in_registry(user_id, updates) # Writes to registry.json
        log_info("config_manager", "update_preferences", f"Registry file update requested for {user_id}")
        registry_update_success = True
    except Exception as e:
        log_error("config_manager", "update_preferences", f"Registry file update failed for {user_id}", e)
        return False # Don't proceed if persistence fails

    # 2. Update In-Memory State via AgentStateManager (If persistence succeeded)
    if registry_update_success and AGENT_STATE_MANAGER_IMPORTED:
        try:
            mem_update_success = update_preferences_in_state(user_id, updates) # Updates live _AGENT_STATE_STORE
            if not mem_update_success:
                log_warning("config_manager", "update_preferences", f"In-memory state update failed or user not found in state for {user_id}.")
                # Should we revert registry? For now, proceed but warn.
        except Exception as mem_e:
             log_error("config_manager", "update_preferences", f"Error updating in-memory state for {user_id}", mem_e)
             # Log error, but persistence succeeded, so arguably return True

    elif registry_update_success: # Log if manager wasn't imported
        log_warning("config_manager", "update_preferences", "AgentStateManager not imported. Skipping in-memory state update.")

    return registry_update_success # Return success based on registry write

def initiate_calendar_auth(user_id: str) -> Dict:
    """Initiates calendar auth flow via calendar_tool."""
    log_info("config_manager", "initiate_calendar_auth", f"Initiating calendar auth for {user_id}")
    if not CALENDAR_TOOL_IMPORTED:
         return {"status": "fails", "message": "Calendar auth component unavailable."}
    current_prefs = get_preferences(user_id) # Use service getter
    if not current_prefs:
        log_error("config_manager", "initiate_calendar_auth", f"Prefs not found for {user_id}")
        return {"status": "fails", "message": "User profile not found."}
    try:
        # Pass current prefs needed by authenticate function
        auth_result = check_calendar_auth_status(user_id, current_prefs)
        return auth_result
    except Exception as e:
        log_error("config_manager", "initiate_calendar_auth", f"Error during calendar auth init: {e}", e)
        return {"status": "fails", "message": "Error starting calendar auth."}

def set_user_status(user_id: str, status: str) -> bool:
    """Helper to specifically update user status in registry and memory."""
    log_info("config_manager", "set_user_status", f"Setting status='{status}' for {user_id}")
    if not status or not isinstance(status, str):
        log_warning("config_manager", "set_user_status", f"Invalid status value: {status}")
        return False
    # Calls the main update function which handles both registry and memory state
    return update_preferences(user_id, {"status": status})
# --- END OF FILE services\config_manager.py ---


================================================================================
üìÑ services\agent_state_manager.py
================================================================================

# --- START OF FILE services\agent_state_manager.py ---
# --- START OF FILE services/agent_state_manager.py ---
# services/agent_state_manager.py
"""
Manages the in-memory state of user agents.
Provides thread-safe functions to access and modify the global agent state dictionary.
Requires initialization via initialize_state_store.
"""
from tools.logger import log_info, log_error, log_warning
from typing import Dict, List, Any, Optional, Set # Added Set
import threading
import copy
from datetime import datetime

# --- Module Level State ---
_AGENT_STATE_STORE: Optional[Dict[str, Dict[str, Any]]] = None
_state_lock = threading.Lock()

def initialize_state_store(agent_dict_ref: Dict):
    """Initializes the state manager with a reference to the global agent state dictionary."""
    global _AGENT_STATE_STORE
    if _AGENT_STATE_STORE is not None:
        log_warning("AgentStateManager", "initialize_state_store", "State store already initialized.")
        return
    if isinstance(agent_dict_ref, dict):
        _AGENT_STATE_STORE = agent_dict_ref
        log_info("AgentStateManager", "initialize_state_store", f"State store initialized with reference (ID: {id(_AGENT_STATE_STORE)}).")
    else:
        log_error("AgentStateManager", "initialize_state_store", "Invalid dictionary reference passed.")
        _AGENT_STATE_STORE = {} # Initialize to empty dict if invalid ref passed

def _is_initialized() -> bool:
    """Checks if the state store has been initialized."""
    if _AGENT_STATE_STORE is None:
        log_error("AgentStateManager", "_is_initialized", "CRITICAL: State store accessed before initialization.")
        return False
    return True

# --- Modifier Functions ---

def register_agent_instance(user_id: str, agent_state: Dict):
    """Adds or replaces the entire state dictionary for a user."""
    if not _is_initialized(): return
    if not isinstance(agent_state, dict):
         log_error("AgentStateManager", "register_agent_instance", f"Invalid agent_state type for {user_id}")
         return
    log_info("AgentStateManager", "register_agent_instance", f"Registering/updating state for user {user_id}")
    with _state_lock:
        _AGENT_STATE_STORE[user_id] = agent_state

def update_preferences_in_state(user_id: str, prefs_updates: Dict) -> bool:
    """Updates the preferences dictionary within the user's in-memory state."""
    if not _is_initialized(): return False
    updated = False
    with _state_lock:
        state = _AGENT_STATE_STORE.get(user_id)
        if state and isinstance(state.get("preferences"), dict):
            state["preferences"].update(prefs_updates)
            log_info("AgentStateManager", "update_preferences_in_state", f"Updated in-memory preferences for {user_id}: {list(prefs_updates.keys())}")
            updated = True
        else:
            log_warning("AgentStateManager", "update_preferences_in_state", f"Cannot update prefs: State or prefs dict missing/invalid for {user_id}")
    return updated

def add_task_to_context(user_id: str, task_data: Dict):
    """Appends or updates a task dictionary in the user's in-memory context list."""
    if not _is_initialized(): return
    with _state_lock:
        state = _AGENT_STATE_STORE.get(user_id)
        if state:
            # Ensure 'active_tasks_context' exists and is a list
            if not isinstance(state.get("active_tasks_context"), list):
                 state["active_tasks_context"] = []

            context = state["active_tasks_context"]
            event_id = task_data.get("event_id") # Use event_id as primary key
            found_idx = -1
            if event_id:
                for i, item in enumerate(context):
                    # Check using event_id which should be unique
                    if item.get("event_id") == event_id:
                        found_idx = i
                        break

            if found_idx != -1:
                 log_info("AgentStateManager", "add_task_to_context", f"Updating task {event_id} in context for {user_id}.")
                 context[found_idx] = task_data # Replace existing entry
            else:
                 context.append(task_data) # Add as new entry
                 log_info("AgentStateManager", "add_task_to_context", f"Added task {event_id} to context for {user_id}. New size: {len(context)}")
        else:
            log_warning("AgentStateManager", "add_task_to_context", f"State missing for {user_id}.")

def update_task_in_context(user_id: str, event_id: str, updated_task_data: Dict):
    """Finds a task by event_id in the context list and replaces it."""
    if not _is_initialized(): return
    with _state_lock:
        state = _AGENT_STATE_STORE.get(user_id)
        if state and isinstance(state.get("active_tasks_context"), list):
            context = state["active_tasks_context"]
            found = False
            for i, item in enumerate(context):
                if item.get("event_id") == event_id:
                    context[i] = updated_task_data # Replace with new data
                    found = True
                    log_info("AgentStateManager", "update_task_in_context", f"Updated task {event_id} in context for {user_id}")
                    break
            if not found:
                 log_warning("AgentStateManager", "update_task_in_context", f"Task {event_id} not found for update. Adding if active.")
                 # Add only if it seems active (optional, depends on desired behavior)
                 if updated_task_data.get("status", "pending").lower() in ["pending", "in_progress", "in progress"]:
                      context.append(updated_task_data)
        else:
             log_warning("AgentStateManager", "update_task_in_context", f"State or active_tasks_context list invalid for {user_id}")

def remove_task_from_context(user_id: str, event_id: str):
    """Removes a task by event_id from the user's in-memory context list."""
    if not _is_initialized(): return
    with _state_lock:
        state = _AGENT_STATE_STORE.get(user_id)
        if state and isinstance(state.get("active_tasks_context"), list):
            original_len = len(state["active_tasks_context"])
            # Use list comprehension for potentially better performance on large lists
            state["active_tasks_context"][:] = [
                item for item in state["active_tasks_context"] if item.get("event_id") != event_id
            ]
            if len(state["active_tasks_context"]) < original_len:
                log_info("AgentStateManager", "remove_task_from_context", f"Removed task {event_id} from context for {user_id}")
            # else: No warning needed if not found, just means it wasn't there
        else:
             log_warning("AgentStateManager", "remove_task_from_context", f"State or active_tasks_context list invalid for {user_id}")

def update_full_context(user_id: str, new_context: List[Dict]):
    """Replaces the entire active_tasks_context list (e.g., after sync)."""
    if not _is_initialized(): return
    with _state_lock:
        state = _AGENT_STATE_STORE.get(user_id)
        if state:
            state["active_tasks_context"] = new_context if isinstance(new_context, list) else []
            log_info("AgentStateManager", "update_full_context", f"Replaced context for {user_id} with {len(state['active_tasks_context'])} items.")
        else:
            log_warning("AgentStateManager", "update_full_context", f"Cannot replace context: State missing for {user_id}")

def add_message_to_user_history(user_id: str, sender: str, message: str):
    """
    Appends a detailed message to the user's conversation history list. Keeps last 50.
    """
    if not _is_initialized(): return
    with _state_lock:
        state = _AGENT_STATE_STORE.get(user_id)
        if not state:
            log_warning("AgentStateManager", "add_message_to_user_history", f"Cannot add message: State missing for {user_id}")
            return

        if not isinstance(state.get("conversation_history"), list):
            log_warning("AgentStateManager", "add_message_to_user_history", f"conversation_history invalid for {user_id}, initializing.")
            state["conversation_history"] = []

        history_list = state["conversation_history"]
        timestamp = datetime.now().isoformat()
        entry = { "sender": sender, "timestamp": timestamp, "content": message }
        history_list.append(entry)
        state["conversation_history"] = history_list[-50:] # Limit size

def update_agent_state_key(user_id: str, key: str, value: Any) -> bool:
    """
    Updates or adds/removes a specific key-value pair in the user's agent state.
    """
    if not _is_initialized(): return False
    updated = False
    with _state_lock:
        state = _AGENT_STATE_STORE.get(user_id)
        if state:
            if value is None:
                if state.pop(key, None) is not None:
                    log_info("AgentStateManager", "update_agent_state_key", f"Removed key '{key}' from state for {user_id}")
            else:
                state[key] = value
                log_info("AgentStateManager", "update_agent_state_key", f"Updated key '{key}' in state for {user_id}")
            updated = True
        else:
            log_warning("AgentStateManager", "update_agent_state_key", f"Cannot update key '{key}': State missing for {user_id}")
    return updated

    # --- Notification Tracking Functions ---
def add_notified_event_id(user_id: str, event_id: str):
    """Adds an event ID to the set of notified events for today."""
    if not _is_initialized(): return
    with _state_lock:
        state = _AGENT_STATE_STORE.get(user_id)
        if state:
            # Ensure the key exists and is a set
            if not isinstance(state.get("notified_event_ids_today"), set):
                state["notified_event_ids_today"] = set()
            state["notified_event_ids_today"].add(event_id)
            # log_info("AgentStateManager", "add_notified_event_id", f"Added {event_id} to notified set for {user_id}") # Maybe too verbose
        else:
            log_warning("AgentStateManager", "add_notified_event_id", f"State missing for {user_id}, cannot add notified event.")

def get_notified_event_ids(user_id: str) -> Set[str]:
    """Gets a copy of the set of notified event IDs for today."""
    if not _is_initialized(): return set()
    with _state_lock:
        state = _AGENT_STATE_STORE.get(user_id)
        if state and isinstance(state.get("notified_event_ids_today"), set):
            return state["notified_event_ids_today"].copy() # Return a copy
    return set() # Return empty set if user or set not found

def clear_notified_event_ids(user_id: str):
    """Clears the set of notified event IDs for the user."""
    if not _is_initialized(): return
    with _state_lock:
        state = _AGENT_STATE_STORE.get(user_id)
        if state:
            # Reset to an empty set, even if key didn't exist before
            state["notified_event_ids_today"] = set()
            log_info("AgentStateManager", "clear_notified_event_ids", f"Cleared notified events set for {user_id}")
        else:
            log_warning("AgentStateManager", "clear_notified_event_ids", f"State missing for {user_id}, cannot clear notified events.")
    # --- End Notification Tracking Functions ---

def get_agent_state(user_id: str) -> Optional[Dict]:
    """
    Safely gets a SHALLOW copy of the full state dictionary for a user.
    """
    if not _is_initialized(): return None
    with _state_lock:
        state = _AGENT_STATE_STORE.get(user_id)
        return state.copy() if state else None

def get_context(user_id: str) -> Optional[List[Dict]]:
    """Gets a deep copy of the active_tasks_context list for a user."""
    if not _is_initialized(): return None
    with _state_lock:
        state = _AGENT_STATE_STORE.get(user_id)
        if state and isinstance(state.get("active_tasks_context"), list):
            return copy.deepcopy(state["active_tasks_context"])
    return [] # Return empty list if user or context list not found/invalid
# --- END OF FILE services/agent_state_manager.py ---
# --- END OF FILE services\agent_state_manager.py ---


================================================================================
üìÑ services\cheats.py
================================================================================

# --- START OF FILE services\cheats.py ---
# --- START OF UPDATED services/cheats.py ---

"""
Service layer for handling direct 'cheat code' commands, bypassing the LLM orchestrator.
Used primarily for testing, debugging, and direct actions.
"""
import json
from typing import List, Optional, Dict, Any
from datetime import datetime, timedelta # Added datetime imports

# Service Imports
from services import task_query_service, task_manager, agent_state_manager
# --- ADDED IMPORTS for morning/evening cheats ---
from services import sync_service
from services import routine_service
from users.user_registry import get_user_preferences
# ----------------------------------------------
from tools import metadata_store # Needed to list all items for /clear

# Utilities
from tools.logger import log_info, log_error, log_warning

# Define constants used by routines (mirroring routine_service)
ROUTINE_CONTEXT_HISTORY_DAYS = 1
ROUTINE_CONTEXT_FUTURE_DAYS = 14

# --- Private Handler Functions ---

def _handle_help() -> str:
    """Provides help text for available cheat commands."""
    return """Available Cheat Commands:
/help - Show this help message
/list [status] - List items (status: active*, pending, completed, all)
/memory - Show summary of current agent in-memory state
/clear - !! DANGER !! Mark all user's items as cancelled (removes from GCal)
/morning - Generate and show today's morning summary
/evening - Generate and show today's evening review"""


def _handle_list(user_id: str, args: List[str]) -> str:
    """Handles the /list command."""
    status_filter = args[0].lower() if args else 'active'
    allowed_statuses = ['active', 'pending', 'in_progress', 'completed', 'all']

    if status_filter not in allowed_statuses:
        return f"Invalid status '{status_filter}'. Use one of: {', '.join(allowed_statuses)}"

    try:
        # Pass user's timezone to formatting function if available
        prefs = get_user_preferences(user_id) or {}
        user_tz_str = prefs.get("TimeZone", "UTC")

        list_body, _ = task_query_service.get_formatted_list(
            user_id=user_id,
            status_filter=status_filter,
            # Pass timezone for formatting (needs get_formatted_list to accept/pass it down)
            # For now, assuming _format_task_line called within get_formatted_list handles it
            # If not, this needs adjustment or get_formatted_list modification
        )
        if list_body:
            # Add user timezone info explicitly to the list output for clarity
            list_intro = f"Items with status '{status_filter}' (Times displayed relative to {user_tz_str}):\n"
            return list_intro + list_body
        else:
            return f"No items found with status '{status_filter}'."
    except Exception as e:
        log_error("cheats", "_handle_list", f"Error calling get_formatted_list: {e}", e)
        return "Error retrieving list."


def _handle_memory(user_id: str) -> str:
    """Handles the /memory command."""
    try:
        agent_state = agent_state_manager.get_agent_state(user_id)
        if agent_state:
            state_summary = {
                "user_id": agent_state.get("user_id"),
                "preferences_keys": list(agent_state.get("preferences", {}).keys()),
                "history_count": len(agent_state.get("conversation_history", [])),
                "context_item_count": len(agent_state.get("active_tasks_context", [])),
                "calendar_object_present": agent_state.get("calendar") is not None
            }
            return f"Agent Memory Summary:\n```json\n{json.dumps(state_summary, indent=2, default=str)}\n```"
        else:
            return "Error: Agent state not found in memory."
    except Exception as e:
        log_error("cheats", "_handle_memory", f"Error retrieving agent state: {e}", e)
        return "Error retrieving agent memory state."


def _handle_clear(user_id: str) -> str:
    """Handles the /clear command. Marks all items as cancelled."""
    log_warning("cheats", "_handle_clear", f"!! Initiating /clear command for user {user_id} !!")
    cancelled_count = 0
    failed_count = 0
    errors = []

    try:
        all_metadata = metadata_store.list_metadata(user_id=user_id)
        item_ids_to_clear = [item.get("event_id") for item in all_metadata if item.get("event_id") and item.get("status") != "cancelled"]

        if not item_ids_to_clear:
            return "No active items found to clear."

        log_info("cheats", "_handle_clear", f"Attempting to cancel {len(item_ids_to_clear)} items for user {user_id}.")

        for item_id in item_ids_to_clear:
            try:
                success = task_manager.cancel_item(user_id, item_id)
                if success:
                    cancelled_count += 1
                else:
                    failed_count += 1
                    errors.append(f"Failed cancel: {item_id[:8]}...")
                    log_warning("cheats", "_handle_clear", f"task_manager.cancel_item failed for {item_id}")
            except Exception as cancel_e:
                failed_count += 1
                errors.append(f"Error cancel: {item_id[:8]}... ({type(cancel_e).__name__})")
                log_error("cheats", "_handle_clear", f"Exception during cancel_item for {item_id}", cancel_e)

        response = f"Clear operation finished.\nSuccessfully cancelled: {cancelled_count}\nFailed/Skipped: {failed_count}"
        if errors:
            response += "\nFailures:\n" + "\n".join(errors[:5])
            if len(errors) > 5: response += "\n..."

        return response

    except Exception as e:
        log_error("cheats", "_handle_clear", f"Critical error during /clear setup or execution for {user_id}", e)
        return "A critical error occurred during the clear operation."


def _handle_morning(user_id: str) -> str:
    """Handles the /morning command by generating the summary."""
    fn_name = "_handle_morning"
    log_info("cheats", fn_name, f"Executing /morning cheat for {user_id}")
    try:
        prefs = get_user_preferences(user_id)
        if not prefs:
            return "Error: Could not retrieve user preferences."

        user_tz_str = prefs.get("TimeZone", "UTC")
        now_local = routine_service.get_local_time(user_tz_str) # Use helper from routine_service

        # Calculate date range for context
        context_start_date = (now_local - timedelta(days=ROUTINE_CONTEXT_HISTORY_DAYS)).strftime("%Y-%m-%d")
        context_end_date = (now_local + timedelta(days=ROUTINE_CONTEXT_FUTURE_DAYS)).strftime("%Y-%m-%d")

        log_info("cheats", fn_name, f"Getting synced context for morning summary (User: {user_id})...")
        aggregated_context = sync_service.get_synced_context_snapshot(user_id, context_start_date, context_end_date)

        # Generate the summary using the function from routine_service
        summary_msg = routine_service.generate_morning_summary(user_id, aggregated_context)

        return summary_msg if summary_msg else "Could not generate morning summary."

    except Exception as e:
        log_error("cheats", fn_name, f"Error generating morning summary via cheat code for {user_id}", e)
        return "An error occurred while generating the morning summary."


def _handle_evening(user_id: str) -> str:
    """Handles the /evening command by generating the review."""
    fn_name = "_handle_evening"
    log_info("cheats", fn_name, f"Executing /evening cheat for {user_id}")
    try:
        prefs = get_user_preferences(user_id)
        if not prefs:
            return "Error: Could not retrieve user preferences."

        user_tz_str = prefs.get("TimeZone", "UTC")
        now_local = routine_service.get_local_time(user_tz_str) # Use helper

        # Calculate date range for context
        context_start_date = (now_local - timedelta(days=ROUTINE_CONTEXT_HISTORY_DAYS)).strftime("%Y-%m-%d")
        context_end_date = (now_local + timedelta(days=ROUTINE_CONTEXT_FUTURE_DAYS)).strftime("%Y-%m-%d")

        log_info("cheats", fn_name, f"Getting synced context for evening review (User: {user_id})...")
        aggregated_context = sync_service.get_synced_context_snapshot(user_id, context_start_date, context_end_date)

        # Generate the review using the function from routine_service
        review_msg = routine_service.generate_evening_review(user_id, aggregated_context)

        return review_msg if review_msg else "Could not generate evening review."

    except Exception as e:
        log_error("cheats", fn_name, f"Error generating evening review via cheat code for {user_id}", e)
        return "An error occurred while generating the evening review."


# --- Main Dispatcher ---

def handle_cheat_command(user_id: str, command: str, args: List[str]) -> str:
    """
    Dispatches cheat commands to the appropriate handler.
    """
    command = command.lower() # Ensure case-insensitivity

    if command == "/help":
        return _handle_help()
    elif command == "/list":
        return _handle_list(user_id, args)
    elif command == "/memory":
        return _handle_memory(user_id)
    elif command == "/clear":
        return _handle_clear(user_id)
    elif command == "/morning":
        return _handle_morning(user_id) # Calls implemented handler
    elif command == "/evening":
        return _handle_evening(user_id) # Calls implemented handler
    else:
        return f"Unknown command: '{command}'. Try /help."

# --- END OF UPDATED services/cheats.py ---
# --- END OF FILE services\cheats.py ---


================================================================================
üìÑ services\llm_interface.py
================================================================================

# --- START OF FILE services\llm_interface.py ---
# llm_interface.py
import os
import openai
import instructor
import threading
from tools.logger import log_info, log_error

OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
_client = None
_client_lock = threading.Lock()

def get_instructor_client():
    """Initializes and returns a singleton, instructor-patched OpenAI client."""
    global _client
    if not OPENAI_API_KEY:
        log_error("llm_interface", "get_instructor_client", "OPENAI_API_KEY not found in environment.")
        return None

    with _client_lock:
        if _client is None:
            try:
                log_info("llm_interface", "get_instructor_client", "Initializing instructor-patched OpenAI client...")
                # Initialize the OpenAI client
                base_client = openai.OpenAI(api_key=OPENAI_API_KEY)
                # Patch it with Instructor
                _client = instructor.patch(base_client)
                log_info("llm_interface", "get_instructor_client", "Instructor-patched OpenAI client initialized.")
            except Exception as e:
                log_error("llm_interface", "get_instructor_client", f"Failed to initialize OpenAI client: {e}", e)
                _client = None # Ensure it remains None on failure
    return _client

# --- END OF FILE services\llm_interface.py ---


================================================================================
üìÑ services\scheduler_service.py
================================================================================

# --- START OF FILE services\scheduler_service.py ---
# --- START OF services/scheduler_service.py ---

from typing import Dict, List, Tuple # Added List, Tuple
from apscheduler.schedulers.background import BackgroundScheduler
from apscheduler.executors.pool import ThreadPoolExecutor
from apscheduler.events import EVENT_JOB_ERROR, EVENT_JOB_EXECUTED
import pytz
from tools.logger import log_info, log_error, log_warning
# --- ADD THIS IMPORT ---
from bridge.request_router import send_message
# ----------------------

# ... (Global scheduler instance, constants, _job_listener) ...
scheduler = None
DEFAULT_TIMEZONE = 'UTC'
NOTIFICATION_CHECK_INTERVAL_MINUTES = 5
ROUTINE_CHECK_INTERVAL_MINUTES = 15
DAILY_CLEANUP_HOUR_UTC = 0
DAILY_CLEANUP_MINUTE_UTC = 5

def _job_listener(event):
    # ... (function remains the same) ...
    fn_name = "_job_listener"
    job = scheduler.get_job(event.job_id) if scheduler else None
    job_name = job.name if job else event.job_id
    if event.exception:
        log_error("scheduler_service", fn_name, f"Job '{job_name}' crashed:", event.exception)
        log_error("scheduler_service", fn_name, f"Traceback: {event.traceback}")
    pass

# --- NEW FUNCTION TO WRAP ROUTINE CHECK AND SENDING ---
def _run_routine_check_and_send():
    """Wrapper function called by scheduler to run checks and send messages."""
    fn_name = "_run_routine_check_and_send"
    log_info("scheduler_service", fn_name, "Scheduler executing routine check job...")
    try:
        # Import the check function here if not already imported globally
        from services.routine_service import check_routine_triggers
        messages_to_send = check_routine_triggers() # This now returns a list

        if messages_to_send:
            log_info("scheduler_service", fn_name, f"Routine check generated {len(messages_to_send)} messages to send.")
            for user_id, message_content in messages_to_send:
                try:
                    send_message(user_id, message_content)
                except Exception as send_err:
                    log_error("scheduler_service", fn_name, f"Error sending routine message to user {user_id}", send_err)
        else:
            log_info("scheduler_service", fn_name, "Routine check completed, no messages to send.")

    except Exception as job_err:
        # Log errors occurring within the job execution itself
        log_error("scheduler_service", fn_name, "Error during scheduled routine check execution", job_err)
# --- END OF NEW WRAPPER FUNCTION ---


def start_scheduler() -> bool:
    global scheduler
    fn_name = "start_scheduler"

    if scheduler and scheduler.running:
        log_warning("scheduler_service", fn_name, "Scheduler is already running.")
        return True

    try:
        log_info("scheduler_service", fn_name, "Initializing APScheduler...")
        executors = {'default': ThreadPoolExecutor(10)}
        job_defaults = {'coalesce': True, 'max_instances': 1}
        scheduler = BackgroundScheduler(
            executors=executors, job_defaults=job_defaults, timezone=pytz.timezone(DEFAULT_TIMEZONE)
        )

        # --- Import Job Functions ---
        check_event_notifications = None
        # We don't import check_routine_triggers here anymore, it's called in the wrapper
        daily_cleanup = None
        try:
            from services.notification_service import check_event_notifications
        except ImportError as e:
            log_error("scheduler_service", fn_name, f"Failed to import 'check_event_notifications': {e}. Notification job NOT scheduled.")
        try:
            # Keep import for daily_cleanup
            from services.routine_service import daily_cleanup
        except ImportError as e:
             log_error("scheduler_service", fn_name, f"Failed to import 'daily_cleanup': {e}. Cleanup job NOT scheduled.")
        # ----------------------------

        # --- Schedule Jobs ---
        jobs_scheduled_count = 0
        if check_event_notifications:
            scheduler.add_job( check_event_notifications, trigger='interval', minutes=NOTIFICATION_CHECK_INTERVAL_MINUTES, id='event_notification_check', name='Check Event Notifications')
            log_info("scheduler_service", fn_name, f"Scheduled 'check_event_notifications' job every {NOTIFICATION_CHECK_INTERVAL_MINUTES} minutes.")
            jobs_scheduled_count += 1
        else:
             log_warning("scheduler_service", fn_name, "'check_event_notifications' job not scheduled.")

        # --- MODIFIED: Schedule the WRAPPER function ---
        scheduler.add_job(
            _run_routine_check_and_send, # Call the wrapper
            trigger='interval',
            minutes=ROUTINE_CHECK_INTERVAL_MINUTES,
            id='routine_trigger_check',
            name='Check Routine Triggers & Send' # Updated name slightly
        )
        log_info("scheduler_service", fn_name, f"Scheduled 'Routine Trigger Check & Send' job every {ROUTINE_CHECK_INTERVAL_MINUTES} minutes.")
        jobs_scheduled_count += 1 # Assuming this job is always added
        # -------------------------------------------------

        if daily_cleanup:
            scheduler.add_job( daily_cleanup, trigger='cron', hour=DAILY_CLEANUP_HOUR_UTC, minute=DAILY_CLEANUP_MINUTE_UTC, timezone=DEFAULT_TIMEZONE, id='daily_cleanup_job', name='Daily Cleanup')
            log_info("scheduler_service", fn_name, f"Scheduled 'daily_cleanup' job daily at {DAILY_CLEANUP_HOUR_UTC:02d}:{DAILY_CLEANUP_MINUTE_UTC:02d} {DEFAULT_TIMEZONE}.")
            jobs_scheduled_count += 1
        else:
            log_warning("scheduler_service", fn_name, "'daily_cleanup' job not scheduled.")

        # ... (rest of the start_scheduler function including listener, start, return True/False) ...
        scheduler.add_listener(_job_listener, EVENT_JOB_ERROR | EVENT_JOB_EXECUTED)
        scheduler.start()
        log_info("scheduler_service", fn_name, "APScheduler started successfully.")
        return True

    except Exception as e:
        log_error("scheduler_service", fn_name, f"Failed to initialize or start APScheduler: {e}", e)
        scheduler = None
        return False

# --- (shutdown_scheduler function remains the same) ---
def shutdown_scheduler():
    # ... (shutdown logic) ...
    global scheduler
    fn_name = "shutdown_scheduler"
    if scheduler and scheduler.running:
        try:
            log_info("scheduler_service", fn_name, "Attempting to shut down scheduler...")
            scheduler.shutdown(wait=False)
            log_info("scheduler_service", fn_name, "Scheduler shut down complete.")
            scheduler = None
        except Exception as e:
            log_error("scheduler_service", fn_name, f"Error during scheduler shutdown: {e}", e)
    elif scheduler:
        log_info("scheduler_service", fn_name, "Scheduler found but was not running.")
        scheduler = None
    else:
        log_info("scheduler_service", fn_name, "No active scheduler instance to shut down.")

# --- END OF services/scheduler_service.py ---
# --- END OF FILE services\scheduler_service.py ---


================================================================================
üìÑ services\sync_service.py
================================================================================

# --- START OF FILE services\sync_service.py ---
# --- START OF FILE services/sync_service.py ---
"""
Provides functionality to get a combined view of WhatsTasker-managed items
and external Google Calendar events for a specific user and time period.
Does NOT modify the persistent metadata store for external events.
"""
import traceback
from datetime import datetime, timedelta, timezone # Import timezone
from typing import List, Dict, Any # Keep Dict, List, Any for internal hints

from tools.logger import log_info, log_error, log_warning
from tools import metadata_store

# Service/Tool Imports
try:
    from users.user_manager import get_agent
except ImportError:
    log_error("sync_service", "import", "Failed to import user_manager.get_agent")
    def get_agent(*args, **kwargs): return None # Dummy

try:
    from services.agent_state_manager import update_task_in_context # For updating context after merge
    AGENT_STATE_IMPORTED = True
except ImportError:
    log_error("sync_service", "import", "Failed to import agent_state_manager functions.")
    AGENT_STATE_IMPORTED = False
    def update_task_in_context(*args, **kwargs): pass # Dummy

# We need GoogleCalendarAPI type for isinstance check, even if not used directly
try:
    from tools.google_calendar_api import GoogleCalendarAPI
except ImportError:
    GoogleCalendarAPI = None # Define as None if import fails


def get_synced_context_snapshot(user_id, start_date_str, end_date_str):
    """
    Fetches WT metadata and GCal events for a period, merges them,
    identifies external events, and returns a combined list of dictionaries.
    Does not persist external events. Updates metadata for WT items if GCal changed.
    """
    fn_name = "get_synced_context_snapshot"
    log_info("sync_service", fn_name, f"Generating synced context for user {user_id}, range: {start_date_str} to {end_date_str}")

    # 1. Get Calendar API instance
    agent_state = get_agent(user_id) if get_agent else None
    calendar_api = None
    if agent_state and GoogleCalendarAPI:
        calendar_api_maybe = agent_state.get("calendar")
        if isinstance(calendar_api_maybe, GoogleCalendarAPI) and calendar_api_maybe.is_active():
            calendar_api = calendar_api_maybe

    # 2. Fetch GCal Events
    gcal_events_list = []
    if calendar_api:
        try:
            log_info("sync_service", fn_name, f"Fetching GCal events for {user_id}...")
            gcal_events_list = calendar_api.list_events(start_date_str, end_date_str)
            log_info("sync_service", fn_name, f"Fetched {len(gcal_events_list)} GCal events for {user_id}.")
        except Exception as e:
            log_error("sync_service", fn_name, f"Error fetching GCal events for {user_id}", e)
            # Continue without GCal events, will only use metadata
    else:
        log_info("sync_service", fn_name, f"GCal API not available or inactive for {user_id}, skipping GCal fetch.")

    # 3. Fetch WT Metadata
    wt_metadata_list = []
    try:
        log_info("sync_service", fn_name, f"Fetching WT metadata for {user_id}...")
        # Fetch within the same range for comparison, but list_metadata filters by 'date' field
        # which might miss tasks scheduled via GCal sessions. Fetching all might be safer?
        # For now, stick to range based on 'date' field as implemented in list_metadata.
        wt_metadata_list = metadata_store.list_metadata(user_id, start_date_str, end_date_str)
        log_info("sync_service", fn_name, f"Fetched {len(wt_metadata_list)} WT metadata items for {user_id}.")
    except Exception as e:
        log_error("sync_service", fn_name, f"Error fetching WT metadata for {user_id}", e)
        # If metadata fails, we might still proceed with just GCal events? Or return empty?
        # Let's return empty for now if metadata fails.
        return []

    # 4. Create Maps for Efficient Lookup
    gcal_events_map = {e['event_id']: e for e in gcal_events_list if e.get('event_id')}
    wt_metadata_map = {m['event_id']: m for m in wt_metadata_list if m.get('event_id')}

    # 5. Merge & Identify Types
    aggregated_context_list: List[Dict[str, Any]] = []
    processed_wt_ids = set() # Keep track of WT items found in GCal

    # Iterate through GCal events first
    for event_id, gcal_data in gcal_events_map.items():
        if event_id in wt_metadata_map:
            # --- WT Item Found in GCal ---
            processed_wt_ids.add(event_id)
            meta_data = wt_metadata_map[event_id]
            # Merge: Start with metadata, update with latest GCal info
            merged_data = meta_data.copy()
            gcal_start = gcal_data.get("gcal_start_datetime")
            gcal_end = gcal_data.get("gcal_end_datetime")
            gcal_title = gcal_data.get("title")
            needs_meta_update = False

            # Update GCal times if they differ
            if gcal_start != merged_data.get("gcal_start_datetime"):
                merged_data["gcal_start_datetime"] = gcal_start
                needs_meta_update = True
            if gcal_end != merged_data.get("gcal_end_datetime"):
                merged_data["gcal_end_datetime"] = gcal_end
                needs_meta_update = True
            # Optionally update title if it changed in GCal? Be cautious.
            # if gcal_title and gcal_title != merged_data.get("title"):
            #    merged_data["title"] = gcal_title
            #    needs_meta_update = True

            aggregated_context_list.append(merged_data)

            # If relevant fields changed, update the persistent metadata store
            if needs_meta_update:
                log_info("sync_service", fn_name, f"GCal data changed for WT item {event_id}. Updating metadata.")
                try:
                    meta_to_save = {fn: merged_data.get(fn) for fn in metadata_store.FIELDNAMES}
                    metadata_store.save_event_metadata(meta_to_save)
                    # Also update in-memory state if possible
                    if AGENT_STATE_IMPORTED:
                        update_task_in_context(user_id, event_id, meta_to_save)
                except Exception as save_err:
                     log_error("sync_service", fn_name, f"Failed to save updated metadata for WT item {event_id} after GCal merge.", save_err)

        else:
            # --- External GCal Event ---
            external_event_data = gcal_data.copy() # Start with GCal data
            external_event_data["type"] = "external_event" # Mark its type
            external_event_data["user_id"] = user_id # Ensure user_id is present
            # Add placeholder status? Or leave as None? Let's leave it.
            # external_event_data["status"] = "calendar_event"
            aggregated_context_list.append(external_event_data)

    # 6. Add WT Items Not Found in GCal Fetch Window
    for event_id, meta_data in wt_metadata_map.items():
        if event_id not in processed_wt_ids:
            # This is a WT item (task/reminder) that wasn't in the GCal list
            # Could be outside the GCal window, deleted from GCal, or never had a GCal entry (local task)
            log_info("sync_service", fn_name, f"Including WT item {event_id} which was not found in GCal fetch window.")
            aggregated_context_list.append(meta_data) # Add the metadata as is

    log_info("sync_service", fn_name, f"Generated aggregated context with {len(aggregated_context_list)} items for {user_id}.")
    return aggregated_context_list

# --- END OF FILE services/sync_service.py ---
# --- END OF FILE services\sync_service.py ---


================================================================================
üìÑ services\notification_service.py
================================================================================

# --- START OF FILE services\notification_service.py ---
# --- START OF FILE services/notification_service.py ---
"""
Handles checking for upcoming events and sending notifications.
Uses an in-memory set within agent_state to track sent notifications for the day.
"""
import traceback
from datetime import datetime, timedelta, timezone
import pytz

from tools.logger import log_info, log_error, log_warning
from users.user_registry import get_registry, get_user_preferences
from services.sync_service import get_synced_context_snapshot
from services.agent_state_manager import get_notified_event_ids, add_notified_event_id
from bridge.request_router import send_message # Direct import for sending
from services.task_manager import _parse_duration_to_minutes # For parsing lead time
from tools import metadata_store
NOTIFICATION_CHECK_INTERVAL_MINUTES = 5

DEFAULT_NOTIFICATION_LEAD_TIME_MINUTES = 15

def generate_event_notification_message(event_data, user_timezone_str="UTC"):
    """
    Formats a simple notification message for an event, converting the start time
    to the user's local timezone.

    Args:
        event_data (dict): Dictionary containing item data, expects 'gcal_start_datetime' and 'title'.
        user_timezone_str (str): The user's Olson timezone string (e.g., 'America/New_York').
                                 Defaults to 'UTC' if not provided or invalid.

    Returns:
        str: Formatted notification string or None if formatting fails.
    """
    fn_name = "generate_event_notification_message"
    title = event_data.get('title', '(No Title)')
    start_time_str = event_data.get('gcal_start_datetime')

    if not start_time_str:
        log_warning("notification_service", fn_name, f"Cannot generate notification for event '{title}' - missing gcal_start_datetime.")
        return None

    try:
        # Determine User Timezone Object
        user_tz = pytz.utc # Default to UTC
        try:
            if user_timezone_str:
                 user_tz = pytz.timezone(user_timezone_str)
        except pytz.UnknownTimeZoneError:
            log_warning("notification_service", fn_name, f"Unknown timezone '{user_timezone_str}'. Using UTC for notification message for event '{title}'.")
            user_timezone_str = "UTC" # Update string for consistency

        # Parse the aware datetime string from GCal
        dt_aware = datetime.fromisoformat(start_time_str.replace('Z', '+00:00'))

        # Convert to user's local timezone
        dt_local = dt_aware.astimezone(user_tz)

        # Format the time string using local time and timezone abbreviation
        time_str = dt_local.strftime('%H:%M %Z') # e.g., 10:00 EDT

        return f"üîî Reminder: '{title}' is starting soon at {time_str}."

    except (ValueError, TypeError) as parse_err:
        log_error("notification_service", fn_name, f"Error parsing/converting start time '{start_time_str}' for event '{title}'. Error: {parse_err}", parse_err)
        return None
    except Exception as e:
        log_error("notification_service", fn_name, f"General error formatting notification for event '{title}': {e}", e)
        return None

def check_event_notifications():
    """
    Scheduled job function. Checks all users for upcoming events needing notification.
    """
    fn_name = "check_event_notifications"
    log_info("notification_service", fn_name, "Running scheduled check for event notifications...")
    now_utc = datetime.now(timezone.utc)

    try:
        registry = get_registry()
        if not registry:
            log_warning("notification_service", fn_name, "User registry is empty. Skipping check.")
            return

        user_ids = list(registry.keys())
        log_info("notification_service", fn_name, f"Checking notifications for {len(user_ids)} users.")

        for user_id in user_ids:
            try:
                prefs = get_user_preferences(user_id)
                if not prefs or not prefs.get("Calendar_Enabled"):
                    # log_info("notification_service", fn_name, f"Skipping user {user_id}: Calendar not enabled.")
                    continue

                lead_time_str = prefs.get("Notification_Lead_Time", "15m")
                lead_time_minutes = _parse_duration_to_minutes(lead_time_str)
                if lead_time_minutes is None:
                     log_warning("notification_service", fn_name, f"Invalid Notification_Lead_Time '{lead_time_str}' for user {user_id}. Using default.")
                     lead_time_minutes = DEFAULT_NOTIFICATION_LEAD_TIME_MINUTES

                # Define the window to check: from now up to lead_time + buffer (e.g., scheduler interval)
                # Fetch slightly ahead to avoid race conditions with scheduler timing
                check_end_utc = now_utc + timedelta(minutes=lead_time_minutes + NOTIFICATION_CHECK_INTERVAL_MINUTES)
                start_date_str = now_utc.strftime("%Y-%m-%d")
                end_date_str = check_end_utc.strftime("%Y-%m-%d")

                # Get combined context (includes external events)
                # Note: sync_service currently filters metadata by 'date' field, which might miss GCal events
                # scheduled far out if the 'date' field wasn't set. Need refinement?
                # For notifications, fetching a narrow window directly from GCal might be better.
                # Let's stick to sync_service for now.
                aggregated_context = get_synced_context_snapshot(user_id, start_date_str, end_date_str)
                if not aggregated_context:
                    # log_info("notification_service", fn_name, f"No context found for user {user_id} in window.")
                    continue

                notified_today_set = get_notified_event_ids(user_id) # Get set of IDs already notified today
                # log_info("notification_service", fn_name, f"User {user_id} notified set size: {len(notified_today_set)}")

                for item in aggregated_context:
                    event_id = item.get("event_id")
                    start_time_iso = item.get("gcal_start_datetime")

                    if not event_id or not start_time_iso:
                        # log_warning("notification_service", fn_name, f"Skipping item for user {user_id} due to missing ID or start time: {item.get('title')}")
                        continue

                    # Check if already notified today
                    if event_id in notified_today_set:
                        # log_info("notification_service", fn_name, f"Event {event_id} already notified today for user {user_id}.")
                        continue

                    try:
                        # Parse the start time (should be timezone-aware)
                        start_dt_aware = datetime.fromisoformat(start_time_iso.replace('Z', '+00:00'))

                        # Calculate notification trigger time (UTC)
                        notification_trigger_time = start_dt_aware - timedelta(minutes=lead_time_minutes)

                        # Check if the notification time is now or in the past
                        if notification_trigger_time <= now_utc:
                             log_info("notification_service", fn_name, f"Triggering notification for user {user_id}, event: {event_id} ('{item.get('title')}')")

                             # Add user timezone to item data for formatting
                             item['user_timezone'] = prefs.get("TimeZone", "UTC")
                             notification_message = generate_event_notification_message(item)

                             if notification_message:
                                 send_message(user_id, notification_message)
                                 # Mark as notified for today in memory
                                 add_notified_event_id(user_id, event_id)
                                 log_info("notification_service", fn_name, f"Sent notification and marked as notified for event {event_id}, user {user_id}")

                                 # Mark WT items as notified in metadata store as well (for persistence)
                                 if item.get("type") in ["task", "reminder"]:
                                     try:
                                         sent_time_utc = datetime.now(timezone.utc).isoformat(timespec='seconds').replace('+00:00', 'Z')
                                         metadata_update = {"internal_reminder_sent": sent_time_utc}
                                         # Use metadata_store directly - TaskManager doesn't have a direct update method for this
                                         current_meta = metadata_store.get_event_metadata(event_id)
                                         if current_meta:
                                             current_meta.update(metadata_update)
                                             meta_to_save = {fn: current_meta.get(fn) for fn in metadata_store.FIELDNAMES}
                                             metadata_store.save_event_metadata(meta_to_save)
                                             log_info("notification_service", fn_name, f"Updated internal_reminder_sent in metadata for WT item {event_id}")
                                         else:
                                              log_warning("notification_service", fn_name, f"Could not find metadata for WT item {event_id} to update sent status.")
                                     except Exception as meta_update_err:
                                          log_error("notification_service", fn_name, f"Failed to update metadata sent status for {event_id}", meta_update_err)

                             else:
                                 log_warning("notification_service", fn_name, f"Failed to generate notification message for event {event_id}.")

                    except ValueError:
                        log_warning("notification_service", fn_name, f"Could not parse start time '{start_time_iso}' for event {event_id}. Skipping notification.")
                    except Exception as item_err:
                        log_error("notification_service", fn_name, f"Error processing item {event_id} for user {user_id}", item_err)

            except Exception as user_err:
                 log_error("notification_service", fn_name, f"Error processing notifications for user {user_id}", user_err)
                 # Continue to the next user

    except Exception as main_err:
        log_error("notification_service", fn_name, f"General error during notification check run", main_err)

    log_info("notification_service", fn_name, "Finished scheduled check for event notifications.")

# --- END OF FILE services/notification_service.py ---
# --- END OF FILE services\notification_service.py ---


================================================================================
üìÑ services\routine_service.py
================================================================================

# --- START OF FILE services\routine_service.py ---
# --- START OF FILE services/routine_service.py ---
"""
Handles scheduled generation of Morning and Evening summaries/reviews.
Includes timezone handling and daily cleanup tasks.
"""

import traceback
from datetime import datetime, timedelta, timezone
import pytz # For timezone handling

from tools.logger import log_info, log_error, log_warning
from users.user_registry import get_registry, get_user_preferences
from services import sync_service # Import the whole module
from services.config_manager import update_preferences # To update last trigger date
from services.agent_state_manager import clear_notified_event_ids, get_agent_state # For daily cleanup
from services.task_query_service import _format_task_line, _sort_tasks # Use helper for formatting

# Define time window for fetching context for routines (e.g., yesterday to 14 days ahead)
ROUTINE_CONTEXT_HISTORY_DAYS = 1
ROUTINE_CONTEXT_FUTURE_DAYS = 14

def get_local_time(user_timezone_str):
    """Gets the current time in the user's specified timezone."""
    fn_name = "get_local_time"
    if not user_timezone_str: user_timezone_str = 'UTC' # Default if None/empty
    try:
        user_tz = pytz.timezone(user_timezone_str)
        return datetime.now(user_tz)
    except pytz.UnknownTimeZoneError:
        log_warning("routine_service", fn_name, f"Unknown timezone '{user_timezone_str}'. Using UTC.")
        return datetime.now(pytz.utc)
    except Exception as e:
        log_error("routine_service", fn_name, f"Error getting local time for tz '{user_timezone_str}'", e)
        return datetime.now(pytz.utc) # Default to UTC on error


def generate_morning_summary(user_id, context):
    """Generates the morning summary message including GCal events and WT tasks."""
    fn_name = "generate_morning_summary"
    prefs = get_user_preferences(user_id)
    user_tz_str = prefs.get("TimeZone", "UTC") if prefs else "UTC"
    user_tz = pytz.timezone(user_tz_str) # Get timezone object
    now_local = get_local_time(user_tz_str)
    today_local_str = now_local.strftime("%Y-%m-%d")

    log_info("routine_service", fn_name, f"Generating morning summary for user {user_id} for date {today_local_str}")

    items_today = []
    for item in context:
        item_date_local_str = None
        start_dt_str = item.get("gcal_start_datetime")
        is_all_day = item.get("is_all_day", False)

        if start_dt_str: # Prioritize GCal time
             try:
                 # Parse ISO string (can be date or datetime, potentially with Z or offset)
                 if 'T' in start_dt_str: # It's likely a datetime
                      dt_aware = datetime.fromisoformat(start_dt_str.replace('Z', '+00:00'))
                      item_date_local_str = dt_aware.astimezone(user_tz).strftime("%Y-%m-%d")
                 elif len(start_dt_str) == 10: # It's likely just a date (all-day event)
                      item_date_local_str = start_dt_str
             except ValueError:
                 log_warning("routine_service", fn_name, f"Could not parse gcal_start_datetime '{start_dt_str}' for item {item.get('event_id')}")
                 pass # Ignore parse errors for this item's date check
        elif item.get("date"): # Fallback to WT date field
             item_date_local_str = item.get("date")

        # Check if the derived local date matches today
        if item_date_local_str == today_local_str:
             # Include tasks, reminders, and external events if they are not completed/cancelled
             current_status = item.get("status", "pending") # Default WT items to pending
             item_type = item.get("type")
             # Include external events, or WT items not completed/cancelled
             if item_type == "external_event" or current_status not in ["completed", "cancelled"]:
                items_today.append(item)

    if not items_today:
        return f"Good morning! ‚òÄÔ∏è Looks like a clear schedule today ({today_local_str}). Anything you'd like to add?"

    # Sort items for display
    sorted_items = _sort_tasks(items_today) # Use the existing sort helper

    message_lines = [f"Good morning! ‚òÄÔ∏è Here's your overview for today, {today_local_str}:"]
    for item in sorted_items:
        # Pass user_tz_str for potential use in formatting (though _format_task_line needs update)
        # Add timezone info to item temporarily for formatting function
        item['_user_timezone_for_display'] = user_tz_str
        formatted_line = _format_task_line(item)
        message_lines.append(f"- {formatted_line}")
        item.pop('_user_timezone_for_display', None) # Clean up temporary key

    message_lines.append("\nHave a productive day!")
    return "\n".join(message_lines)


def generate_evening_review(user_id, context):
    """Generates the evening review message, listing only active WT items for the day."""
    fn_name = "generate_evening_review"
    prefs = get_user_preferences(user_id)
    user_tz_str = prefs.get("TimeZone", "UTC") if prefs else "UTC"
    user_tz = pytz.timezone(user_tz_str)
    now_local = get_local_time(user_tz_str)
    today_local_str = now_local.strftime("%Y-%m-%d")

    log_info("routine_service", fn_name, f"Generating evening review for user {user_id} for date {today_local_str}")

    wt_items_today_active = []
    for item in context:
        item_type = item.get("type")
        # *** Filter for WT items ONLY ***
        if item_type in ["task", "reminder"]:
            item_date_local_str = None
            start_dt_str = item.get("gcal_start_datetime")
            is_all_day = item.get("is_all_day", False)

            if start_dt_str:
                 try:
                     if 'T' in start_dt_str:
                          dt_aware = datetime.fromisoformat(start_dt_str.replace('Z', '+00:00'))
                          item_date_local_str = dt_aware.astimezone(user_tz).strftime("%Y-%m-%d")
                     elif len(start_dt_str) == 10:
                          item_date_local_str = start_dt_str
                 except ValueError: pass
            elif item.get("date"):
                 item_date_local_str = item.get("date")

            # Include if it's for today AND its status is pending or in progress
            if item_date_local_str == today_local_str and item.get("status") in ["pending", "in_progress", "in progress"]:
                 wt_items_today_active.append(item)

    if not wt_items_today_active:
        return f"Good evening! üëã No active tasks or reminders were scheduled for today ({today_local_str}). Time to relax or plan for tomorrow?"

    # Sort items for display
    sorted_items = _sort_tasks(wt_items_today_active)

    message_lines = [f"Good evening! üëã Let's review your day ({today_local_str}). Here are the tasks/reminders still marked as active:"]
    for i, item in enumerate(sorted_items):
        # Pass user_tz_str for potential use in formatting
        item['_user_timezone_for_display'] = user_tz_str
        formatted_line = _format_task_line(item)
        message_lines.append(f"{i+1}. {formatted_line}")
        item.pop('_user_timezone_for_display', None)

    message_lines.append("\nHow did it go? You can update items by replying (e.g., 'complete 1', 'cancel 2') or add new ones for tomorrow.")
    return "\n".join(message_lines)


def check_routine_triggers(): # Remove -> List[Tuple[str, str]] hint if not allowed
    """
    Scheduled job function. Checks all users if morning/evening routines should run.
    Calls sync service before generating summaries.
    Returns a list of (user_id, message_content) tuples for messages to be sent.
    """
    fn_name = "check_routine_triggers"
    log_info("routine_service", fn_name, "Running scheduled check for routine triggers...")
    messages_to_send = [] # Initialize list to store messages

    try:
        registry = get_registry()
        if not registry:
            log_warning("routine_service", fn_name, "User registry is empty. Skipping check.")
            return []

        user_ids = list(registry.keys())
        log_info("routine_service", fn_name, f"Checking routines for {len(user_ids)} users.")

        for user_id in user_ids:
            prefs = None
            try:
                prefs = get_user_preferences(user_id)
                if not prefs or prefs.get("status") != "active":
                    continue

                user_tz_str = prefs.get("TimeZone")
                if not user_tz_str:
                    continue

                now_local = get_local_time(user_tz_str)
                today_local_str = now_local.strftime("%Y-%m-%d")
                current_local_hm = now_local.strftime("%H:%M")

                aggregated_context = None
                context_fetched = False

                # --- Check Morning Routine ---
                morning_time_str = prefs.get("Morning_Summary_Time")
                if prefs.get("Enable_Morning") and morning_time_str:
                    last_triggered_morning = prefs.get("last_morning_trigger_date")
                    if current_local_hm >= morning_time_str and last_triggered_morning != today_local_str:
                        log_info("routine_service", fn_name, f"Triggering Morning Summary for user {user_id} at {current_local_hm} {user_tz_str}")
                        if not context_fetched:
                            context_start_date = (now_local - timedelta(days=ROUTINE_CONTEXT_HISTORY_DAYS)).strftime("%Y-%m-%d")
                            context_end_date = (now_local + timedelta(days=ROUTINE_CONTEXT_FUTURE_DAYS)).strftime("%Y-%m-%d")
                            log_info("routine_service", fn_name, f"Getting synced context for routines (User: {user_id})...")
                            aggregated_context = sync_service.get_synced_context_snapshot(user_id, context_start_date, context_end_date)
                            context_fetched = True

                        summary_msg = generate_morning_summary(user_id, aggregated_context or [])
                        if summary_msg:
                            # --- REPLACE send_message WITH append ---
                            messages_to_send.append((user_id, summary_msg))
                            # ----------------------------------------
                            update_success = update_preferences(user_id, {"last_morning_trigger_date": today_local_str})
                            if not update_success: log_error("routine_service", fn_name, f"Failed to update last_morning_trigger_date for {user_id}")
                        else:
                            log_warning("routine_service", fn_name, f"Morning summary generated empty message for {user_id}")

                # --- Check Evening Routine ---
                evening_time_str = prefs.get("Evening_Summary_Time")
                if prefs.get("Enable_Evening") and evening_time_str:
                    last_triggered_evening = prefs.get("last_evening_trigger_date")
                    if current_local_hm >= evening_time_str and last_triggered_evening != today_local_str:
                        log_info("routine_service", fn_name, f"Triggering Evening Review for user {user_id} at {current_local_hm} {user_tz_str}")
                        if not context_fetched:
                            context_start_date = (now_local - timedelta(days=ROUTINE_CONTEXT_HISTORY_DAYS)).strftime("%Y-%m-%d")
                            context_end_date = (now_local + timedelta(days=ROUTINE_CONTEXT_FUTURE_DAYS)).strftime("%Y-%m-%d")
                            log_info("routine_service", fn_name, f"Getting synced context for routines (User: {user_id})...")
                            aggregated_context = sync_service.get_synced_context_snapshot(user_id, context_start_date, context_end_date)
                            context_fetched = True

                        review_msg = generate_evening_review(user_id, aggregated_context or [])
                        if review_msg:
                            # --- REPLACE send_message WITH append ---
                            messages_to_send.append((user_id, review_msg))
                            # ----------------------------------------
                            update_success = update_preferences(user_id, {"last_evening_trigger_date": today_local_str})
                            if not update_success: log_error("routine_service", fn_name, f"Failed to update last_evening_trigger_date for {user_id}")
                        else:
                            log_warning("routine_service", fn_name, f"Evening review generated empty message for {user_id}")

            except Exception as user_err:
                 log_error("routine_service", fn_name, f"Error processing routines for user {user_id}. Prefs: {prefs}", user_err)
                 traceback.print_exc()

    except Exception as main_err:
        log_error("routine_service", fn_name, f"General error during routine check run", main_err)
        traceback.print_exc()

    log_info("routine_service", fn_name, f"Finished scheduled check for routine triggers. Found {len(messages_to_send)} messages to send.")
    return messages_to_send # Return the list

def daily_cleanup():
    """Scheduled job to perform daily cleanup tasks (e.g., reset notification tracker)."""
    fn_name = "daily_cleanup"
    log_info("routine_service", fn_name, "Running daily cleanup job...")

    try:
        registry = get_registry()
        user_ids = list(registry.keys())
        if not user_ids:
            log_info("routine_service", fn_name, "No users found for daily cleanup.")
            return

        log_info("routine_service", fn_name, f"Performing daily cleanup for {len(user_ids)} users...")
        cleared_count = 0
        for user_id in user_ids:
            try:
                if AGENT_STATE_IMPORTED:
                    # Check if state exists before trying to clear
                    if get_agent_state(user_id): # Use get_agent_state to check existence
                       clear_notified_event_ids(user_id)
                       cleared_count += 1
                    # else: No state in memory, nothing to clear
                # else: Cannot clear if state manager not imported
            except Exception as e:
                 log_error("routine_service", fn_name, f"Error during daily cleanup for user {user_id}", e)

        log_info("routine_service", fn_name, f"Daily cleanup finished. Cleared notification sets for {cleared_count} users.")

    except Exception as e:
        log_error("routine_service", fn_name, "Error during daily cleanup main loop", e)

# --- END OF FILE services/routine_service.py ---
# --- END OF FILE services\routine_service.py ---


================================================================================
üìÑ tools\google_calendar_api.py
================================================================================

# --- START OF FILE tools\google_calendar_api.py ---
# --- START OF FILE tools/google_calendar_api.py ---

import os
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Any, TYPE_CHECKING

# --- Try importing Google libraries ---
Credentials = None
build = None
HttpError = Exception
GoogleAuthRequest = None
RefreshError = Exception
GOOGLE_LIBS_AVAILABLE = False

try:
    from tools.logger import log_info, log_error, log_warning
except ImportError:
    import logging
    logging.basicConfig(level=logging.INFO, format='%(levelname)s:google_calendar_api:%(message)s')
    log_info = logging.info; log_error = logging.error; log_warning = logging.warning
    log_error("google_calendar_api", "import", "Failed to import project logger.")

try:
    from google.oauth2.credentials import Credentials as ImportedCredentials
    from googleapiclient.discovery import build as imported_build
    from googleapiclient.errors import HttpError as ImportedHttpError
    from google.auth.transport.requests import Request as ImportedGoogleAuthRequest
    from google.auth.exceptions import RefreshError as ImportedRefreshError

    Credentials = ImportedCredentials
    build = imported_build
    HttpError = ImportedHttpError
    GoogleAuthRequest = ImportedGoogleAuthRequest
    RefreshError = ImportedRefreshError
    GOOGLE_LIBS_AVAILABLE = True
    log_info("google_calendar_api", "import", "Successfully imported Google API libraries.")
except ImportError as import_error_exception:
    log_error("google_calendar_api", "import", f"Failed to import one or more Google API libraries: {import_error_exception}. GoogleCalendarAPI will be non-functional.", import_error_exception)

# --- Other Local Project Imports ---
try:
    from tools.token_store import get_user_token, save_user_token_encrypted
except ImportError as e:
     log_error("google_calendar_api", "import", f"Failed to import from token_store: {e}", e)
     def get_user_token(*args, **kwargs): return None
     def save_user_token_encrypted(*args, **kwargs): return False

if TYPE_CHECKING:
    from googleapiclient.discovery import Resource
    if Credentials:
         from google.oauth2.credentials import Credentials

# --- Configuration ---
GOOGLE_CLIENT_ID = os.getenv("GOOGLE_CLIENT_ID")
GOOGLE_CLIENT_SECRET = os.getenv("GOOGLE_CLIENT_SECRET")
if not GOOGLE_CLIENT_ID: log_error("google_calendar_api", "config", "CRITICAL: GOOGLE_CLIENT_ID not set.")
if not GOOGLE_CLIENT_SECRET: log_error("google_calendar_api", "config", "CRITICAL: GOOGLE_CLIENT_SECRET not set.")
DEFAULT_TIMEZONE = "Asia/Jerusalem" # TODO: Make user-specific via preferences
GOOGLE_TOKEN_URI = "https://oauth2.googleapis.com/token"


class GoogleCalendarAPI:
    """Handles interactions with the Google Calendar API for a specific user."""
    def __init__(self, user_id: str):
        # ... (existing __init__ logic remains the same) ...
        fn_name = "__init__"
        self.user_id = user_id
        self.service = None # Initialize service to None
        self.user_timezone = DEFAULT_TIMEZONE

        log_info("GoogleCalendarAPI", fn_name, f"Initializing for user {self.user_id}")
        if not GOOGLE_LIBS_AVAILABLE:
            log_error("GoogleCalendarAPI", fn_name, "Google API libraries not available. Initialization skipped.")
            return

        credentials = self._load_credentials()

        if credentials:
            try:
                if build is None:
                    raise ImportError("Build function ('googleapiclient.discovery.build') not available.")
                # Assign the built service to self.service
                self.service = build("calendar", "v3", credentials=credentials, cache_discovery=False)
                log_info("GoogleCalendarAPI", fn_name, f"GCal service built successfully for {self.user_id}")
            except ImportError as e:
                 log_error("GoogleCalendarAPI", fn_name, f"Import error during service build: {e}", e)
                 self.service = None # Ensure service is None on error
            except Exception as e:
                log_error("GoogleCalendarAPI", fn_name, f"Failed to build GCal service: {e}", e)
                self.service = None # Ensure service is None on error
        else:
            log_warning("GoogleCalendarAPI", fn_name, f"Initialization incomplete for {self.user_id} due to credential failure.")
            self.service = None # Ensure service is None if creds fail


    def _load_credentials(self): # REMOVED TYPE HINT
        # ... (existing _load_credentials logic remains the same) ...
        # (Make sure this function correctly returns the credentials object or None)
        fn_name = "_load_credentials"
        log_info("GoogleCalendarAPI", fn_name, f"Attempting credentials load for {self.user_id}")

        if not GOOGLE_LIBS_AVAILABLE or Credentials is None:
            log_error("GoogleCalendarAPI", fn_name, "Google libraries or Credentials class not available.")
            return None

        if not GOOGLE_CLIENT_ID or not GOOGLE_CLIENT_SECRET:
             log_error("GoogleCalendarAPI", fn_name, "Client ID or Secret missing in environment config.")
             return None

        token_data = get_user_token(self.user_id)
        if not token_data:
             log_info("GoogleCalendarAPI", fn_name, f"No token data found for user {self.user_id}.")
             return None

        if "refresh_token" not in token_data:
             log_error("GoogleCalendarAPI", fn_name, f"FATAL: refresh_token missing in stored data for {self.user_id}. Re-auth needed.")
             return None

        credential_info_for_lib = {
            'token': token_data.get('access_token'),
            'refresh_token': token_data.get('refresh_token'),
            'token_uri': GOOGLE_TOKEN_URI,
            'client_id': GOOGLE_CLIENT_ID,
            'client_secret': GOOGLE_CLIENT_SECRET,
            'scopes': token_data.get('scopes', [])
        }
        if isinstance(credential_info_for_lib['scopes'], str):
            credential_info_for_lib['scopes'] = credential_info_for_lib['scopes'].split()

        creds = None
        try:
            creds = Credentials.from_authorized_user_info(credential_info_for_lib)

            if not creds.valid:
                log_warning("GoogleCalendarAPI", fn_name, f"Credentials invalid/expired for {self.user_id}. Checking refresh token.")
                if creds.refresh_token:
                    log_info("GoogleCalendarAPI", fn_name, f"Attempting explicit token refresh for {self.user_id}...")
                    try:
                        if GoogleAuthRequest is None: raise ImportError("GoogleAuthRequest class not available for refresh.")
                        creds.refresh(GoogleAuthRequest())
                        log_info("GoogleCalendarAPI", fn_name, f"Token refresh successful for {self.user_id}.")
                        refreshed_token_data_to_save = { 'access_token': creds.token, 'refresh_token': creds.refresh_token, 'token_uri': creds.token_uri, 'client_id': creds.client_id, 'client_secret': creds.client_secret, 'scopes': creds.scopes, 'expiry_iso': creds.expiry.isoformat() if creds.expiry else None }
                        if save_user_token_encrypted is None: log_error("GoogleCalendarAPI", fn_name, "save_user_token_encrypted function not available.")
                        elif not save_user_token_encrypted(self.user_id, refreshed_token_data_to_save): log_warning("GoogleCalendarAPI", fn_name, f"Failed to save refreshed token for {self.user_id}.")
                    except RefreshError as refresh_err:
                        log_error("GoogleCalendarAPI", fn_name, f"Token refresh FAILED for {self.user_id} (RefreshError): {refresh_err}. Re-authentication required.", refresh_err)
                        token_file_path = os.path.join("data", f"tokens_{self.user_id}.json.enc")
                        if os.path.exists(token_file_path):
                            log_warning("GoogleCalendarAPI", fn_name, f"Deleting invalid token file due to refresh failure: {token_file_path}")
                            try: os.remove(token_file_path)
                            except OSError as rm_err: log_error("GoogleCalendarAPI", fn_name, f"Failed to remove token file: {rm_err}")
                        return None
                    except ImportError as imp_err:
                         log_error("GoogleCalendarAPI", fn_name, f"Import error during refresh for {self.user_id}: {imp_err}")
                         return None
                    except Exception as refresh_err:
                        log_error("GoogleCalendarAPI", fn_name, f"Unexpected error during token refresh for {self.user_id}: {refresh_err}", refresh_err)
                        return None
                else:
                     log_error("GoogleCalendarAPI", fn_name, f"Credentials invalid for {self.user_id}, and no refresh token available. Re-authentication needed.")
                     return None

            if creds and creds.valid:
                 log_info("GoogleCalendarAPI", fn_name, f"Credentials loaded and valid for {self.user_id}.")
                 return creds
            else:
                 log_error("GoogleCalendarAPI", fn_name, f"Failed to obtain valid credentials for {self.user_id} after potential refresh.")
                 return None
        except Exception as e:
            log_error("GoogleCalendarAPI", fn_name, f"Error creating/validating credentials object for {self.user_id}: {e}", e)
            return None


    # --- ADD THIS METHOD ---
    def is_active(self):
        """Checks if the Google Calendar service object was successfully initialized."""
        # The core check is whether self.service was assigned the built resource object
        return self.service is not None
    # --- END OF ADDED METHOD ---

    # --- Public API Methods ---
    def create_event(self, event_data: Dict): # REMOVED TYPE HINT -> Optional[str]
        # ... (existing create_event logic remains the same) ...
        fn_name = "create_event"
        if not self.is_active():
            log_error("GoogleCalendarAPI", fn_name, f"Service not active for user {self.user_id}.")
            return None
        assert self.service is not None

        try:
            event_date = event_data.get('date')
            event_time = event_data.get('time')
            duration_minutes = None
            if event_data.get('duration'):
                try:
                    duration_str = str(event_data['duration']).lower().replace(' ','')
                    if 'h' in duration_str:
                        parts = duration_str.split('h')
                        hours = float(parts[0])
                        duration_minutes = hours * 60
                        if len(parts) > 1 and 'm' in parts[1]:
                            minutes = int(parts[1].replace('m',''))
                            duration_minutes += minutes
                    elif 'm' in duration_str:
                        duration_minutes = int(duration_str.replace('m',''))
                    else:
                        duration_minutes = int(float(duration_str))
                    duration_minutes = round(duration_minutes)
                except (ValueError, TypeError):
                    log_warning("GoogleCalendarAPI", fn_name, f"Could not parse duration: {event_data.get('duration')}")
                    duration_minutes = None

            if not event_date:
                 log_error("GoogleCalendarAPI", fn_name, f"Missing mandatory 'date' field.")
                 return None

            start_obj = {}
            end_obj = {}
            time_zone = self.user_timezone

            if event_time:
                 try:
                     start_dt = datetime.strptime(f"{event_date} {event_time}", "%Y-%m-%d %H:%M")
                     delta = timedelta(minutes=duration_minutes if duration_minutes is not None else 30)
                     end_dt = start_dt + delta
                     start_obj = {"dateTime": start_dt.isoformat(), "timeZone": time_zone}
                     end_obj = {"dateTime": end_dt.isoformat(), "timeZone": time_zone}
                 except ValueError as time_err:
                      log_error("GoogleCalendarAPI", fn_name, f"Invalid date/time format: {event_date} {event_time}", time_err)
                      return None
            else: # All day
                try:
                    start_dt_date = datetime.strptime(event_date, "%Y-%m-%d").date()
                    end_date_dt = start_dt_date + timedelta(days=1)
                    start_obj = {"date": start_dt_date.strftime("%Y-%m-%d")}
                    end_obj = {"date": end_date_dt.strftime("%Y-%m-%d")}
                except ValueError as date_err:
                     log_error("GoogleCalendarAPI", fn_name, f"Invalid date format '{event_date}' for all-day event", date_err)
                     return None

            google_event_body = { "summary": event_data.get("title", event_data.get("description", "New Item")), "description": event_data.get("description", ""), "start": start_obj, "end": end_obj }

            log_info("GoogleCalendarAPI", fn_name, f"Creating GCal event for user {self.user_id}: {google_event_body.get('summary')}")
            created_event = self.service.events().insert(calendarId='primary', body=google_event_body).execute()
            google_event_id = created_event.get("id")

            if google_event_id:
                log_info("GoogleCalendarAPI", fn_name, f"Successfully created GCal event ID: {google_event_id}")
                return google_event_id
            else:
                log_error("GoogleCalendarAPI", fn_name, f"GCal API response missing 'id'. Response: {created_event}")
                return None
        except HttpError as http_err:
             log_error("GoogleCalendarAPI", fn_name, f"HTTP error creating event for user {self.user_id}: Status {http_err.resp.status}", http_err)
             return None
        except Exception as e:
            log_error("GoogleCalendarAPI", fn_name, f"Unexpected error creating event for user {self.user_id}", e)
            return None

    def update_event(self, event_id: str, updates: Dict): # REMOVED TYPE HINT -> bool:
        # ... (existing update_event logic remains the same) ...
        fn_name = "update_event"
        if not self.is_active():
            log_error("GoogleCalendarAPI", fn_name, f"Service not active for user {self.user_id}, cannot update event {event_id}.")
            return False
        assert self.service is not None

        try:
            update_payload = {}
            needs_update = False
            time_zone = self.user_timezone

            if "title" in updates: update_payload["summary"] = updates["title"]; needs_update = True
            if "description" in updates: update_payload["description"] = updates["description"]; needs_update = True

            new_date_str = updates.get("date")
            new_time_str = updates.get("time")

            if new_date_str or new_time_str is not None:
                 try:
                      existing_event = self.service.events().get(calendarId='primary', eventId=event_id).execute()
                      start_current = existing_event.get('start', {})
                      end_current = existing_event.get('end', {})
                 except HttpError as get_err:
                      log_error("GoogleCalendarAPI", fn_name, f"Failed to get existing event {event_id} details before update: {get_err}", get_err)
                      return False

                 current_date = start_current.get('date')
                 current_dt_str = start_current.get('dateTime')

                 target_date_str = new_date_str if new_date_str else (datetime.fromisoformat(current_dt_str).strftime('%Y-%m-%d') if current_dt_str else current_date)
                 target_time_str = new_time_str

                 if target_date_str:
                      if target_time_str: # Timed event
                          try:
                              start_dt = datetime.strptime(f"{target_date_str} {target_time_str}", "%Y-%m-%d %H:%M")
                              duration = timedelta(minutes=30) # Default fallback
                              if start_current.get('dateTime') and end_current.get('dateTime'):
                                   try: duration = datetime.fromisoformat(end_current['dateTime']) - datetime.fromisoformat(start_current['dateTime'])
                                   except Exception: pass
                              end_dt = start_dt + duration
                              update_payload["start"] = {"dateTime": start_dt.isoformat(), "timeZone": time_zone}
                              update_payload["end"] = {"dateTime": end_dt.isoformat(), "timeZone": time_zone}
                              needs_update = True
                          except ValueError as e: log_error("GoogleCalendarAPI", fn_name, f"Invalid date/time '{target_date_str} {target_time_str}' on update: {e}");
                      else: # All day event
                           try:
                               start_dt_date = datetime.strptime(target_date_str, "%Y-%m-%d").date()
                               end_date_dt = start_dt_date + timedelta(days=1)
                               update_payload["start"] = {"date": start_dt_date.strftime("%Y-%m-%d")}
                               update_payload["end"] = {"date": end_date_dt.strftime("%Y-%m-%d")}
                               needs_update = True
                           except ValueError as e: log_error("GoogleCalendarAPI", fn_name, f"Invalid date '{target_date_str}' on update: {e}");

            if not needs_update:
                 log_info("GoogleCalendarAPI", fn_name, f"No fields require patching for GCal event {event_id}")
                 return True

            log_info("GoogleCalendarAPI", fn_name, f"Patching GCal event {event_id} for user {self.user_id}. Fields: {list(update_payload.keys())}")
            self.service.events().patch(calendarId='primary', eventId=event_id, body=update_payload).execute()
            log_info("GoogleCalendarAPI", fn_name, f"Successfully updated GCal event {event_id}")
            return True
        except HttpError as http_err:
             log_error("GoogleCalendarAPI", fn_name, f"HTTP error updating event {event_id}: Status {http_err.resp.status}", http_err)
             return False
        except Exception as e:
            log_error("GoogleCalendarAPI", fn_name, f"Unexpected error updating event {event_id}", e)
            return False


    def delete_event(self, event_id: str): # REMOVED TYPE HINT -> bool:
        # ... (existing delete_event logic remains the same) ...
        fn_name = "delete_event"
        if not self.is_active():
            log_error("GoogleCalendarAPI", fn_name, f"Service not active for user {self.user_id}, cannot delete event {event_id}.")
            return False
        assert self.service is not None
        try:
            log_info("GoogleCalendarAPI", fn_name, f"Attempting delete GCal event {event_id} for user {self.user_id}")
            self.service.events().delete(calendarId='primary', eventId=event_id).execute()
            log_info("GoogleCalendarAPI", fn_name, f"Successfully deleted GCal event {event_id}.")
            return True
        except HttpError as http_err:
            if http_err.resp.status in [404, 410]:
                log_warning("GoogleCalendarAPI", fn_name, f"GCal event {event_id} not found or already gone (Status {http_err.resp.status}). Assuming deleted.")
                return True
            else:
                 log_error("GoogleCalendarAPI", fn_name, f"HTTP error deleting event {event_id}: Status {http_err.resp.status}", http_err)
                 return False
        except Exception as e:
            log_error("GoogleCalendarAPI", fn_name, f"Unexpected error deleting event {event_id}", e)
            return False

    def list_events(self, start_date: str, end_date: str): # REMOVED TYPE HINT -> List[Dict]:
        # ... (existing list_events logic remains the same) ...
        fn_name = "list_events"
        if not self.is_active():
            log_error("GoogleCalendarAPI", fn_name, f"Service not active for user {self.user_id}, cannot list events.")
            return []
        assert self.service is not None

        try:
            start_dt = datetime.strptime(start_date, "%Y-%m-%d").replace(hour=0, minute=0, second=0, microsecond=0)
            end_dt_exclusive = datetime.strptime(end_date, "%Y-%m-%d").replace(hour=0, minute=0, second=0, microsecond=0) + timedelta(days=1)
            time_min = start_dt.isoformat() + "Z"
            time_max = end_dt_exclusive.isoformat() + "Z"
            log_info("GoogleCalendarAPI", fn_name, f"Listing GCal events for user {self.user_id} from {time_min} to {time_max}")
        except ValueError as date_err:
            log_error("GoogleCalendarAPI", fn_name, f"Invalid date format for listing events: {start_date} / {end_date}", date_err)
            return []

        try:
            all_items = []
            page_token = None
            while True:
                events_result = self.service.events().list( calendarId='primary', timeMin=time_min, timeMax=time_max, maxResults=250, singleEvents=True, orderBy='startTime', pageToken=page_token ).execute()
                items = events_result.get("items", [])
                all_items.extend(items)
                page_token = events_result.get('nextPageToken')
                if not page_token: break
            log_info("GoogleCalendarAPI", fn_name, f"Found {len(all_items)} GCal events for user {self.user_id} in range.")
            return [self._parse_google_event(e) for e in all_items]
        except HttpError as http_err:
             log_error("GoogleCalendarAPI", fn_name, f"HTTP error listing events for user {self.user_id}: Status {http_err.resp.status}", http_err)
             return []
        except Exception as e:
            log_error("GoogleCalendarAPI", fn_name, f"Unexpected error listing events for user {self.user_id}", e)
            return []

    def _get_single_event(self, event_id: str): # REMOVED TYPE HINT -> Optional[Dict]:
        # ... (existing _get_single_event logic remains the same) ...
        fn_name = "_get_single_event"
        if not self.is_active():
            log_error("GoogleCalendarAPI", fn_name, f"Service not active for user {self.user_id}.")
            return None
        assert self.service is not None
        try:
            event = self.service.events().get(calendarId='primary', eventId=event_id).execute()
            return event
        except HttpError as http_err:
             if http_err.resp.status in [404, 410]:
                  log_warning("GoogleCalendarAPI", fn_name, f"Event {event_id} not found (Status {http_err.resp.status}).")
             else:
                  log_error("GoogleCalendarAPI", fn_name, f"HTTP error getting event {event_id}: Status {http_err.resp.status}", http_err)
             return None
        except Exception as e:
            log_error("GoogleCalendarAPI", fn_name, f"Unexpected error getting event {event_id}", e)
            return None

    def _parse_google_event(self, event: Dict): # REMOVED TYPE HINT -> Dict:
        # ... (existing _parse_google_event logic remains the same, ensuring it returns keys 'gcal_start_datetime' etc.) ...
        start_info = event.get("start", {})
        end_info = event.get("end", {})
        start_datetime_str = start_info.get("dateTime", start_info.get("date"))
        end_datetime_str = end_info.get("dateTime", end_info.get("date"))
        is_all_day = "date" in start_info and "dateTime" not in start_info
        parsed = {
            "event_id": event.get("id"),
            "title": event.get("summary", ""),
            "description": event.get("description", ""),
            "gcal_start_datetime": start_datetime_str, # Store the full string
            "gcal_end_datetime": end_datetime_str,     # Store the full string
            "is_all_day": is_all_day,
            "gcal_link": event.get("htmlLink", ""),
            "status_gcal": event.get("status", ""),
            "created_gcal": event.get("created"),
            "updated_gcal": event.get("updated"),
        }
        return parsed
# --- END OF FILE tools/google_calendar_api.py ---
# --- END OF FILE tools\google_calendar_api.py ---


================================================================================
üìÑ tools\calendar_tool.py
================================================================================

# --- START OF FILE tools\calendar_tool.py ---
# tools/calendar_tool.py

import os
import requests
import json # For logging potentially
from fastapi import APIRouter, Request, Response
from fastapi.responses import HTMLResponse
from tools.logger import log_info, log_error, log_warning
from tools.encryption import encrypt_data # Need encrypt
import jwt
import requests.compat # Needed for urlencode in authenticate
from datetime import datetime # Keep if used elsewhere

# --- TEMPORARY PHASE 1 IMPORTS ---
# This direct import is specific to Phase 1 testing.
# Later phases might use a service (ConfigManager) to update preferences.
from users.user_registry import update_preferences as update_prefs_direct
# ----------------------------------

router = APIRouter()

# --- Configuration ---
GOOGLE_CLIENT_SECRET = os.getenv("GOOGLE_CLIENT_SECRET")
CLIENT_ID = os.getenv("GOOGLE_CLIENT_ID")
REDIRECT_URI = os.getenv("GOOGLE_REDIRECT_URI", "http://localhost:8000/oauth2callback")
SCOPE = "https://www.googleapis.com/auth/calendar"
TOKEN_URL = "https://oauth2.googleapis.com/token"
AUTH_URL_BASE = "https://accounts.google.com/o/oauth2/auth"

# Check essential config on load
if not GOOGLE_CLIENT_SECRET or not CLIENT_ID:
    log_error("calendar_tool", "config", "CRITICAL: GOOGLE_CLIENT_ID or GOOGLE_CLIENT_SECRET env var not set.")

log_info("calendar_tool", "config", f"Calendar Tool loaded. Client ID starts with: {str(CLIENT_ID)[:10]}")


# --- Core Authentication Check Function (Phase 1) ---
def authenticate(user_id: str, prefs: dict) -> dict:
    """
    Checks user's calendar auth status based on provided preferences (Phase 1).
    Returns status and auth URL if needed, or status/message if token exists.
    Does NOT attempt to load/test the token here.
    """
    log_info("calendar_tool", "authenticate", f"Checking auth status for user {user_id}")
    token_file_path = prefs.get("token_file")
    calendar_enabled = prefs.get("Calendar_Enabled", False)

    # Scenario 1: Token file exists and system thinks it's enabled
    if calendar_enabled and token_file_path and os.path.exists(token_file_path):
        log_info("calendar_tool", "authenticate", f"Token file exists and enabled flag is True for {user_id}.")
        # Report that token exists, actual validation happens when GCalAPI is initialized
        return {"status": "token_exists", "message": "Stored calendar credentials found. Attempting to use..."}

    # Scenario 2: Initiate new authorization
    else:
        log_info("calendar_tool", "authenticate", f"No valid token file found or not enabled for {user_id}. Initiating auth.")
        # Check essential config needed to generate URL
        if not CLIENT_ID or not REDIRECT_URI:
             log_error("calendar_tool", "authenticate", f"Client ID or Redirect URI missing for auth URL generation.")
             return {"status": "fails", "message": "Server configuration error prevents authentication."}

        normalized_state = user_id.replace("@c.us", "").replace("+","") # Basic normalization
        params = {
            "client_id": CLIENT_ID,
            "redirect_uri": REDIRECT_URI,
            "scope": SCOPE,
            "response_type": "code",
            "access_type": "offline", # Crucial for refresh token
            "state": normalized_state,
            "prompt": "consent" # Force consent screen for refresh token reliability
        }
        try:
             encoded_params = requests.compat.urlencode(params)
             auth_url = f"{AUTH_URL_BASE}?{encoded_params}"
             log_info("calendar_tool", "authenticate", f"Generated auth URL for {user_id}")
             return {"status": "pending", "message": f"Please authenticate your calendar by visiting this URL: {auth_url}"}
        except Exception as url_e:
             log_error("calendar_tool", "authenticate", f"Failed to build auth URL for {user_id}", url_e)
             return {"status": "fails", "message": "Failed to generate authentication URL."}


# --- OAuth Callback Endpoint (Cleaned "OLD STYLE" Logic) ---
@router.get("/oauth2callback", response_class=HTMLResponse)
async def oauth2callback(request: Request, code: str | None = None, state: str | None = None, error: str | None = None):
    """
    Handles the OAuth2 callback from Google.
    Exchanges code, saves token (WITHOUT immediate verification), updates registry.
    """
    html_error_template = "<html><body><h1>Authentication Error</h1><p>Details: {details}</p><p>Please try authenticating again or contact support if the issue persists.</p></body></html>"
    # Simple success message assuming token *will* work later
    html_success_template = "<html><body><h1>Authentication Successful!</h1><p>Your credentials have been saved. The connection will be fully tested when first used. You can close this window and return to the chat.</p></body></html>"

    if error:
        log_error("calendar_tool", "oauth2callback", f"OAuth error received from Google: {error}")
        return HTMLResponse(content=html_error_template.format(details=f"Google reported an error: {error}"), status_code=400)
    if not code or not state:
        log_error("calendar_tool", "oauth2callback", "Callback missing code or state.")
        return HTMLResponse(content=html_error_template.format(details="Invalid response received from Google (missing code or state)."), status_code=400)

    user_id = state
    log_info("calendar_tool", "oauth2callback", f"Callback received for user {user_id}.")

    if not GOOGLE_CLIENT_SECRET or not CLIENT_ID:
         log_error("calendar_tool", "oauth2callback", "Server configuration error: Client ID/Secret not set.")
         return HTMLResponse(content=html_error_template.format(details="Server configuration error."), status_code=500)

    try:
        # --- 1. Exchange Code for Tokens ---
        payload = {
            "code": code, "client_id": CLIENT_ID, "client_secret": GOOGLE_CLIENT_SECRET,
            "redirect_uri": REDIRECT_URI, "grant_type": "authorization_code"
        }
        log_info("calendar_tool", "oauth2callback", f"Exchanging authorization code for tokens for user {user_id}.")
        token_response = requests.post(TOKEN_URL, data=payload)
        token_response.raise_for_status()
        tokens = token_response.json()
        log_info("calendar_tool", "oauth2callback", f"Tokens received successfully. Keys: {list(tokens.keys())}")

        # --- Basic Token Checks ---
        if 'access_token' not in tokens:
             log_error("calendar_tool", "oauth2callback", f"Access token *NOT* received for user {user_id}.")
             return HTMLResponse(content=html_error_template.format(details="Failed to obtain access token from Google."), status_code=500)
        if 'refresh_token' not in tokens:
             log_warning("calendar_tool", "oauth2callback", f"Refresh token *NOT* received for user {user_id}. Offline access might fail later.")
        # --- End Basic Checks ---

        # --- Verification Block REMOVED ---

        # --- 3. Extract Email ---
        email = ""
        id_token = tokens.get("id_token")
        if id_token:
            try:
                decoded = jwt.decode(id_token, options={"verify_signature": False})
                email = decoded.get("email", "")
                log_info("calendar_tool", "oauth2callback", f"Extracted email '{email}' for user {user_id}.")
            except jwt.exceptions.DecodeError as jwt_e:
                 log_warning("calendar_tool", "oauth2callback", f"Failed to decode id_token for {user_id}, proceeding without email.", jwt_e)

        # --- 4. Encrypt and Save Tokens ---
        log_info("calendar_tool", "oauth2callback", f"Proceeding to encrypt and save token for {user_id} (no immediate verification).")
        encrypted_tokens = encrypt_data(tokens)
        if not encrypted_tokens:
             log_error("calendar_tool", "oauth2callback", f"Encryption failed for {user_id}'s tokens.")
             return HTMLResponse(content=html_error_template.format(details="Failed to secure credentials."), status_code=500)

        token_file_path = os.path.join("data", f"tokens_{user_id}.json.enc")
        try:
            os.makedirs(os.path.dirname(token_file_path), exist_ok=True)
            with open(token_file_path, "wb") as f: f.write(encrypted_tokens)
            log_info("calendar_tool", "oauth2callback", f"Tokens stored successfully for {user_id}.")

            # --- 5. Update Preferences (DIRECTLY in Registry - Phase 1 Only) ---
            prefs_update = {
                "email": email,
                "token_file": token_file_path,
                "Calendar_Enabled": True,
                "status": "active" # Set user active after successful save for Phase 1
            }
            try:
                update_prefs_direct(user_id, prefs_update)
                log_info("calendar_tool", "oauth2callback", f"Preferences updated DIRECTLY for {user_id}: {prefs_update}")
                # Return simple SUCCESS response to user
                return HTMLResponse(content=html_success_template, status_code=200)
            except Exception as pref_e:
                log_error("calendar_tool", "oauth2callback", f"Failed to update preferences registry for {user_id} after token save.", pref_e)
                return HTMLResponse(content=html_error_template.format(details="Credentials saved, but failed to update user profile. Contact support."), status_code=500)

        except IOError as io_e:
             log_error("calendar_tool", "oauth2callback", f"Failed to write token file {token_file_path}", io_e)
             return HTMLResponse(content=html_error_template.format(details="Failed to save credentials locally."), status_code=500)

    except requests.exceptions.HTTPError as http_e:
        response_text = http_e.response.text; status_code = http_e.response.status_code
        error_details = f"Error {status_code} during token exchange.";
        try: error_json = http_e.response.json(); error_details = error_json.get('error_description', error_json.get('error', f"HTTP {status_code}"))
        except ValueError: pass
        log_error("calendar_tool", "oauth2callback", f"HTTP error {status_code} during token exchange for {user_id}. Details: {error_details}", http_e)
        return HTMLResponse(content=html_error_template.format(details=f"Could not get authorization from Google: {error_details}."), status_code=status_code)
    except Exception as e:
        log_error("calendar_tool", "oauth2callback", f"Generic unexpected error during callback for {user_id}", e)
        return HTMLResponse(content=html_error_template.format(details=f"An unexpected server error occurred: {e}."), status_code=500)
# --- END OF FILE tools\calendar_tool.py ---


================================================================================
üìÑ tools\metadata_store.py
================================================================================

# --- START OF FILE tools\metadata_store.py ---
# --- START OF FILE tools/metadata_store.py ---

import csv
import os
from datetime import datetime
from tools.logger import log_info, log_error, log_warning

# --- Configuration ---
METADATA_DIR = "data"
METADATA_FILE = os.path.join(METADATA_DIR, "events_metadata.csv")

# --- UPDATED FIELDNAMES (Phase 1 - Scheduler Prep) ---
FIELDNAMES = [
    # Core Identifiers & Status
    "event_id",                 # Primary Key (Matches Google Calendar Event ID if applicable, otherwise local_uuid)
    "user_id",                  # User identifier
    "type",                     # 'task', 'reminder', 'event' (event type added later by sync if needed)
    "status",                   # 'pending', 'in progress', 'completed', 'cancelled'

    # Core Content
    "title",                    # Task/Reminder/Event Title (potentially synced from GCal)
    "description",              # Task/Reminder/Event Description (potentially synced from GCal)
    "date",                     # User's intended local Due Date / Reminder Date (YYYY-MM-DD)
    "time",                     # User's intended local Due Time / Reminder Time (HH:MM or None)

    # Task Specific Fields
    "estimated_duration",       # User's estimate of work time (e.g., "4h", "90m")
    "sessions_planned",         # Integer: Total work sessions needed/scheduled based on estimate/prefs
    "sessions_completed",       # Integer: Work sessions marked as completed
    "progress_percent",         # Integer: 0-100 calculated progress (primarily for tasks)
    "session_event_ids",        # String: JSON encoded list of GCal IDs for work sessions ["id1", "id2"]

    # Scheduling & Sync Related (Some potentially updated by GCal)
    "project",                  # Associated project tag (or None)
    "series_id",                # ID for recurring events (if implemented later)
    "gcal_start_datetime",      # Full ISO 8601 Aware Timestamp from GCal (e.g., 2023-10-27T10:00:00+03:00)
    "gcal_end_datetime",        # Full ISO 8601 Aware Timestamp from GCal
    "duration",                 # Original GCal event duration string (e.g., PT1H) - informational

    # Internal Tracking & Metadata
    "created_at",               # ISO 8601 UTC timestamp ('Z') when metadata record was created
    "completed_at",             # ISO 8601 UTC timestamp ('Z') when task was completed (or None)
    "internal_reminder_sent",   # ISO 8601 UTC timestamp ('Z') when reminder was sent, or "" if not sent
    "original_date",            # Original user input for date (e.g., 'tomorrow') - informational
    "progress",                 # Qualitative progress notes (Potentially deprecated - keep for now)
    "internal_reminder_minutes",# (DEPRECATED - use Notification_Lead_Time pref instead) - Keep field for backward compat? Or remove? Let's remove.
]
# --- END OF UPDATED FIELDNAMES ---

# --- Initialization ---
def init_metadata_store():
    """Ensures the data directory and metadata CSV file exist with headers."""
    fn_name = "init_metadata_store"
    try:
        os.makedirs(METADATA_DIR, exist_ok=True)
        header_correct = False
        file_exists = os.path.exists(METADATA_FILE)

        if file_exists:
             try:
                 with open(METADATA_FILE, "r", newline="", encoding="utf-8") as f_check:
                      first_line = f_check.readline()
                      if first_line:
                           # Handle potential BOM (Byte Order Mark)
                           if first_line.startswith('\ufeff'):
                               first_line = first_line[1:]
                           reader = csv.reader([first_line])
                           try:
                               header = next(reader)
                               header_correct = (header == FIELDNAMES)
                           except StopIteration: # Empty file after BOM or similar
                               header_correct = False
                      else: # File exists but is empty
                           header_correct = False
             except Exception as read_err:
                 log_warning("metadata_store", fn_name, f"Could not read or parse existing header: {read_err}")
                 header_correct = False # Assume incorrect if read fails

        if not file_exists or not header_correct:
            action = "Overwriting" if file_exists else "Creating"
            log_warning("metadata_store", fn_name, f"{action} {METADATA_FILE} with new headers (Required: {len(FIELDNAMES)}, Found Correct: {header_correct}).")
            # Optional: Backup existing file before overwriting
            # if file_exists and not header_correct:
            #     backup_path = METADATA_FILE + f".bak_{datetime.now():%Y%m%d_%H%M%S}"
            #     try:
            #         os.rename(METADATA_FILE, backup_path)
            #         log_info("metadata_store", fn_name, f"Backed up existing file to {backup_path}")
            #     except OSError as bak_err:
            #         log_error("metadata_store", fn_name, f"Could not back up existing metadata file: {bak_err}")

            try:
                with open(METADATA_FILE, "w", newline="", encoding="utf-8") as f:
                    writer = csv.DictWriter(f, fieldnames=FIELDNAMES)
                    writer.writeheader()
                log_info("metadata_store", fn_name, f"Successfully wrote headers to {METADATA_FILE}.")
            except IOError as write_err:
                 log_error("metadata_store", fn_name, f"Failed to write headers to {METADATA_FILE}", write_err)
                 raise # Reraise critical error

    except IOError as e:
        log_error("metadata_store", fn_name, f"Failed to initialize metadata store directory/file at {METADATA_FILE}", e)
        raise


# --- Core CRUD Functions ---

def save_event_metadata(event_metadata: dict):
    """
    Saves or updates a single event's metadata using atomic write.
    Ensures only fields defined in FIELDNAMES are saved.
    """
    fn_name = "save_event_metadata"
    init_metadata_store() # Ensure file exists with correct header

    event_id = event_metadata.get("event_id")
    if not event_id:
        log_error("metadata_store", fn_name, "Cannot save metadata: 'event_id' is missing.")
        raise KeyError("'event_id' is required in event_metadata dictionary.")

    all_events = []
    updated = False

    # Read existing data safely
    try:
        all_events = load_all_metadata()
    except Exception as load_e:
         log_error("metadata_store", fn_name, f"Error loading existing metadata before save: {load_e}. Proceeding cautiously.", load_e)
         all_events = [] # Start fresh if load fails

    # Prepare the row to be saved, ensuring only valid fields are included
    # And convert values to string for CSV compatibility (handle None)
    row_to_save = {}
    for field in FIELDNAMES:
        value = event_metadata.get(field)
        # Convert non-string/non-None values to string. Keep None as None (will become empty string in CSV).
        if value is not None and not isinstance(value, str):
            row_to_save[field] = str(value)
        else:
            row_to_save[field] = value # Keep strings and None as they are

    # Update or append
    found_index = -1
    for i, existing_event in enumerate(all_events):
        if existing_event.get("event_id") == event_id:
            found_index = i
            break

    if found_index != -1:
        log_info("metadata_store", fn_name, f"Updating existing metadata for event {event_id}")
        all_events[found_index] = row_to_save
        updated = True
    else:
        log_info("metadata_store", fn_name, f"Adding new metadata for event {event_id}")
        all_events.append(row_to_save)

    # Write data back safely using atomic write
    temp_file_path = METADATA_FILE + ".tmp"
    try:
        with open(temp_file_path, "w", newline="", encoding="utf-8") as f:
            # Use extrasaction='ignore' - shouldn't be needed due to explicit field loop, but safe.
            # Handle None values by writing empty strings using DictWriter's default behavior.
            writer = csv.DictWriter(f, fieldnames=FIELDNAMES, extrasaction='ignore')
            writer.writeheader()
            writer.writerows(all_events) # Pass the list of dictionaries
        os.replace(temp_file_path, METADATA_FILE)
        log_info("metadata_store", fn_name, f"Successfully saved metadata. Total records: {len(all_events)}")
    except (IOError, csv.Error) as e: # Catch specific CSV/IO errors
         log_error("metadata_store", fn_name, f"Error writing metadata file {METADATA_FILE}: {e}", e)
         if os.path.exists(temp_file_path):
             try: os.remove(temp_file_path)
             except OSError: pass
         raise # Reraise after logging
    except Exception as e:
         log_error("metadata_store", fn_name, f"Unexpected error writing metadata file {METADATA_FILE}", e)
         if os.path.exists(temp_file_path):
             try: os.remove(temp_file_path)
             except OSError: pass
         raise

def get_event_metadata(event_id: str) -> dict:
    """
    Retrieves metadata for a specific event ID. Returns empty dict if not found/error.
    """
    fn_name = "get_event_metadata"
    init_metadata_store()
    try:
        with open(METADATA_FILE, "r", newline="", encoding="utf-8") as f:
            reader = csv.DictReader(f)
            for row in reader:
                if row.get("event_id") == event_id:
                    # Convert numeric/json fields back if needed, or handle downstream
                    # Example: row['sessions_planned'] = int(row['sessions_planned'] or 0)
                    return row
        return {} # Not found
    except FileNotFoundError:
        log_info("metadata_store", fn_name, f"Metadata file not found when getting {event_id}.")
        return {}
    except Exception as e:
        log_error("metadata_store", fn_name, f"Error reading metadata file for event {event_id}", e)
        return {}


def delete_event_metadata(event_id: str) -> bool:
    """Deletes metadata for a specific event ID. Rewrites the file."""
    fn_name = "delete_event_metadata"
    init_metadata_store()
    if not event_id:
        log_warning("metadata_store", fn_name, "Attempted to delete metadata with empty event_id.")
        return False

    all_events = []
    found = False
    try:
        all_events = load_all_metadata() # Read all current data
        original_count = len(all_events)
        # Filter out the event to delete
        filtered_events = [e for e in all_events if e.get("event_id") != event_id]
        found = len(filtered_events) < original_count

        if not found:
             log_warning("metadata_store", fn_name, f"Event ID {event_id} not found in metadata. No deletion performed.")
             return True # Consider it success if it wasn't there to delete

        all_events = filtered_events # Use the filtered list for saving

    except Exception as load_e:
         log_error("metadata_store", fn_name, f"Error loading metadata before delete operation for {event_id}", load_e)
         return False # Fail if we couldn't load existing data

    # Write the filtered data back using atomic write
    temp_file_path = METADATA_FILE + ".tmp"
    try:
        with open(temp_file_path, "w", newline="", encoding="utf-8") as f:
            writer = csv.DictWriter(f, fieldnames=FIELDNAMES, extrasaction='ignore')
            writer.writeheader()
            writer.writerows(all_events)
        os.replace(temp_file_path, METADATA_FILE)
        log_info("metadata_store", fn_name, f"Successfully deleted metadata for {event_id}. Remaining records: {len(all_events)}")
        return True
    except Exception as e:
        log_error("metadata_store", fn_name, f"Error writing metadata after delete operation for {event_id}", e)
        if os.path.exists(temp_file_path):
            try: os.remove(temp_file_path)
            except OSError: pass
        return False


def list_metadata(user_id: str, start_date_str: str | None = None, end_date_str: str | None = None) -> list[dict]:
    """Lists metadata for a user, optionally filtered by 'date' field."""
    fn_name = "list_metadata"
    init_metadata_store()
    results = []
    try:
        with open(METADATA_FILE, "r", newline="", encoding="utf-8") as f:
            reader = csv.DictReader(f)
            for row in reader:
                if row.get("user_id") == user_id:
                    include = True
                    # Date filtering logic (uses 'date' field, not gcal_start_datetime)
                    row_date_str = row.get("date")
                    if row_date_str and (start_date_str or end_date_str):
                        try:
                            row_date = datetime.strptime(row_date_str, "%Y-%m-%d").date()
                            if start_date_str:
                                start_date = datetime.strptime(start_date_str, "%Y-%m-%d").date()
                                if row_date < start_date: include = False
                            if include and end_date_str:
                                end_date = datetime.strptime(end_date_str, "%Y-%m-%d").date()
                                if row_date > end_date: include = False
                        except (ValueError, TypeError):
                             # Ignore rows with invalid date format if filtering is active
                             include = False
                    if include:
                        results.append(row)
        return results
    except FileNotFoundError:
        log_info("metadata_store", fn_name, "Metadata file not found.")
        return []
    except Exception as e:
        log_error("metadata_store", fn_name, f"Error reading metadata for user {user_id}", e)
        return []


def load_all_metadata() -> list[dict]:
    """Loads all records from the metadata CSV file."""
    fn_name = "load_all_metadata"
    init_metadata_store()
    try:
        with open(METADATA_FILE, "r", newline="", encoding="utf-8") as f:
            # Check if file is empty before trying to read
            first_char = f.read(1)
            if not first_char:
                return [] # Return empty list for empty file
            f.seek(0) # Reset cursor
            reader = csv.DictReader(f)
            data = list(reader) # Read all rows into a list
            return data
    except FileNotFoundError:
        log_info("metadata_store", fn_name, "Metadata file not found.")
        return []
    except Exception as e:
        log_error("metadata_store", fn_name, f"Error reading all metadata: {e}", e)
        return []


# --- Ensure store is initialized when module is loaded ---
init_metadata_store()

# --- END OF FILE tools/metadata_store.py ---
# --- END OF FILE tools\metadata_store.py ---


================================================================================
üìÑ tools\token_store.py
================================================================================

# --- START OF FILE tools\token_store.py ---
# tools/token_store.py

import os
import json
# Remove: from cryptography.fernet import Fernet # No longer needed here
from tools.encryption import decrypt_data # <-- IMPORT THIS
from tools.logger import log_info, log_error, log_warning # <-- Added log_error
from tools.encryption import decrypt_data, encrypt_data

TOKEN_DIR = "data" # Keep this if already defined

# Remove or comment out: ENCRYPTION_KEY = os.getenv("ENCRYPTION_KEY") # No longer needed here

def get_user_token(user_id: str) -> dict | None: # <-- Return type changed to dict | None
    """Loads and decrypts the user's token from the encrypted file."""
    path = os.path.join(TOKEN_DIR, f"tokens_{user_id}.json.enc")
    if not os.path.exists(path):
        # This is not necessarily an error, just means no token yet.
        log_info("token_store", "get_user_token", f"No token file found for user {user_id} at {path}")
        return None # <-- Return None if file not found

    try:
        with open(path, "rb") as f:
            encrypted_data = f.read()

        # Use the new decrypt function
        token_data = decrypt_data(encrypted_data) # <-- USE THIS

        if token_data:
            # Log a summary of the token data without exposing full details.
            log_info("token_store", "get_user_token", f"Decrypted token data keys for user {user_id}: {list(token_data.keys())}")
            return token_data
        else:
            # Decryption failed (logged within decrypt_data)
            log_error("token_store", "get_user_token", f"Failed to decrypt token for user {user_id} from {path}.")
            # Consider deleting the corrupted file? os.remove(path)
            return None

    except FileNotFoundError:
         log_info("token_store", "get_user_token", f"Token file not found for user {user_id} at {path} (race condition?).")
         return None
    except Exception as e:
        # Catch any other unexpected errors during file reading etc.
        log_error("token_store", "get_user_token", f"Unexpected error loading token for user {user_id} from {path}: {str(e)}", e)
        return None


def save_user_token_encrypted(user_id: str, token_data: dict) -> bool:
    """
    Encrypts and saves the user's token data to a file using atomic write.

    Args:
        user_id: The user's identifier.
        token_data: The dictionary containing token information. It expects keys like
                    'access_token', 'refresh_token', 'token_uri', etc. It handles if
                    the access token key is 'token' coming from the Credentials object.

    Returns:
        True if saving was successful, False otherwise.
    """
    log_info("token_store", "save_user_token_encrypted", f"Attempting to save token for user {user_id}")
    path = os.path.join(TOKEN_DIR, f"tokens_{user_id}.json.enc")
    temp_path = path + ".tmp" # Temporary file path

    try:
        # Standardize access token key to 'access_token' before saving
        data_to_save = token_data.copy()
        if 'token' in data_to_save and 'access_token' not in data_to_save:
            data_to_save['access_token'] = data_to_save.pop('token')

        # Basic validation: Check if critical tokens are present AFTER standardization
        if not data_to_save.get('access_token'):
             log_error("token_store", "save_user_token_encrypted", f"Attempted to save token data missing access_token for {user_id}.")
             return False
        if not data_to_save.get('refresh_token'):
             # Allow saving even if refresh token is missing, but log warning.
             log_warning("token_store", "save_user_token_encrypted", f"Saving token data potentially missing refresh_token for {user_id}.")

        # Encrypt the standardized data
        encrypted_tokens = encrypt_data(data_to_save)
        if not encrypted_tokens:
            # Error already logged by encrypt_data
            log_error("token_store", "save_user_token_encrypted", f"Encryption failed for {user_id}'s tokens during save.")
            return False

        # Ensure directory exists
        os.makedirs(os.path.dirname(path), exist_ok=True)

        # Write to temporary file first
        with open(temp_path, "wb") as f:
            f.write(encrypted_tokens)

        # Atomically replace original file with temp file
        os.replace(temp_path, path)

        log_info("token_store", "save_user_token_encrypted", f"Token stored successfully for {user_id} at {path}.")
        return True

    except Exception as e:
        log_error("token_store", "save_user_token_encrypted", f"Failed to write token file {path}", e)
        # Clean up temp file if it exists after an error
        if os.path.exists(temp_path):
            try: os.remove(temp_path)
            except OSError as rm_err: log_error("...", f"Failed to remove temp file {temp_path}: {rm_err}")
        return False
# --- END OF FILE tools\token_store.py ---


================================================================================
üìÑ tools\encryption.py
================================================================================

# --- START OF FILE tools\encryption.py ---
# tools/encryption.py

import os
import json
from cryptography.fernet import Fernet, InvalidToken
from tools.logger import log_error, log_info

# --- Key Management ---
# Load the encryption key from environment variables
# CRITICAL: This key MUST be kept secret and secure.
# Generate one using generate_key() below and set it as an environment variable.
ENCRYPTION_KEY_ENV_VAR = "ENCRYPTION_KEY"
_encryption_key = os.getenv(ENCRYPTION_KEY_ENV_VAR)

if not _encryption_key:
    log_error("encryption", "__init__",
              f"CRITICAL ERROR: Environment variable '{ENCRYPTION_KEY_ENV_VAR}' not set. Encryption disabled.")
    # You might want to raise an exception here to halt execution
    # raise ValueError(f"Environment variable '{ENCRYPTION_KEY_ENV_VAR}' is required for encryption.")
    _fernet = None
else:
    try:
        # Ensure the key is bytes
        _key_bytes = _encryption_key.encode('utf-8')
        _fernet = Fernet(_key_bytes)
        log_info("encryption", "__init__", "Fernet encryption service initialized successfully.")
    except Exception as e:
        log_error("encryption", "__init__", f"Failed to initialize Fernet. Invalid key format? Error: {e}", e)
        _fernet = None
        # raise ValueError(f"Invalid encryption key format: {e}") # Optional: Halt execution

# --- Encryption/Decryption Functions ---

def encrypt_data(data: dict) -> bytes | None:
    """
    Encrypts a dictionary using Fernet.

    Args:
        data: The dictionary to encrypt.

    Returns:
        Encrypted bytes if successful, None otherwise.
    """
    if not _fernet:
        log_error("encryption", "encrypt_data", "Encryption service not available (key missing or invalid).")
        return None
    try:
        # Serialize the dictionary to a JSON string, then encode to bytes
        data_bytes = json.dumps(data).encode('utf-8')
        encrypted_data = _fernet.encrypt(data_bytes)
        return encrypted_data
    except Exception as e:
        log_error("encryption", "encrypt_data", f"Encryption failed: {e}", e)
        return None

def decrypt_data(encrypted_data: bytes) -> dict | None:
    """
    Decrypts data encrypted with Fernet back into a dictionary.

    Args:
        encrypted_data: The encrypted bytes to decrypt.

    Returns:
        The original dictionary if successful, None otherwise.
    """
    if not _fernet:
        log_error("encryption", "decrypt_data", "Encryption service not available (key missing or invalid).")
        return None
    try:
        decrypted_bytes = _fernet.decrypt(encrypted_data)
        # Decode bytes back to JSON string, then parse into dictionary
        decrypted_json = decrypted_bytes.decode('utf-8')
        original_data = json.loads(decrypted_json)
        return original_data
    except InvalidToken:
        log_error("encryption", "decrypt_data", "Decryption failed: Invalid token (key mismatch or data corrupted).")
        return None
    except Exception as e:
        log_error("encryption", "decrypt_data", f"Decryption failed: {e}", e)
        return None

# --- Key Generation Utility ---

def generate_key() -> str:
    """Generates a new Fernet key (URL-safe base64 encoded)."""
    return Fernet.generate_key().decode('utf-8')

# Example usage for generating a key (run this file directly: python -m tools.encryption)
if __name__ == "__main__":
    new_key = generate_key()
    print("Generated Fernet Key (set this as your ENCRYPTION_KEY environment variable):")
    print(new_key)
    print("\nWARNING: Keep this key secure and secret!")
# --- END OF FILE tools\encryption.py ---


================================================================================
üìÑ tools\logger.py
================================================================================

# --- START OF FILE tools\logger.py ---
import os
import pytz
from datetime import datetime
import traceback

# === Config ===
DEBUG_MODE = True  # Set to False in production
LOG_DIR = "logs"
LOG_FILE = os.path.join(LOG_DIR, "whats_tasker.log")

# Ensure logs directory exists
os.makedirs(LOG_DIR, exist_ok=True)

# === Helpers ===
def _timestamp():
    tz = pytz.timezone("Asia/Jerusalem")
    return datetime.now(tz).strftime("%Y-%m-%d %H:%M:%S")

def _format_log(level: str, module: str, func: str, message: str):
    return f"[{_timestamp()}] [{level}] [{module}:{func}] {message}"

# === Log functions ===
def log_info(module: str, func: str, message: str):
    entry = _format_log("INFO", module, func, message)
    if DEBUG_MODE:
        print(entry)

def log_error(module: str, func: str, message: str, exception: Exception = None):
    entry = _format_log("ERROR", module, func, message)
    if DEBUG_MODE:
        print(entry)
        # --- ADD THIS PART ---
        if exception:
            print(traceback.format_exc()) # Print traceback to console in debug mode
        # ---------------------
    else:
        with open(LOG_FILE, "a", encoding="utf-8") as f:
            f.write(entry + "\n")
            if exception:
                f.write(traceback.format_exc())
                
def log_warning(module: str, func: str, message: str, exception: Exception = None):
    entry = _format_log("WARNING", module, func, message)
    if DEBUG_MODE:
        print(entry)
    else:
        with open(LOG_FILE, "a", encoding="utf-8") as f:
            f.write(entry + "\n")
            if exception:
                f.write(traceback.format_exc())
# --- END OF FILE tools\logger.py ---


================================================================================
üìÑ users\user_manager.py
================================================================================

# --- START OF FILE users\user_manager.py ---
# --- START OF FILE users/user_manager.py ---

import os
import re
from datetime import datetime, timedelta
from typing import Dict, Any, Optional, List
import traceback
from tools.logger import log_info, log_error, log_warning
from users.user_registry import get_registry, register_user, get_user_preferences

# --- State Manager Import ---
try:
    from services.agent_state_manager import (
        register_agent_instance,
        get_agent_state,
        initialize_state_store
    )
    AGENT_STATE_MANAGER_IMPORTED = True
except ImportError:
     log_error("user_manager", "import", "AgentStateManager not found.")
     AGENT_STATE_MANAGER_IMPORTED = False
     # Fallback local dictionary (not recommended for production)
     _user_agents_in_memory: Dict[str, Dict[str, Any]] = {}
     def register_agent_instance(uid, state): _user_agents_in_memory[uid] = state
     def get_agent_state(uid): return _user_agents_in_memory.get(uid)
     def initialize_state_store(ref): global _user_agents_in_memory; _user_agents_in_memory = ref

# --- Service/Tool Imports ---
try:
    from tools.google_calendar_api import GoogleCalendarAPI
    GCAL_API_IMPORTED = True
except ImportError:
    GCAL_API_IMPORTED = False
    GoogleCalendarAPI = None # Define as None if import fails
    log_warning("user_manager","import", "GoogleCalendarAPI not found.")
try:
    from tools.metadata_store import list_metadata
    METADATA_STORE_IMPORTED = True
    log_info("user_manager", "import", "Successfully imported metadata_store.")
except ImportError:
    METADATA_STORE_IMPORTED = False
    def list_metadata(*args, **kwargs): return [] # Dummy function if import fails
    log_error("user_manager", "import", "metadata_store not found.")


# --- In-Memory State Dictionary Reference ---
_user_agents_in_memory: Dict[str, Dict[str, Any]] = {}
if AGENT_STATE_MANAGER_IMPORTED:
    # Initialize the state store via the manager
    initialize_state_store(_user_agents_in_memory)


# --- Preload Context ---
def _preload_initial_context(user_id: str) -> list:
    """Loads initial context ONLY from the metadata store during startup."""
    fn_name = "_preload_initial_context"
    log_info("user_manager", fn_name, f"Preloading initial context for {user_id} from metadata store.")
    if not METADATA_STORE_IMPORTED:
        log_error("user_manager", fn_name, "Metadata store not imported. Cannot preload context.")
        return []
    try:
        # Fetch metadata without date filters initially
        metadata_list = list_metadata(user_id=user_id)
        log_info("user_manager", fn_name, f"Preloaded {len(metadata_list)} items from metadata store for {user_id}.")
        return metadata_list
    except Exception as e:
        log_error("user_manager", fn_name, f"Error preloading context for {user_id} from metadata", e)
        return []


# --- Agent State Creation ---
def create_and_register_agent_state(user_id: str): # REMOVED TYPE HINT
    """Creates the full agent state dictionary and registers it."""
    fn_name = "create_and_register_agent_state"
    log_info("user_manager", fn_name, f"Creating FULL agent state for {user_id}")
    norm_user_id = re.sub(r'\D', '', user_id)
    if not norm_user_id:
        log_error("user_manager", fn_name, f"Invalid user_id after normalization: '{user_id}'")
        return None

    register_user(norm_user_id)
    preferences = get_user_preferences(norm_user_id)
    if not preferences:
        log_error("user_manager", fn_name, f"Failed to get/create prefs for {norm_user_id} after registration attempt.")
        return None

    # --- Refined GCal Initialization ---
    calendar_api_instance = None # Default to None
    if GCAL_API_IMPORTED and GoogleCalendarAPI is not None and preferences.get("Calendar_Enabled"):
        token_file = preferences.get("token_file")
        if token_file and os.path.exists(token_file):
            log_info("user_manager", fn_name, f"Attempting GCalAPI init for {norm_user_id}")
            temp_cal_api = None # Initialize temporary variable
            try:
                # Step 1: Create the instance (this calls __init__ which loads creds and builds service)
                temp_cal_api = GoogleCalendarAPI(norm_user_id)
                # Step 2: Explicitly check if it's active *after* __init__ completes
                if temp_cal_api.is_active():
                    calendar_api_instance = temp_cal_api # Assign the created instance
                    log_info("user_manager", fn_name, f"GCalAPI initialized and active for {norm_user_id}")
                else:
                    # This case means __init__ completed but self.service was None or is_active() returned False
                    log_warning("user_manager", fn_name, f"GCalAPI initialized but NOT active for {norm_user_id}. Calendar features disabled.")
                    calendar_api_instance = None
            except Exception as cal_e:
                 # *** ADDED TRACEBACK LOGGING ***
                 tb_str = traceback.format_exc()
                 log_error("user_manager", fn_name, f"Exception during GCalAPI initialization or is_active() check for {norm_user_id}. Traceback:\n{tb_str}", cal_e)
                 # *******************************
                 calendar_api_instance = None # Ensure None on any exception
        else:
            if preferences.get("Calendar_Enabled"):
                 log_warning("user_manager", fn_name, f"GCal enabled for {norm_user_id} but token file missing/invalid path: {token_file}")
    elif not GCAL_API_IMPORTED:
         log_warning("user_manager", fn_name, f"GoogleCalendarAPI library not imported, skipping calendar init for {norm_user_id}.")
    elif not preferences.get("Calendar_Enabled"):
         log_info("user_manager", fn_name, f"Calendar not enabled for {norm_user_id}, skipping GCal init.")
    # --- End Refined GCal Initialization ---

    # Preload initial task context
    initial_context = _preload_initial_context(norm_user_id)

    # Assemble the full agent state dictionary
    agent_state = {
        "user_id": norm_user_id,
        "preferences": preferences,
        "active_tasks_context": initial_context,
        "calendar": calendar_api_instance, # Store the final instance (or None)
        "conversation_history": [],
        "notified_event_ids_today": set()
    }

    # Register state using AgentStateManager
    try:
        register_agent_instance(norm_user_id, agent_state)
        log_info("user_manager", fn_name, f"Successfully registered agent state for {norm_user_id}")
        return agent_state
    except Exception as e:
        log_error("user_manager", fn_name, f"Failed state registration for {norm_user_id}", e)
        return None

# --- Initialize All Agents ---
def init_all_agents():
    """Initializes states for all users found in the registry."""
    fn_name = "init_all_agents"
    log_info("user_manager", fn_name, "Initializing states for all registered users...")
    registry_data = get_registry()
    registered_users = list(registry_data.keys())
    initialized_count = 0
    failed_count = 0

    if not registered_users:
        log_info("user_manager", fn_name, "No users found in registry.")
        return

    log_info("user_manager", fn_name, f"Found {len(registered_users)} users. Initializing...")
    for user_id in registered_users:
        # Normalize ID just in case registry contains non-normalized ones
        norm_user_id = re.sub(r'\D', '', user_id)
        if not norm_user_id:
             log_warning("user_manager", fn_name, f"Skipping invalid user_id found in registry: '{user_id}'")
             failed_count += 1
             continue
        try:
            created_state = create_and_register_agent_state(norm_user_id)
            if created_state:
                initialized_count += 1
            else:
                # Error already logged by create_and_register_agent_state
                failed_count += 1
        except Exception as e:
            log_error("user_manager", fn_name, f"Unexpected error initializing agent state for user {norm_user_id}", e)
            failed_count += 1

    log_info("user_manager", fn_name, f"Agent state initialization complete. Success: {initialized_count}, Failed: {failed_count}")


# --- Get Agent State ---
def get_agent(user_id: str) -> Optional[Dict]:
    """Retrieves or creates and registers the agent state for a user."""
    fn_name = "get_agent"
    norm_user_id = re.sub(r'\D', '', user_id)
    if not norm_user_id:
         log_error("user_manager", fn_name, f"Cannot get agent state for invalid normalized user_id from '{user_id}'")
         return None

    agent_state = None
    try:
        # Use state manager function (or local fallback)
        agent_state = get_agent_state(norm_user_id)
        if not agent_state:
            log_warning("user_manager", fn_name, f"State for {norm_user_id} not in memory. Creating now.")
            agent_state = create_and_register_agent_state(norm_user_id)
    except Exception as e:
         log_error("user_manager", fn_name, f"Error retrieving/creating agent state for {norm_user_id}", e)
         agent_state = None # Ensure None is returned on error

    return agent_state

# --- END OF FILE users/user_manager.py ---
# --- END OF FILE users\user_manager.py ---


================================================================================
üìÑ users\user_registry.py
================================================================================

# --- START OF FILE users\user_registry.py ---
# --- START OF FILE users/user_registry.py ---

import json
import os
from datetime import datetime
from tools.logger import log_info, log_error, log_warning

USER_REGISTRY_PATH = "data/users/registry.json"

# --- UPDATED Default Preferences ---
DEFAULT_PREFERENCES = {
    "status": "new", # 'new', 'onboarding', 'active'
    # Time & Scheduling Preferences
    "TimeZone": None, # REQUIRED during onboarding (e.g., "Asia/Jerusalem", "America/New_York")
    "Work_Start_Time": None, # REQUIRED during onboarding (HH:MM)
    "Work_End_Time": None,   # REQUIRED during onboarding (HH:MM)
    "Work_Days": ["Monday", "Tuesday", "Wednesday", "Thursday", "Friday"], # Default, modifiable
    "Preferred_Session_Length": None, # REQUIRED during onboarding (e.g., "60m", "1.5h")
    # Routine Preferences
    "Morning_Summary_Time": None , # User local time (HH:MM), default None
    "Evening_Summary_Time": None , # User local time (HH:MM), default None
    "Enable_Morning": True, # Default enabled if time is set
    "Enable_Evening": True, # Default enabled if time is set
    "Enable_Weekly_Reflection": False, # Future use
    # Notification Preferences (NEW)
    "Notification_Lead_Time": "15m", # Default lead time for event notifications
    # Calendar Integration
    "Calendar_Enabled": False, # Flag if GCal connected
    "Calendar_Type": "", # "Google" or potentially others later
    "email": "", # User's Google email (extracted during auth)
    "token_file": None, # Path to encrypted token file
    # Internal Tracking
    "Last_Sync": "", # ISO 8601 UTC timestamp (e.g., "2025-04-22T15:30:00Z")
    "last_morning_trigger_date": "", # YYYY-MM-DD string
    "last_evening_trigger_date": "", # YYYY-MM-DD string
    # Misc/Future Use
    "Holiday_Dates": [], # List of YYYY-MM-DD strings
}
# --- END OF UPDATED DEFAULT PREFERENCES ---


# Global in-memory registry variable.
_registry = {}

def load_registry():
    """Loads the registry from disk into memory."""
    global _registry
    if os.path.exists(USER_REGISTRY_PATH):
        try:
            with open(USER_REGISTRY_PATH, "r", encoding="utf-8") as f:
                # Handle empty file case
                content = f.read()
                if not content.strip():
                    _registry = {}
                else:
                    f.seek(0) # Go back to start if not empty
                    _registry = json.load(f)
            # Ensure existing users have all default keys
            updated_registry = False
            for user_id, user_data in _registry.items():
                 if "preferences" not in user_data:
                      user_data["preferences"] = DEFAULT_PREFERENCES.copy()
                      updated_registry = True
                 else:
                      for key, default_value in DEFAULT_PREFERENCES.items():
                           if key not in user_data["preferences"]:
                                user_data["preferences"][key] = default_value
                                updated_registry = True
            if updated_registry:
                 log_info("user_registry", "load_registry", "Added missing default preference keys to existing users.")
                 save_registry() # Save immediately if defaults were added

        except (json.JSONDecodeError, IOError) as e:
            log_error("user_registry", "load_registry", f"Failed to load or parse registry file {USER_REGISTRY_PATH}", e)
            _registry = {} # Fallback to empty registry on error
    else:
        _registry = {}
    log_info("user_registry", "load_registry", f"Registry loaded with {len(_registry)} users.")
    return _registry

def get_registry():
    """Returns the in-memory registry. Loads it if not already loaded."""
    global _registry
    # Check if registry is empty dictionary, load only if file exists
    if not _registry and os.path.exists(USER_REGISTRY_PATH):
         load_registry()
    # If still empty after attempt, it's genuinely empty or failed load
    return _registry

# Compatibility alias
def load_registered_users():
    return get_registry()

def save_registry():
    """Saves the current in-memory registry to disk."""
    global _registry
    try:
        # Ensure directory exists
        os.makedirs(os.path.dirname(USER_REGISTRY_PATH), exist_ok=True)
        # Use atomic write pattern
        temp_path = USER_REGISTRY_PATH + ".tmp"
        with open(temp_path, "w", encoding="utf-8") as f:
            json.dump(_registry, f, indent=2, ensure_ascii=False)
        os.replace(temp_path, USER_REGISTRY_PATH)
        # log_info("user_registry", "save_registry", "Registry saved to disk.") # Can be noisy
    except IOError as e:
        log_error("user_registry", "save_registry", f"Failed to write registry file {USER_REGISTRY_PATH}", e)
        if os.path.exists(temp_path):
            try: os.remove(temp_path)
            except OSError: pass
    except Exception as e:
        log_error("user_registry", "save_registry", f"Unexpected error saving registry", e)
        if os.path.exists(temp_path):
             try: os.remove(temp_path)
             except OSError: pass


def register_user(user_id):
    """Registers a new user with default preferences if not already present."""
    reg = get_registry() # Ensures registry is loaded
    if user_id not in reg:
        log_info("user_registry", "register_user", f"Registering new user {user_id}...")
        # Use deep copy to avoid modifying the original DEFAULT_PREFERENCES
        reg[user_id] = {"preferences": DEFAULT_PREFERENCES.copy()}
        save_registry() # Save after adding the new user
        log_info("user_registry", "register_user", f"Registered new user {user_id} with default preferences.")
    # else: User already exists, do nothing silently


def update_preferences(user_id, new_preferences):
    """Updates preferences for a given user and saves the registry."""
    reg = get_registry()
    if user_id in reg:
        # Ensure the preferences key exists and is a dict
        if not isinstance(reg[user_id].get("preferences"), dict):
             reg[user_id]["preferences"] = DEFAULT_PREFERENCES.copy()

        # Validate keys before updating? Optional, but good practice.
        valid_updates = {k: v for k, v in new_preferences.items() if k in DEFAULT_PREFERENCES}
        invalid_keys = set(new_preferences.keys()) - set(valid_updates.keys())
        if invalid_keys:
            log_warning("user_registry", "update_preferences", f"Ignoring invalid preference keys for user {user_id}: {invalid_keys}")

        if not valid_updates:
             log_warning("user_registry", "update_preferences", f"No valid preference keys provided for update for user {user_id}.")
             return False # Or True if ignoring invalid keys is considered success? Let's say False.

        reg[user_id]["preferences"].update(valid_updates)
        save_registry() # Save after updating
        log_info("user_registry", "update_preferences", f"Updated preferences for user {user_id}: {list(valid_updates.keys())}")
        return True
    else:
        log_error("user_registry", "update_preferences", f"User {user_id} not registered, cannot update preferences.")
        return False

def get_user_preferences(user_id):
    """Gets preferences for a user, returns None if user not found."""
    reg = get_registry()
    user_data = reg.get(user_id)
    if user_data:
        # Ensure preferences key exists and return a copy with all defaults ensured
        prefs = user_data.get("preferences", {})
        if not isinstance(prefs, dict):
             prefs = {} # Reset if not a dict

        # Create a copy of defaults, update with user's saved prefs
        # This ensures all keys exist in the returned dict
        full_prefs = DEFAULT_PREFERENCES.copy()
        full_prefs.update(prefs)
        return full_prefs
    else:
        return None

# Load registry into memory on module import.
load_registry()

# (Keep __main__ block for testing if desired)

# --- END OF FILE users/user_registry.py ---
# --- END OF FILE users\user_registry.py ---


================================================================================
üìÑ tests\mock_sender.py
================================================================================

# --- START OF FILE tests\mock_sender.py ---
# --- START of tests/mock_sender.py ---
import requests
import sys
import os
import json
import time
import threading

# Add project root to sys.path
project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
sys.path.insert(0, project_root)

# Use the project logger, but primarily for errors/warnings in this script
from tools.logger import log_info, log_error, log_warning

DEFAULT_USER = "972547778005" # Example default
BASE_URL = "http://localhost:8000"
INCOMING_URL = f"{BASE_URL}/incoming"
OUTGOING_URL = f"{BASE_URL}/outgoing"
ACK_URL = f"{BASE_URL}/ack"

_stop_polling = threading.Event()

def poll_for_messages(user_id_raw: str):
    # ... (polling function remains the same) ...
    log_info("mock_sender", "poll_thread", f"Polling thread started for user {user_id_raw}.") # Keep start message
    session = requests.Session()
    connection_lost = False # Flag to track connection state

    while not _stop_polling.is_set():
        try:
            res = session.get(OUTGOING_URL, timeout=10)
            res.raise_for_status() # Raise HTTPError for bad responses (4xx or 5xx)

            if connection_lost:
                print("[SYSTEM]: Connection to server restored.")
                log_warning("mock_sender", "poll_thread", "Connection restored.") # Use warning for visibility
                connection_lost = False

            data = res.json()
            messages = data.get("messages", [])

            if messages:
                for msg in messages:
                    print(f"\n[BOT]: {msg.get('message', '[No message content]')}")
                    try:
                        ack_payload = {"message_id": msg.get("message_id"), "user_id": msg.get("user_id")}
                        ack_res = session.post(ACK_URL, json=ack_payload, timeout=5)
                        if ack_res.status_code != 200:
                             log_warning("mock_sender", "poll_thread", f"Failed ACK for {msg.get('message_id')}. Status: {ack_res.status_code}")
                    except Exception as ack_e:
                         log_warning("mock_sender", "poll_thread", f"Error sending ACK for {msg.get('message_id')}: {ack_e}")
                print(f"[YOU]: ", end="", flush=True)

        except requests.exceptions.Timeout:
             if not connection_lost: # Log only the first time timeout happens after connection was okay
                 log_warning("mock_sender", "poll_thread", "Polling request timed out.")
                 print("[SYSTEM]: Polling timed out...")
                 connection_lost = True # Assume connection might be shaky
             time.sleep(2) # Wait longer on timeout
        except requests.exceptions.RequestException as e:
             if not connection_lost: # Only log the first time connection fails
                 # Check for connection refused specifically
                 if "actively refused it" in str(e):
                      error_msg = "Connection error: Target machine actively refused connection. Is the server running?"
                 else:
                      error_msg = f"Connection error: {e}"
                 log_error("mock_sender", "poll_thread", error_msg)
                 print(f"[SYSTEM]: {error_msg}")
                 connection_lost = True
             time.sleep(5) # Wait significantly longer if connection refused/error
        except json.JSONDecodeError as e:
             log_error("mock_sender", "poll_thread", f"Failed to decode JSON response: {e}. Response text: {res.text[:100]}")
             print("[SYSTEM]: Received invalid response from server.")
             time.sleep(2)
        except Exception as e:
             log_error("mock_sender", "poll_thread", f"Unexpected error in polling thread: {e}", e)
             if not connection_lost:
                 print(f"[SYSTEM]: Unexpected polling error: {e}")
                 connection_lost = True
             time.sleep(5)

        if not connection_lost:
            time.sleep(0.5)

    log_info("mock_sender", "poll_thread", "Polling thread stopped.")


# --- UPDATED send_mock_message ---
def send_mock_message(user_id_raw: str, message: str):
    """Sends message, expects only ACK back directly."""
    payload = {"user_id": user_id_raw, "message": message}
    # Increase timeout significantly for LLM-heavy operations
    # Try 60 or 90 seconds
    ack_timeout = 60
    try:
        # Use the increased timeout here
        res = requests.post(INCOMING_URL, json=payload, timeout=ack_timeout)
        res.raise_for_status()
        response_data = res.json()
        if not response_data.get("ack"):
             log_warning("mock_sender", "send_mock_message", f"Server response did not contain expected ack: {response_data}")

    except requests.exceptions.ReadTimeout:
        # Log specifically that the ACK timed out, but the message was likely sent
        log_warning("mock_sender", "send_mock_message", f"ACK response timed out after {ack_timeout}s (message likely sent, server processing).")
        print(f"[SYSTEM]: Server took too long to acknowledge message (>{ack_timeout}s), but it was likely sent. Check for BOT response.")
    except requests.exceptions.RequestException as e:
        log_error("mock_sender", "send_mock_message", f"Failed to send message", e)
        print(f"[SYSTEM]: Error connecting to server to send message: {e}")
    except json.JSONDecodeError:
        log_error("mock_sender", "send_mock_message", f"Received non-JSON ACK response: {res.text}")
        print(f"[SYSTEM ERROR]: Received invalid ACK response from server.")
    except Exception as e:
        log_error("mock_sender", "send_mock_message", f"Unexpected error sending message", e)
        print(f"[SYSTEM]: Unexpected error sending message: {e}")

# --- (main function remains the same) ---
def main():
    default_display_user = DEFAULT_USER
    user_input_raw = input(f"Enter user ID (default: {default_display_user}): ").strip()
    user_id_to_send = user_input_raw if user_input_raw else default_display_user

    print(f"--- Mock Sender for User: {user_id_to_send} ---")
    print("Polling for messages... Type your message. Use :exit to quit.")

    polling_thread = threading.Thread(target=poll_for_messages, args=(user_id_to_send,), daemon=True)
    polling_thread.start()

    while True:
        try:
            msg = input(f"[YOU]: ")
            if msg.strip().lower() == ":exit":
                break
            if msg.strip() == "": continue
            send_mock_message(user_id_to_send, msg)
        except (EOFError, KeyboardInterrupt):
            print("\nCtrl+C or EOF detected.")
            break

    print("\nStopping polling thread...")
    _stop_polling.set()
    polling_thread.join(timeout=2)
    print("Mock chat ended.")

if __name__ == "__main__":
     try: log_info("mock_sender", "main", "Starting mock sender...")
     except NameError: print("FATAL ERROR: Logger not loaded."); sys.exit(1)
     except Exception as e: print(f"FATAL ERROR during logging setup: {e}"); sys.exit(1)
     main()
# --- END of tests/mock_sender.py ---
# --- END OF FILE tests\mock_sender.py ---


