# WhatsTasker Project Code Dump (v0.8 Architecture Target)
# Generated: 2025-04-21 14:04:26

================================================================================
üìÑ README.md
================================================================================

# --- START OF FILE README.md ---

# --- END OF FILE README.md ---


================================================================================
üìÑ WhatsTasker_PRD_08.txt
================================================================================

# --- START OF FILE WhatsTasker_PRD_08.txt ---
ÔªøWhatsTasker: Product Requirements (v0.8 - Orchestrator Focused)
üß≠ Overview
WhatsTasker is a personal productivity assistant designed for individuals seeking to manage their tasks, improve focus, and reduce procrastination through a conversational interface. Accessed via WhatsApp, it functions as an intelligent Time Management Expert, integrating directly with the user's primary calendar (initially Google Calendar) and utilizing an internal metadata store for enhanced tracking. WhatsTasker aims to streamline daily planning, facilitate frictionless task capture, and support helpful routines.
üåü Core System Goal
The system shall act as a proactive and intelligent Time Management Expert. It must understand user context (history, preferences, schedule, tasks), interpret requests accurately, facilitate effective task and schedule management, and anticipate user needs where feasible within the defined scope, moving beyond simple command execution.
üîë Core Functional Requirements
1. Multi-User Support & Identification:
   * Uniquely identify users via their WhatsApp phone number (user_id).
   * Maintain consistent user_id linkage across all system components.
2. Calendar-Based Task & Reminder Management:
   * Direct Calendar Integration: Synchronize confirmed tasks and scheduled events with the user's designated primary calendar (initially Google Calendar).
   * Metadata Augmentation: Maintain an internal metadata store linked to calendar events/internal IDs to track additional details crucial for system operation (e.g., item status [pending, in progress, completed, cancelled], type [task, reminder], estimated effort, project tags, timestamps).
3. Natural Language Interaction (via WhatsApp):
   * Conversational Interface: Enable users to interact using free-form, natural language commands and requests.
   * Contextual Understanding: Maintain and utilize conversational context (recent history, user state) to understand follow-up questions, resolve ambiguities, and provide relevant, coherent responses.
4. User Capabilities & Outcomes:
   * Onboarding: Guide new users through an initial setup process to configure essential preferences (e.g., work parameters, calendar connection authorization) and activate their account.
   * Task/Reminder Capture & Classification:
      * Allow users to efficiently capture intentions (tasks, reminders) via natural language.
      * Intelligently distinguish between simple Reminders (typically requiring only description and time) and effortful Tasks (implying duration/effort and eligibility for scheduling work sessions).
      * Reliably extract key details: description, due date/time, type (task or reminder), and estimated effort (for tasks).
   *    * Task Modification: Enable users to accurately modify details of existing tasks or reminders (e.g., description, due date/time, status, estimated effort) based on their requests.
   * Task & Reminder Status Updates: Allow users to easily update the status of items (e.g., mark as completed, pending), including handling interactive replies to system-generated lists.
   * Task & Reminder Viewing & Filtering: Enable users to view their items (tasks and reminders), with options to filter by relevant criteria such as date range, status (active, completed, etc.), or an associated project tag/label.
   * Clarification Handling: Intelligently request clarification from the user when input is ambiguous, incomplete, or conflicts with existing information, guiding the user towards providing necessary details.
   * Task Scheduling Assistance (for Type: task only):
      * Proactively offer to find and schedule dedicated work time in the calendar for newly created Tasks (unless scheduling was part of the initial request).
      * Upon user request, propose suitable, potentially distributed, time slots for Tasks requiring work sessions, considering task requirements, user preferences, and calendar availability.
      * Allow users to easily confirm and book proposed work session slots into their calendar.
5. Automated Routines & Summaries:
   * Morning Check-in: Provide a concise overview of the user's upcoming day, including scheduled calendar events, due tasks, and reminders. Goal: Enhance daily awareness and readiness.
   * Evening Review: Facilitate a quick end-of-day review:
      * Present Tasks and Reminders scheduled for or due that day.
      * Allow users to easily update the completion status of these items.
      * For Tasks identified as incomplete, prompt the user (optionally asking for a reason) and proactively offer to reschedule the task or its remaining work, potentially suggesting new time slots. Goal: Prevent tasks from being dropped and adapt planning.
   * 6. Fallback Operation:
   * Support basic task and reminder management (capture, status updates, viewing lists) even without calendar integration. Smart scheduling proposals, calendar-based summaries, and rescheduling offers will be unavailable in this mode.
üí° Future Capabilities (Beyond Initial Scope)
* Advanced Scheduling Intelligence (Optimal times, breaks, conflict handling)
* Proactive Time Management Advice & Goal Tracking
* Weekly Reflection Module (Structured prompts, insight storage)
* Recurring Tasks/Reminders Support
* Advanced Daily Planning Assistance ("Just One Thing", backlog suggestions)
* Timed Pre-Event Notifications (Requires external scheduler)
* Shared/Team Task Collaboration Features
* Expanded Calendar Integrations (Outlook, etc.)
* User Subscription Tiers & Monetization
‚ö†Ô∏è MVP Scope & Limitations (Initial Release)
* Calendar: Integration only with Google Calendar; one active calendar per user.
* Task Types: No built-in support for recurring tasks/reminders.
* Collaboration: No shared tasks or team features.
* Proactivity: Limited to offering scheduling for new tasks and rescheduling incomplete tasks in the evening review. No broader unsolicited advice.
* Notifications: Timed pre-event notifications are out of scope.
* Reflection: Weekly reflection module is out of scope.
* Monetization: No user plans or payments.
‚úÖ Acceptance Criteria (High-Level)
* Users can successfully onboard, configure preferences, and connect their Google Calendar.
* The system correctly distinguishes between Tasks and Reminders based on user input.
* Users can reliably create, view (with date/status/project filters), update, and mark tasks/reminders as complete via natural language.
* The system proactively offers scheduling for newly created Tasks.
* The system can propose schedule slots for Tasks based on calendar availability and preferences, and book confirmed slots.
* The system provides functional Morning Check-in and Evening Review summaries.
* The Evening Review correctly identifies incomplete Tasks and offers rescheduling options.
* The system handles clarifications when user input is ambiguous.
* Basic reminder functionality works without calendar integration.
# --- END OF FILE WhatsTasker_PRD_08.txt ---


================================================================================
üìÑ WhatsTasker_SRS_08.txt
================================================================================

# --- START OF FILE WhatsTasker_SRS_08.txt ---
Ôªø# --- START OF FILE WhatsTasker_SRS_08.txt ---
ÔªøWhatsTasker: SRS (v0.8h - Pure LLM Flow & Onboarding)

**1. Goals & Principles**

*   **Primary Goal:** Create a WhatsApp-based personal productivity assistant that acts as an intelligent Time Management Expert. It should understand user context, facilitate task/schedule management via natural language, and integrate seamlessly with the user's primary calendar (Google Calendar initially).
*   **Core Architecture:**
    *   An **OrchestratorAgent** (for active users) and an **OnboardingAgent** (for new/onboarding users) serve as the central reasoning hubs and primary user interfaces, processing user input within its full context. Routing is determined by user status (`new`, `onboarding`, `active`).
    *   Agents leverage Structured Tool Use (via Instructor/Pydantic and OpenAI Function Calling/Tool Use) to delegate specific actions to well-defined Tools.
    *   **Pure LLM Control Flow:** Agents rely **entirely on the LLM** (guided by specific system prompts) to manage conversational state, ask clarifying questions, interpret user replies, decide which tool to call (if any), and formulate responses based on history and tool results. Python code primarily executes validated tool calls requested by the LLM.
    *   **Two-Step LLM Interaction (with Tools):**
        1.  Agent sends context + history + user message + tool definitions to LLM.
        2.  LLM responds either with direct text OR a tool call request.
        3.  If a tool call is requested, the Agent executes the tool via the Tool Layer/Service Layer.
        4.  Agent sends the original context + history + LLM's tool call request + the tool's execution result back to the LLM.
        5.  LLM analyzes the tool result and generates the final text response for the user.
    *   A **Service Layer** (TaskManager, TaskQueryService, ConfigManager, Cheats) encapsulates business logic and data manipulation using plain Python, interacting with persistent stores and external APIs based on Tool invocations or direct cheat code calls.
*   **Modularity & Reusability:** Components (Agents, Tools, Services, Data Stores) are clearly separated. Services contain reusable business logic. Tools provide reliable interfaces for specific actions callable by the LLM.
*   **Reliability & Maintainability:** Prioritize reliable execution via structured Tool Use and Pydantic validation. Code includes type hinting and clear documentation. Conversational logic resides primarily within the LLM prompts.
*   **LLM Interaction:** Utilize OpenAI's Tool Use capabilities, facilitated by Instructor/Pydantic, for Agents to reliably select and invoke Tools with validated parameters. The `propose_task_slots` tool uses a focused LLM sub-call internally for scheduling logic. Input interpretation (e.g., flexible time formats) relies on LLM interpretation guided by prompt instructions before tool calls.
*   **Data Handling & State:**
    *   Persistence: Metadata Store (CSV/DB), User Registry (JSON/DB), and Google Calendar are sources of truth. Encrypted Token Store for credentials. MVP accepts CSV/JSON.
    *   User Status: User state is tracked via a `status` field in preferences (`new`, `onboarding`, `active`), managed by `user_registry.py` and `config_manager.py`. This status dictates routing in `request_router.py`.
    *   Runtime State: A central, thread-safe AgentStateManager manages the in-memory state for each active user (preferences, history, task context snapshot, API clients). State loaded/created on demand via `user_manager.py`.
    *   Context Provision: Relevant context (history, prefs, task/calendar snapshot) loaded via AgentStateManager and provided to the appropriate Agent (Onboarding or Orchestrator) on each interaction.
    *   State Updates: Services invoked by Tools update persistent stores and signal necessary updates to in-memory state via AgentStateManager.
    *   Synchronization: (Deferred) SyncService needed for long-term reconciliation. MVP relies on action-driven state updates.
*   **Error Handling:**
    *   Tools/Services handle expected errors and return clear status/messages via their result dictionaries.
    *   Pydantic validation handles malformed parameters before tool execution.
    *   Agents interpret tool failures (passed back via the second LLM call) and rely on the LLM (guided by prompts) to communicate appropriately with the user.

**2. Architecture Overview**

1.  **Interface Layer:** User message arrives (CLI/WhatsApp) -> `bridge.cli_interface` (FastAPI).
2.  **Routing Layer (`request_router.py`):**
    *   Receives message, normalizes `user_id`.
    *   Ensures user state exists via `user_manager.get_agent` (creates default state with `status="new"` if first time).
    *   Retrieves current `agent_state` (including `preferences` with `status`).
    *   **Status Check & Routing:**
        *   If `status == "new"`: Sends welcome message, calls `config_manager.set_user_status` to change status to `"onboarding"`, returns welcome message.
        *   If message starts with `/` and `CHEATS_IMPORTED`: Calls `services.cheats.handle_cheat_command`, sends result, returns.
        *   If `status == "onboarding"` and `ONBOARDING_AGENT_IMPORTED`: Adds user message to history, loads basic context (prefs, history), invokes `agents.onboarding_agent.handle_onboarding_request`.
        *   If `status == "active"` and `ORCHESTRATOR_IMPORTED`: Adds user message to history, loads full context (prefs, history, tasks, calendar) via `task_query_service.get_context_snapshot`, invokes `agents.orchestrator_agent.handle_user_request`.
        *   Otherwise: Handles error/unknown status.
3.  **Agent Layer (`onboarding_agent.py` or `orchestrator_agent.py`):**
    *   Receives user message and relevant context snapshot.
    *   Loads appropriate system prompt (`onboarding_agent_system_prompt` or `orchestrator_agent_system_prompt`).
    *   Defines the specific set of Tools available for its context (onboarding tools vs. full toolset).
    *   **LLM Call 1 (Planner):** Interacts with LLM via Instructor, providing context, history, user message, and available tool definitions.
    *   LLM decides next step: Respond directly or call a tool.
4.  **Execution Layer:**
    *   **If LLM Responds Directly:** Agent receives text.
    *   **If LLM Calls a Tool:**
        *   Agent receives structured tool call request (Pydantic object).
        *   Agent validates parameters (redundant check, Pydantic/Instructor handle primary).
        *   Agent calls the corresponding Tool function in `agents.tool_definitions.py`.
        *   Tool function interacts with **Service Layer** (`TaskManager`, `ConfigManager`, `TaskQueryService`).
        *   Services perform logic, interact with **Data Layer** (`MetadataStore`, `UserRegistry`, `TokenStore`) and external APIs (`GoogleCalendarAPI`).
        *   Services update persistent data and signal updates to **State Layer** (`AgentStateManager`).
        *   Tool function returns result dictionary (`{"success": bool, "message": str, ...}`) back to the Agent.
5.  **Agent Layer (Response Generation):**
    *   **If LLM Responded Directly (Step 3):** Agent uses that text as the final response.
    *   **If Tool Was Called (Step 4):**
        *   Agent prepares messages for **LLM Call 2 (Responder)**, including the original messages plus the assistant's tool call request and the `role: "tool"` message containing the tool's result (JSON string).
        *   Agent sends these messages to the LLM.
        *   LLM analyzes the tool result and history, then generates the final text response based on its system prompt instructions.
        *   Agent uses the LLM's generated text as the final response.
6.  **Response Flow:** Agent returns the final response string -> Router calls `send_message` (which adds agent response to history and sends via Bridge) -> User.

*Diagram:* (The existing diagram is still largely representative at a high level, showing the main components. The key change is the conditional routing in the Router and the two-step LLM call pattern within the Agents when tools are used.)

**3. Module & Function Breakdown**

*   **Core Infrastructure:** (Largely Unchanged - main.py, cli_interface.py, logger.py, encryption.py, token_store.py, metadata_store.py, google_calendar_api.py, calendar_tool.py, agent_state_manager.py, user_manager.py, user_registry.py)
    *   `bridge/request_router.py`: **Updated** - Handles status checking (`new`, `onboarding`, `active`), welcome message for new users, routing to appropriate agent (onboarding or orchestrator), cheat code detection and routing to `services/cheats.py`.
*   **Service Layer:**
    *   `services/task_manager.py`: **Updated** - Core task/reminder logic. Contains `create_task`, `update_task` (details only), `update_task_status` (excludes cancel), `cancel_item` (handles GCal cleanup + sets status), `schedule_work_sessions`, `cancel_sessions`. Accepts structured input from tools.
    *   `services/config_manager.py`: **Updated** - User configuration logic. Includes `set_user_status`.
    *   `services/task_query_service.py`: **Updated** - Data retrieval. `get_formatted_list` accepts `project_filter`. `get_context_snapshot` provides context for Orchestrator.
    *   `services/cheats.py`: **NEW** - Contains logic for handling cheat code commands (`/help`, `/list`, `/memory`, `/clear`, placeholders for `/morning`, `/evening`).
    *   `services/sync_service.py`: (Deferred)
    *   `services/llm_interface.py`: Initializes Instructor-patched OpenAI client.
*   **Agent Layer:**
    *   `agents/orchestrator_agent.py`: **Updated** - Central reasoning hub for **active** users. Implements pure LLM control flow with two-step LLM calls for tool execution. Uses full toolset. Relies on prompt for conversational logic.
    *   `agents/onboarding_agent.py`: **NEW** - Central reasoning hub for **onboarding** users. Implements pure LLM control flow. Uses limited toolset (`update_user_preferences`, `initiate_calendar_connection`). Relies on specific onboarding prompt to guide preference collection and status update via `update_user_preferences`.
    *   `agents/tool_definitions.py`: **Updated** - Defines Pydantic models and Python functions for the **current, refined toolset** (v0.8f): `create_item`, `update_item_details`, `update_item_status`, `propose_task_slots`, `book_task_slots`, `cancel_task_sessions`, `update_user_preferences`, `initiate_calendar_connection`, `get_formatted_task_list`, `interpret_list_reply`. Includes helpers for scheduler parsing/API access.
    *   **(Obsolete Files):** `agents/intention_agent.py`, `agents/task_agent.py`, `agents/config_agent.py`, `agents/scheduler_agent.py`, `agents/scheduling_logic.py`, `agents/list_reply_logic.py`.
*   **LLM Interaction Layer:** (Primarily `llm_interface.py` and Instructor library)
    *   **(Obsolete Files):** `langchain_chains/chain_builders.py`.

**4. Configuration Files**

*   `config/prompts.yaml`:
    *   **Updated:** Contains `orchestrator_agent_system_prompt` (detailed instructions for pure LLM flow, tool usage, response generation based on tool results, multi-step scheduling flow).
    *   **Updated:** Contains `onboarding_agent_system_prompt` (specific instructions for guiding preference collection, using limited tools, interpreting inputs, calling `update_user_preferences` to set status `active`).
    *   **Updated:** Contains `session_scheduler_system_prompt` (instructions for the sub-LLM called by `propose_task_slots_tool`).
*   `config/messages.yaml`: **Updated** - Includes refined `welcome_confirmation_message` and other standard messages.
*   `config/settings.yaml`: (If used) General settings.
*   `.env`: Secrets.

**5. Key Considerations / Future Work**

*   **Prompt Engineering:** Success heavily relies on the clarity, detail, and robustness of the prompts (especially `orchestrator_agent_system_prompt` and `onboarding_agent_system_prompt`) to guide the LLM's reasoning, interpretation, tool use, and conversational state management. Continuous refinement will be needed.
*   **LLM Reliability for State:** The pure LLM flow depends on the LLM maintaining context and correctly interpreting history for multi-turn interactions (like scheduling confirmations). This might be less reliable than explicit state management for very complex flows.
*   **Error Handling & Presentation:** The LLM needs to effectively interpret `success: false` results from tools and present user-friendly error messages based on the provided `message` field, guided by the prompt.
*   **Input Flexibility:** Relies on LLM's ability (guided by prompt examples/instructions) to parse flexible user inputs (like time formats) before calling tools. May require prompt adjustments or fallback to clarification if LLM fails interpretation.
*   **Synchronization (`SyncService`):** Still deferred but crucial for long-term data consistency.
*   **Testing:** Requires testing the agents' ability to follow prompts, select/invoke tools correctly, handle tool results, and manage conversation flow, including edge cases and clarifications. Mocking LLM responses is essential.
*   **(Future):** Implement remaining features (Summaries, Reflection, Recurring Tasks, etc.) likely as new Tools callable by the Orchestrator, potentially requiring prompt updates.

# --- END OF FILE WhatsTasker_SRS_08.txt ---
# --- END OF FILE WhatsTasker_SRS_08.txt ---


================================================================================
üìÑ requirements.txt
================================================================================

# --- START OF FILE requirements.txt ---
# Web Framework & Server 
fastapi>=0.110.0,<0.112.0 
uvicorn[standard]>=0.29.0,<0.30.0 
 
# Langchain Core & OpenAI Integration 
langchain>=0.1.16,<0.2.0 
langchain-core>=0.1.40,<0.2.0 
langchain-openai>=0.1.3,<0.2.0 
 
# Google API Libraries 
google-api-python-client>=2.120.0,<3.0.0 
google-auth-oauthlib>=1.2.0,<2.0.0 
google-auth>=2.29.0,<3.0.0 
 
# Configuration & Environment 
python-dotenv>=1.0.1,<2.0.0 
PyYAML>=6.0.1,<7.0.0 
 
# Utilities 
requests>=2.31.0,<3.0.0 
pytz>=2024.1 
cryptography>=42.0.0,<43.0.0 
PyJWT>=2.8.0,<3.0.0 
 
# Pydantic (Core dependency for FastAPI & Langchain) 
pydantic>=2.7.0,<3.0.0 
 
# Optional: If using pandas checks (e.g., pd.isna) - uncomment if needed 
pandas>=2.0.0,<3.0.0 

# --- END OF FILE requirements.txt ---


================================================================================
üìÑ config\prompts.yaml
================================================================================

# --- START OF FILE config\prompts.yaml ---
# --- START OF FILE config/prompts.yaml ---

# =====================================================
# Prompt(s) for OrchestratorAgent (v0.8f Toolset - Scheduling Implemented)
# =====================================================
orchestrator_agent_system_prompt: |
  You are WhatsTasker, an expert Time Management Assistant communicating via WhatsApp. Your goal is to help the user manage their tasks, reminders, and schedule effectively and conversationally by understanding their requests and utilizing the appropriate tools. **You control the entire conversation flow.**

  **Core Principles:**
  - Be helpful, concise, and friendly.
  - **Use the provided Conversation History extensively** to understand the context of the user's latest message, especially for follow-up questions or confirmations.
  - Differentiate between simple 'reminders' (alerts) and effortful 'tasks' (work items, possibly with sessions).
  - **If essential information is missing** (like date, duration for a task, etc.), **ask a clear clarifying question** before attempting to call a tool.
  - **Decide which tool to call** based on the user's confirmed intent and the available information. Call tools only when you have all required parameters in the correct format.
  - **After a tool executes**, you will receive its result in a 'tool' role message (as a JSON string). **Analyze this result** (check 'success' status, 'message', and any data fields like 'item_id', 'proposed_slots', 'list_body').
  - **Formulate the final response to the user based on the tool result.** Confirm success, relay failure messages, present data (like proposed slots or task lists), or ask the *next* logical question in a multi-step flow (like scheduling).
  - **NEVER perform complex formatting or conversational logic yourself in Python; rely on the LLM (that's you!) for all interaction decisions and message generation.**

  **Context Provided:**
  - Current Reference Time ({timezone}): Date: YYYY-MM-DD, Day: Weekday, Time: HH:MM. Use this to resolve relative dates/times.
  - Current User Preferences (JSON Object): User settings (work hours, session length, etc.).
  - Recent Conversation History: Use this to understand sequences and follow-ups.
  - Relevant Active Items (List of JSON Objects): Snapshot of tasks/reminders (use 'item_id').
  - Upcoming Calendar Events (List of JSON Objects): Snapshot of calendar entries.

  **Multi-Step Flow Examples (Your Responsibility to Manage):**

  *   **Task Creation & Scheduling Flow:**
      1.  User asks to create a task (e.g., "make presentation").
      2.  You realize duration is missing. Ask: "Okay, I can create that task. How long do you estimate it will take?"
      3.  User replies (e.g., "3 hours").
      4.  You now have description, type='task', estimated_duration. You still need a date. Ask: "Got it, 3 hours. What date is this task for?"
      5.  User replies (e.g., "next wednesday").
      6.  You resolve the date. Now you have all info. Call `create_item` tool with description, date, type='task', estimated_duration='3h'.
      7.  Tool runs. You get a 'tool' message back like `{"role": "tool", "content": "{\"success\": true, \"item_id\": \"local_123\", \"item_type\": \"task\", \"estimated_duration\": \"3h\", \"message\": \"Task 'make presentation...' created successfully.\"}"}`.
      8.  Analyze the tool result: success=true, type=task, duration=3h. Calculate num_sessions (e.g., 3 sessions if pref=1h). Since num_sessions > 1, ask the user: "Task 'make presentation...' created successfully. This task is estimated at 3h. Does this need to be one continuous block, or can I schedule it as 3 separate 1h sessions? (Respond 'continuous' or 'separate')"
      9.  User replies (e.g., "separate").
      10. From history, you know the task_id is "local_123". Call `propose_task_slots` with `task_id="local_123"`. Let the tool calculate the number of slots based on estimate/prefs by omitting `number_of_slots`. (Optional: If user had specified "next week" earlier, you *could* pass search dates here, but maybe start simple).
      11. Tool runs. You get a 'tool' message back with `proposed_slots` list and a `message`.
      12. Analyze the tool result. If `proposed_slots` is not empty: Format the slots clearly for the user: "Okay, I found 3 potential slots: \n- Slot 1: 2025-04-23 from 10:00 to 11:00\n- Slot 2: ... \n Shall I book these slots? (yes/no/cancel)". If empty, relay the failure message from the tool result.
      13. User replies (e.g., "yes").
      14. From history, identify the relevant `task_id` and the `proposed_slots` from the previous turn. Call `book_task_slots` with `task_id="local_123"` and the `slots_to_book` list.
      15. Tool runs. You get a 'tool' message back. Formulate final confirmation to user based on the tool's success/message.

  *   **Listing Tasks:**
      1.  User asks "show my tasks for tomorrow".
      2.  You resolve date, call `get_formatted_task_list` with `date_range=["YYYY-MM-DD", "YYYY-MM-DD"]`, `status_filter="active"`.
      3.  Tool runs. You get a 'tool' message back with `list_body`, `list_mapping`, `count`, `message`.
      4.  Analyze result. If `count > 0`, respond to user: "{message_from_tool}\n{list_body_from_tool}". If `count == 0`, respond with the `message_from_tool` (e.g., "No items found...").

  **Tool Usage Instructions:**
  - Decide the *single next step*: Respond directly, ask clarification, or call ONE tool.
  - If calling a tool, provide *all required* parameters in the correct format. Check descriptions below.
  - After a tool runs, formulate your response to the user based on the JSON result provided in the `tool` message.

  **Available Tools & Required Parameter Structures:**
  [Keep the exact tool definitions from the previous prompt - Ensure descriptions are clear for the LLM]

  1.  **`create_item`**: Creates task/reminder metadata. Also creates GCal event for Reminders with time.
      *   `description` (string, required), `date` (string, required, YYYY-MM-DD), `type` (string, required, "task" | "reminder"), `time` (string, optional, HH:MM), `estimated_duration` (string, optional), `project` (string, optional)
  2.  **`update_item_details`**: Modifies core details ONLY (desc, date, time, estimate, project). Not status.
      *   `item_id` (string, required), `updates` (object, required, non-empty, allowed keys: description, date, time, estimated_duration, project)
  3.  **`update_item_status`**: Changes status OR cancels/deletes item.
      *   `item_id` (string, required), `new_status` (string, required, "pending"|"in_progress"|"completed"|"cancelled")
  4.  **`update_user_preferences`**: Changes user settings.
      *   `updates` (object, required, non-empty)
  5.  **`initiate_calendar_connection`**: Starts GCal OAuth flow. No parameters needed.
  6.  **`propose_task_slots`**: Finds available work session slots for a Task.
      *   `task_id` (string, required), `number_of_slots` (integer, optional, >0), `search_start_date` (string, optional, YYYY-MM-DD), `search_end_date` (string, optional, YYYY-MM-DD)
  7.  **`book_task_slots`**: Books confirmed GCal work sessions.
      *   `task_id` (string, required), `slots_to_book` (list of objects, required, non-empty)
  8.  **`cancel_task_sessions`**: Deletes specific GCal work sessions.
      *   `task_id` (string, required), `session_ids_to_cancel` (list of strings, required, non-empty)
  9.  **`interpret_list_reply`**: (Placeholder) Parses replies to numbered lists.
      *   `user_reply` (string, required), `list_mapping` (object, required)
  10. **`get_formatted_task_list`**: Gets a filtered/formatted list.
      *   `date_range` (list of strings, optional, [YYYY-MM-DD, YYYY-MM-DD]), `status_filter` (string, optional, "active"|"pending"|"in_progress"|"completed"|"all"), `project_filter` (string, optional)

  **Your Task Summary:**
  Read history/context -> Understand user message -> Decide next step (clarify, call tool, respond directly) -> If calling tool: ensure params valid -> If tool called: wait for result -> Analyze tool result -> Formulate final response/next step based *only* on history and tool results.

# =====================================================
# Prompt(s) for ConfigAgent (Keep as is for now)
# =====================================================
config_agent_system_prompt: |
  You are the Configuration Agent for WhatsTasker...
  # ... (rest of config agent prompt remains unchanged) ...

config_agent_human_prompt: |
  **Current Situation Analysis:**
  # ... (rest of config agent prompt remains unchanged) ...

# =====================================================
# OBSOLETE PROMPTS
# =====================================================
# task_agent_system_prompt: | ...
# task_agent_human_prompt: | ...
# list_reply_agent_system_prompt: | ...
# list_reply_agent_human_prompt: | ...

# =====================================================
# Prompt(s) for Session Scheduling LLM (Used BY propose_task_slots TOOL)
# =====================================================
session_scheduler_system_prompt: |
  You are an expert Scheduler assistant used by the propose_task_slots tool for WhatsTasker.
  Your goal is to propose a schedule of work sessions for a specific task, distributing them reasonably over the available time, based on user preferences, task details, and existing calendar events.

  **Core Task:** Given the task details, user preferences, existing calendar events, the number of slots requested (`num_slots_requested`), and the desired duration for *each* slot (`user_session_length` - which might be the user's preference OR the task's total duration if only 1 slot is requested), generate a list of proposed work session slots.

  **Input Variables Provided:**
  - Task Description: {task_description}
  - Task Due Date: {task_due_date} (YYYY-MM-DD) - Use for context and buffer.
  - Task Estimated Duration: {task_estimated_duration} (e.g., "3h", "90m") - Total work estimate.
  - User Working Days: {user_working_days} (List of strings)
  - User Work Start Time: {user_work_start_time} (HH:MM)
  - User Work End Time: {user_work_end_time} (HH:MM)
  - User Session Length: {user_session_length} (String, e.g., "60m", "3h") - **This is the DURATION of EACH slot you need to find.** It might be the user's preferred length OR the total task duration if num_slots_requested=1.
  - Existing Calendar Events (JSON list): {existing_events_json} (Format: [{"start_datetime": "...", "end_datetime": "...", "summary": "..."}, ...])
  - Current Date: {current_date} (YYYY-MM-DD)
  - Number of Slots to Propose: {num_slots_requested} (Integer) - Find exactly this many slots.

  **Processing Logic:**
  1.  **Calculate Slot Duration:** Determine the required slot duration in minutes from the provided `user_session_length` input variable.
  2.  **Identify Available Time Slots:**
      - Consider dates from the day after `current_date` up to **1-2 days BEFORE** `task_due_date`.
      - Filter these dates based on `user_working_days`.
      - Within each valid date, consider the time window between `user_work_start_time` and `user_work_end_time`.
      - Check the `existing_events_json` for conflicts. An existing event conflicts if its time range overlaps *at all* with a potential session slot of the **required duration** calculated in step 1.
      - Find time slots within the working hours that are free and match the **required duration**.
  3.  **Select & Distribute Sessions:** From the available slots, select exactly `num_slots_requested` sessions. **Attempt to distribute these sessions reasonably** across the available period (between tomorrow and the buffer before the due date). Don't bunch them all up unless necessary. Prioritize earlier dates if distribution allows. Ensure sufficient buffer before the due date.
  4.  **Calculate End Times:** For each selected session start time (`date`, `time`), calculate the corresponding `end_time` by adding the **required duration** (from step 1). Ensure the `end_time` does not exceed `user_work_end_time`.
  5.  **Format Output:** Create the JSON output with unique `slot_ref` (starting from 1) for each proposal. Ensure the response is ONLY the JSON object.

  **Output Format Requirements:**
  Respond ONLY with a single, valid JSON object containing exactly two keys:
  1.  `"proposed_sessions"`: A JSON list of proposed sessions. Each element MUST be a dictionary with "slot_ref" (integer, starting from 1), "date" (YYYY-MM-DD), "time" (HH:MM - start time), and "end_time" (HH:MM) keys. Return an empty list `[]` if no suitable slots found.
  2.  `"response_message"`: A user-facing message summarizing the proposal or explaining failure.
      - Success Example: "Okay, I found {N} potential slots for '{Task Description}'."
      - Failure Example: "I looked at your calendar but couldn't find {N} free slots for '{Task Description}' (each needing {duration} duration) before the due date based on your preferences. You might need to adjust the task details or your work settings."

session_scheduler_human_prompt: |
  **Task Details:**
  - Description: {task_description}
  - Due Date: {task_due_date}
  - Estimated Duration: {task_estimated_duration}

  **User Preferences & Slot Request:**
  - Working Days: {user_working_days}
  - Work Start Time: {user_work_start_time}
  - Work End Time: {user_work_end_time}
  - **Duration of EACH slot to find**: {user_session_length}
  - **Number of Slots to Find**: {num_slots_requested}

  **Calendar Context:**
  - Today's Date: {current_date}
  - Existing Events (JSON): {existing_events_json}

  **Your Task:** Propose exactly {num_slots_requested} schedule slots, each of duration {user_session_length}. Distribute slots reasonably, leaving buffer before due date. Respond ONLY in the specified JSON format (`proposed_sessions` with slot_ref, date, time, end_time; `response_message`). Ensure JSON validity.

# =====================================================
# Prompt(s) for OnboardingAgent
# =====================================================
onboarding_agent_system_prompt: |
  You are the Onboarding Assistant for WhatsTasker. Your goal is to guide a new user through the initial setup process conversationally by collecting essential preferences and ensuring data is in the correct format BEFORE calling any tools.

  **Core Task & Rules:**
  1. Examine the `Current User Preferences` provided in the context.
  2. Identify the *first* essential preference that is missing (`null`). The required preferences and their **STRICT required formats** are:
     *   `TimeZone`: Must be a valid Olson Timezone Name (e.g., `America/New_York`, `Europe/London`, `Asia/Jerusalem`).
     *   `Work_Start_Time`: Must be in **HH:MM format (24-hour clock)** (e.g., `09:00`, `17:30`).
     *   `Work_End_Time`: Must be in **HH:MM format (24-hour clock)** (e.g., `18:00`).
     *   `Preferred_Session_Length`: Must be a string indicating duration (e.g., `60m`, `90m`, `1.5h`, `2h`).
  3. Ask the user a clear, friendly question for **only that specific missing preference**. Clearly state the required format in your question.
  4. **Interpret the user's reply.** Try to understand common variations:
     *   For **Time**: Convert inputs like `6pm` to `18:00`, `9am` to `09:00`, `noon` to `12:00`, `midnight` to `00:00`. Handle `0900` as `09:00`, `1730` as `17:30`.
     *   For **TimeZone**: If the user gives a city like `london` or `new york` or `tel aviv`, try to infer the correct Olson name (`Europe/London`, `America/New_York`, `Asia/Jerusalem`). If they give an abbreviation like `EST`, try to map it to a common Olson name (but be cautious, ask if unsure, e.g., "Do you mean America/New_York for EST?").
     *   For **Duration**: Convert `1 hour` to `60m`, `1.5 hours` to `90m`, `2 hours` to `2h`.
  5. **If you can confidently convert the user's reply to the EXACT required format:** Call the `update_user_preferences` tool. The parameters MUST be `{{"updates": {{KEY: "FORMATTED_VALUE"}}}}`. Example: `{{"updates": {{"Work_End_Time": "18:00"}}}}`.
  6. **If you CANNOT confidently interpret or convert the user's reply to the required format:** DO NOT call the tool. Instead, **ask the user for clarification**, reminding them of the specific format needed. Example: "Sorry, I need the time in HH:MM format like 09:00 or 17:30. Could you please provide your work start time again?"
  7. After the tool runs successfully (you'll see `success: true` in the tool result), repeat from step 1: check the preferences for the *next* missing item and ask for it.
  8. Once all four required preferences (`TimeZone`, `Work_Start_Time`, `Work_End_Time`, `Preferred_Session_Length`) have been collected and successfully saved via the tool:
     - Ask the user if they want to connect Google Calendar: "I've got your basic preferences setup. Would you like to connect your Google Calendar now...? (yes/no)"
     - If 'yes': Call `initiate_calendar_connection`. Relay the message/URL from the tool result. End interaction for now.
     - If 'no': Acknowledge their choice. Proceed to the final step.
  9. **Final Step (After prefs collected AND calendar handled):** Call `update_user_preferences` with `{{"updates": {{"status": "active"}}}}`.
  10. After the final status update succeeds, respond with a concluding message like: "Great, setup is complete! ..."

  **Important:** Be precise. Ask for one thing. Interpret carefully. Format correctly *before* calling the tool. Ask again if unsure or if the format is wrong.

  **Context Provided:**
  - Current User Preferences (JSON Object): Check for `null` values for required keys.
  - Conversation History: See what you last asked for and how the user replied.

  **Tools Available During Onboarding:**
  - `update_user_preferences`: `{{"updates": {{KEY: "VALUE"}}}}` (VALUE must be correctly formatted by you).
  - `initiate_calendar_connection`: No parameters.

onboarding_agent_human_prompt: |
  Current Preferences:
  ```json
  {current_preferences_json}
    History:
    {conversation_history}
    User message: {message}
    Your Task: Based on the system instructions, determine the next step: Ask for the next missing REQUIRED preference (TimeZone, Work_Start_Time, Work_End_Time, Preferred_Session_Length), ask about calendar connection, call a tool (update_user_preferences or initiate_calendar_connection), or finalize onboarding (by calling update_user_preferences with status: active). Formulate your response or tool call.
# --- END OF FILE config/prompts.yaml ---
# --- END OF FILE config\prompts.yaml ---


================================================================================
üìÑ config\messages.yaml
================================================================================

# --- START OF FILE config\messages.yaml ---
# config/messages.yaml

# --- Add these ---
welcome_confirmation_message: |
  Hello! üëã Ready to take control of your time and focus? Welcome to WhatsTasker!
  Imagine having a personal Time Management Expert right here in WhatsApp, helping you:
  üß† **Capture Everything Instantly:** Just text me your tasks and reminders like you normally would ‚Äì "remind me about the team sync at 10", "add task finish the report by Friday, takes about 3 hours". No more switching apps!
  üìÖ **Stay Aligned with Your Calendar:** I connect directly to your Google Calendar, ensuring your tasks and scheduled work time fit seamlessly into your day.
  ‚è±Ô∏è **Beat Procrastination:** Need focused time for important tasks? I can find empty slots in your calendar and block out dedicated work sessions for you.
  ‚òÄÔ∏è **Start Strong, End Clear:** Get ahead with helpful morning check-ins and wrap up your day effectively with quick evening reviews to ensure nothing falls through the cracks (coming soon!).
  Think of me as your partner in productivity, helping you plan better, focus deeply, and get more done with less stress.
  Ready to unlock WhatsTasker? 

setup_starting_message: "Great! Let's set things up. I'll ask a few questions to configure your preferences."
setup_declined_message: "Okay, no problem. Just message me again whenever you're ready to set things up!"
ask_confirmation_again_message: "Sorry, I didn't quite understand that. Are you ready to start the setup process? (yes/no)"
user_registered_already_message: "Welcome back! How can I help you today?" # Optional: For returning users found by get_agent

# --- Keep existing messages ---
generic_error_message: "Sorry, an unexpected error occurred. Please try again."
intent_parse_error_message: "Sorry, I had trouble understanding the structure of that request."
intent_unknown_message: "Sorry, I'm not sure how to help with that. You can ask me to add tasks, list tasks, or change settings."
intent_clarify_message: "Sorry, I didn't quite understand that. Could you please rephrase?"
# ... add any other messages you have ...
# --- END OF FILE config\messages.yaml ---


================================================================================
üìÑ config\settings.yaml
================================================================================

# --- START OF FILE config\settings.yaml ---

# --- END OF FILE config\settings.yaml ---


================================================================================
üìÑ main.py
================================================================================

# --- START OF FILE main.py ---
# --- START OF FILE main.py ---

import os
import sys
import asyncio # Import asyncio
from dotenv import load_dotenv
load_dotenv()

from tools.logger import log_info, log_error
import uvicorn
from users.user_manager import init_all_agents
import traceback

def main():
    log_info("main", "main", "Initializing agent states...")
    try:
        init_all_agents()
        log_info("main", "main", "Agent state initialization complete.")
    except Exception as init_e:
        log_error("main", "main", "CRITICAL error during init_all_agents.", init_e)
        # Decide if you want to exit if init fails critically
        # sys.exit(1)

    reload_enabled = os.getenv("APP_ENV", "production").lower() == "development"
    log_level = "debug" if reload_enabled else "info"

    log_info("main", "main", f"Starting FastAPI server (Reload: {reload_enabled}, Log Level: {log_level})...")

    config = uvicorn.Config(
        "bridge.cli_interface:app",
        host="0.0.0.0",
        port=8000,
        reload=reload_enabled,
        access_log=False,
        log_level=log_level
    )
    server = uvicorn.Server(config)

    try:
        # Use server.run() which allows catching KeyboardInterrupt more directly
        # NOTE: This might behave slightly differently regarding auto-reload handling
        # compared to uvicorn.run(). Test carefully if using reload.
        # For production (reload=False), this is fine.
        asyncio.run(server.serve())
    except KeyboardInterrupt:
        log_info("main", "main", "KeyboardInterrupt received. Shutting down gracefully...")
        # Uvicorn's signal handlers should already be taking care of shutdown.
        # We just catch the final interrupt here to prevent its traceback.
    except asyncio.CancelledError:
        # This might still happen depending on timing, but often the KeyboardInterrupt is caught first.
        log_info("main", "main", "Asyncio operation cancelled during shutdown.")
    except ImportError as e:
         log_error("main", "main", f"Could not import FastAPI app.", e)
         log_error("main", "main", f"ImportError Traceback:\n{traceback.format_exc()}")
         sys.exit(1)
    except Exception as e:
         log_error("main", "main", f"Failed to start or run FastAPI server.", e)
         log_error("main", "main", f"Startup/Runtime Exception Traceback:\n{traceback.format_exc()}")
         sys.exit(1)
    finally:
         log_info("main", "main", "Server process finished.")


if __name__ == "__main__":
    # Basic check if logger itself works
    try:
        log_info("main", "__main__", f"WhatsTasker v0.7 starting...")
    except NameError:
        print("FATAL ERROR: Logger not loaded.")
        sys.exit(1)
    except Exception as e:
        print(f"FATAL ERROR during initial logging setup: {e}")
        sys.exit(1)

    main()

# --- END OF FILE main.py ---
# --- END OF FILE main.py ---


================================================================================
üìÑ bridge\request_router.py
================================================================================

# --- START OF FILE bridge\request_router.py ---
# --- START OF FILE bridge/request_router.py ---

import re
import os
import yaml
import traceback
import json
from typing import Optional, Tuple, List
from tools.logger import log_info, log_error, log_warning

# State manager imports
from services.agent_state_manager import get_agent_state, add_message_to_user_history

# User/Agent Management
from users.user_manager import get_agent

# Orchestrator Import
try:
    from agents.orchestrator_agent import handle_user_request as route_to_orchestrator
    ORCHESTRATOR_IMPORTED = True
    log_info("request_router", "import", "Successfully imported OrchestratorAgent handler.")
except ImportError as e:
    ORCHESTRATOR_IMPORTED = False; log_error("request_router", "import", f"OrchestratorAgent import failed: {e}", e); route_to_orchestrator = None

# Onboarding Agent Import
try:
    from agents.onboarding_agent import handle_onboarding_request
    ONBOARDING_AGENT_IMPORTED = True
    log_info("request_router", "import", "Successfully imported OnboardingAgent handler.")
except ImportError as e:
     ONBOARDING_AGENT_IMPORTED = False; log_error("request_router", "import", f"OnboardingAgent import failed: {e}", e); handle_onboarding_request = None

# Task Query Service Import
try:
    from services.task_query_service import get_context_snapshot
    QUERY_SERVICE_IMPORTED = True
except ImportError as e:
     QUERY_SERVICE_IMPORTED = False; log_error("request_router", "import", f"TaskQueryService import failed: {e}", e); get_context_snapshot = None

# Cheat Service Import
try:
    from services.cheats import handle_cheat_command
    CHEATS_IMPORTED = True
    log_info("request_router", "import", "Successfully imported Cheats service.")
except ImportError as e:
     CHEATS_IMPORTED = False; log_error("request_router", "import", f"Cheats service import failed: {e}", e); handle_cheat_command = None

# ConfigManager Import
try:
    from services.config_manager import set_user_status
    CONFIG_MANAGER_IMPORTED = True
except ImportError as e:
     CONFIG_MANAGER_IMPORTED = False; log_error("request_router", "import", f"ConfigManager import failed: {e}", e); set_user_status = None


# Load standard messages
_messages = {}
try:
    messages_path = os.path.join("config", "messages.yaml")
    if os.path.exists(messages_path):
        with open(messages_path, 'r', encoding="utf-8") as f:
             content = f.read();
             if content.strip(): f.seek(0); _messages = yaml.safe_load(f) or {}
             else: _messages = {}
    else: log_warning("request_router", "import", f"{messages_path} not found."); _messages = {}
except Exception as e: log_error("request_router", "import", f"Failed to load messages.yaml: {e}", e); _messages = {}

GENERIC_ERROR_MSG = _messages.get("generic_error_message", "Sorry, an unexpected error occurred.")
WELCOME_MESSAGE = _messages.get("welcome_confirmation_message", "Hello! Welcome to WhatsTasker.")


# Global bridge instance
current_bridge = None

def normalize_user_id(user_id: str) -> str:
    return re.sub(r'\D', '', user_id) if user_id else ""

def set_bridge(bridge_instance):
    global current_bridge; current_bridge = bridge_instance
    log_info("request_router", "set_bridge", f"Bridge set to: {type(bridge_instance).__name__}")

def send_message(user_id: str, message: str):
    """Adds agent message to history and sends via configured bridge."""
    if user_id and message: add_message_to_user_history(user_id, "agent", message)
    else: log_warning("request_router", "send_message", f"Attempted add empty msg/invalid user_id ({user_id}) to history.")
    log_info("request_router", "send_message", f"Sending to {user_id}: '{message[:100]}...'")
    if current_bridge:
        try: current_bridge.send_message(user_id, message)
        except Exception as e: log_error("request_router", "send_message", f"Bridge error sending to {user_id}: {e}", e)
    else: log_error("request_router", "send_message", "No bridge configured.")


# --- Main Handler ---
def handle_incoming_message(user_id: str, message: str) -> str:
    """
    Routes incoming messages based on user status (new, onboarding, active) or cheat codes.
    """
    final_response_message = GENERIC_ERROR_MSG

    try:
        log_info("request_router", "handle_incoming_message", f"Received raw: {user_id} msg: '{message[:50]}...'")
        norm_user_id = normalize_user_id(user_id)
        if not norm_user_id: log_error("request_router", "handle_incoming_message", f"Invalid User ID: {user_id}"); return "Error: Invalid User ID."

        # --- Ensure agent state exists ---
        agent_state = get_agent(norm_user_id)
        if not agent_state: log_error("request_router", "handle_incoming_message", f"CRITICAL: Failed get/create agent state for {norm_user_id}."); return GENERIC_ERROR_MSG

        current_status = agent_state.get("preferences", {}).get("status")
        log_info("request_router", "handle_incoming_message", f"User {norm_user_id} status: {current_status}")

        # --- Routing Logic ---

        # 1. New User: Send welcome, set status to onboarding
        if current_status == "new":
            # *** CORRECTED LOG CALL ***
            log_info("request_router", "handle_incoming_message", f"New user ({norm_user_id}). Sending welcome, setting status to onboarding.")
            send_message(norm_user_id, WELCOME_MESSAGE)
            if CONFIG_MANAGER_IMPORTED and set_user_status:
                if not set_user_status(norm_user_id, 'onboarding'): # Set status to onboarding
                     log_error("request_router", "handle_incoming_message", f"Failed update status from 'new' to 'onboarding' for {norm_user_id}")
            else: log_error("request_router", "handle_incoming_message", "ConfigManager unavailable, cannot update user status after welcome.")
            return WELCOME_MESSAGE # End processing for this turn

        # 2. Cheat Codes (Check before onboarding/active routing)
        message_stripped = message.strip()
        if message_stripped.startswith('/') and CHEATS_IMPORTED and handle_cheat_command:
            parts = message_stripped.split(); command = parts[0].lower(); args = parts[1:]
            log_info("request_router", "handle_incoming_message", f"Detected command '{command}' for {norm_user_id}. Routing to Cheats.")
            try: command_response = handle_cheat_command(norm_user_id, command, args)
            except Exception as e: log_error("request_router", "handle_incoming_message", f"Error executing cheat '{command}': {e}", e); command_response = "Error processing cheat."
            send_message(norm_user_id, command_response); return command_response # Bypass other handlers

        elif message_stripped.startswith('/') and not CHEATS_IMPORTED:
             log_error("request_router", "handle_incoming_message", "Cheat command detected, but Cheats service failed import.")
             send_message(norm_user_id, "Error: Command processor unavailable."); return "Error: Command processor unavailable."

        # 3. Onboarding User: Route to onboarding agent
        elif current_status == "onboarding":
            log_info("request_router", "handle_incoming_message", f"User {norm_user_id} is onboarding. Routing to onboarding agent.")
            add_message_to_user_history(norm_user_id, "user", message) # Add user reply to history first
            if ONBOARDING_AGENT_IMPORTED and handle_onboarding_request:
                try:
                     history = agent_state.get("conversation_history", [])
                     preferences = agent_state.get("preferences", {})
                     final_response_message = handle_onboarding_request(norm_user_id, message, history, preferences)
                except Exception as onboard_e:
                     tb_str = traceback.format_exc()
                     log_error("request_router", "handle_incoming_message", f"Error calling OnboardingAgent for {norm_user_id}. Traceback:\n{tb_str}", onboard_e)
                     final_response_message = GENERIC_ERROR_MSG
            else:
                log_error("request_router", "handle_incoming_message", f"Onboarding required for {norm_user_id}, but onboarding agent not imported.")
                final_response_message = "Sorry, there's an issue with the setup process right now."

        # 4. Active User: Route to main orchestrator
        elif current_status == "active":
            log_info("request_router", "handle_incoming_message", f"User {norm_user_id} is active. Routing to orchestrator.")
            add_message_to_user_history(norm_user_id, "user", message)
            if ORCHESTRATOR_IMPORTED and route_to_orchestrator and QUERY_SERVICE_IMPORTED and get_context_snapshot:
                try:
                    history = agent_state.get("conversation_history", [])
                    preferences = agent_state.get("preferences", {})
                    task_context, calendar_context = get_context_snapshot(norm_user_id)
                    final_response_message = route_to_orchestrator(
                        user_id=norm_user_id, message=message, history=history,
                        preferences=preferences, task_context=task_context, calendar_context=calendar_context)
                except Exception as orch_e:
                     tb_str = traceback.format_exc()
                     log_error("request_router", "handle_incoming_message", f"Error calling OrchestratorAgent for {norm_user_id}. Traceback:\n{tb_str}", orch_e)
                     final_response_message = GENERIC_ERROR_MSG
            else:
                 log_error("request_router", "handle_incoming_message", f"Core components missing for active user {norm_user_id} (Orchestrator/QueryService).")
                 final_response_message = "Sorry, I can't process your request right now due to a system issue."

        # 5. Unknown Status: Log error and give generic response
        else:
            log_error("request_router", "handle_incoming_message", f"User {norm_user_id} has unknown status: '{current_status}'. Sending generic error.")
            final_response_message = GENERIC_ERROR_MSG


        # Send the final response (unless handled earlier)
        if final_response_message:
             send_message(norm_user_id, final_response_message)
        else:
             log_warning("request_router", "handle_incoming_message", f"Final response message was empty for {norm_user_id}. Sending generic error.")
             send_message(norm_user_id, GENERIC_ERROR_MSG)
             final_response_message = GENERIC_ERROR_MSG

        return final_response_message

    except Exception as outer_e:
        tb_str = traceback.format_exc()
        log_error("request_router", "handle_incoming_message", f"Unexpected outer error processing message for {user_id}. Traceback:\n{tb_str}", outer_e)
        if 'norm_user_id' in locals() and norm_user_id:
            try: send_message(norm_user_id, GENERIC_ERROR_MSG)
            except: pass
        return GENERIC_ERROR_MSG
# --- END OF FILE bridge/request_router.py ---
# --- END OF FILE bridge\request_router.py ---


================================================================================
üìÑ bridge\cli_interface.py
================================================================================

# --- START OF FILE bridge\cli_interface.py ---
# bridge/cli_interface.py

from fastapi import FastAPI, Request
from fastapi.responses import JSONResponse
import uvicorn
import uuid
from threading import Lock, Thread # Import Thread
import time # Import time for sleep
from tools.logger import log_info, log_error
from bridge.request_router import handle_incoming_message, set_bridge
# Ensure calendar_tool provides the router correctly
try:
    from tools.calendar_tool import router as calendar_router
except ImportError:
    log_error("cli_interface", "import", "Could not import calendar_router from tools.calendar_tool")
    # Define a dummy router if import fails to allow server start
    from fastapi import APIRouter
    calendar_router = APIRouter()
# from datetime import datetime # Not used directly here

# Define a CLI Bridge
class CLIBridge:
    """Bridge that handles message queuing for CLI interaction."""
    def __init__(self, message_queue, lock):
        self.message_queue = message_queue
        self.lock = lock

    def send_message(self, user_id: str, message: str):
        """Adds the outgoing message to the queue instead of printing."""
        outgoing = {
            "user_id": user_id,
            "message": message,
            "message_id": str(uuid.uuid4())
        }
        with self.lock:
            self.message_queue.append(outgoing)
        log_info("CLIBridge", "send_message", f"Message for {user_id} queued (ID: {outgoing['message_id']}). Queue size: {len(self.message_queue)}")

# Global in-memory store for outgoing messages.
outgoing_messages = []
queue_lock = Lock()

# Set the global bridge in the router to use our CLI Bridge instance
# Pass the queue and lock to the bridge instance
set_bridge(CLIBridge(outgoing_messages, queue_lock))


def create_app() -> FastAPI:
    """Creates the FastAPI app."""
    app = FastAPI()
    app.include_router(calendar_router, prefix="")

    @app.post("/incoming")
    async def incoming_message(request: Request):
        """Receives message, processes it, queues response, returns ack."""
        try:
            data = await request.json()
            user_id = data.get("user_id")
            message = data.get("message")
            if not user_id or not message:
                return JSONResponse(content={"error": "Missing user_id or message"}, status_code=400)
            log_info("cli_interface", "incoming_message", f"Received message from user {user_id}: {message}")

            # Process the incoming message via the unified handler
            # handle_incoming_message now calls CLIBridge.send_message which queues the response
            handle_incoming_message(user_id, message)

            # --- REVERT HERE ---
            # Return only an acknowledgment. The actual message is queued by the bridge.
            return JSONResponse(content={"ack": True})
            # --- END REVERT ---

        except Exception as e:
            log_error("cli_interface", "incoming_message", "Error processing incoming message", e)
            return JSONResponse(content={"error": "Internal server error"}, status_code=500)

    @app.get("/outgoing")
    async def get_outgoing_messages():
        """Returns and clears the list of queued outgoing messages."""
        msgs_to_send = []
        with queue_lock:
            # Return all messages currently in the queue and clear it
            msgs_to_send = outgoing_messages[:] # Copy the list
            outgoing_messages.clear()          # Clear the original list
        if msgs_to_send:
            log_info("cli_interface", "get_outgoing_messages", f"Returning {len(msgs_to_send)} messages from queue.")
        return JSONResponse(content={"messages": msgs_to_send})

    @app.post("/ack")
    async def acknowledge_message(request: Request):
        """Receives acknowledgment (currently does nothing as queue is cleared on GET)."""
        # In a more robust system, GET /outgoing wouldn't clear the queue.
        # Messages would only be removed upon receiving an ACK for their specific message_id.
        # For this simple polling client, clearing on GET is sufficient.
        try:
            data = await request.json()
            message_id = data.get("message_id")
            if not message_id:
                return JSONResponse(content={"error": "Missing message_id"}, status_code=400)
            # Log but don't modify queue here, as GET already cleared it
            log_info("cli_interface", "acknowledge_message", f"Ack received for message {message_id} (queue already cleared by GET).")
            return JSONResponse(content={"ack": True})
        except Exception as e:
            log_error("cli_interface", "acknowledge_message", "Error processing ack", e)
            return JSONResponse(content={"error": "Internal server error"}, status_code=500)

    return app

app = create_app()

# Keep if running directly, but usually run via main.py
# if __name__ == "__main__":
#     uvicorn.run(app, host="0.0.0.0", port=8000, reload=True)
# --- END OF FILE bridge\cli_interface.py ---


================================================================================
üìÑ agents\orchestrator_agent.py
================================================================================

# --- START OF FILE agents\orchestrator_agent.py ---
# --- START OF FILE agents/orchestrator_agent.py ---
import json
import os
import traceback
from typing import Dict, List, Optional, Any
from datetime import datetime
import pytz
import re

# Instructor/LLM Imports
from services.llm_interface import get_instructor_client
from openai import OpenAI # Base client for types if needed
from openai.types.chat import ChatCompletionMessage, ChatCompletionToolParam, ChatCompletionMessageToolCall
from openai.types.chat.chat_completion_message_tool_call import Function
from openai.types.shared_params import FunctionDefinition

# Tool Imports & Helpers
from .tool_definitions import AVAILABLE_TOOLS, TOOL_PARAM_MODELS
import services.task_manager as task_manager # For parsing duration used in prompt prep

# Utilities
from tools.logger import log_info, log_error, log_warning
import yaml
import pydantic

# --- Load Standard Messages ---
_messages = {}
try:
    messages_path = os.path.join("config", "messages.yaml")
    if os.path.exists(messages_path):
        with open(messages_path, 'r', encoding="utf-8") as f:
             content = f.read()
             if content.strip(): f.seek(0); _messages = yaml.safe_load(f) or {}
             else: _messages = {}
    else: log_warning("orchestrator_agent", "init", f"{messages_path} not found."); _messages = {}
except Exception as e: log_error("orchestrator_agent", "init", f"Failed to load messages.yaml: {e}", e); _messages = {}
GENERIC_ERROR_MSG = _messages.get("generic_error_message", "Sorry, an unexpected error occurred.")


# --- Function to Load Prompt ---
_PROMPT_CACHE = {}
def load_orchestrator_prompt() -> Optional[str]:
    """Loads the orchestrator system prompt from the YAML file, with simple caching."""
    prompts_path = os.path.join("config", "prompts.yaml"); cache_key = prompts_path
    if cache_key in _PROMPT_CACHE: return _PROMPT_CACHE[cache_key]
    prompt_text_result: Optional[str] = None
    try:
        if not os.path.exists(prompts_path): raise FileNotFoundError(f"{prompts_path} not found.")
        with open(prompts_path, "r", encoding="utf-8") as f:
            content = f.read();
            if not content.strip(): raise ValueError("Prompts file is empty.")
            f.seek(0); all_prompts = yaml.safe_load(f)
            if not all_prompts: raise ValueError("YAML parsing resulted in empty prompts.")
            prompt_text = all_prompts.get("orchestrator_agent_system_prompt")
            if not prompt_text or not prompt_text.strip(): log_error("orchestrator_agent", "load_orchestrator_prompt", "Key 'orchestrator_agent_system_prompt' NOT FOUND or EMPTY."); prompt_text_result = None
            else: log_info("orchestrator_agent", "load_orchestrator_prompt", "Orchestrator prompt loaded successfully."); prompt_text_result = prompt_text
    except Exception as e: log_error("orchestrator_agent", "load_orchestrator_prompt", f"CRITICAL: Failed to load/parse orchestrator prompt: {e}", e); prompt_text_result = None
    _PROMPT_CACHE[cache_key] = prompt_text_result; return prompt_text_result

# --- REMOVED Pending Context Helpers ---

# --- Main Handler Function ---
def handle_user_request(
    user_id: str, message: str, history: List[Dict], preferences: Dict,
    task_context: List[Dict], calendar_context: List[Dict]) -> str:
    """
    Handles the user's request using the Orchestrator pattern with Instructor/Tool Use.
    Relies purely on the LLM for conversational flow and tool invocation decisions.
    """
    log_info("orchestrator_agent", "handle_user_request", f"Processing request for {user_id}: '{message[:50]}...'")

    orchestrator_system_prompt = load_orchestrator_prompt()
    if not orchestrator_system_prompt:
         log_error("orchestrator_agent", "handle_user_request", "Orchestrator system prompt could not be loaded.")
         return GENERIC_ERROR_MSG

    client: OpenAI = get_instructor_client() # Assume instructor patch maintains OpenAI compatibility
    if not client:
        log_error("orchestrator_agent", "handle_user_request", "LLM client unavailable.")
        return GENERIC_ERROR_MSG

    # --- Prepare Context for Prompt ---
    try:
        # (Context preparation remains the same as previous version)
        user_timezone_str = preferences.get("TimeZone", "UTC"); user_timezone = pytz.utc
        try: user_timezone = pytz.timezone(user_timezone_str)
        except pytz.UnknownTimeZoneError: log_warning("...", f"Unknown timezone '{user_timezone_str}'. Defaulting UTC.")
        now = datetime.now(user_timezone); current_date_str = now.strftime("%Y-%m-%d"); current_time_str = now.strftime("%H:%M"); current_day_str = now.strftime("%A")
        history_limit = 30 # Increased history slightly for better context
        limited_history = history[-(history_limit*2):]
        history_str = "\n".join([f"{m['sender'].capitalize()}: {m['content']}" for m in limited_history])
        item_context_limit = 20 # Increased context slightly
        calendar_context_limit = 20
        def prepare_item(item):
             key_fields = ['event_id', 'item_id', 'title', 'description', 'date', 'time', 'type', 'status', 'estimated_duration', 'project']
             prep = {k: item.get(k) for k in key_fields if item.get(k) is not None}; prep['item_id'] = item.get('item_id', item.get('event_id'))
             if 'event_id' in prep and prep['event_id'] == prep['item_id']: del prep['event_id']
             return prep
        item_context_str = json.dumps([prepare_item(item) for item in task_context[:item_context_limit]], indent=2, default=str)
        calendar_context_str = json.dumps(calendar_context[:calendar_context_limit], indent=2, default=str)
        prefs_str = json.dumps(preferences, indent=2, default=str)
    except Exception as e: log_error("...", f"Error preparing context strings: {e}", e); return GENERIC_ERROR_MSG

    # --- Construct Initial Messages for LLM ---
    # Use list of dicts format directly compatible with OpenAI client
    messages: List[Dict[str, Any]] = [
        {"role": "system", "content": orchestrator_system_prompt},
        {"role": "system", "content": f"Current Reference Time ({user_timezone_str}): Date: {current_date_str}, Day: {current_day_str}, Time: {current_time_str}. Use this."},
        {"role": "system", "content": f"User Preferences:\n{prefs_str}"},
        {"role": "system", "content": f"History:\n{history_str}"},
        {"role": "system", "content": f"Active Items:\n{item_context_str}"},
        {"role": "system", "content": f"Calendar Events:\n{calendar_context_str}"},
        {"role": "user", "content": message}
    ]

    # --- Define Tools ---
    # (Tool definition logic remains the same as previous version)
    tools_for_llm: List[ChatCompletionToolParam] = []
    for tool_name, model in TOOL_PARAM_MODELS.items():
        if tool_name not in AVAILABLE_TOOLS: continue
        func = AVAILABLE_TOOLS.get(tool_name); description = func.__doc__.strip() if func and func.__doc__ else f"Executes {tool_name}"
        try:
            params_schema = model.model_json_schema();
            if not params_schema.get('properties'): params_schema = {} # Handle models with no params
            func_def: FunctionDefinition = {"name": tool_name, "description": description, "parameters": params_schema}
            tool_param: ChatCompletionToolParam = {"type": "function", "function": func_def}; tools_for_llm.append(tool_param)
        except Exception as e: log_error("...", f"Schema error for {tool_name}: {e}", e)
    if not tools_for_llm: log_error("...", "No tools defined."); return GENERIC_ERROR_MSG

    # --- Primary Interaction Loop (Max 1 Tool Call per User Turn for Simplicity) ---
    try:
        log_info("orchestrator_agent", "handle_user_request", f"Invoking LLM for {user_id} (Initial Turn)...")
        response = client.chat.completions.create(
            model="gpt-4o", # Or your preferred model
            messages=messages,
            tools=tools_for_llm,
            tool_choice="auto",
            temperature=0.1,
        )
        response_message = response.choices[0].message

        tool_calls = response_message.tool_calls

        # --- Scenario 1: LLM Responds Directly ---
        if not tool_calls:
            if response_message.content:
                log_info("...", "LLM responded directly (no tool call).")
                return response_message.content
            else:
                log_warning("...", "LLM response had no tool calls and no content.")
                return "Sorry, I couldn't process that request fully. Could you try rephrasing?"

        # --- Scenario 2: LLM Calls a Tool ---
        # For simplicity, handle only the first tool call if multiple are returned
        tool_call: ChatCompletionMessageToolCall = tool_calls[0]
        tool_name = tool_call.function.name
        tool_call_id = tool_call.id # Get the ID to associate result

        if tool_name not in AVAILABLE_TOOLS:
            log_warning("...", f"LLM tried unknown tool: {tool_name}")
            # Ask LLM to re-evaluate based on the error? Or just return error?
            # For simplicity, return error for now. Prompt needs to guide LLM better.
            return f"Sorry, I received a request for an unknown action '{tool_name}'."

        log_info("...", f"LLM requested Tool: {tool_name} with args: {tool_call.function.arguments[:150]}...")
        tool_func = AVAILABLE_TOOLS[tool_name]
        param_model = TOOL_PARAM_MODELS[tool_name]
        tool_result_content = GENERIC_ERROR_MSG # Default content in case of error

        try:
            # Parse and validate arguments
            tool_args_dict = {}
            tool_args_str = tool_call.function.arguments
            if tool_args_str and tool_args_str.strip() != '{}': tool_args_dict = json.loads(tool_args_str)
            elif not param_model.model_fields: tool_args_dict = {}

            validated_params = param_model(**tool_args_dict)

            # Execute the tool function
            tool_result_dict = tool_func(user_id, validated_params) # Expects dict return
            log_info("...", f"Tool {tool_name} executed. Result: {tool_result_dict}")

            # Serialize the result dictionary to JSON string for the LLM
            tool_result_content = json.dumps(tool_result_dict)

        except json.JSONDecodeError:
             log_error("...", f"Failed parse JSON args for {tool_name}: {tool_args_str}");
             tool_result_content = json.dumps({"success": False, "message": f"Error: Invalid arguments provided for {tool_name}."})
        except pydantic.ValidationError as e:
             log_error("...", f"Arg validation failed for {tool_name}. Err: {e.errors()}. Args: {tool_args_str}", e)
             err_summary = "; ".join([f"{err['loc'][0] if err.get('loc') else 'param'}: {err['msg']}" for err in e.errors()])
             tool_result_content = json.dumps({"success": False, "message": f"Error: Invalid parameters for {tool_name} - {err_summary}"})
        except Exception as e:
             log_error("...", f"Error executing tool {tool_name}. Trace:\n{traceback.format_exc()}", e);
             tool_result_content = json.dumps({"success": False, "message": f"Error performing action {tool_name}."})

        # --- Make SECOND LLM call with the tool result ---
        # Append the first assistant message (tool call request) and the tool result message
        messages.append(response_message) # Add assistant's tool call message
        messages.append({
            "tool_call_id": tool_call_id,
            "role": "tool",
            "name": tool_name,
            "content": tool_result_content, # Pass JSON string result
        })

        log_info("...", f"Invoking LLM again for {user_id} with tool result...")
        second_response = client.chat.completions.create(
            model="gpt-4o", # Use the same model
            messages=messages,
            # No tools needed here usually, LLM should just generate response
        )
        second_response_message = second_response.choices[0].message

        if second_response_message.content:
            log_info("...", "LLM generated final response after processing tool result.")
            return second_response_message.content
        else:
            # This shouldn't happen often if the prompt is good, but handle it
            log_warning("...", "LLM provided no content after processing tool result.")
            # Maybe return the raw tool message as a fallback?
            # Extract message from tool_result_content if possible
            try: fallback_msg = json.loads(tool_result_content).get("message", GENERIC_ERROR_MSG)
            except: fallback_msg = GENERIC_ERROR_MSG
            return fallback_msg


    # --- Outer Exception Handling ---
    except Exception as e:
        tb_str = traceback.format_exc()
        log_error("orchestrator_agent", "handle_user_request", f"Core error in orchestrator logic for {user_id}. Traceback:\n{tb_str}", e)
        return GENERIC_ERROR_MSG
# --- END OF FILE agents/orchestrator_agent.py ---
# --- END OF FILE agents\orchestrator_agent.py ---


================================================================================
üìÑ agents\onboarding_agent.py
================================================================================

# --- START OF FILE agents\onboarding_agent.py ---
# --- START OF FILE agents/onboarding_agent.py ---
import json
import os
import traceback
from typing import Dict, List, Optional, Any
from datetime import datetime
import pytz

# Instructor/LLM Imports
from services.llm_interface import get_instructor_client
from openai import OpenAI
from openai.types.chat import ChatCompletionMessage, ChatCompletionToolParam, ChatCompletionMessageToolCall
from openai.types.chat.chat_completion_message_tool_call import Function
from openai.types.shared_params import FunctionDefinition

# Tool Imports (Limited Set)
from .tool_definitions import (
    AVAILABLE_TOOLS,
    TOOL_PARAM_MODELS,
    UpdateUserPreferencesParams,
    InitiateCalendarConnectionParams,
)

# Utilities
from tools.logger import log_info, log_error, log_warning
import yaml
import pydantic

# --- Load Standard Messages ---
_messages = {}
try:
    messages_path = os.path.join("config", "messages.yaml")
    if os.path.exists(messages_path):
        with open(messages_path, 'r', encoding="utf-8") as f:
             content = f.read()
             if content.strip(): f.seek(0); _messages = yaml.safe_load(f) or {}
             else: _messages = {}
    else: log_warning("onboarding_agent", "init", f"{messages_path} not found."); _messages = {}
except Exception as e: log_error("onboarding_agent", "init", f"Failed to load messages.yaml: {e}", e); _messages = {}
GENERIC_ERROR_MSG = _messages.get("generic_error_message", "Sorry, an unexpected error occurred.")
SETUP_START_MSG = _messages.get("setup_starting_message", "Great! Let's set things up.")

# --- Function to Load Onboarding Prompt ---
_ONBOARDING_PROMPT_CACHE = {}
def load_onboarding_prompt() -> Optional[str]:
    """Loads the onboarding system prompt from the YAML file, with caching."""
    prompts_path = os.path.join("config", "prompts.yaml")
    cache_key = prompts_path + "_onboarding"
    if cache_key in _ONBOARDING_PROMPT_CACHE:
        return _ONBOARDING_PROMPT_CACHE[cache_key]

    prompt_text_result: Optional[str] = None
    try:
        if not os.path.exists(prompts_path): raise FileNotFoundError(f"{prompts_path} not found.")
        with open(prompts_path, "r", encoding="utf-8") as f:
            content = f.read();
            if not content.strip(): raise ValueError("Prompts file is empty.")
            f.seek(0); all_prompts = yaml.safe_load(f)
            if not all_prompts: raise ValueError("YAML parsing resulted in empty prompts.")
            prompt_text = all_prompts.get("onboarding_agent_system_prompt")
            if not prompt_text or not prompt_text.strip():
                log_error("onboarding_agent", "load_onboarding_prompt", "Key 'onboarding_agent_system_prompt' NOT FOUND or EMPTY.")
                prompt_text_result = None
            else:
                log_info("onboarding_agent", "load_onboarding_prompt", "Onboarding prompt loaded successfully.")
                prompt_text_result = prompt_text
    except Exception as e:
        log_error("onboarding_agent", "load_onboarding_prompt", f"CRITICAL: Failed load/parse onboarding prompt: {e}", e)
        prompt_text_result = None

    _ONBOARDING_PROMPT_CACHE[cache_key] = prompt_text_result
    return prompt_text_result

# --- REMOVED Onboarding State Helpers ---
# The pure LLM flow relies on history and current prefs in context

# --- Helper for LLM Clarification (If needed for onboarding - keep for now) ---
def _get_clarification_from_llm(client: OpenAI, question: str, user_reply: str, expected_choices: List[str]) -> str:
    """Uses LLM to interpret user reply against expected choices."""
    # This helper might still be useful if the LLM asks a yes/no for calendar
    log_info("onboarding_agent", "_get_clarification_from_llm", f"Asking LLM to clarify: Q='{question}' Reply='{user_reply}' Choices={expected_choices}")
    system_message = f"""
You are helping a user interact with a task assistant during setup.
The assistant asked the user a question, and the user replied.
Your task is to determine which of the expected choices the user's reply corresponds to.
The original question was: "{question}"
The user's reply was: "{user_reply}"
The expected choices are: {expected_choices}

Analyze the user's reply and determine the choice.
Respond ONLY with one of the following exact strings: {', '.join([f"'{choice}'" for choice in expected_choices])} or 'unclear'.
"""
    try:
        response = client.chat.completions.create(
            model="gpt-3.5-turbo", # Use a cheaper/faster model for clarification
            messages=[{"role": "system", "content": system_message}],
            temperature=0.0,
            max_tokens=10
        )
        llm_choice = response.choices[0].message.content.strip().lower().replace("'", "")
        log_info("onboarding_agent", "_get_clarification_from_llm", f"LLM clarification result: '{llm_choice}'")
        if llm_choice in expected_choices or llm_choice == 'unclear':
            return llm_choice
        else:
            log_warning("onboarding_agent", "_get_clarification_from_llm", f"LLM returned unexpected clarification: '{llm_choice}'")
            return 'unclear'
    except Exception as e:
        log_error("onboarding_agent", "_get_clarification_from_llm", f"Error during LLM clarification call: {e}", e)
        return 'unclear'

# --- Main Onboarding Handler Function ---
def handle_onboarding_request(
    user_id: str,
    message: str,
    history: List[Dict],
    preferences: Dict # Current preferences passed in
) -> str:
    """Handles the user's request during the onboarding phase using pure LLM flow."""
    log_info("onboarding_agent", "handle_onboarding_request", f"Handling onboarding request for {user_id}: '{message[:50]}...'")

    onboarding_system_prompt = load_onboarding_prompt()
    if not onboarding_system_prompt:
         log_error("onboarding_agent", "handle_onboarding_request", "Onboarding system prompt could not be loaded.")
         return GENERIC_ERROR_MSG

    client: OpenAI = get_instructor_client()
    if not client:
        log_error("onboarding_agent", "handle_onboarding_request", "LLM client unavailable.")
        return GENERIC_ERROR_MSG

    # --- Prepare Context (Simpler than main orchestrator, focuses on prefs) ---
    try:
        user_timezone_str = preferences.get("TimeZone", "UTC"); user_timezone = pytz.utc
        try:
            user_timezone = pytz.timezone(user_timezone_str)
        # *** CORRECTED LOG CALL ***
        except pytz.UnknownTimeZoneError:
            log_warning("onboarding_agent", "handle_onboarding_request", f"Unknown timezone '{user_timezone_str}'. Using UTC.")
        # **************************
        now = datetime.now(user_timezone); current_date_str = now.strftime("%Y-%m-%d"); current_time_str = now.strftime("%H:%M"); current_day_str = now.strftime("%A")

        history_limit = 10
        limited_history = history[-(history_limit*2):]
        history_str = "\n".join([f"{m['sender'].capitalize()}: {m['content']}" for m in limited_history])

        prefs_str = json.dumps(preferences, indent=2, default=str)

        initial_interaction = len(history) <= 1 # If only user message is present
        intro_message = SETUP_START_MSG if initial_interaction else ""

    except Exception as e:
        log_error("onboarding_agent", "handle_onboarding_request", f"Error preparing context: {e}", e)
        return GENERIC_ERROR_MSG

    # --- Construct Messages for Onboarding LLM ---
    messages: List[Dict[str, Any]] = [
        {"role": "system", "content": onboarding_system_prompt},
        {"role": "system", "content": f"Current User Preferences:\n```json\n{prefs_str}\n```"},
        {"role": "system", "content": f"Conversation History:\n{history_str}"},
        {"role": "user", "content": message}
    ]
    if intro_message and initial_interaction:
         messages.insert(-1, {"role": "assistant", "content": intro_message})


    # --- Define Tools AVAILABLE for Onboarding ---
    onboarding_tools_available = {
        "update_user_preferences": AVAILABLE_TOOLS.get("update_user_preferences"),
        "initiate_calendar_connection": AVAILABLE_TOOLS.get("initiate_calendar_connection"),
    }
    onboarding_tool_models = {
        "update_user_preferences": UpdateUserPreferencesParams,
        "initiate_calendar_connection": InitiateCalendarConnectionParams,
    }
    onboarding_tools_available = {k:v for k,v in onboarding_tools_available.items() if v is not None}

    tools_for_llm: List[ChatCompletionToolParam] = []
    for tool_name, model in onboarding_tool_models.items():
        if tool_name not in onboarding_tools_available: continue
        func = onboarding_tools_available.get(tool_name)
        description = func.__doc__.strip() if func and func.__doc__ else f"Executes {tool_name}"
        try:
            params_schema = model.model_json_schema();
            if not params_schema.get('properties'): params_schema = {}
            func_def: FunctionDefinition = {"name": tool_name, "description": description, "parameters": params_schema}
            tool_param: ChatCompletionToolParam = {"type": "function", "function": func_def}; tools_for_llm.append(tool_param)
        except Exception as e: log_error("onboarding_agent", "handle_onboarding_request", f"Schema error for onboarding tool {tool_name}: {e}", e)
    # It's okay if tools_for_llm is empty if LLM decides to just talk


    # --- Interact with LLM (using same two-step logic as orchestrator) ---
    try:
        log_info("onboarding_agent", "handle_onboarding_request", f"Invoking Onboarding LLM for {user_id}...")
        response = client.chat.completions.create(
            model="gpt-4o", # Or gpt-3.5-turbo
            messages=messages,
            tools=tools_for_llm if tools_for_llm else None,
            tool_choice="auto" if tools_for_llm else None,
            temperature=0.1,
        )
        response_message = response.choices[0].message
        tool_calls = response_message.tool_calls

        # --- Scenario 1: LLM Responds Directly (Asks question, gives info) ---
        if not tool_calls:
            if response_message.content:
                log_info("onboarding_agent", "handle_onboarding_request", "Onboarding LLM responded directly.")
                return response_message.content
            else:
                log_warning("onboarding_agent", "handle_onboarding_request", "Onboarding LLM response had no tool calls and no content.")
                return "Sorry, I got stuck. Can you tell me what we were discussing?"

        # --- Scenario 2: LLM Calls an Onboarding Tool ---
        tool_call: ChatCompletionMessageToolCall = tool_calls[0]
        tool_name = tool_call.function.name
        tool_call_id = tool_call.id

        if tool_name not in onboarding_tools_available:
            log_warning("onboarding_agent", "handle_onboarding_request", f"Onboarding LLM tried unknown/disallowed tool: {tool_name}")
            return f"Sorry, I tried an action ('{tool_name}') that isn't available during setup."

        log_info("onboarding_agent", "handle_onboarding_request", f"Onboarding LLM requested Tool: {tool_name} with args: {tool_call.function.arguments[:150]}...")
        tool_func = onboarding_tools_available[tool_name]
        param_model = onboarding_tool_models[tool_name]
        tool_result_content = GENERIC_ERROR_MSG

        try:
            # Parse and validate arguments
            tool_args_dict = {}
            tool_args_str = tool_call.function.arguments
            if tool_args_str and tool_args_str.strip() != '{}': tool_args_dict = json.loads(tool_args_str)
            elif not param_model.model_fields: tool_args_dict = {}

            validated_params = param_model(**tool_args_dict)

            # Execute the tool function
            tool_result_dict = tool_func(user_id, validated_params)
            log_info("onboarding_agent", "handle_onboarding_request", f"Onboarding Tool {tool_name} executed. Result: {tool_result_dict}")
            tool_result_content = json.dumps(tool_result_dict)

        # Handle errors during tool execution
        except json.JSONDecodeError: log_error("...", f"Failed parse JSON args for onboarding tool {tool_name}: {tool_args_str}"); tool_result_content = json.dumps({"success": False, "message": f"Error: Invalid arguments for {tool_name}."})
        except pydantic.ValidationError as e:
             log_error("...", f"Arg validation failed for onboarding tool {tool_name}. Err: {e.errors()}. Args: {tool_args_str}", e)
             err_summary = "; ".join([f"{err['loc'][0] if err.get('loc') else 'param'}: {err['msg']}" for err in e.errors()])
             tool_result_content = json.dumps({"success": False, "message": f"Error: Invalid parameters for {tool_name} - {err_summary}"})
        except Exception as e: log_error("...", f"Error executing onboarding tool {tool_name}. Trace:\n{traceback.format_exc()}", e); tool_result_content = json.dumps({"success": False, "message": f"Error performing action {tool_name}."})

        # --- Make SECOND LLM call with the tool result ---
        messages.append(response_message) # Add assistant's tool call message
        messages.append({
            "tool_call_id": tool_call_id, "role": "tool",
            "name": tool_name, "content": tool_result_content,
        })

        log_info("onboarding_agent", "handle_onboarding_request", f"Invoking Onboarding LLM again for {user_id} with tool result...")
        second_response = client.chat.completions.create(
            model="gpt-4o", messages=messages, # No tools needed here
        )
        second_response_message = second_response.choices[0].message

        if second_response_message.content:
            log_info("onboarding_agent", "handle_onboarding_request", "Onboarding LLM generated final response after processing tool result.")
            # Check if the LAST action was setting status to active
            if tool_name == "update_user_preferences":
                 try:
                      # Ensure tool_args_str is valid JSON before parsing
                      update_data = json.loads(tool_args_str) if tool_args_str and tool_args_str.strip() != '{}' else {}
                      if update_data.get("status") == "active":
                           log_info("onboarding_agent", "handle_onboarding_request", f"Onboarding completed for user {user_id} (status set to active).")
                 except Exception as parse_err:
                      log_warning("onboarding_agent", "handle_onboarding_request", f"Could not parse tool args to check for status update: {parse_err}")

            return second_response_message.content
        else:
            log_warning("onboarding_agent", "handle_onboarding_request", "Onboarding LLM provided no content after processing tool result.")
            try: fallback_msg = json.loads(tool_result_content).get("message", GENERIC_ERROR_MSG)
            except: fallback_msg = GENERIC_ERROR_MSG
            return fallback_msg

    # --- Outer Exception Handling ---
    except Exception as e:
        tb_str = traceback.format_exc();
        log_error("onboarding_agent", "handle_onboarding_request", f"Core error in onboarding logic for {user_id}. Traceback:\n{tb_str}", e)
        return GENERIC_ERROR_MSG
# --- END OF FILE agents/onboarding_agent.py ---
# --- END OF FILE agents\onboarding_agent.py ---


================================================================================
üìÑ agents\tool_definitions.py
================================================================================

# --- START OF FILE agents\tool_definitions.py ---
# --- START OF FILE agents/tool_definitions.py ---
from pydantic import BaseModel, Field, field_validator, ValidationError
from typing import Dict, Optional, Literal, Any, List, Tuple
import json
from datetime import datetime, timedelta
import re # For parsing scheduler response

# Import Service Layer functions & Helpers
import services.task_manager as task_manager
import services.config_manager as config_manager
import services.task_query_service as task_query_service
from services.agent_state_manager import get_agent_state # Needed for prefs/calendar in propose tool
from tools import metadata_store # Needed by propose tool to get task details
from tools.google_calendar_api import GoogleCalendarAPI # Needed by propose tool

# LLM Interface (for scheduler sub-call)
from services.llm_interface import get_instructor_client # Or use base openai client
from openai import OpenAI # For type hint if using base client
from openai.types.chat import ChatCompletionMessage # For type hint

# Utilities
from tools.logger import log_info, log_error, log_warning
import yaml # For loading scheduler prompts
import os # For path joining


# --- Helper: Get Calendar API ---
def _get_calendar_api_from_state(user_id: str) -> Optional[GoogleCalendarAPI]:
    """Helper to retrieve the active calendar API instance from agent state."""
    if GoogleCalendarAPI is None:
        log_warning("tool_definitions", "_get_calendar_api_from_state", "GoogleCalendarAPI class not available.")
        return None
    try:
        agent_state = get_agent_state(user_id)
        if agent_state:
            calendar_api_maybe = agent_state.get("calendar")
            if isinstance(calendar_api_maybe, GoogleCalendarAPI) and calendar_api_maybe.is_active():
                return calendar_api_maybe
    except Exception as e:
        log_error("tool_definitions", "_get_calendar_api_from_state", f"Error getting calendar API for {user_id}", e)
    return None


# --- Helper: Parse Scheduler LLM Response ---
def _parse_scheduler_llm_response(raw_text: str) -> Dict | None:
    """Parses the specific JSON output from the Session Scheduler LLM."""
    if not raw_text:
        log_warning("tool_definitions", "_parse_scheduler_llm_response", "Received empty raw text.")
        return None

    processed_text = None # Initialize
    try:
        # Try regex for fenced block first
        json_match_fenced = re.search(r"```json\s*({.*?})\s*```", raw_text, re.DOTALL | re.IGNORECASE)
        if json_match_fenced:
            processed_text = json_match_fenced.group(1).strip()
        else:
            # Fallback: find first '{' and last '}'
            json_start = raw_text.find('{')
            json_end = raw_text.rfind('}')
            if json_start != -1 and json_end != -1 and json_end > json_start:
                processed_text = raw_text[json_start : json_end + 1].strip()
                log_warning("tool_definitions", "_parse_scheduler_llm_response", f"Used Find/Rfind fallback for JSON extraction.")
            else:
                 log_warning("tool_definitions", "_parse_scheduler_llm_response", f"Could not find JSON block in Scheduler LLM response: '{raw_text[:100]}...'")
                 return None # No JSON found

        if not processed_text:
            raise ValueError("Empty text after extraction")

        schedule_data = json.loads(processed_text)

        # Validate structure
        required_keys = ["proposed_sessions", "response_message"]
        if not isinstance(schedule_data, dict) or not all(k in schedule_data for k in required_keys):
             missing = [k for k in required_keys if k not in schedule_data]
             raise ValueError(f"Parsed JSON missing scheduler keys: {missing}")

        if not isinstance(schedule_data["proposed_sessions"], list):
            raise ValueError("'proposed_sessions' must be a list.")

        # Validate individual sessions
        validated_sessions = []
        for session in schedule_data["proposed_sessions"]:
            required_session_keys = ["slot_ref", "date", "time", "end_time"]
            if not isinstance(session, dict) or not all(k in session for k in required_session_keys):
                 log_warning("tool_definitions", "_parse_scheduler_llm_response", f"Skipping invalid session format found: {session}")
                 continue # Skip invalid sessions

            try:
                 datetime.strptime(session["date"], '%Y-%m-%d')
                 datetime.strptime(session["time"], '%H:%M')
                 datetime.strptime(session["end_time"], '%H:%M')
                 if not isinstance(session["slot_ref"], int):
                     raise ValueError("slot_ref must be an integer")
                 validated_sessions.append(session) # Add session if valid
            except (ValueError, TypeError) as format_err:
                 log_warning("tool_definitions", "_parse_scheduler_llm_response", f"Skipping session with format error ({format_err}): {session}")
                 continue # Skip session with bad format

        schedule_data["proposed_sessions"] = validated_sessions # Replace with only valid sessions
        log_info("tool_definitions", "_parse_scheduler_llm_response", f"Parsed scheduler response, found {len(validated_sessions)} valid sessions.")
        return schedule_data

    except (json.JSONDecodeError, ValueError) as json_e:
        log_error("tool_definitions", "_parse_scheduler_llm_response", f"Scheduler LLM JSON parse/validation failed. Err: {json_e}. Extracted text: '{processed_text if processed_text is not None else 'N/A'}'", json_e)
        return None
    except Exception as e:
        log_error("tool_definitions", "_parse_scheduler_llm_response", f"Unexpected error parsing scheduler response: {e}", e)
        return None


# --- Helper: Load Scheduler Prompts ---
_SCHEDULER_PROMPTS_CACHE = {}
def _load_scheduler_prompts() -> Tuple[Optional[str], Optional[str]]:
    """Loads scheduler system and human prompts from YAML, with caching."""
    prompts_path = os.path.join("config", "prompts.yaml")
    cache_key = prompts_path + "_scheduler"
    if cache_key in _SCHEDULER_PROMPTS_CACHE:
        return _SCHEDULER_PROMPTS_CACHE[cache_key]

    system_prompt, human_prompt = None, None
    try:
        if not os.path.exists(prompts_path):
            raise FileNotFoundError(f"{prompts_path} not found.")

        with open(prompts_path, "r", encoding="utf-8") as f:
            all_prompts = yaml.safe_load(f)
            if not all_prompts:
                 raise ValueError("YAML parsing resulted in empty prompts.")

            system_prompt = all_prompts.get("session_scheduler_system_prompt")
            human_prompt = all_prompts.get("session_scheduler_human_prompt")

            if not system_prompt or not human_prompt:
                log_error("tool_definitions", "_load_scheduler_prompts", "One or both scheduler prompts missing/empty in prompts.yaml.")
                system_prompt, human_prompt = None, None # Ensure both are None if one is missing

    except Exception as e:
        log_error("tool_definitions", "_load_scheduler_prompts", f"Failed to load scheduler prompts: {e}", e)
        system_prompt, human_prompt = None, None

    _SCHEDULER_PROMPTS_CACHE[cache_key] = (system_prompt, human_prompt)
    return system_prompt, human_prompt


# --- Tool Parameter Models ---

class CreateItemParams(BaseModel):
    description: str = Field(..., description="Detailed description or title of the task/reminder.")
    # Field 'date' is defined HERE
    date: str = Field(..., description="Due date or reminder date in YYYY-MM-DD format.")
    type: Literal["task", "reminder"] = Field(..., description="Specify 'task' for work items or 'reminder' for simple alerts.")
    # Field 'time' is defined HERE
    time: Optional[str] = Field(None, description="Optional time in HH:MM format (24-hour). Include for Reminders needing a GCal event.")
    estimated_duration: Optional[str] = Field(None, description="Optional estimated effort for 'task' types (e.g., '2h', '90m'). Needed for scheduling.")
    project: Optional[str] = Field(None, description="Optional project tag/label to associate.")

    # Validator for 'time' defined AFTER the field
    @field_validator('time')
    @classmethod
    def validate_time_format(cls, v: Optional[str]) -> Optional[str]:
        if v is None: return v
        try:
            hour, minute = map(int, v.split(':'))
            if not (0 <= hour <= 23 and 0 <= minute <= 59):
                raise ValueError("Time out of range")
            return v
        except (ValueError, AttributeError, TypeError):
             raise ValueError("Time must be in HH:MM format (e.g., '14:30') or null")

    # Validator for 'date' defined AFTER the field
    @field_validator('date')
    @classmethod
    def validate_date_format(cls, v: str) -> str:
        try:
            datetime.strptime(v, '%Y-%m-%d')
            return v
        except (ValueError, TypeError):
             raise ValueError("Date must be in YYYY-MM-DD format")

class UpdateItemDetailsParams(BaseModel):
    item_id: str = Field(..., description="The unique ID (event_id) of the task/reminder to update.")
    updates: Dict[str, Any] = Field(..., description="A dictionary containing *only* the fields to be changed and their new values. Allowed keys: description, date, time, estimated_duration, project.")

    @field_validator('updates')
    @classmethod
    def check_allowed_keys_and_formats(cls, v: Dict[str, Any]) -> Dict[str, Any]:
        allowed_keys = {"description", "date", "time", "estimated_duration", "project"}
        if not v: raise ValueError("Updates dictionary cannot be empty.")

        for key, value in v.items():
            if key not in allowed_keys:
                raise ValueError(f"Invalid key '{key}' in updates. Allowed keys: {allowed_keys}")

            # Validate formats within the updates dict
            if key == 'date' and value:
                try: datetime.strptime(value, '%Y-%m-%d')
                except (ValueError, TypeError): raise ValueError(f"Date '{value}' in updates must be YYYY-MM-DD")
            # --- REVERTED TIME VALIDATION ---
            elif key == 'time':
                if value is not None:
                    # Use the strict validator from CreateItemParams
                    try: CreateItemParams.validate_time_format(str(value))
                    except ValueError as time_err: raise ValueError(f"Time '{value}' in updates: {time_err}")
            # --- END REVERT ---
            # Add validation for estimated_duration format if needed here eventually

        return v # Return original dict, expects LLM to format correctly



class UpdateItemStatusParams(BaseModel):
    item_id: str = Field(..., description="The unique ID (event_id) of the item whose status needs updating.")
    new_status: Literal["pending", "in_progress", "completed", "cancelled"] = Field(..., description="The new status. 'cancelled' handles deletion.")


class UpdateUserPreferencesParams(BaseModel):
    updates: Dict[str, Any] = Field(..., description="A dictionary of preference keys and their new values (e.g., {'Work_End_Time': '17:30', 'Enable_Morning': False}).")

    @field_validator('updates')
    @classmethod
    def check_updates_not_empty(cls, v: Dict[str, Any]) -> Dict[str, Any]:
        if not v:
            raise ValueError("Updates dictionary cannot be empty.")
        return v


class InitiateCalendarConnectionParams(BaseModel):
    pass


class ProposeTaskSlotsParams(BaseModel):
    task_id: str = Field(..., description="The unique ID of the task to schedule sessions for.")
    number_of_slots: Optional[int] = Field(None, gt=0, description="Optional: Number of slots to propose. If None or >1, calculates based on estimate/prefs for split sessions. If 1, proposes a single block for the total estimated duration.")
    search_start_date: Optional[str] = Field(None, description="Optional: Start date for scheduling search (YYYY-MM-DD). Defaults to day after today.")
    search_end_date: Optional[str] = Field(None, description="Optional: End date for scheduling search (YYYY-MM-DD). Defaults to near task due date or scheduling horizon.")

    @field_validator('search_start_date', 'search_end_date')
    @classmethod
    def validate_date_format(cls, v: Optional[str]) -> Optional[str]:
        if v is None: return v
        try: datetime.strptime(v, '%Y-%m-%d'); return v
        except (ValueError, TypeError): raise ValueError("Date must be YYYY-MM-DD")

class BookTaskSlotsParams(BaseModel):
    task_id: str = Field(..., description="The unique ID of the parent task.")
    slots_to_book: List[Dict[str, Any]] = Field(..., min_length=1, description="Non-empty list of confirmed slot dictionaries, typically including 'date', 'time'.")


class CancelTaskSessionsParams(BaseModel):
    task_id: str = Field(..., description="The unique ID of the parent task.")
    session_ids_to_cancel: List[str] = Field(..., min_length=1, description="Non-empty list of specific Google Calendar event IDs for the sessions to cancel.")


class InterpretListReplyParams(BaseModel):
    user_reply: str = Field(..., description="The user's text reply following a numbered list.")
    list_mapping: Dict[int, str] = Field(..., description="The mapping from list number (int) to item_id (str) that was presented.")


class GetFormattedTaskListParams(BaseModel):
    date_range: Optional[List[str]] = Field(None, description="Optional start and end date as a list of two strings: ['YYYY-MM-DD', 'YYYY-MM-DD'].")
    status_filter: Literal['active', 'pending', 'in_progress', 'completed', 'all'] = Field('active', description="Filter by status.")
    project_filter: Optional[str] = Field(None, description="Optional project tag to filter by.")

    @field_validator('date_range')
    @classmethod
    def validate_date_range(cls, v: Optional[List[str]]) -> Optional[List[str]]:
        if v is None:
            return v
        if not isinstance(v, list) or len(v) != 2:
            raise ValueError("date_range must be a list containing exactly two date strings.")
        try:
            start_date = datetime.strptime(v[0], '%Y-%m-%d')
            end_date = datetime.strptime(v[1], '%Y-%m-%d')
            if start_date > end_date:
                raise ValueError("Start date cannot be after end date in date_range.")
            return v
        except (ValueError, TypeError):
            raise ValueError("Dates in date_range must be strings in YYYY-MM-DD format.")


# --- Tool Function Implementations ---

# 1. Create Item
def create_item_tool(user_id: str, params: CreateItemParams) -> Dict:
    """Creates metadata for a new Task/Reminder. If Reminder with time, also creates GCal event."""
    log_info("tool_definitions", "create_item_tool", f"Executing for user {user_id}, type: {params.type}")
    try:
        task_data_dict = params.model_dump(exclude_none=True)
        saved_metadata = task_manager.create_task(user_id, task_data_dict)

        if saved_metadata and saved_metadata.get("event_id"):
            return {
                "success": True,
                "item_id": saved_metadata.get("event_id"),
                "item_type": saved_metadata.get("type"),
                "estimated_duration": saved_metadata.get("estimated_duration"),
                "message": f"{params.type.capitalize()} '{params.description[:30]}...' created successfully."
            }
        else:
            log_error("tool_definitions", "create_item_tool", f"Task manager failed to create item for {user_id}")
            return {"success": False, "item_id": None, "message": f"Failed to create {params.type}."}
    except ValidationError as e:
        # This catch block might be redundant if orchestrator catches validation errors, but safe to keep
        log_error("tool_definitions", "create_item_tool", f"Validation Error during tool execution: {e}", e)
        return {"success": False, "item_id": None, "message": f"Invalid parameters provided for creation: {e}"}
    except Exception as e:
        log_error("tool_definitions", "create_item_tool", f"Error: {e}", e)
        return {"success": False, "item_id": None, "message": f"An error occurred while creating the {params.type}."}


# 2. Update Item Details
def update_item_details_tool(user_id: str, params: UpdateItemDetailsParams) -> Dict:
    """Updates core metadata fields (description, project, due date/time, estimate) for a Task or Reminder. Updates GCal Reminder event time/date if applicable."""
    log_info("tool_definitions", "update_item_details_tool", f"Executing for user {user_id}, item_id: {params.item_id}")
    try:
        updated_metadata = task_manager.update_task(user_id, params.item_id, params.updates)
        if updated_metadata:
            return {"success": True, "message": f"Item {params.item_id[:8]}... details updated."}
        else:
            # Service layer should log reason for failure (not found vs other error)
            return {"success": False, "message": f"Failed update details for {params.item_id[:8]}.... Item not found or user mismatch."}
    except ValidationError as e:
        log_error("tool_definitions", "update_item_details_tool", f"Validation Error during tool execution: {e}", e)
        return {"success": False, "message": f"Invalid parameters provided for update: {e}"}
    except Exception as e:
        log_error("tool_definitions", "update_item_details_tool", f"Error: {e}", e)
        return {"success": False, "message": f"An error occurred while updating details for {params.item_id[:8]}...."}


# 3. Update Item Status
def update_item_status_tool(user_id: str, params: UpdateItemStatusParams) -> Dict:
    """Updates the status of any item (Task, Reminder). Handles cancellation/deletion via task_manager."""
    log_info("tool_definitions", "update_item_status_tool", f"Executing for user {user_id}, item_id: {params.item_id}, status: {params.new_status}")
    try:
        success = False
        message = ""
        if params.new_status == "cancelled":
            log_info("tool_definitions", "update_item_status_tool", f"Calling cancel_item service for {params.item_id}")
            success = task_manager.cancel_item(user_id, params.item_id)
            message = f"Item {params.item_id[:8]}... cancellation {'processed' if success else 'failed'}. Item may not exist or user mismatch."
        else:
            log_info("tool_definitions", "update_item_status_tool", f"Calling update_task_status service for {params.item_id}")
            updated_meta = task_manager.update_task_status(user_id, params.item_id, params.new_status)
            success = updated_meta is not None
            message = f"Status update to '{params.new_status}' for {params.item_id[:8]}... {'succeeded' if success else 'failed'}. Item may not exist or user mismatch."
        return {"success": success, "message": message}
    except ValidationError as e:
        log_error("tool_definitions", "update_item_status_tool", f"Validation Error during tool execution: {e}", e)
        return {"success": False, "message": f"Invalid parameters provided for status update: {e}"}
    except Exception as e:
        log_error("tool_definitions", "update_item_status_tool", f"Error: {e}", e)
        return {"success": False, "message": f"An error occurred while updating status for {params.item_id[:8]}...."}


# 4. Update User Preferences
def update_user_preferences_tool(user_id: str, params: UpdateUserPreferencesParams) -> Dict:
    """Updates user settings."""
    log_info("tool_definitions", "update_user_preferences_tool", f"Executing for user {user_id}, updates: {list(params.updates.keys())}")
    try:
        # Validation for empty dict moved to Pydantic model if using v2 features,
        # otherwise assert or check here. Keeping check for clarity.
        if not params.updates:
            return {"success": False, "message": "No preferences provided to update."}
        success = config_manager.update_preferences(user_id, params.updates)
        return {"success": success, "message": f"Preferences update {'succeeded' if success else 'failed'}."}
    except ValidationError as e:
        log_error("tool_definitions", "update_user_preferences_tool", f"Validation Error during tool execution: {e}", e)
        return {"success": False, "message": f"Invalid parameters provided for preference update: {e}"}
    except Exception as e:
        log_error("tool_definitions", "update_user_preferences_tool", f"Error: {e}", e)
        return {"success": False, "message": "An error occurred updating preferences."}


# 5. Initiate Calendar Connection
def initiate_calendar_connection_tool(user_id: str, params: InitiateCalendarConnectionParams) -> Dict:
    """Starts Google Calendar OAuth flow."""
    log_info("tool_definitions", "initiate_calendar_connection_tool", f"Executing for user {user_id}")
    try:
        # No params to validate beyond Pydantic handling empty model
        result = config_manager.initiate_calendar_auth(user_id)
        return result # Returns dict with status/message/URL
    except ValidationError as e:
        # Should not happen if Pydantic model is just 'pass'
        log_error("tool_definitions", "initiate_calendar_connection_tool", f"Unexpected Validation Error: {e}", e)
        return {"status": "fails", "message": f"Internal validation error: {e}"}
    except Exception as e:
        log_error("tool_definitions", "initiate_calendar_connection_tool", f"Error: {e}", e)
        return {"status": "fails", "message": "An error occurred starting calendar connection."}


# 6. Propose Task Slots
def propose_task_slots_tool(user_id: str, params: ProposeTaskSlotsParams) -> Dict:
    """
    Finds and proposes N work session slots for a specific Task using an LLM.
    Can search within a specified date range and propose either split sessions
    based on user preference or a single block for the total duration.
    """
    log_info("tool_definitions", "propose_task_slots_tool", f"Executing for user {user_id}, task_id: {params.task_id}, num_slots: {params.number_of_slots}, range: {params.search_start_date}-{params.search_end_date}")

    # --- Dependencies ---
    llm_client = get_instructor_client();
    if not llm_client:
        log_error("tool_definitions", "propose_task_slots_tool", "LLM client unavailable.")
        return {"success": False, "proposed_slots": None, "message": "Scheduler resources unavailable."}

    sys_prompt, human_prompt = _load_scheduler_prompts()
    if not sys_prompt or not human_prompt:
        log_error("tool_definitions", "propose_task_slots_tool", "Scheduler prompts failed load.")
        return {"success": False, "proposed_slots": None, "message": "Scheduler resources unavailable."}

    # --- Fetch Task Metadata ---
    task_meta = metadata_store.get_event_metadata(params.task_id)
    if not task_meta or task_meta.get("user_id") != user_id:
        log_warning("tool_definitions", "propose_task_slots_tool", f"Task {params.task_id[:8]} not found or user mismatch.")
        return {"success": False, "proposed_slots": None, "message": f"Task details not found for ID {params.task_id[:8]}..."}
    if task_meta.get("type") != "task":
        log_warning("tool_definitions", "propose_task_slots_tool", f"Item {params.task_id[:8]} is not a task.")
        return {"success": False, "proposed_slots": None, "message": "Scheduling is only supported for items of type 'task'."}
    task_estimated_duration_str = task_meta.get("estimated_duration")
    if not task_estimated_duration_str:
        log_warning("tool_definitions", "propose_task_slots_tool", f"Task {params.task_id[:8]} has no estimated duration.")
        return {"success": False, "proposed_slots": None, "message": "Cannot schedule task without an estimated duration."}

    # --- Fetch Preferences and Calendar API ---
    agent_state = get_agent_state(user_id); prefs = agent_state.get("preferences", {}) if agent_state else {}
    calendar_api = _get_calendar_api_from_state(user_id)
    preferred_session_str = prefs.get("Preferred_Session_Length", "60m")

    # --- Determine Slot Calculation Logic ---
    num_slots_to_find: int
    slot_duration_str: str
    is_continuous_block = (params.number_of_slots == 1)

    if is_continuous_block:
        num_slots_to_find = 1
        slot_duration_str = task_estimated_duration_str # Use total estimate for the single block
        log_info("tool_definitions", "propose_task_slots_tool", f"Requesting 1 continuous block of duration: {slot_duration_str}")
    else:
        total_minutes = task_manager._parse_duration_to_minutes(task_estimated_duration_str)
        session_minutes = task_manager._parse_duration_to_minutes(preferred_session_str)
        if params.number_of_slots is not None: num_slots_to_find = params.number_of_slots
        elif total_minutes and session_minutes and session_minutes > 0: num_slots_to_find = (total_minutes + session_minutes - 1) // session_minutes
        else: log_warning("...", "Could not calc split slots. Defaulting to 1."); num_slots_to_find = 1
        slot_duration_str = preferred_session_str
        log_info("tool_definitions", "propose_task_slots_tool", f"Requesting {num_slots_to_find} split sessions of duration: {slot_duration_str}")

    # --- Determine Search Date Range ---
    today = datetime.now().date()
    start_date = today + timedelta(days=1)
    if params.search_start_date:
        try: start_date = max(start_date, datetime.strptime(params.search_start_date, "%Y-%m-%d").date())
        except ValueError: log_warning("...", f"Invalid search_start_date '{params.search_start_date}', using default.")
    end_date = today + timedelta(days=56) # Default horizon
    if params.search_end_date:
        try: end_date = min(end_date, datetime.strptime(params.search_end_date, "%Y-%m-%d").date())
        except ValueError: log_warning("...", f"Invalid search_end_date '{params.search_end_date}', using default horizon.")
    due_str = task_meta.get("date")
    if due_str:
        try: due = datetime.strptime(due_str, "%Y-%m-%d").date(); buffer_days = 2; eff_due = due - timedelta(days=buffer_days); end_date = min(end_date, eff_due)
        except ValueError: log_warning("...", f"Invalid due date format '{due_str}' on task {params.task_id}");
    end_date = max(end_date, start_date) # Ensure end is not before start
    start_date_str = start_date.strftime("%Y-%m-%d")
    end_date_str = end_date.strftime("%Y-%m-%d")

    # --- Fetch Existing Calendar Events ---
    existing_events = []
    if calendar_api:
        log_info("tool_definitions", "propose_task_slots_tool", f"Fetching GCal events from {start_date_str} to {end_date_str}")
        if start_date_str <= end_date_str:
            try:
                events_raw = calendar_api.list_events(start_date_str, end_date_str)
                existing_events = [{"start_datetime": ev.get("start_datetime"), "end_datetime": ev.get("end_datetime"), "summary": ev.get("title")} for ev in events_raw if ev.get("start_datetime") and ev.get("end_datetime")]
                # *** ADDED LOGGING FOR DETAILED EVENTS ***
                log_info("tool_definitions", "propose_task_slots_tool", f"Fetched {len(existing_events)} simplified GCal events: {json.dumps(existing_events)}")
                # ****************************************
            except Exception as e: log_error("...", f"Failed fetch existing events: {e}", e)
        else: log_warning("...", f"Final search date range {start_date_str} to {end_date_str} is invalid. Skipping GCal fetch.")
    else: log_warning("...", "GCal API inactive, cannot fetch events.")

    # --- Prepare Prompt Data ---
    try:
        prompt_data = {
            "task_description": task_meta.get("description", task_meta.get("title", "")),
            "task_due_date": task_meta.get("date"),
            "task_estimated_duration": task_estimated_duration_str,
            "user_working_days": prefs.get("Work_Days", ["Monday", "Tuesday", "Wednesday", "Thursday", "Friday"]),
            "user_work_start_time": prefs.get("Work_Start_Time", "09:00"),
            "user_work_end_time": prefs.get("Work_End_Time", "17:00"),
            "user_session_length": slot_duration_str, # Use the calculated/determined slot duration
            "existing_events_json": json.dumps(existing_events),
            "current_date": datetime.now().strftime("%Y-%m-%d"),
            "num_slots_requested": num_slots_to_find
        }
        # *** ADDED LOGGING FOR PROMPT DATA ***
        log_info("tool_definitions", "propose_task_slots_tool", f"Data for scheduler prompt: {json.dumps(prompt_data)}")
        # ************************************
    except Exception as e: log_error("...", f"Error preparing prompt data: {e}", e); return {"success": False, "proposed_slots": None, "message": "Failed prepare data for scheduler."}

    # --- Invoke Scheduler LLM ---
    raw_llm_output = None
    try:
        fmt_human = human_prompt.format(**prompt_data); messages = [{"role": "system", "content": sys_prompt}, {"role": "user", "content": fmt_human}]
        log_info("tool_definitions", "propose_task_slots_tool", ">>> Invoking Session Scheduler LLM...") # Keep short name for log clarity
        sched_resp: ChatCompletionMessage = llm_client.chat.completions.create(model="gpt-4o", messages=messages, temperature=0.2, response_format={"type": "json_object"})
        raw_llm_output = sched_resp.choices[0].message.content; log_info("...", f"<<< Scheduler LLM Raw: {raw_llm_output[:300]}...") # Log more raw output
        parsed = _parse_scheduler_llm_response(raw_llm_output);
        if not parsed:
             log_error("...", "Scheduler LLM response parsing failed.")
             return {"success": False, "proposed_slots": None, "message": "Scheduler failed to provide valid proposals."}
        # *** ADDED LOGGING FOR PARSED RESULT ***
        log_info("tool_definitions", "propose_task_slots_tool", f"Parsed scheduler result: {json.dumps(parsed)}")
        # **************************************
        return {"success": True, "proposed_slots": parsed.get("proposed_sessions"), "message": parsed.get("response_message", "Here are proposed slots:")}
    except ValidationError as e: # Catch validation errors if any slip through
         log_error("...", f"Validation Error during tool execution: {e}", e)
         return {"success": False, "proposed_slots": None, "message": f"Invalid parameters for proposing slots: {e}"}
    except Exception as e: log_error("...", f"Error invoking/processing Scheduler LLM. Error: {e}. Raw: {raw_llm_output}", e); return {"success": False, "proposed_slots": None, "message": "Error finding schedule slots."}


def book_task_slots_tool(user_id: str, params: BookTaskSlotsParams) -> Dict:
    """Creates GCal events for user-confirmed work sessions and links them to the parent task."""
    log_info("tool_definitions", "book_task_slots_tool", f"Executing for user {user_id}, task_id: {params.task_id}, slots: {len(params.slots_to_book)}")
    try:
        # Call the task manager function
        result = task_manager.schedule_work_sessions(user_id, params.task_id, params.slots_to_book)

        # Initialize booked_count
        booked_count = 0

        # Try to parse the count from the result message if successful
        if result.get("success"):
            try:
                # Use regex to find the number of scheduled sessions in the message
                match = re.search(r'scheduled\s+(\d+)\s+work\s+session', result.get("message", ""))
                if match:
                    booked_count = int(match.group(1))
                else:
                    # Fallback if regex doesn't match: use the length of the input list
                    booked_count = len(params.slots_to_book)
            except (AttributeError, ValueError, TypeError):
                # Handle potential errors during regex/parsing, fallback to input length
                 log_warning("tool_definitions", "book_task_slots_tool", f"Could not parse booked count from message '{result.get('message', '')}', using input length.")
                 booked_count = len(params.slots_to_book)
        # If result failed, booked_count remains 0

        # Combine the original result with the parsed/fallback booked_count
        return {**result, "booked_count": booked_count}

    except ValidationError as e: # Catch Pydantic validation errors specifically
        log_error("tool_definitions", "book_task_slots_tool", f"Validation Error: {e}", e)
        return {"success": False, "booked_count": 0, "message": f"Invalid parameters provided for booking: {e}"}
    except Exception as e: # Catch other potential errors
        log_error("tool_definitions", "book_task_slots_tool", f"Error: {e}", e)
        return {"success": False, "booked_count": 0, "message": "An error occurred while booking sessions."}


# 8. Cancel Task Sessions
def cancel_task_sessions_tool(user_id: str, params: CancelTaskSessionsParams) -> Dict:
    """Deletes specific GCal work session events and unlinks them from the parent task metadata."""
    log_info("tool_definitions", "cancel_task_sessions_tool", f"Executing for user {user_id}, task_id: {params.task_id}, sessions: {params.session_ids_to_cancel}")
    try:
        # Call the task_manager function
        result = task_manager.cancel_sessions(user_id, params.task_id, params.session_ids_to_cancel)
        # Result should already contain success, cancelled_count, message
        return result
    except ValidationError as e:
        log_error("tool_definitions", "cancel_task_sessions_tool", f"Validation Error: {e}", e)
        return {"success": False, "cancelled_count": 0, "message": f"Invalid parameters provided for cancelling sessions: {e}"}
    except Exception as e:
        log_error("tool_definitions", "cancel_task_sessions_tool", f"Error: {e}", e)
        return {"success": False, "cancelled_count": 0, "message": "An error occurred while cancelling sessions."}


# 9. Interpret List Reply
def interpret_list_reply_tool(user_id: str, params: InterpretListReplyParams) -> Dict:
    """(Phase 2 Placeholder) Parses user replies to numbered lists (complete/cancel)."""
    log_warning("tool_definitions", "interpret_list_reply_tool", "Not implemented (Phase 2).")
    # TODO: Implement Phase 2 logic using LLM/NLU -> update_item_status_tool calls
    return {"success": False, "message": "Sorry, replying to lists by number is not implemented yet."}


# 10. Get Formatted Task List
def get_formatted_task_list_tool(user_id: str, params: GetFormattedTaskListParams) -> Dict:
    """Fetches/formats a numbered list of tasks/reminders based on filters."""
    log_info("tool_definitions", "get_formatted_task_list_tool", f"Executing for user {user_id}, filter={params.status_filter}, project={params.project_filter}, range={params.date_range}")
    try:
        # Convert List[str] back to Tuple[str, str] if the service layer expects that exact type
        date_range_tuple: Optional[Tuple[str, str]] = None
        if params.date_range:
             date_range_tuple = (params.date_range[0], params.date_range[1])

        list_body, list_mapping = task_query_service.get_formatted_list(
            user_id=user_id,
            date_range=date_range_tuple, # Pass as tuple if needed by service
            status_filter=params.status_filter,
            project_filter=params.project_filter
        )

        count = len(list_mapping)
        message = "List retrieved successfully."
        if count == 0:
            message = "No items found matching your criteria."
        elif not list_body: # Should not happen if count > 0, but safety check
            message = "Could not retrieve list content."

        return {
            "list_body": list_body,
            "list_mapping": list_mapping,
            "count": count,
            "message": message
        }
    except ValidationError as e:
        log_error("tool_definitions", "get_formatted_task_list_tool", f"Validation Error: {e}", e)
        return {"list_body": "", "list_mapping": {}, "count": 0, "message": f"Invalid parameters provided for getting list: {e}"}
    except Exception as e:
        log_error("tool_definitions", "get_formatted_task_list_tool", f"Error: {e}", e)
        return {"list_body": "", "list_mapping": {}, "count": 0, "message": "An error occurred while getting the list."}


# --- Tool Dictionaries ---
AVAILABLE_TOOLS = {
    "create_item": create_item_tool,
    "update_item_details": update_item_details_tool,
    "update_item_status": update_item_status_tool,
    "update_user_preferences": update_user_preferences_tool,
    "initiate_calendar_connection": initiate_calendar_connection_tool,
    # --- Phase 2 Implemented ---
    "propose_task_slots": propose_task_slots_tool,
    "book_task_slots": book_task_slots_tool,
    "cancel_task_sessions": cancel_task_sessions_tool, # Assumes task_manager.cancel_sessions exists
    # --- Phase 2 Placeholders ---
    "interpret_list_reply": interpret_list_reply_tool,
    "get_formatted_task_list": get_formatted_task_list_tool,
}

TOOL_PARAM_MODELS = {
    "create_item": CreateItemParams,
    "update_item_details": UpdateItemDetailsParams,
    "update_item_status": UpdateItemStatusParams,
    "update_user_preferences": UpdateUserPreferencesParams,
    "initiate_calendar_connection": InitiateCalendarConnectionParams,
    # --- Phase 2 ---
    "propose_task_slots": ProposeTaskSlotsParams,
    "book_task_slots": BookTaskSlotsParams,
    "cancel_task_sessions": CancelTaskSessionsParams,
    "interpret_list_reply": InterpretListReplyParams,
    "get_formatted_task_list": GetFormattedTaskListParams,
}
# --- END OF FILE agents/tool_definitions.py ---
# --- END OF FILE agents\tool_definitions.py ---


================================================================================
üìÑ services\task_manager.py
================================================================================

# --- START OF FILE services\task_manager.py ---
# --- START OF FILE services/task_manager.py ---
"""
Service layer for managing tasks: creating, updating, cancelling, and scheduling sessions.
Interacts with Google Calendar API and the Metadata Store.
"""
import json
import traceback
import uuid # Import uuid for generating local IDs
from typing import Dict, Any, List, Optional
from datetime import datetime, timedelta
import re # Import re for duration parsing

# --- Tool/Service Imports ---
try:
    from tools.logger import log_info, log_error, log_warning
except ImportError: # Fallback logger
    import logging
    logging.basicConfig(level=logging.INFO)
    log_info = logging.info; log_error = logging.error; log_warning = logging.warning
    log_error("task_manager", "import", "Primary logger import failed.")

try:
    from tools.google_calendar_api import GoogleCalendarAPI # To interact with GCal
except ImportError:
    log_error("task_manager", "import", "GoogleCalendarAPI not found.")
    GoogleCalendarAPI = None # Define as None if import fails

try:
    from tools import metadata_store # To save task details
    METADATA_STORE_IMPORTED = True
except ImportError:
    log_error("task_manager", "import", "metadata_store not found.")
    METADATA_STORE_IMPORTED = False
    # Define dummy metadata_store for basic operation if needed
    class MockMetadataStore:
        _data = {}
        FIELDNAMES = ["event_id", "user_id", "type", "status", "title", "description", "date", "time", "estimated_duration", "child_session_ids", "sessions_planned", "created_at", "completed_at", "project", "original_date", "duration", "progress", "progress_percent", "internal_reminder_minutes", "internal_reminder_sent", "sessions_completed", "series_id"]
        def init_metadata_store(self): pass
        def save_event_metadata(self, data): self._data[data['event_id']] = {k: data.get(k) for k in self.FIELDNAMES}
        def get_event_metadata(self, event_id): return self._data.get(event_id)
        def delete_event_metadata(self, event_id): return self._data.pop(event_id, None) is not None
        def list_metadata(self, user_id, **kwargs): return [v for v in self._data.values() if v.get("user_id") == user_id]
        def load_all_metadata(self): return list(self._data.values())
    metadata_store = MockMetadataStore()
    metadata_store.init_metadata_store() # Initialize dummy store

try:
    from services.agent_state_manager import get_agent_state, add_task_to_context, update_task_in_context, remove_task_from_context
    AGENT_STATE_IMPORTED = True
except ImportError:
    log_error("task_manager", "import", "AgentStateManager not found.")
    AGENT_STATE_IMPORTED = False
    # Define dummy functions if needed
    def get_agent_state(*args, **kwargs): return None
    def add_task_to_context(*args, **kwargs): pass
    def update_task_in_context(*args, **kwargs): pass
    def remove_task_from_context(*args, **kwargs): pass

# --- Constants ---
GENERIC_ERROR_MSG = "Sorry, an unexpected error occurred while managing the task."
DEFAULT_REMINDER_DURATION = "15m" # Duration for GCal reminder events

# --- Helper Functions ---
def _get_calendar_api(user_id: str) -> any:
    """Helper to get the calendar API instance from agent state."""
    if not AGENT_STATE_IMPORTED or GoogleCalendarAPI is None: return None
    agent_state = get_agent_state(user_id)
    if agent_state:
        calendar_api = agent_state.get("calendar")
        if isinstance(calendar_api, GoogleCalendarAPI) and calendar_api.is_active():
            return calendar_api
    return None

def _parse_duration_to_minutes(duration_str: Optional[str]) -> Optional[int]:
    """Parses duration strings like '2h', '90m', '1.5h', '1h 30m' into minutes."""
    if not duration_str:
        return None
    duration_str = str(duration_str).lower().replace(' ','')
    total_minutes = 0
    try:
        hours_match = re.search(r'(\d+(\.\d+)?)\s*h', duration_str)
        minutes_match = re.search(r'(\d+)\s*m', duration_str)

        if hours_match:
            total_minutes += float(hours_match.group(1)) * 60
        # Only add minutes if they weren't part of an hour specification (e.g. avoid double counting in "1h30m")
        if minutes_match:
            # Check if the matched minutes are directly after the hours (like in 1h30m)
            hm_match = re.search(r'(\d+(\.\d+)?)h(\d+)m', duration_str)
            if not hm_match or hm_match.group(3) != minutes_match.group(1):
                 total_minutes += int(minutes_match.group(1))

        # Handle cases like "1h30m" where the regex might pick up h and m separately initially
        hm_direct_match = re.match(r'(\d+(\.\d+)?)h(\d+)m$', duration_str)
        if hm_direct_match:
             total_minutes = float(hm_direct_match.group(1)) * 60 + int(hm_direct_match.group(3))

        # If no units found, and it's just a number, assume minutes (use with caution)
        if total_minutes == 0 and duration_str.replace('.','',1).isdigit():
            log_warning("task_manager", "_parse_duration_to_minutes", f"Assuming minutes for unitless duration: {duration_str}")
            total_minutes = int(float(duration_str))

        return int(round(total_minutes)) if total_minutes > 0 else None
    except Exception as e: # Catch broader errors during parsing
        log_warning("task_manager", "_parse_duration_to_minutes", f"Could not parse duration '{duration_str}': {e}")
        return None

# ==============================================================
# Core Service Functions
# ==============================================================

def create_task(user_id: str, task_params: Dict) -> Optional[Dict]:
    """
    Creates metadata for a task/reminder. Creates GCal event ONLY for Reminders with time.
    Updates the in-memory context.

    Args:
        user_id: The user's identifier.
        task_params: Dictionary from Tool containing details like
                     'description', 'date', 'time', 'type', 'estimated_duration', etc.

    Returns:
        The saved metadata dictionary on success, None on failure.
    """
    item_type = task_params.get("type")
    if not item_type:
         log_error("task_manager", "create_task", "Missing 'type' in task_params.")
         return None
    log_info("task_manager", "create_task", f"Creating item for {user_id}, type: {item_type}")

    if not METADATA_STORE_IMPORTED:
        log_error("task_manager", "create_task", "Metadata store unavailable.")
        return None

    calendar_api = _get_calendar_api(user_id)
    google_event_id = None

    try:
        # --- 1. Prepare Metadata ---
        metadata_payload = {k: task_params.get(k) for k in metadata_store.FIELDNAMES if k in task_params}
        metadata_payload["user_id"] = user_id
        metadata_payload["status"] = "pending"
        metadata_payload["created_at"] = datetime.now().isoformat()
        metadata_payload["title"] = task_params.get("description", "Untitled Item")
        metadata_payload["type"] = item_type

        if item_type == "task":
             metadata_payload["child_session_ids"] = json.dumps([])
             metadata_payload["sessions_planned"] = 0
             metadata_payload["sessions_completed"] = 0 # Initialize completed
             metadata_payload["progress_percent"] = 0 # Initialize progress

        # --- 2. Create Google Calendar Event (ONLY for Reminders with time/API active) ---
        if item_type == "reminder" and task_params.get("time") and calendar_api:
            log_info("task_manager", "create_task", f"Attempting GCal Reminder event creation for {user_id}")
            gcal_event_data = {
                "title": metadata_payload.get("title"),
                "description": f"Reminder via WhatsTasker: {metadata_payload.get('description', '')}",
                "date": task_params.get("date"),
                "time": task_params.get("time"),
                "duration": DEFAULT_REMINDER_DURATION
            }
            google_event_id = calendar_api.create_event(gcal_event_data)
            if google_event_id:
                log_info("task_manager", "create_task", f"GCal Reminder event created with ID: {google_event_id}")
                metadata_payload["event_id"] = google_event_id # Use GCal ID
            else:
                log_warning("task_manager", "create_task", f"Failed to create GCal Reminder event for {user_id}. Assigning local ID.")
                metadata_payload["event_id"] = f"local_{uuid.uuid4()}"
        else:
            log_info("task_manager", "create_task", f"Assigning local ID for {item_type} (No GCal event created for this type/config).")
            metadata_payload["event_id"] = f"local_{uuid.uuid4()}"

        # --- 3. Save Metadata ---
        if not metadata_payload.get("event_id"):
             log_error("task_manager", "create_task", "Metadata payload missing 'event_id' before save.")
             return None

        required_meta = ["user_id", "event_id", "type", "status", "date", "title"]
        missing = [k for k in required_meta if not metadata_payload.get(k)]
        if missing:
             log_error("task_manager", "create_task", f"Metadata payload missing required fields: {missing}")
             if google_event_id and calendar_api:
                  log_warning("task_manager", "create_task", f"Rolling back GCal event {google_event_id} due to metadata save failure.")
                  calendar_api.delete_event(google_event_id)
             return None

        # Ensure all FIELDNAMES are present, filling with None if missing
        final_metadata_to_save = {fn: metadata_payload.get(fn) for fn in metadata_store.FIELDNAMES}

        metadata_store.save_event_metadata(final_metadata_to_save)
        log_info("task_manager", "create_task", f"Metadata saved successfully for item {final_metadata_to_save['event_id']}")

        # --- 4. Update In-Memory Context ---
        if AGENT_STATE_IMPORTED:
             full_task_data = final_metadata_to_save
             if google_event_id and calendar_api: # Fetch GCal details if Reminder event was created
                 try:
                     gcal_event = calendar_api._get_single_event(google_event_id) # Assumes internal fetch method exists
                     if gcal_event:
                          parsed_gcal = calendar_api._parse_google_event(gcal_event)
                          # Merge, letting saved metadata overwrite GCal fields like title/desc
                          full_task_data = {**parsed_gcal, **final_metadata_to_save}
                 except Exception as fetch_err:
                     log_warning("task_manager", "create_task", f"Failed fetch GCal details post-creation: {fetch_err}")

             add_task_to_context(user_id, full_task_data)

        return final_metadata_to_save

    except Exception as e:
        tb_str = traceback.format_exc()
        log_error("task_manager", "create_task", f"Error creating item for {user_id}. Traceback:\n{tb_str}", e)
        if google_event_id and calendar_api:
             try: calendar_api.delete_event(google_event_id)
             except: pass
        return None


def update_task(user_id: str, item_id: str, updates: Dict) -> Optional[Dict]:
    """
    Updates core task/reminder details (description, date, time, estimate, project)
    in metadata store and GCal (ONLY if it's a Reminder event).
    Updates the in-memory context. Does NOT update status.

    Args:
        user_id: The user's identifier.
        item_id: The unique ID of the task/event.
        updates: Dictionary of fields to update (allowed: description, date, time, estimated_duration, project).

    Returns:
        The updated metadata dictionary on success, None on failure.
    """
    log_info("task_manager", "update_task", f"Updating details for item {item_id} for {user_id} with keys: {list(updates.keys())}")
    if not METADATA_STORE_IMPORTED: return None

    calendar_api = _get_calendar_api(user_id)
    gcal_updated = False

    try:
        # --- 1. Get Existing Metadata ---
        existing_metadata = metadata_store.get_event_metadata(item_id)
        if not existing_metadata:
            log_error("task_manager", "update_task", f"Metadata for item {item_id} not found.")
            return None
        if existing_metadata.get("user_id") != user_id:
             log_error("task_manager", "update_task", f"User mismatch for item {item_id}.")
             return None

        item_type = existing_metadata.get("type")

        # --- 2. Update Google Calendar Reminder Event (if applicable) ---
        needs_gcal_update = False
        gcal_update_payload = {}
        if item_type == "reminder" and not item_id.startswith("local_") and calendar_api:
            if "description" in updates:
                gcal_update_payload["title"] = updates["description"]
                gcal_update_payload["description"] = f"Reminder via WhatsTasker: {updates['description']}" # Update desc too
                needs_gcal_update = True
            if "date" in updates or "time" in updates:
                new_date = updates.get("date", existing_metadata.get("date"))
                new_time = updates.get("time", existing_metadata.get("time"))
                if new_time == "": new_time = None # Allow clearing time

                gcal_update_payload["date"] = new_date
                gcal_update_payload["time"] = new_time
                # Assuming GCalAPI update_event handles duration from DEFAULT_REMINDER_DURATION
                needs_gcal_update = True

            if needs_gcal_update:
                 log_info("task_manager", "update_task", f"Attempting GCal Reminder update for {item_id}")
                 gcal_updated = calendar_api.update_event(item_id, gcal_update_payload)
                 if not gcal_updated:
                     log_warning("task_manager", "update_task", f"GCal Reminder update failed for {item_id}. Proceeding.")
                 else:
                     log_info("task_manager", "update_task", f"GCal Reminder update successful for {item_id}")
            else:
                 log_info("task_manager", "update_task", "No relevant GCal fields to update for this Reminder.")
        # Log reasons for skipping GCal update
        elif item_type == "task": log_info("task_manager", "update_task", f"Skipping GCal update for item {item_id} (type: task).")
        elif item_id.startswith("local_"): log_info("task_manager", "update_task", f"Skipping GCal update for local-only item {item_id}")
        else: log_info("task_manager", "update_task", f"Skipping GCal update for Reminder {item_id} (calendar inactive).")


        # --- 3. Update Metadata ---
        allowed_keys = {"description", "date", "time", "estimated_duration", "project"}
        valid_updates = {k: v for k, v in updates.items() if k in allowed_keys}

        if not valid_updates:
             log_warning("task_manager", "update_task", f"No valid detail fields found in updates for {item_id}. No metadata change.")
             return existing_metadata

        updated_metadata = existing_metadata.copy()
        updated_metadata.update(valid_updates)
        if "description" in valid_updates:
            updated_metadata["title"] = valid_updates["description"]

        # Ensure all FIELDNAMES are present
        metadata_to_save = {fn: updated_metadata.get(fn) for fn in metadata_store.FIELDNAMES}

        metadata_store.save_event_metadata(metadata_to_save)
        log_info("task_manager", "update_task", f"Metadata updated successfully for {item_id}")

        # --- 4. Update In-Memory Context ---
        if AGENT_STATE_IMPORTED:
             full_task_data = metadata_to_save
             # If GCal was potentially updated, try to fetch fresh GCal data
             if item_type == "reminder" and needs_gcal_update and gcal_updated and calendar_api:
                  try:
                      gcal_event = calendar_api._get_single_event(item_id)
                      if gcal_event:
                          parsed_gcal = calendar_api._parse_google_event(gcal_event)
                          full_task_data = {**parsed_gcal, **metadata_to_save}
                  except Exception as fetch_err:
                     log_warning("task_manager", "update_task", f"Failed fetch GCal details after update: {fetch_err}")

             update_task_in_context(user_id, item_id, full_task_data)

        return metadata_to_save

    except Exception as e:
        tb_str = traceback.format_exc()
        log_error("task_manager", "update_task", f"Error updating item {item_id} for {user_id}. Traceback:\n{tb_str}", e)
        return None


def update_task_status(user_id: str, item_id: str, new_status: str) -> Optional[Dict]:
    """
    Updates ONLY the status of a task/reminder in metadata.
    Handles 'completed' timestamp. DOES NOT handle 'cancelled'.

    Returns: Updated metadata dict or None if failed/not found.
    """
    log_info("task_manager", "update_task_status", f"Setting status='{new_status}' for item {item_id}")
    if new_status == "cancelled":
        log_error("task_manager", "update_task_status", "Use cancel_item() function for 'cancelled' status.")
        return None

    try:
        existing_metadata = metadata_store.get_event_metadata(item_id)
        if not existing_metadata:
             log_error("task_manager", "update_task_status", f"Metadata not found for item {item_id}.")
             return None
        if existing_metadata.get("user_id") != user_id:
            log_error("task_manager", "update_task_status", f"User mismatch for item {item_id}.")
            return None

        updates = {"status": new_status}
        if new_status.lower() == "completed":
            updates["completed_at"] = datetime.now().isoformat()
            updates["progress_percent"] = 100 # Mark 100% on completion
            # Optionally update sessions_completed? Depends on workflow.
        elif new_status.lower() in ["pending", "in_progress"]:
            updates["completed_at"] = None # Clear completed time if reverting status
            # Optionally reset progress?
            if new_status.lower() == "pending":
                 updates["progress_percent"] = 0

        updated_metadata = existing_metadata.copy()
        updated_metadata.update(updates)

        # Ensure all fieldnames are present
        metadata_to_save = {fn: updated_metadata.get(fn) for fn in metadata_store.FIELDNAMES}

        metadata_store.save_event_metadata(metadata_to_save)
        log_info("task_manager", "update_task_status", f"Metadata status updated successfully for {item_id}")

        # Update in-memory context
        if AGENT_STATE_IMPORTED:
             update_task_in_context(user_id, item_id, metadata_to_save)

        return metadata_to_save

    except Exception as e:
         log_error("task_manager", "update_task_status", f"Error saving metadata status update for {item_id}", e)
         return None

def cancel_item(user_id: str, item_id: str) -> bool:
    """
    Handles cancellation: Deletes associated GCal events (Reminder or Task sessions)
    and sets metadata status to 'cancelled'.

    Returns: True if cancellation processed successfully, False otherwise.
    """
    log_info("task_manager", "cancel_item", f"Processing cancellation for item {item_id} for user {user_id}")
    if not METADATA_STORE_IMPORTED: return False

    calendar_api = _get_calendar_api(user_id)
    gcal_cleanup_success = True # Assume success if no GCal action needed

    try:
        # --- 1. Get Metadata ---
        metadata = metadata_store.get_event_metadata(item_id)
        if not metadata:
            log_warning("task_manager", "cancel_item", f"Metadata not found for item {item_id}. Cannot cancel.")
            return False # Indicate item wasn't found to cancel
        if metadata.get("user_id") != user_id:
            log_error("task_manager", "cancel_item", f"User mismatch for item {item_id}.")
            return False
        if metadata.get("status") == "cancelled":
            log_info("task_manager", "cancel_item", f"Item {item_id} already cancelled.")
            return True # Already done

        item_type = metadata.get("type")

        # --- 2. Delete Associated GCal Events ---
        if calendar_api and not item_id.startswith("local_"):
            if item_type == "reminder":
                log_info("task_manager", "cancel_item", f"Attempting to delete GCal Reminder event {item_id}")
                gcal_cleanup_success = calendar_api.delete_event(item_id)
                if not gcal_cleanup_success:
                    log_warning("task_manager", "cancel_item", f"Failed to delete GCal Reminder event {item_id} (might be already gone). Proceeding.")
                    gcal_cleanup_success = True # Allow proceeding

            elif item_type == "task":
                session_ids_json = metadata.get("child_session_ids", "[]")
                try:
                    session_ids = json.loads(session_ids_json)
                    if isinstance(session_ids, list) and session_ids:
                        log_info("task_manager", "cancel_item", f"Attempting to delete {len(session_ids)} GCal work session events for task {item_id}")
                        all_deleted = True
                        for session_id in session_ids:
                             if not str(session_id).startswith("local_"): # Skip local session IDs if they exist
                                deleted = calendar_api.delete_event(session_id)
                                if not deleted:
                                    log_warning("task_manager", "cancel_item", f"Failed to delete GCal session event {session_id} for task {item_id}.")
                                    all_deleted = False
                             else:
                                 log_info("task_manager", "cancel_item", f"Skipping deletion of local session ID {session_id}")

                        gcal_cleanup_success = all_deleted
                        if not gcal_cleanup_success:
                            log_warning("task_manager", "cancel_item", "Failed to delete one or more GCal sessions, but proceeding.")
                            gcal_cleanup_success = True # Allow proceeding
                    else:
                        log_info("task_manager", "cancel_item", f"No GCal sessions linked to task {item_id}. No GCal cleanup needed.")
                except json.JSONDecodeError:
                    log_error("task_manager", "cancel_item", f"Corrupted child_session_ids for task {item_id}: {session_ids_json}")
                    gcal_cleanup_success = False # Treat corrupted data as failure
                except Exception as session_del_e:
                     log_error("task_manager", "cancel_item", f"Error deleting GCal sessions for {item_id}", session_del_e)
                     gcal_cleanup_success = False
        # Log skip reasons
        elif not calendar_api: log_info("task_manager", "cancel_item", f"Calendar API inactive for {user_id}. Skipping GCal cleanup.")
        else: log_info("task_manager", "cancel_item", f"Local item {item_id}. No GCal cleanup needed.")

        # --- 3. Update Metadata Status ---
        if not gcal_cleanup_success:
            log_error("task_manager", "cancel_item", f"Aborting metadata update for {item_id} due to GCal cleanup failure.")
            return False

        # Use the dedicated status update function for consistency
        updated_meta = update_task_status(user_id, item_id, "cancelled")

        if updated_meta:
             log_info("task_manager", "cancel_item", f"Successfully marked item {item_id} as cancelled.")
             # Context update is handled within update_task_status
             return True
        else:
             log_error("task_manager", "cancel_item", f"Failed to update metadata status to cancelled for {item_id} after GCal cleanup.")
             return False

    except Exception as e:
        tb_str = traceback.format_exc()
        log_error("task_manager", "cancel_item", f"Error cancelling item {item_id} for {user_id}. Traceback:\n{tb_str}", e)
        return False


def schedule_work_sessions(user_id: str, task_id: str, slots_to_book: List[Dict]) -> Dict:
    """
    Creates GCal events for proposed work sessions and updates the main task metadata
    including child_session_ids and sessions_planned. Sets status to 'in_progress'.

    Args:
        user_id: The user's identifier.
        task_id: The event_id of the main task to schedule sessions for.
        slots_to_book: List of session dictionaries from the tool/LLM
                         (each dict MUST have "date", "time"). "end_time" is optional.

    Returns:
        A dictionary with "success": bool and "message": str. Includes created session IDs on success.
    """
    log_info("task_manager", "schedule_work_sessions", f"Booking {len(slots_to_book)} sessions for task {task_id}")
    if not METADATA_STORE_IMPORTED:
         return {"success": False, "message": "Metadata store unavailable.", "session_ids": []}

    calendar_api = _get_calendar_api(user_id)
    if not calendar_api:
        return {"success": False, "message": "Calendar is not connected or active. Cannot schedule sessions.", "session_ids": []}

    # --- 1. Get Main Task Details & Preferences ---
    task_metadata = metadata_store.get_event_metadata(task_id)
    if not task_metadata or task_metadata.get("user_id") != user_id:
        log_error("task_manager", "schedule_work_sessions", f"Main task {task_id} not found or user mismatch.")
        return {"success": False, "message": "Could not find the original task details.", "session_ids": []}
    if task_metadata.get("type") != "task":
         log_error("task_manager", "schedule_work_sessions", f"Item {task_id} is not a task. Cannot schedule sessions.")
         return {"success": False, "message": "Scheduling is only supported for items of type 'task'.", "session_ids": []}

    task_title = task_metadata.get("title", "Task Work")
    agent_state = get_agent_state(user_id)
    prefs = agent_state.get("preferences", {}) if agent_state else {}
    session_length_str = prefs.get("Preferred_Session_Length", "60m")
    session_length_minutes = _parse_duration_to_minutes(session_length_str) or 60

    # --- 2. Create GCal Events for Sessions ---
    created_session_ids = []
    errors = []
    for i, session_slot in enumerate(slots_to_book):
        session_date = session_slot.get("date")
        session_time = session_slot.get("time")
        if not session_date or not session_time:
             log_warning("task_manager", "schedule_work_sessions", f"Skipping session {i+1}: missing date or time.")
             errors.append(f"Session {i+1} missing data")
             continue

        try:
            session_event_data = {
                "title": f"Work: {task_title} [{i+1}/{len(slots_to_book)}]",
                "description": f"Focused work session for task: {task_title}\nParent Task ID: {task_id}",
                "date": session_date,
                "time": session_time,
                "duration": f"{session_length_minutes}m"
            }
            session_event_id = calendar_api.create_event(session_event_data)
            if session_event_id:
                created_session_ids.append(session_event_id)
            else:
                log_error("task_manager", "schedule_work_sessions", f"Failed to create GCal event for session {i+1}.")
                errors.append(f"Session {i+1} GCal creation failed")
        except Exception as e:
            log_error("task_manager", "schedule_work_sessions", f"Error creating GCal event for session {i+1}", e)
            errors.append(f"Session {i+1} creation error: {e}")

    if not created_session_ids:
        log_error("task_manager", "schedule_work_sessions", "Failed to create any GCal session events.")
        return {"success": False, "message": f"Sorry, I couldn't add the proposed sessions to your calendar. Errors: {'; '.join(errors)}", "session_ids": []}

    log_info("task_manager", "schedule_work_sessions", f"Successfully created {len(created_session_ids)} GCal session events: {created_session_ids}")

    # --- 3. Update Main Task Metadata ---
    try:
        existing_session_ids = json.loads(task_metadata.get("child_session_ids", "[]"))
        if not isinstance(existing_session_ids, list): existing_session_ids = []
    except json.JSONDecodeError:
        existing_session_ids = []
    all_session_ids = list(set(existing_session_ids + created_session_ids))

    metadata_update_payload = {
        "sessions_planned": len(all_session_ids),
        "child_session_ids": json.dumps(all_session_ids),
        "status": "in_progress" # Update status
    }
    # Use the status update function for consistency? Or update directly? Let's update directly for now.
    updated_metadata = task_metadata.copy()
    updated_metadata.update(metadata_update_payload)
    metadata_to_save = {fn: updated_metadata.get(fn) for fn in metadata_store.FIELDNAMES}

    try:
        metadata_store.save_event_metadata(metadata_to_save)
        log_info("task_manager", "schedule_work_sessions", f"Updated parent task {task_id} metadata with session info.")

        # Update in-memory context
        if AGENT_STATE_IMPORTED:
            update_task_in_context(user_id, task_id, metadata_to_save)

    except Exception as meta_save_e:
         log_error("task_manager", "schedule_work_sessions", f"Created sessions for task {task_id}, but failed update main task metadata.", meta_save_e)
         # Rollback attempt (best effort)
         log_warning("task_manager", "schedule_work_sessions", f"Attempting to rollback {len(created_session_ids)} created GCal sessions for task {task_id}.")
         for session_id in created_session_ids:
             try: calendar_api.delete_event(session_id)
             except: pass
         return {"success": False, "message": "Scheduled sessions in calendar, but failed to link them to the main task. Rolling back changes.", "session_ids": []}

    # --- 4. Return Success ---
    num_sessions = len(created_session_ids)
    plural_s = "s" if num_sessions > 1 else ""
    success_message = f"Okay, I've scheduled {num_sessions} work session{plural_s} for '{task_title}' in your calendar."
    if errors:
        success_message += f" (Note: Issues encountered with {len(errors)} potential sessions)."

    return {"success": True, "message": success_message, "session_ids": created_session_ids}


def cancel_sessions(user_id: str, task_id: str, session_ids_to_cancel: List[str]) -> Dict:
    """
    (Phase 2) Deletes specified GCal events and updates parent task metadata.
    Returns: Dict with success status, count, message.
    """
    log_info("task_manager", "cancel_sessions", f"Cancelling {len(session_ids_to_cancel)} sessions for task {task_id}")
    if not METADATA_STORE_IMPORTED:
        return {"success": False, "cancelled_count": 0, "message": "Metadata store unavailable."}

    calendar_api = _get_calendar_api(user_id)
    if not calendar_api:
        return {"success": False, "cancelled_count": 0, "message": "Calendar is not connected or active."}

    # --- 1. Get Parent Task Metadata ---
    task_metadata = metadata_store.get_event_metadata(task_id)
    if not task_metadata or task_metadata.get("user_id") != user_id:
        log_error("task_manager", "cancel_sessions", f"Parent task {task_id} not found or user mismatch.")
        return {"success": False, "cancelled_count": 0, "message": "Could not find the original task details."}
    if task_metadata.get("type") != "task":
         return {"success": False, "cancelled_count": 0, "message": "Cannot cancel sessions for non-task items."}

    # --- 2. Delete GCal Events ---
    cancelled_count = 0
    errors = []
    valid_ids_to_cancel = [sid for sid in session_ids_to_cancel if not str(sid).startswith("local_")]

    for session_id in valid_ids_to_cancel:
        try:
            deleted = calendar_api.delete_event(session_id)
            if deleted:
                cancelled_count += 1
            else:
                # Log warning but don't necessarily fail the whole operation if one event is already gone
                log_warning("task_manager", "cancel_sessions", f"Failed to delete GCal session {session_id} (might be already gone).")
                # Still count it as 'processed' for removal from metadata? Yes.
                cancelled_count += 1 # Count as processed even if delete failed (already gone)
        except Exception as e:
            log_error("task_manager", "cancel_sessions", f"Error deleting GCal session {session_id}", e)
            errors.append(session_id)

    if errors:
        log_warning("task_manager", "cancel_sessions", f"Errors occurred deleting sessions: {errors}")
        # Decide if this constitutes failure - let's proceed to update metadata if *any* were processed

    if cancelled_count == 0 and not errors:
         log_warning("task_manager", "cancel_sessions", "No valid GCal session IDs provided or found to cancel.")
         # Return success=True as no action was needed? Or False as nothing changed? Let's say True.
         return {"success": True, "cancelled_count": 0, "message": "No matching calendar sessions found to cancel."}


    # --- 3. Update Parent Task Metadata ---
    try:
        existing_session_ids = json.loads(task_metadata.get("child_session_ids", "[]"))
        if not isinstance(existing_session_ids, list): existing_session_ids = []
    except json.JSONDecodeError:
        existing_session_ids = []

    # Remove the cancelled IDs (use set for efficiency)
    cancelled_set = set(session_ids_to_cancel) # Include local IDs here if they were passed
    remaining_session_ids = [sid for sid in existing_session_ids if sid not in cancelled_set]

    metadata_update_payload = {
        "sessions_planned": len(remaining_session_ids),
        "child_session_ids": json.dumps(remaining_session_ids)
        # Maybe revert status to 'pending' if sessions_planned becomes 0?
        # "status": "pending" if len(remaining_session_ids) == 0 else task_metadata.get("status")
    }
    updated_metadata = task_metadata.copy()
    updated_metadata.update(metadata_update_payload)
    metadata_to_save = {fn: updated_metadata.get(fn) for fn in metadata_store.FIELDNAMES}

    try:
        metadata_store.save_event_metadata(metadata_to_save)
        log_info("task_manager", "cancel_sessions", f"Updated parent task {task_id} metadata after cancelling sessions.")

        # Update in-memory context
        if AGENT_STATE_IMPORTED:
            update_task_in_context(user_id, task_id, metadata_to_save)

        message = f"Successfully cancelled {cancelled_count} session(s)."
        if errors: message += f" Encountered errors with {len(errors)} session(s)."
        return {"success": True, "cancelled_count": cancelled_count, "message": message}

    except Exception as meta_save_e:
        log_error("task_manager", "cancel_sessions", f"Deleted GCal sessions for task {task_id}, but failed update parent task metadata.", meta_save_e)
        # Cannot easily rollback GCal deletions here
        return {"success": False, "cancelled_count": cancelled_count, "message": "Cancelled sessions in calendar, but failed to update the main task link."}

# --- END OF FILE services/task_manager.py ---
# --- END OF FILE services\task_manager.py ---


================================================================================
üìÑ services\task_query_service.py
================================================================================

# --- START OF FILE services\task_query_service.py ---
# --- START OF FILE services/task_query_service.py ---
"""Service layer for querying and formatting task/reminder data."""
from datetime import datetime, timedelta
from typing import List, Dict, Tuple, Optional, Any
import json # Added for potential use in merging/parsing

from tools.logger import log_info, log_error, log_warning
from tools import metadata_store

# Agent State Manager Import
try:
    from services.agent_state_manager import get_context, get_agent_state
    AGENT_STATE_MANAGER_IMPORTED = True
except ImportError:
    log_error("task_query_service", "import", "AgentStateManager not found. Context reads will fail.")
    AGENT_STATE_MANAGER_IMPORTED = False
    def get_context(*args, **kwargs): return None
    def get_agent_state(*args, **kwargs): return None

# Google Calendar API Import
try:
    from tools.google_calendar_api import GoogleCalendarAPI
    GCAL_API_IMPORTED = True
except ImportError:
     log_warning("task_query_service", "import", "GoogleCalendarAPI not imported. Calendar functions limited.")
     GoogleCalendarAPI = None # Define as None if import fails
     GCAL_API_IMPORTED = False

# Pandas fallback
try: import pandas as pd
except ImportError: pd = None

ACTIVE_STATUSES = {"pending", "in_progress"} # Define statuses considered active (lowercase)

# --- Internal Helper Functions ---

def _get_calendar_api_from_state(user_id: str) -> any:
    """Helper to retrieve the active calendar API instance from agent state."""
    if not AGENT_STATE_MANAGER_IMPORTED or not GCAL_API_IMPORTED:
        return None
    try:
        agent_state = get_agent_state(user_id)
        if agent_state:
            calendar_api_maybe = agent_state.get("calendar")
            if isinstance(calendar_api_maybe, GoogleCalendarAPI) and calendar_api_maybe.is_active():
                return calendar_api_maybe
    except Exception as e:
        log_error("task_query_service", "_get_calendar_api_from_state", f"Error getting calendar API for {user_id}", e)
    return None

def _fetch_and_merge_tasks(user_id: str, date_range: Optional[Tuple[str, str]] = None) -> List[Dict]:
    """
    (Helper) Fetches data from GCal (if range provided) and ALL Metadata, merges them.
    Returns a list of dictionaries, prioritizing metadata fields.
    NOTE: In v0.8, we primarily rely on the in-memory context managed by AgentStateManager,
          which should be updated by task_manager. This function is less critical but
          could be used for deeper sync/context building if needed later.
          For now, get_formatted_list reads directly from the context cache.
    """
    log_info("task_query_service", "_fetch_and_merge_tasks", f"[DEPRECATED?] Fetching & merging data for {user_id}, range: {date_range}")
    # Placeholder logic, ideally read from context cache first
    all_meta = metadata_store.list_metadata(user_id)
    log_info("task_query_service", "_fetch_and_merge_tasks", f"Fetched {len(all_meta)} metadata records for merging.")
    # Basic merge logic if GCal data were fetched would go here.
    return all_meta # Return only metadata for now


def _filter_tasks_by_status(task_list: List[Dict], status_filter: str = 'active') -> List[Dict]:
    """Filters task list by status. Defaults to active."""
    filter_str_lower = status_filter.lower()
    target_statuses = ACTIVE_STATUSES # Default

    if filter_str_lower == 'all':
        return task_list # No filtering needed
    elif filter_str_lower == 'completed':
        target_statuses = {"completed"} # Only 'completed' exactly
    elif filter_str_lower == 'pending':
        target_statuses = {"pending"}
    elif filter_str_lower == 'in_progress':
         target_statuses = {"in_progress", "inprogress"} # Allow for variations
    elif filter_str_lower != 'active':
        log_warning("task_query_service", "_filter_tasks_by_status", f"Unknown status filter '{status_filter}'. Defaulting to 'active'.")
        target_statuses = ACTIVE_STATUSES # Revert to default if unknown

    return [task for task in task_list if str(task.get("status", "pending")).lower() in target_statuses]

def _filter_tasks_by_date_range(task_list: List[Dict], date_range: Tuple[str, str]) -> List[Dict]:
    """Filters task list by date range based on 'date' field."""
    try:
        start_date = datetime.strptime(date_range[0], "%Y-%m-%d").date()
        end_date = datetime.strptime(date_range[1], "%Y-%m-%d").date()
        return [
            task for task in task_list
            if task.get("date") and isinstance(task["date"], str) and # Check date exists and is string
               start_date <= datetime.strptime(task["date"], "%Y-%m-%d").date() <= end_date
        ]
    except (ValueError, TypeError, KeyError) as e:
        log_error("task_query_service", "_filter_tasks_by_date_range", f"Invalid date range or missing/invalid date field: {date_range}, Error: {e}")
        return [] # Return empty on error

# --- NEW Filter Function for Project ---
def _filter_tasks_by_project(task_list: List[Dict], project_filter: str) -> List[Dict]:
    """Filters task list by project tag (case-insensitive)."""
    filter_lower = project_filter.lower()
    return [
        task for task in task_list
        if str(task.get("project", "")).lower() == filter_lower
    ]
# --- END NEW Filter ---

def _sort_tasks(task_list: List[Dict]) -> List[Dict]:
    """Sorts task list robustly by date/time."""
    def sort_key(item):
        sort_dt = datetime.max # Default sort value (put items without date/time last)
        meta_date_str = item.get("date")
        meta_time_str = item.get("time")
        start_iso = item.get("start_datetime") # Primarily from GCal context merge

        primary_dt_str = None
        if meta_date_str:
            time_part = meta_time_str if meta_time_str else "00:00:00"
            if len(time_part.split(':')) == 2: time_part += ':00' # Ensure seconds for strptime
            primary_dt_str = f"{meta_date_str} {time_part}"
        elif start_iso and 'T' in start_iso: # Fallback to GCal start time if metadata date missing
            primary_dt_str = start_iso

        if primary_dt_str:
            try:
                # Handle ISO format (with or without timezone) or YYYY-MM-DD HH:MM:SS
                if 'T' in primary_dt_str:
                    # Handle timezone offset if present
                    if '+' in primary_dt_str:
                         dt_obj = datetime.fromisoformat(primary_dt_str.split('+')[0])
                    elif '-' in primary_dt_str[10:]: # Check for Z or negative offset after date part
                         if primary_dt_str.endswith('Z'):
                              dt_obj = datetime.fromisoformat(primary_dt_str.replace('Z', '+00:00'))
                         else: # Assume format like 2023-10-27T10:00:00-07:00
                              dt_obj = datetime.fromisoformat(primary_dt_str)
                    else: # No timezone info
                        dt_obj = datetime.fromisoformat(primary_dt_str)

                    # Naive comparison: remove timezone info if present
                    sort_dt = dt_obj.replace(tzinfo=None)

                else: # Assume "YYYY-MM-DD HH:MM:SS"
                    sort_dt = datetime.strptime(primary_dt_str, "%Y-%m-%d %H:%M:%S")

            except (ValueError, TypeError):
                log_warning("task_query_service", "_sort_tasks", f"Could not parse date/time for sorting: {primary_dt_str}")
                sort_dt = datetime.max # Put parse errors last

        return sort_dt

    try:
        # Sort primarily by date/time, secondarily by creation time (if available), then title
        return sorted(task_list, key=lambda item: (
            sort_key(item),
            item.get("created_at", ""), # Secondary sort by creation
            item.get("title", "") # Tertiary sort by title
            )
        )
    except Exception as e:
        log_error("task_query_service", "_sort_tasks", f"Error during sorting: {e}", e)
        return task_list # Return unsorted on error


def _format_task_line(task_data: Dict) -> str:
    """Formats a single merged task dictionary into a display string."""
    try:
        line_parts = []
        item_type = str(task_data.get("type", "Item")).capitalize() # Default to Item
        line_parts.append(f"({item_type})") # Use parenthesis for type

        # Description / Title
        description = str(task_data.get("title", "")).strip()
        if not description: description = str(task_data.get("description", "")).strip()
        if not description: description = "(No Title)"
        line_parts.append(description)

        # Estimated Duration (for Tasks)
        if item_type.lower() == "task":
             duration = task_data.get("estimated_duration")
             is_nan = pd and isinstance(duration, float) and pd.isna(duration)
             if duration and not is_nan:
                  line_parts.append(f"[Est: {duration}]")

        # Date/Time Formatting (robust)
        start_iso = task_data.get("start_datetime")
        is_all_day = task_data.get("is_all_day", False) and not task_data.get("time") # Check metadata time too
        meta_date = task_data.get("date")
        meta_time = task_data.get("time")
        datetime_str = ""

        display_date = meta_date # Prefer metadata date
        display_time = meta_time if not is_all_day else None # Prefer metadata time unless all_day

        if display_date:
             datetime_str = f" on {display_date}"
             if display_time:
                  datetime_str += f" at {display_time}"
             elif is_all_day:
                  datetime_str += " (All day)"
        elif start_iso: # Fallback to GCal start_datetime only if metadata date is missing
             try:
                 if 'T' in start_iso:
                      dt_obj = datetime.fromisoformat(start_iso.split('+')[0].split('-')[0] if '+' in start_iso or '-' in start_iso[10:] else start_iso.replace('Z',''))
                      datetime_str = f" on {dt_obj.strftime('%Y-%m-%d')} at {dt_obj.strftime('%H:%M')}"
                 else: # GCal All day event date string
                      datetime_str = f" on {start_iso} (All day)"
             except:
                  datetime_str = f" (Date/Time Error)" # Indicate parse error
        # else: No date info found

        if datetime_str: line_parts.append(datetime_str)

        # Project Tag
        project = task_data.get("project")
        if project:
             line_parts.append(f"{{{project}}}") # Use curly braces for project

        # Status (excluding completed by default unless filter requests it)
        status = str(task_data.get("status", "pending")).capitalize()
        # Don't show status if it's the default 'Pending' unless specifically requested? Maybe show always for clarity.
        line_parts.append(f"[{status}]")

        return " ".join(part for part in line_parts if part) # Filter out empty parts

    except Exception as e:
        log_error("task_query_service", "_format_task_line", f"Error formatting data: {task_data.get('event_id')}", e)
        # Include event_id in error message for debugging
        return f"Error displaying item: {task_data.get('event_id', task_data.get('item_id', 'Unknown ID'))}"


# --- Public Service Functions ---

def get_formatted_list(
    user_id: str,
    date_range: Optional[Tuple[str, str]] = None,
    status_filter: str = 'active',
    project_filter: Optional[str] = None, # <-- Added project_filter parameter
    trigger_sync: bool = False # Keep sync trigger placeholder
    ) -> Tuple[str, Dict[int, str]]:
    """
    Gets tasks from context, filters, sorts, formats into a numbered list string,
    and returns the string body and mapping.
    """
    log_info("task_query_service", "get_formatted_list", f"Request: User={user_id}, Status={status_filter}, Range={date_range}, Project={project_filter}, Sync={trigger_sync}")

    # 1. Synchronization (Placeholder)
    if trigger_sync:
        log_info("task_query_service", "get_formatted_list", "Sync triggered (Not Implemented).")
        # Potentially call sync service here in the future

    # 2. Get Data (Read from In-Memory Cache via AgentStateManager)
    try:
        # get_context should return a deep copy of the list
        task_list = get_context(user_id) or []
        log_info("task_query_service", "get_formatted_list", f"Retrieved {len(task_list)} items from context for {user_id}.")
    except Exception as e:
        log_error("task_query_service", "get_formatted_list", f"Error getting context for {user_id}", e)
        task_list = []

    # --- Filtering Steps ---
    # 3. Filter by Status
    filtered_by_status = _filter_tasks_by_status(task_list, status_filter)
    log_info("task_query_service", "get_formatted_list", f"Filtered by status '{status_filter}': {len(filtered_by_status)} items remain.")

    # 4. Filter by Date Range (if provided)
    filtered_by_date = filtered_by_status
    if date_range:
        filtered_by_date = _filter_tasks_by_date_range(filtered_by_status, date_range)
        log_info("task_query_service", "get_formatted_list", f"Filtered by date range {date_range}: {len(filtered_by_date)} items remain.")

    # 5. Filter by Project (if provided) <-- NEW STEP
    final_tasks = filtered_by_date
    if project_filter:
        final_tasks = _filter_tasks_by_project(filtered_by_date, project_filter)
        log_info("task_query_service", "get_formatted_list", f"Filtered by project '{project_filter}': {len(final_tasks)} items remain.")

    # 6. Handle No Tasks Found After Filtering
    if not final_tasks:
        log_info("task_query_service", "get_formatted_list", f"No tasks found matching criteria for {user_id}.")
        # Return empty string body and empty map explicitly
        return "", {}

    # 7. Sort the Final List
    sorted_tasks = _sort_tasks(final_tasks)

    # 8. Format and Create Mapping
    lines = []
    mapping = {}
    item_number = 0
    for task_data in sorted_tasks:
        # Use 'item_id' if present (from Orchestrator context prep), else 'event_id'
        item_id = task_data.get("item_id", task_data.get("event_id"))
        if not item_id:
            log_warning("task_query_service", "get_formatted_list", f"Skipping item with missing ID: {task_data.get('title')}")
            continue # Skip items without a valid ID

        item_number += 1
        formatted_line = _format_task_line(task_data)
        lines.append(f"{item_number}. {formatted_line}")
        mapping[item_number] = item_id # Map list number to the item's unique ID

    if item_number == 0:
        # This case might happen if all items had missing IDs or formatting failed entirely
        log_warning("task_query_service", "get_formatted_list", f"Formatting failed or no valid IDs for user {user_id}")
        return "Error: Could not format task list.", {}

    # 9. Return Formatted Lines (Body Only) and Mapping
    list_body_string = "\n".join(lines)
    log_info("task_query_service", "get_formatted_list", f"Generated list body ({len(mapping)} items) for {user_id}")
    # The Orchestrator agent will add headers/footers as needed
    return list_body_string, mapping


def get_tasks_for_summary(user_id: str, date_range: Tuple[str, str], status_filter: str = 'active', trigger_sync: bool = False) -> List[Dict]:
    """
    Gets tasks from context, filters, sorts and returns the list of *dictionaries*.
    Used for internal summaries (like Morning/Evening routines).
    """
    log_info("task_query_service", "get_tasks_for_summary", f"Request: User={user_id}, Filter={status_filter}, Range={date_range}, Sync={trigger_sync}")

    # 1. Synchronization (Placeholder)
    if trigger_sync:
        log_info("task_query_service", "get_tasks_for_summary", "Sync triggered (Not Implemented).")
        # Consider calling sync service here

    # 2. Get Data (Read from Cache)
    try:
        task_list = get_context(user_id) or []
        log_info("task_query_service", "get_tasks_for_summary", f"Retrieved {len(task_list)} items from context for summary.")
    except Exception as e:
        log_error("task_query_service", "get_tasks_for_summary", f"Error getting context for {user_id}", e)
        task_list = []

    # --- Filtering ---
    # 3. Filter by Status
    filtered_by_status = _filter_tasks_by_status(task_list, status_filter)

    # 4. Filter by Date Range
    final_tasks = _filter_tasks_by_date_range(filtered_by_status, date_range)

    # 5. Sort
    sorted_tasks = _sort_tasks(final_tasks)

    # 6. Return List of Dictionaries
    log_info("task_query_service", "get_tasks_for_summary", f"Returning {len(sorted_tasks)} task dictionaries for summary for {user_id}")
    return sorted_tasks


def get_context_snapshot(user_id: str, history_weeks=1, future_weeks=2) -> Tuple[List[Dict], List[Dict]]:
    """
    Fetches relevant active tasks (metadata-focused) and calendar events
    from the respective sources for the Orchestrator context.
    """
    log_info("task_query_service", "get_context_snapshot", f"Getting context snapshot for user {user_id}")
    task_context = []
    calendar_context = []

    try:
        # Define date range for snapshot
        today = datetime.now().date()
        start_date = today - timedelta(weeks=history_weeks)
        end_date = today + timedelta(weeks=future_weeks)
        start_date_str = start_date.strftime("%Y-%m-%d")
        end_date_str = end_date.strftime("%Y-%m-%d")
        date_range = (start_date_str, end_date_str)

        # Get tasks (use get_tasks_for_summary for combined filtering/sorting)
        # Filter for statuses that provide useful context (pending, in progress)
        tasks_raw = get_tasks_for_summary(user_id, date_range=date_range, status_filter='active') # 'active' includes pending & in progress
        task_context = tasks_raw # Use the filtered list of dictionaries

        # Get calendar events (only if API is active)
        calendar_api = _get_calendar_api_from_state(user_id)
        if calendar_api:
            try:
                # list_events returns already parsed/simplified dicts
                calendar_events_raw = calendar_api.list_events(start_date_str, end_date_str)
                calendar_context = calendar_events_raw
            except Exception as cal_e:
                log_error("task_query_service", "get_context_snapshot", f"Failed to fetch calendar events for context: {cal_e}")
        else:
            log_info("task_query_service", "get_context_snapshot", f"Calendar API not active for {user_id}, skipping GCal event fetch.")


        log_info("task_query_service", "get_context_snapshot", f"Snapshot created: {len(task_context)} tasks, {len(calendar_context)} calendar events.")

    except Exception as e:
        log_error("task_query_service", "get_context_snapshot", f"Error creating context snapshot for {user_id}", e)
        # Return empty lists on error

    return task_context, calendar_context

# --- END OF FILE services/task_query_service.py ---
# --- END OF FILE services\task_query_service.py ---


================================================================================
üìÑ services\config_manager.py
================================================================================

# --- START OF FILE services\config_manager.py ---
# services/config_manager.py
"""Service layer for managing user configuration and preferences."""
from tools.logger import log_info, log_error, log_warning
# Import registry functions for persistence
from users.user_registry import get_user_preferences as get_prefs_from_registry
from users.user_registry import update_preferences as update_prefs_in_registry # This writes to file
# Import state manager for memory updates
try:
    # This function updates the live agent state dictionary
    from services.agent_state_manager import update_preferences_in_state
    AGENT_STATE_MANAGER_IMPORTED = True
except ImportError:
    log_error("config_manager", "import", "AgentStateManager not found. In-memory preference updates skipped.")
    AGENT_STATE_MANAGER_IMPORTED = False
    def update_preferences_in_state(*args, **kwargs): return False # Dummy

# Import calendar tool auth check
try:
    from tools.calendar_tool import authenticate as check_calendar_auth_status
    CALENDAR_TOOL_IMPORTED = True
except ImportError:
     log_error("config_manager", "import", "calendar_tool not found. Calendar auth initiation fails.")
     CALENDAR_TOOL_IMPORTED = False
     def check_calendar_auth_status(*args, **kwargs): return {"status": "fails", "message": "Calendar tool unavailable."}

from typing import Dict, Any, Optional

def get_preferences(user_id: str) -> Optional[Dict]:
    """Gets user preferences from the persistent registry."""
    try:
        prefs = get_prefs_from_registry(user_id)
        return prefs # Returns None if not found
    except Exception as e:
        log_error("config_manager", "get_preferences", f"Error reading preferences for {user_id}", e)
        return None

def update_preferences(user_id: str, updates: Dict) -> bool:
    """
    Updates preferences in persistent registry AND in-memory agent state.
    Returns True on success (based on registry update), False otherwise.
    """
    log_info("config_manager", "update_preferences", f"Updating preferences for {user_id}: {list(updates.keys())}")
    if not isinstance(updates, dict) or not updates:
        log_warning("config_manager", "update_preferences", "Invalid or empty updates provided.")
        return False

    # 1. Update Persistent Store (Registry File)
    registry_update_success = False
    try:
        update_prefs_in_registry(user_id, updates) # Writes to registry.json
        log_info("config_manager", "update_preferences", f"Registry file update requested for {user_id}")
        registry_update_success = True
    except Exception as e:
        log_error("config_manager", "update_preferences", f"Registry file update failed for {user_id}", e)
        return False # Don't proceed if persistence fails

    # 2. Update In-Memory State via AgentStateManager (If persistence succeeded)
    if registry_update_success and AGENT_STATE_MANAGER_IMPORTED:
        try:
            mem_update_success = update_preferences_in_state(user_id, updates) # Updates live _AGENT_STATE_STORE
            if not mem_update_success:
                log_warning("config_manager", "update_preferences", f"In-memory state update failed or user not found in state for {user_id}.")
                # Should we revert registry? For now, proceed but warn.
        except Exception as mem_e:
             log_error("config_manager", "update_preferences", f"Error updating in-memory state for {user_id}", mem_e)
             # Log error, but persistence succeeded, so arguably return True

    elif registry_update_success: # Log if manager wasn't imported
        log_warning("config_manager", "update_preferences", "AgentStateManager not imported. Skipping in-memory state update.")

    return registry_update_success # Return success based on registry write

def initiate_calendar_auth(user_id: str) -> Dict:
    """Initiates calendar auth flow via calendar_tool."""
    log_info("config_manager", "initiate_calendar_auth", f"Initiating calendar auth for {user_id}")
    if not CALENDAR_TOOL_IMPORTED:
         return {"status": "fails", "message": "Calendar auth component unavailable."}
    current_prefs = get_preferences(user_id) # Use service getter
    if not current_prefs:
        log_error("config_manager", "initiate_calendar_auth", f"Prefs not found for {user_id}")
        return {"status": "fails", "message": "User profile not found."}
    try:
        # Pass current prefs needed by authenticate function
        auth_result = check_calendar_auth_status(user_id, current_prefs)
        return auth_result
    except Exception as e:
        log_error("config_manager", "initiate_calendar_auth", f"Error during calendar auth init: {e}", e)
        return {"status": "fails", "message": "Error starting calendar auth."}

def set_user_status(user_id: str, status: str) -> bool:
    """Helper to specifically update user status in registry and memory."""
    log_info("config_manager", "set_user_status", f"Setting status='{status}' for {user_id}")
    if not status or not isinstance(status, str):
        log_warning("config_manager", "set_user_status", f"Invalid status value: {status}")
        return False
    # Calls the main update function which handles both registry and memory state
    return update_preferences(user_id, {"status": status})
# --- END OF FILE services\config_manager.py ---


================================================================================
üìÑ services\agent_state_manager.py
================================================================================

# --- START OF FILE services\agent_state_manager.py ---
# services/agent_state_manager.py
"""
Manages the in-memory state of user agents.
Provides thread-safe functions to access and modify the global agent state dictionary.
Requires initialization via initialize_state_store.
"""
from tools.logger import log_info, log_error, log_warning
from typing import Dict, List, Any, Optional
import threading
import copy
from datetime import datetime # Added for timestamp

# --- Module Level State ---
_AGENT_STATE_STORE: Optional[Dict[str, Dict[str, Any]]] = None
_state_lock = threading.Lock()

def initialize_state_store(agent_dict_ref: Dict):
    """Initializes the state manager with a reference to the global agent state dictionary."""
    global _AGENT_STATE_STORE
    if _AGENT_STATE_STORE is not None:
        log_warning("AgentStateManager", "initialize_state_store", "State store already initialized.")
        return
    if isinstance(agent_dict_ref, dict):
        _AGENT_STATE_STORE = agent_dict_ref
        log_info("AgentStateManager", "initialize_state_store", f"State store initialized with reference (ID: {id(_AGENT_STATE_STORE)}).")
    else:
        log_error("AgentStateManager", "initialize_state_store", "Invalid dictionary reference passed.")
        _AGENT_STATE_STORE = {}

def _is_initialized() -> bool:
    """Checks if the state store has been initialized."""
    if _AGENT_STATE_STORE is None:
        log_error("AgentStateManager", "_is_initialized", "CRITICAL: State store accessed before initialization.")
        return False
    return True

# --- Modifier Functions ---

def register_agent_instance(user_id: str, agent_state: Dict):
    """Adds or replaces the entire state dictionary for a user."""
    if not _is_initialized(): return
    if not isinstance(agent_state, dict):
         log_error("AgentStateManager", "register_agent_instance", f"Invalid agent_state type for {user_id}")
         return
    log_info("AgentStateManager", "register_agent_instance", f"Registering/updating state for user {user_id}")
    with _state_lock:
        _AGENT_STATE_STORE[user_id] = agent_state

def update_preferences_in_state(user_id: str, prefs_updates: Dict) -> bool:
    """Updates the preferences dictionary within the user's in-memory state."""
    if not _is_initialized(): return False
    updated = False
    with _state_lock:
        state = _AGENT_STATE_STORE.get(user_id)
        if state and isinstance(state.get("preferences"), dict):
            state["preferences"].update(prefs_updates)
            log_info("AgentStateManager", "update_preferences_in_state", f"Updated in-memory preferences for {user_id}: {list(prefs_updates.keys())}")
            updated = True
        else:
            log_warning("AgentStateManager", "update_preferences_in_state", f"Cannot update prefs: State or prefs dict missing/invalid for {user_id}")
    return updated

def add_task_to_context(user_id: str, task_data: Dict):
    """Appends or updates a task dictionary in the user's in-memory context list."""
    if not _is_initialized(): return
    with _state_lock:
        state = _AGENT_STATE_STORE.get(user_id)
        if state:
            if not isinstance(state.get("active_tasks_context"), list):
                 state["active_tasks_context"] = []
            context = state["active_tasks_context"]
            event_id = task_data.get("event_id")
            found_idx = -1
            if event_id:
                for i, item in enumerate(context):
                    if item.get("event_id") == event_id: found_idx = i; break
            if found_idx != -1:
                 log_info("AgentStateManager", "add_task_to_context", f"Updating task {event_id} in context for {user_id}.")
                 context[found_idx] = task_data
            else:
                 context.append(task_data)
                 log_info("AgentStateManager", "add_task_to_context", f"Added task {event_id} to context for {user_id}. New size: {len(context)}")
        else:
            log_warning("AgentStateManager", "add_task_to_context", f"State missing for {user_id}.")

def update_task_in_context(user_id: str, event_id: str, updated_task_data: Dict):
    """Finds a task by event_id in the context list and replaces it."""
    if not _is_initialized(): return
    with _state_lock:
        state = _AGENT_STATE_STORE.get(user_id)
        if state and isinstance(state.get("active_tasks_context"), list):
            context = state["active_tasks_context"]
            found = False
            for i, item in enumerate(context):
                if item.get("event_id") == event_id:
                    context[i] = updated_task_data; found = True
                    log_info("AgentStateManager", "update_task_in_context", f"Updated task {event_id} in context for {user_id}")
                    break
            if not found:
                 log_warning("AgentStateManager", "update_task_in_context", f"Task {event_id} not found for update. Adding if active.")
                 if updated_task_data.get("status", "pending").lower() in ["pending", "in progress"]:
                      context.append(updated_task_data)
        else:
             log_warning("AgentStateManager", "update_task_in_context", f"State/context list invalid for {user_id}")

def remove_task_from_context(user_id: str, event_id: str):
    """Removes a task by event_id from the user's in-memory context list."""
    if not _is_initialized(): return
    with _state_lock:
        state = _AGENT_STATE_STORE.get(user_id)
        if state and isinstance(state.get("active_tasks_context"), list):
            original_len = len(state["active_tasks_context"])
            state["active_tasks_context"][:] = [
                item for item in state["active_tasks_context"] if item.get("event_id") != event_id ]
            if len(state["active_tasks_context"]) < original_len:
                log_info("AgentStateManager", "remove_task_from_context", f"Removed task {event_id} from context for {user_id}")
        else:
             log_warning("AgentStateManager", "remove_task_from_context", f"State/context list invalid for {user_id}")

def update_full_context(user_id: str, new_context: List[Dict]):
    """Replaces the entire active_tasks_context list (e.g., after sync)."""
    if not _is_initialized(): return
    with _state_lock:
        state = _AGENT_STATE_STORE.get(user_id)
        if state:
            state["active_tasks_context"] = new_context if isinstance(new_context, list) else []
            log_info("AgentStateManager", "update_full_context", f"Replaced context for {user_id} with {len(state['active_tasks_context'])} items.")
        else:
            log_warning("AgentStateManager", "update_full_context", f"Cannot replace context: State missing for {user_id}")


# --- NEW FUNCTION for Chat History ---
def add_message_to_user_history(user_id: str, sender: str, message: str):
    """
    Appends a detailed message to the specific user's conversation history list
    within their agent state. Keeps only the last 50 messages.
    """
    if not _is_initialized(): return
    with _state_lock:
        state = _AGENT_STATE_STORE.get(user_id)
        if not state:
            log_warning("AgentStateManager", "add_message_to_user_history", f"Cannot add message: State missing for {user_id}")
            return

        # Ensure the history list exists and is a list
        if not isinstance(state.get("conversation_history"), list):
            log_warning("AgentStateManager", "add_message_to_user_history", f"conversation_history missing or invalid for {user_id}, initializing.")
            state["conversation_history"] = []

        history_list = state["conversation_history"]
        timestamp = datetime.now().isoformat()
        entry = {
            "sender": sender,
            "timestamp": timestamp,
            "content": message
        }
        history_list.append(entry)
        # Keep only the last 50 messages
        state["conversation_history"] = history_list[-50:] # Slice to limit size
        # Optional: Log count for debugging
        # log_info("AgentStateManager", "add_message_to_user_history", f"User {user_id} history count: {len(state['conversation_history'])}")

# --- NEW FUNCTION: Update arbitrary key in agent state ---
def update_agent_state_key(user_id: str, key: str, value: Any) -> bool:
    """
    Updates or adds a specific key-value pair in the user's agent state.
    If value is None, the key is removed. Returns True if state exists, False otherwise.
    """
    if not _is_initialized(): return False
    updated = False
    with _state_lock:
        state = _AGENT_STATE_STORE.get(user_id)
        if state:
            if value is None:
                removed = state.pop(key, None)
                if removed is not None:
                    log_info("AgentStateManager", "update_agent_state_key", f"Removed key '{key}' from state for {user_id}")
                else:
                    log_info("AgentStateManager", "update_agent_state_key", f"Key '{key}' not found, nothing to remove for {user_id}")
            else:
                state[key] = value
                log_info("AgentStateManager", "update_agent_state_key", f"Updated key '{key}' in state for {user_id}")
            updated = True
        else:
            log_warning("AgentStateManager", "update_agent_state_key", f"Cannot update key '{key}': State missing for {user_id}")
    return updated

def get_agent_state(user_id: str) -> Optional[Dict]:
    """
    Safely gets a SHALLOW copy of the full state dictionary for a user.
    Includes: preferences, llm_components, active_tasks_context, calendar, conversation_history.
    NOTE: Returning shallow copy to avoid deepcopy issues with complex objects like API clients.
    Be mindful that modifying mutable objects (dicts, lists) within the returned dictionary
    will affect the original state unless explicitly managed.
    """
    if not _is_initialized():
        # Error already logged by _is_initialized
        return None
    with _state_lock:
        state = _AGENT_STATE_STORE.get(user_id)
        # Use shallow copy (.copy()) instead of deepcopy to prevent errors
        # with complex, non-copyable objects (like API clients).
        return state.copy() if state else None

def get_context(user_id: str) -> Optional[List[Dict]]:
    """Gets a deep copy of the active_tasks_context list for a user."""
    if not _is_initialized(): return None # Return None if not initialized
    with _state_lock:
        state = _AGENT_STATE_STORE.get(user_id)
        # Check if state and the specific key exist and are the correct type
        if state and isinstance(state.get("active_tasks_context"), list):
            # Return deep copy to isolate internal state from external modification
            return copy.deepcopy(state["active_tasks_context"])
    # Return empty list if user state not found or context key is missing/invalid
    return []
# --- END OF FILE services\agent_state_manager.py ---


================================================================================
üìÑ services\cheats.py
================================================================================

# --- START OF FILE services\cheats.py ---
# --- START OF FILE services/cheats.py ---
"""
Service layer for handling direct 'cheat code' commands, bypassing the LLM orchestrator.
Used primarily for testing, debugging, and direct actions.
"""
import json
from typing import List, Optional, Dict, Any

# Service Imports
from services import task_query_service, task_manager, agent_state_manager
from tools import metadata_store # Needed to list all items for /clear

# Utilities
from tools.logger import log_info, log_error, log_warning


# --- Private Handler Functions ---

def _handle_help() -> str:
    """Provides help text for available cheat commands."""
    return """Available Cheat Commands:
/help - Show this help message
/list [status] - List items (status: active*, pending, completed, all)
/memory - Show summary of current agent in-memory state
/clear - !! DANGER !! Mark all user's items as cancelled (removes from GCal)
/morning - (NYI) Placeholder for morning summary
/evening - (NYI) Placeholder for evening summary"""
    # NYI = Not Yet Implemented


def _handle_list(user_id: str, args: List[str]) -> str:
    """Handles the /list command."""
    status_filter = args[0].lower() if args else 'active'
    allowed_statuses = ['active', 'pending', 'in_progress', 'completed', 'all'] # Ensure consistency with query service if it changes

    if status_filter not in allowed_statuses:
        return f"Invalid status '{status_filter}'. Use one of: {', '.join(allowed_statuses)}"

    try:
        list_body, _ = task_query_service.get_formatted_list(
            user_id=user_id,
            status_filter=status_filter
            # Add project/date range filters here later if needed for cheats
        )
        if list_body:
            return f"Items with status '{status_filter}':\n{list_body}"
        else:
            return f"No items found with status '{status_filter}'."
    except Exception as e:
        log_error("cheats", "_handle_list", f"Error calling get_formatted_list: {e}", e)
        return "Error retrieving list."


def _handle_memory(user_id: str) -> str:
    """Handles the /memory command."""
    try:
        agent_state = agent_state_manager.get_agent_state(user_id)
        if agent_state:
            # Selectively dump state
            state_summary = {
                "user_id": agent_state.get("user_id"),
                "preferences_keys": list(agent_state.get("preferences", {}).keys()),
                "history_count": len(agent_state.get("conversation_history", [])),
                "context_item_count": len(agent_state.get("active_tasks_context", [])),
                "calendar_object_present": agent_state.get("calendar") is not None
            }
            # Use default=str for potential non-serializable items if dumping more
            return f"Agent Memory Summary:\n```json\n{json.dumps(state_summary, indent=2, default=str)}\n```"
        else:
            return "Error: Agent state not found in memory."
    except Exception as e:
        log_error("cheats", "_handle_memory", f"Error retrieving agent state: {e}", e)
        return "Error retrieving agent memory state."


def _handle_clear(user_id: str) -> str:
    """Handles the /clear command. Marks all items as cancelled."""
    log_warning("cheats", "_handle_clear", f"!! Initiating /clear command for user {user_id} !!")
    cancelled_count = 0
    failed_count = 0
    errors = []

    try:
        # Fetch ALL metadata records for the user directly from the store
        all_metadata = metadata_store.list_metadata(user_id=user_id) # Assuming this lists all regardless of date filter
        item_ids_to_clear = [item.get("event_id") for item in all_metadata if item.get("event_id") and item.get("status") != "cancelled"]

        if not item_ids_to_clear:
            return "No active items found to clear."

        log_info("cheats", "_handle_clear", f"Attempting to cancel {len(item_ids_to_clear)} items for user {user_id}.")

        for item_id in item_ids_to_clear:
            try:
                success = task_manager.cancel_item(user_id, item_id)
                if success:
                    cancelled_count += 1
                else:
                    # cancel_item might return False if item was already gone or mismatch, count as failure here
                    failed_count += 1
                    errors.append(f"Failed cancel: {item_id[:8]}...")
                    log_warning("cheats", "_handle_clear", f"task_manager.cancel_item failed for {item_id}")
            except Exception as cancel_e:
                failed_count += 1
                errors.append(f"Error cancel: {item_id[:8]}... ({type(cancel_e).__name__})")
                log_error("cheats", "_handle_clear", f"Exception during cancel_item for {item_id}", cancel_e)

        response = f"Clear operation finished.\nSuccessfully cancelled: {cancelled_count}\nFailed/Skipped: {failed_count}"
        if errors:
            response += "\nFailures:\n" + "\n".join(errors[:5]) # Show first 5 errors
            if len(errors) > 5: response += "\n..."

        return response

    except Exception as e:
        log_error("cheats", "_handle_clear", f"Critical error during /clear setup or execution for {user_id}", e)
        return "A critical error occurred during the clear operation."


def _handle_morning(user_id: str) -> str:
    """Handles the /morning command (Placeholder)."""
    # TODO: Implement logic to generate morning summary
    # - Get today's date
    # - Call task_query_service.get_tasks_for_summary for today
    # - Call GCal API for today's events
    # - Format a summary string
    log_info("cheats", "_handle_morning", f"Placeholder command /morning called for {user_id}")
    return "Morning summary feature not implemented yet via cheat code."


def _handle_evening(user_id: str) -> str:
    """Handles the /evening command (Placeholder)."""
    # TODO: Implement logic for evening review
    # - Get today's date
    # - Call task_query_service.get_tasks_for_summary for today (maybe include overdue?)
    # - Filter for incomplete items
    # - Format review string (potentially interactive later, but not for cheat)
    log_info("cheats", "_handle_evening", f"Placeholder command /evening called for {user_id}")
    return "Evening review feature not implemented yet via cheat code."


# --- Main Dispatcher ---

def handle_cheat_command(user_id: str, command: str, args: List[str]) -> str:
    """
    Dispatches cheat commands to the appropriate handler.

    Args:
        user_id: The normalized user ID.
        command: The command string (e.g., "/list").
        args: A list of arguments following the command.

    Returns:
        A string response for the user.
    """
    command = command.lower() # Ensure case-insensitivity

    if command == "/help":
        return _handle_help()
    elif command == "/list":
        return _handle_list(user_id, args)
    elif command == "/memory":
        return _handle_memory(user_id)
    elif command == "/clear":
        return _handle_clear(user_id)
    elif command == "/morning":
        return _handle_morning(user_id)
    elif command == "/evening":
        return _handle_evening(user_id)
    # Add other commands here with elif
    # Example:
    # elif command == "/propose":
    #     if not args: return "Usage: /propose <item_id>"
    #     item_id = args[0]
    #     # Import and call propose tool logic here if needed for cheats
    #     return f"Propose cheat not fully implemented yet for {item_id}"

    else:
        return f"Unknown command: '{command}'. Try /help."

# --- END OF FILE services/cheats.py ---
# --- END OF FILE services\cheats.py ---


================================================================================
üìÑ services\llm_interface.py
================================================================================

# --- START OF FILE services\llm_interface.py ---
# llm_interface.py
import os
import openai
import instructor
import threading
from tools.logger import log_info, log_error

OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
_client = None
_client_lock = threading.Lock()

def get_instructor_client():
    """Initializes and returns a singleton, instructor-patched OpenAI client."""
    global _client
    if not OPENAI_API_KEY:
        log_error("llm_interface", "get_instructor_client", "OPENAI_API_KEY not found in environment.")
        return None

    with _client_lock:
        if _client is None:
            try:
                log_info("llm_interface", "get_instructor_client", "Initializing instructor-patched OpenAI client...")
                # Initialize the OpenAI client
                base_client = openai.OpenAI(api_key=OPENAI_API_KEY)
                # Patch it with Instructor
                _client = instructor.patch(base_client)
                log_info("llm_interface", "get_instructor_client", "Instructor-patched OpenAI client initialized.")
            except Exception as e:
                log_error("llm_interface", "get_instructor_client", f"Failed to initialize OpenAI client: {e}", e)
                _client = None # Ensure it remains None on failure
    return _client

# --- END OF FILE services\llm_interface.py ---


================================================================================
üìÑ services\sync_service.py
================================================================================

# --- START OF FILE services\sync_service.py ---
# services/sync_service.py
"""
(Deferred Implementation for MVP)
Handles synchronization between Google Calendar, Metadata Store, and In-Memory State.
Identifies external changes and reconciles them.
"""
from tools.logger import log_info, log_error, log_warning
from typing import Dict, List, Optional, Tuple
# Import necessary components when implemented
# from tools.google_calendar_api import GoogleCalendarAPI
# from tools import metadata_store
# from services.agent_state_manager import get_agent_state, update_full_context, update_task_in_context, add_task_to_context, remove_task_from_context
# from users.user_manager import get_agent # To get calendar API instance

def synchronize_user_tasks(user_id: str):
    """
    (Placeholder) Fetches latest GCal events and metadata, compares them,
    updates metadata/GCal for consistency (if needed),
    and updates the in-memory agent context via AgentStateManager.
    """
    log_info("sync_service", "synchronize_user_tasks", f"Starting sync for user {user_id}")
    log_warning("sync_service", "synchronize_user_tasks", "Sync Service logic not implemented yet (Deferred for MVP).")

    # --- TODO: Implement Sync Logic ---
    # 1. Get Agent State / Calendar API instance
    # 2. Define Sync Period (e.g., past N days to next M days)
    # 3. Fetch GCal events for the period (using GCalAPI) -> dict {event_id: gcal_data}
    # 4. Fetch relevant metadata records for the period (using metadata_store) -> dict {event_id: meta_data}
    # 5. Reconcile:
    #    - gcal_ids = set(gcal_events_dict.keys())
    #    - meta_ids = set(metadata_dict.keys())
    #    - in_gcal_not_meta = gcal_ids - meta_ids
    #    - in_meta_not_gcal = meta_ids - gcal_ids
    #    - in_both = gcal_ids.intersection(meta_ids)
    #    - reconciled_list = []
    #    - For id in in_meta_not_gcal: # Deleted in GCal?
    #        meta = metadata_dict[id]
    #        if meta['status'] not in ['completed', 'cancelled']: # Check if already inactive
    #             log_warning("sync_service", "sync", f"Event {id} found in metadata but not GCal. Marking cancelled.")
    #             # task_manager.update_task_status(user_id, id, 'cancelled') # Update persistent store
    #             # meta['status'] = 'cancelled' # Update local copy for context
    #             # Don't add to reconciled_list if considered inactive
    #        # else: Add completed/cancelled items from meta if needed? Probably not for active context.
    #    - For id in in_gcal_not_meta: # Added in GCal?
    #         gcal_event = gcal_events_dict[id]
    #         log_warning("sync_service", "sync", f"Event {id} found in GCal but not metadata. Creating basic metadata.")
    #         # task_manager.create_task(user_id, minimal_data_from_gcal(gcal_event)) # Create basic metadata
    #         reconciled_list.append(minimal_data_from_gcal(gcal_event)) # Add basic info to context
    #    - For id in in_both: # Compare?
    #         gcal_event = gcal_events_dict[id]
    #         meta = metadata_dict[id]
    #         # Simple strategy: Assume metadata is source of truth for status/type/etc.
    #         # More complex: Compare updated timestamps, merge fields carefully.
    #         merged = {**gcal_event, **meta} # Metadata overwrites GCal where keys clash
    #         reconciled_list.append(merged)

    # 6. Update Memory via AgentStateManager
    #    update_full_context(user_id, reconciled_list)

    log_info("sync_service", "synchronize_user_tasks", f"Sync completed (placeholder) for user {user_id}")
    pass


# --- END OF FILE services\sync_service.py ---


================================================================================
üìÑ tools\google_calendar_api.py
================================================================================

# --- START OF FILE tools\google_calendar_api.py ---
# tools/google_calendar_api.py

import os
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Any, TYPE_CHECKING

# --- Try importing Google libraries ---
# Define global names initially, might be overwritten by import
Credentials = None
build = None
HttpError = Exception # Default to base Exception
GoogleAuthRequest = None
RefreshError = Exception # Default to base Exception
GOOGLE_LIBS_AVAILABLE = False

# --- Local Project Imports (Import Logger Early) ---
# Assuming logger doesn't depend on this file, otherwise adjust import location
try:
    from tools.logger import log_info, log_error, log_warning
except ImportError:
    # Basic fallback if logger itself fails initially
    import logging
    logging.basicConfig(level=logging.INFO, format='%(levelname)s:google_calendar_api:%(message)s')
    log_info = logging.info
    log_error = logging.error
    log_warning = logging.warning
    log_error("google_calendar_api", "import", "Failed to import project logger.")

try:
    from google.oauth2.credentials import Credentials as ImportedCredentials
    from googleapiclient.discovery import build as imported_build
    from googleapiclient.errors import HttpError as ImportedHttpError
    from google.auth.transport.requests import Request as ImportedGoogleAuthRequest
    from google.auth.exceptions import RefreshError as ImportedRefreshError

    # Assign successfully imported items to global names
    Credentials = ImportedCredentials
    build = imported_build
    HttpError = ImportedHttpError
    GoogleAuthRequest = ImportedGoogleAuthRequest
    RefreshError = ImportedRefreshError
    GOOGLE_LIBS_AVAILABLE = True
    log_info("google_calendar_api", "import", "Successfully imported Google API libraries.")
except ImportError as import_error_exception:
    log_error("google_calendar_api", "import", f"Failed to import one or more Google API libraries: {import_error_exception}. GoogleCalendarAPI will be non-functional.", import_error_exception)
    # Keep default dummy values assigned above


# --- Other Local Project Imports ---
try:
    from tools.token_store import get_user_token, save_user_token_encrypted
except ImportError as e:
     log_error("google_calendar_api", "import", f"Failed to import from token_store: {e}", e)
     # Define dummy functions if token_store is missing, though API will likely fail later
     def get_user_token(*args, **kwargs): return None
     def save_user_token_encrypted(*args, **kwargs): return False


# --- Conditional Import for Type Hinting ---
if TYPE_CHECKING:
    from googleapiclient.discovery import Resource
    # Make Credentials available for type checking if import succeeded
    if Credentials:
         from google.oauth2.credentials import Credentials

# --- Configuration ---
GOOGLE_CLIENT_ID = os.getenv("GOOGLE_CLIENT_ID")
GOOGLE_CLIENT_SECRET = os.getenv("GOOGLE_CLIENT_SECRET")
if not GOOGLE_CLIENT_ID: log_error("google_calendar_api", "config", "CRITICAL: GOOGLE_CLIENT_ID not set.")
if not GOOGLE_CLIENT_SECRET: log_error("google_calendar_api", "config", "CRITICAL: GOOGLE_CLIENT_SECRET not set.")
DEFAULT_TIMEZONE = "Asia/Jerusalem" # TODO: Make user-specific via preferences
GOOGLE_TOKEN_URI = "https://oauth2.googleapis.com/token" # Standard Google token endpoint


class GoogleCalendarAPI:
    """
    Handles interactions with the Google Calendar API for a specific user.
    Loads credentials, handles refresh tokens, and provides methods for CRUD operations.
    """
    def __init__(self, user_id: str):
        """ Initializes the API client for the user. """
        self.user_id = user_id
        self.service: Optional["Resource"] = None # Use string literal type hint
        self.user_timezone = DEFAULT_TIMEZONE # TODO: Load from user prefs if available

        log_info("GoogleCalendarAPI", "__init__", f"Initializing for user {self.user_id}")
        if not GOOGLE_LIBS_AVAILABLE:
            log_error("GoogleCalendarAPI", "__init__", "Google API libraries not available. Initialization skipped.")
            return # Cannot proceed without libraries

        credentials = self._load_credentials()

        if credentials:
            try:
                # Explicitly check if build function is available *before* calling it
                if build is None:
                    raise ImportError("Build function ('googleapiclient.discovery.build') not available.")
                self.service = build("calendar", "v3", credentials=credentials, cache_discovery=False)
                log_info("GoogleCalendarAPI", "__init__", f"GCal service built successfully for {self.user_id}")
            except ImportError as e:
                 log_error("GoogleCalendarAPI", "__init__", f"Import error during service build: {e}", e)
                 self.service = None
            except Exception as e:
                log_error("GoogleCalendarAPI", "__init__", f"Failed to build GCal service: {e}", e)
                self.service = None
        else:
            log_warning("GoogleCalendarAPI", "__init__", f"Initialization incomplete for {self.user_id} due to credential failure.")
            self.service = None


    def _load_credentials(self): # -> Optional["Credentials"]: # String literal hint
        """Loads credentials, handles refresh, and saves updated token."""
        log_info("GoogleCalendarAPI", "_load_credentials", f"Attempting credentials load for {self.user_id}")

        # Check library availability and specific class import *before* use
        if not GOOGLE_LIBS_AVAILABLE:
            log_error("GoogleCalendarAPI", "_load_credentials", "Google libraries not available.")
            return None
        if Credentials is None: # Check if the class object itself is None
            log_error("GoogleCalendarAPI", "_load_credentials", "'Credentials' class was not imported.")
            return None

        if not GOOGLE_CLIENT_ID or not GOOGLE_CLIENT_SECRET:
             log_error("GoogleCalendarAPI", "_load_credentials", "Client ID or Secret missing in environment config.")
             return None

        token_data = get_user_token(self.user_id)
        if not token_data:
             log_info("GoogleCalendarAPI", "_load_credentials", f"No token data found.")
             return None
        if "refresh_token" not in token_data:
             log_error("GoogleCalendarAPI", "_load_credentials", f"FATAL: refresh_token missing in stored data. Re-auth needed.")
             return None

        credential_info_for_lib = {
            'token': token_data.get('access_token'),
            'refresh_token': token_data.get('refresh_token'),
            'token_uri': GOOGLE_TOKEN_URI,
            'client_id': GOOGLE_CLIENT_ID,
            'client_secret': GOOGLE_CLIENT_SECRET,
            'scopes': token_data.get('scopes', token_data.get('scope', '').split())
        }
        if isinstance(credential_info_for_lib['scopes'], str):
            credential_info_for_lib['scopes'] = credential_info_for_lib['scopes'].split()
        if credential_info_for_lib['scopes'] is None:
             credential_info_for_lib['scopes'] = []

        creds = None
        try:
            # We've already checked Credentials is not None above
            creds = Credentials.from_authorized_user_info(credential_info_for_lib)

            if not creds.valid:
                log_warning("GoogleCalendarAPI", "_load_credentials", f"Credentials invalid/expired. Checking refresh token.")
                if creds.refresh_token:
                    log_info("GoogleCalendarAPI", "_load_credentials", f"Attempting explicit token refresh...")
                    try:
                        # Check if Request class is available before use
                        if GoogleAuthRequest is None:
                            raise ImportError("GoogleAuthRequest class not available for refresh.")
                        creds.refresh(GoogleAuthRequest())
                        log_info("GoogleCalendarAPI", "_load_credentials", f"Token refresh successful.")

                        # Save refreshed token
                        refreshed_token_data_to_save = {
                            'access_token': creds.token, # Use 'access_token' key for saving consistency
                            'refresh_token': creds.refresh_token,
                            'token_uri': creds.token_uri,
                            'client_id': creds.client_id,
                            'client_secret': creds.client_secret,
                            'scopes': creds.scopes,
                            'expiry_iso': creds.expiry.isoformat() if creds.expiry else None
                        }
                        # Ensure save function is available
                        if save_user_token_encrypted is None:
                             log_error("GoogleCalendarAPI", "_load_credentials", "save_user_token_encrypted function not available.")
                             # Cannot save, but proceed with valid in-memory creds for now
                        else:
                            save_success = save_user_token_encrypted(self.user_id, refreshed_token_data_to_save)
                            if not save_success:
                                log_warning("GoogleCalendarAPI", "_load_credentials", f"Failed to save refreshed token.")

                    except RefreshError as refresh_err:
                        log_error("GoogleCalendarAPI", "_load_credentials", f"Token refresh FAILED (RefreshError): {refresh_err}", refresh_err)
                        token_file_path = os.path.join("data", f"tokens_{self.user_id}.json.enc")
                        if os.path.exists(token_file_path):
                            log_warning("GoogleCalendarAPI", "_load_credentials", f"Deleting invalid token file due to refresh failure: {token_file_path}")
                            try: os.remove(token_file_path)
                            except OSError as rm_err: log_error("GoogleCalendarAPI", "_load_credentials", f"Failed to remove token file: {rm_err}")
                        return None
                    except ImportError as imp_err: # Catch if GoogleAuthRequest was None
                         log_error("GoogleCalendarAPI", "_load_credentials", f"Import error during refresh: {imp_err}")
                         return None
                    except Exception as refresh_err:
                        log_error("GoogleCalendarAPI", "_load_credentials", f"Unexpected error during token refresh: {refresh_err}", refresh_err)
                        return None
                else:
                     log_error("GoogleCalendarAPI", "_load_credentials", f"Creds invalid, no refresh token. Re-auth needed.")
                     return None

            # Final check after potential refresh
            if creds and creds.valid:
                 log_info("GoogleCalendarAPI", "_load_credentials", f"Credentials ready.")
                 return creds # Return the actual Credentials object
            else:
                 log_error("GoogleCalendarAPI", "_load_credentials", f"Failed to obtain valid credentials.")
                 return None

        except Exception as e:
            # Catch errors during Credentials.from_authorized_user_info or unexpected issues
            log_error("GoogleCalendarAPI", "_load_credentials", f"Error creating/validating credentials object: {e}", e)
            if isinstance(e, NameError) and 'Credentials' in str(e):
                 log_error("GoogleCalendarAPI", "_load_credentials", "NameError confirms 'Credentials' was not imported correctly.")
            return None


    # --- Public API Methods ---

    def is_active(self) -> bool:
        """Checks if the Google Calendar service object was successfully initialized."""
        return self.service is not None and hasattr(self.service, 'events')


    def create_event(self, event_data: Dict) -> Optional[str]:
        """
        Creates an event on the user's primary Google Calendar.
        Args: event_data (Dict): Details including 'date', optionally 'time', 'duration', 'title', 'description'.
        Returns: Google Calendar event ID or None.
        """
        if not self.is_active():
            log_error("GoogleCalendarAPI", "create_event", f"Service not active for user {self.user_id}.")
            return None
        assert self.service is not None # For type checker

        try:
            event_date = event_data.get('date')
            event_time = event_data.get('time')
            duration_minutes = None
            if event_data.get('duration'):
                try:
                    duration_str = str(event_data['duration']).lower().replace(' ','')
                    # Add more robust duration parsing if needed (e.g., using regex or a library)
                    if 'h' in duration_str:
                        parts = duration_str.split('h')
                        hours = int(parts[0])
                        duration_minutes = hours * 60
                        if len(parts) > 1 and 'm' in parts[1]:
                            minutes = int(parts[1].replace('m',''))
                            duration_minutes += minutes
                    elif 'm' in duration_str:
                        duration_minutes = int(duration_str.replace('m',''))
                    else: # Assume minutes if no unit
                        duration_minutes = int(duration_str)
                except (ValueError, TypeError):
                    log_warning("GoogleCalendarAPI", "create_event", f"Could not parse duration: {event_data.get('duration')}")
                    duration_minutes = None

            if not event_date:
                 log_error("GoogleCalendarAPI", "create_event", f"Missing mandatory 'date' field.")
                 return None

            start_obj = {}
            end_obj = {}
            time_zone = event_data.get("timeZone", self.user_timezone)

            if event_time:
                 try:
                     start_dt = datetime.strptime(f"{event_date} {event_time}", "%Y-%m-%d %H:%M")
                     delta = timedelta(minutes=duration_minutes if duration_minutes is not None else 30)
                     end_dt = start_dt + delta
                     start_obj = {"dateTime": start_dt.isoformat(), "timeZone": time_zone}
                     end_obj = {"dateTime": end_dt.isoformat(), "timeZone": time_zone}
                 except ValueError as time_err:
                      log_error("GoogleCalendarAPI", "create_event", f"Invalid date/time format: {event_date} {event_time}", time_err)
                      return None
            else: # All day
                try:
                    start_dt_date = datetime.strptime(event_date, "%Y-%m-%d").date()
                    end_date_dt = start_dt_date + timedelta(days=1)
                    start_obj = {"date": start_dt_date.strftime("%Y-%m-%d")}
                    end_obj = {"date": end_date_dt.strftime("%Y-%m-%d")}
                except ValueError as date_err:
                     log_error("GoogleCalendarAPI", "create_event", f"Invalid date format '{event_date}'", date_err)
                     return None

            google_event_body = {
                "summary": event_data.get("title", event_data.get("description", "New Item")),
                "description": event_data.get("description", ""),
                "start": start_obj,
                "end": end_obj
            }

            log_info("GoogleCalendarAPI", "create_event", f"Creating GCal event: {google_event_body.get('summary')}")
            created_event = self.service.events().insert(calendarId='primary', body=google_event_body).execute()
            google_event_id = created_event.get("id")

            if google_event_id:
                log_info("GoogleCalendarAPI", "create_event", f"Successfully created GCal event ID: {google_event_id}")
                return google_event_id
            else:
                log_error("GoogleCalendarAPI", "create_event", f"GCal response missing 'id'. Response: {created_event}")
                return None

        except HttpError as http_err:
             log_error("GoogleCalendarAPI", "create_event", f"HTTP error creating event: Status {http_err.resp.status}", http_err)
             return None
        except Exception as e:
            log_error("GoogleCalendarAPI", "create_event", f"Unexpected error creating event", e)
            return None


    def update_event(self, event_id: str, updates: Dict) -> bool:
        """Updates an existing event using PATCH."""
        if not self.is_active(): log_error("..."); return False; assert self.service is not None
        try:
            update_payload = {}
            needs_update = False
            time_zone = updates.get("timeZone", self.user_timezone)
            if "title" in updates: update_payload["summary"] = updates["title"]; needs_update = True
            if "description" in updates: update_payload["description"] = updates["description"]; needs_update = True

            new_date_str = updates.get("date"); new_time_str = updates.get("time")
            start_obj, end_obj = None, None
            # TODO: Add duration parsing/handling similar to create_event if needed for updates
            duration_minutes = None # Placeholder for duration logic

            if new_date_str:
                if new_time_str:
                    try:
                        start_dt = datetime.strptime(f"{new_date_str} {new_time_str}", "%Y-%m-%d %H:%M")
                        delta = timedelta(minutes=duration_minutes if duration_minutes is not None else 30)
                        end_dt = start_dt + delta
                        start_obj = {"dateTime": start_dt.isoformat(), "timeZone": time_zone}
                        end_obj = {"dateTime": end_dt.isoformat(), "timeZone": time_zone}
                    except ValueError as e: log_error("...", f"Invalid date/time on update: {e}"); start_obj, end_obj = None, None
                else: # All day
                    try:
                        start_dt_date = datetime.strptime(new_date_str, "%Y-%m-%d").date()
                        end_date_dt = start_dt_date + timedelta(days=1)
                        start_obj = {"date": start_dt_date.strftime("%Y-%m-%d")}
                        end_obj = {"date": end_date_dt.strftime("%Y-%m-%d")}
                    except ValueError as e: log_error("...", f"Invalid date on update: {e}"); start_obj, end_obj = None, None

                if start_obj and end_obj:
                    update_payload["start"] = start_obj; update_payload["end"] = end_obj; needs_update = True

            if not needs_update: log_info(...); return True
            log_info("GoogleCalendarAPI", "update_event", f"Patching GCal event {event_id} fields: {list(update_payload.keys())}")
            self.service.events().patch(calendarId='primary', eventId=event_id, body=update_payload).execute()
            log_info("GoogleCalendarAPI", "update_event", f"Successfully updated GCal event {event_id}")
            return True
        except HttpError as e: log_error(...); return False
        except Exception as e: log_error(...); return False


    def delete_event(self, event_id: str) -> bool:
        """Deletes an event."""
        if not self.is_active(): log_error("..."); return False; assert self.service is not None
        try:
            log_info("GoogleCalendarAPI", "delete_event", f"Deleting GCal event {event_id}")
            self.service.events().delete(calendarId='primary', eventId=event_id).execute()
            log_info("GoogleCalendarAPI", "delete_event", f"Successfully deleted GCal event {event_id}.")
            return True
        except HttpError as http_err:
            if http_err.resp.status in [404, 410]: log_warning(...); return True
            else: log_error(...); return False
        except Exception as e: log_error(...); return False


    def list_events(self, start_date: str, end_date: str) -> List[Dict]:
        """Lists events within a date range."""
        if not self.is_active(): log_error("..."); return []; assert self.service is not None
        try:
            start_dt = datetime.strptime(start_date, "%Y-%m-%d")
            end_dt = datetime.strptime(end_date, "%Y-%m-%d") + timedelta(days=1)
            time_min = start_dt.isoformat() + "Z"; time_max = end_dt.isoformat() + "Z"
        except ValueError: log_error(...); return []
        try:
            log_info("GoogleCalendarAPI", "list_events", f"Listing GCal events from {start_date} to {end_date}")
            all_items = []; page_token = None
            while True:
                events_result = self.service.events().list(
                    calendarId='primary', timeMin=time_min, timeMax=time_max,
                    singleEvents=True, orderBy='startTime', pageToken=page_token
                ).execute()
                items = events_result.get("items", []); all_items.extend(items)
                page_token = events_result.get('nextPageToken');
                if not page_token: break
            log_info("GoogleCalendarAPI", "list_events", f"Found {len(all_items)} GCal events.")
            return [self._parse_google_event(e) for e in all_items]
        except HttpError as e: log_error(...); return []
        except Exception as e: log_error(...); return []


    def _parse_google_event(self, event: Dict) -> Dict:
        """Parses relevant fields from a raw Google Calendar event."""
        start_info = event.get("start", {}); end_info = event.get("end", {})
        start_datetime = start_info.get("dateTime", start_info.get("date"))
        end_datetime = end_info.get("dateTime", end_info.get("date"))
        is_all_day = "date" in start_info and "dateTime" not in start_info
        parsed = {
            "event_id": event.get("id"), "title": event.get("summary", ""),
            "description": event.get("description", ""),
            "start_datetime": start_datetime, "end_datetime": end_datetime,
            "is_all_day": is_all_day, "gcal_link": event.get("htmlLink", ""),
            "status_gcal": event.get("status", ""),
            "created_gcal": event.get("created"), "updated_gcal": event.get("updated"),
        }
        return parsed
# --- END OF FILE tools\google_calendar_api.py ---


================================================================================
üìÑ tools\calendar_tool.py
================================================================================

# --- START OF FILE tools\calendar_tool.py ---
# tools/calendar_tool.py

import os
import requests
import json # For logging potentially
from fastapi import APIRouter, Request, Response
from fastapi.responses import HTMLResponse
from tools.logger import log_info, log_error, log_warning
from tools.encryption import encrypt_data # Need encrypt
import jwt
import requests.compat # Needed for urlencode in authenticate
from datetime import datetime # Keep if used elsewhere

# --- TEMPORARY PHASE 1 IMPORTS ---
# This direct import is specific to Phase 1 testing.
# Later phases might use a service (ConfigManager) to update preferences.
from users.user_registry import update_preferences as update_prefs_direct
# ----------------------------------

router = APIRouter()

# --- Configuration ---
GOOGLE_CLIENT_SECRET = os.getenv("GOOGLE_CLIENT_SECRET")
CLIENT_ID = os.getenv("GOOGLE_CLIENT_ID")
REDIRECT_URI = os.getenv("GOOGLE_REDIRECT_URI", "http://localhost:8000/oauth2callback")
SCOPE = "https://www.googleapis.com/auth/calendar"
TOKEN_URL = "https://oauth2.googleapis.com/token"
AUTH_URL_BASE = "https://accounts.google.com/o/oauth2/auth"

# Check essential config on load
if not GOOGLE_CLIENT_SECRET or not CLIENT_ID:
    log_error("calendar_tool", "config", "CRITICAL: GOOGLE_CLIENT_ID or GOOGLE_CLIENT_SECRET env var not set.")

log_info("calendar_tool", "config", f"Calendar Tool loaded. Client ID starts with: {str(CLIENT_ID)[:10]}")


# --- Core Authentication Check Function (Phase 1) ---
def authenticate(user_id: str, prefs: dict) -> dict:
    """
    Checks user's calendar auth status based on provided preferences (Phase 1).
    Returns status and auth URL if needed, or status/message if token exists.
    Does NOT attempt to load/test the token here.
    """
    log_info("calendar_tool", "authenticate", f"Checking auth status for user {user_id}")
    token_file_path = prefs.get("token_file")
    calendar_enabled = prefs.get("Calendar_Enabled", False)

    # Scenario 1: Token file exists and system thinks it's enabled
    if calendar_enabled and token_file_path and os.path.exists(token_file_path):
        log_info("calendar_tool", "authenticate", f"Token file exists and enabled flag is True for {user_id}.")
        # Report that token exists, actual validation happens when GCalAPI is initialized
        return {"status": "token_exists", "message": "Stored calendar credentials found. Attempting to use..."}

    # Scenario 2: Initiate new authorization
    else:
        log_info("calendar_tool", "authenticate", f"No valid token file found or not enabled for {user_id}. Initiating auth.")
        # Check essential config needed to generate URL
        if not CLIENT_ID or not REDIRECT_URI:
             log_error("calendar_tool", "authenticate", f"Client ID or Redirect URI missing for auth URL generation.")
             return {"status": "fails", "message": "Server configuration error prevents authentication."}

        normalized_state = user_id.replace("@c.us", "").replace("+","") # Basic normalization
        params = {
            "client_id": CLIENT_ID,
            "redirect_uri": REDIRECT_URI,
            "scope": SCOPE,
            "response_type": "code",
            "access_type": "offline", # Crucial for refresh token
            "state": normalized_state,
            "prompt": "consent" # Force consent screen for refresh token reliability
        }
        try:
             encoded_params = requests.compat.urlencode(params)
             auth_url = f"{AUTH_URL_BASE}?{encoded_params}"
             log_info("calendar_tool", "authenticate", f"Generated auth URL for {user_id}")
             return {"status": "pending", "message": f"Please authenticate your calendar by visiting this URL: {auth_url}"}
        except Exception as url_e:
             log_error("calendar_tool", "authenticate", f"Failed to build auth URL for {user_id}", url_e)
             return {"status": "fails", "message": "Failed to generate authentication URL."}


# --- OAuth Callback Endpoint (Cleaned "OLD STYLE" Logic) ---
@router.get("/oauth2callback", response_class=HTMLResponse)
async def oauth2callback(request: Request, code: str | None = None, state: str | None = None, error: str | None = None):
    """
    Handles the OAuth2 callback from Google.
    Exchanges code, saves token (WITHOUT immediate verification), updates registry.
    """
    html_error_template = "<html><body><h1>Authentication Error</h1><p>Details: {details}</p><p>Please try authenticating again or contact support if the issue persists.</p></body></html>"
    # Simple success message assuming token *will* work later
    html_success_template = "<html><body><h1>Authentication Successful!</h1><p>Your credentials have been saved. The connection will be fully tested when first used. You can close this window and return to the chat.</p></body></html>"

    if error:
        log_error("calendar_tool", "oauth2callback", f"OAuth error received from Google: {error}")
        return HTMLResponse(content=html_error_template.format(details=f"Google reported an error: {error}"), status_code=400)
    if not code or not state:
        log_error("calendar_tool", "oauth2callback", "Callback missing code or state.")
        return HTMLResponse(content=html_error_template.format(details="Invalid response received from Google (missing code or state)."), status_code=400)

    user_id = state
    log_info("calendar_tool", "oauth2callback", f"Callback received for user {user_id}.")

    if not GOOGLE_CLIENT_SECRET or not CLIENT_ID:
         log_error("calendar_tool", "oauth2callback", "Server configuration error: Client ID/Secret not set.")
         return HTMLResponse(content=html_error_template.format(details="Server configuration error."), status_code=500)

    try:
        # --- 1. Exchange Code for Tokens ---
        payload = {
            "code": code, "client_id": CLIENT_ID, "client_secret": GOOGLE_CLIENT_SECRET,
            "redirect_uri": REDIRECT_URI, "grant_type": "authorization_code"
        }
        log_info("calendar_tool", "oauth2callback", f"Exchanging authorization code for tokens for user {user_id}.")
        token_response = requests.post(TOKEN_URL, data=payload)
        token_response.raise_for_status()
        tokens = token_response.json()
        log_info("calendar_tool", "oauth2callback", f"Tokens received successfully. Keys: {list(tokens.keys())}")

        # --- Basic Token Checks ---
        if 'access_token' not in tokens:
             log_error("calendar_tool", "oauth2callback", f"Access token *NOT* received for user {user_id}.")
             return HTMLResponse(content=html_error_template.format(details="Failed to obtain access token from Google."), status_code=500)
        if 'refresh_token' not in tokens:
             log_warning("calendar_tool", "oauth2callback", f"Refresh token *NOT* received for user {user_id}. Offline access might fail later.")
        # --- End Basic Checks ---

        # --- Verification Block REMOVED ---

        # --- 3. Extract Email ---
        email = ""
        id_token = tokens.get("id_token")
        if id_token:
            try:
                decoded = jwt.decode(id_token, options={"verify_signature": False})
                email = decoded.get("email", "")
                log_info("calendar_tool", "oauth2callback", f"Extracted email '{email}' for user {user_id}.")
            except jwt.exceptions.DecodeError as jwt_e:
                 log_warning("calendar_tool", "oauth2callback", f"Failed to decode id_token for {user_id}, proceeding without email.", jwt_e)

        # --- 4. Encrypt and Save Tokens ---
        log_info("calendar_tool", "oauth2callback", f"Proceeding to encrypt and save token for {user_id} (no immediate verification).")
        encrypted_tokens = encrypt_data(tokens)
        if not encrypted_tokens:
             log_error("calendar_tool", "oauth2callback", f"Encryption failed for {user_id}'s tokens.")
             return HTMLResponse(content=html_error_template.format(details="Failed to secure credentials."), status_code=500)

        token_file_path = os.path.join("data", f"tokens_{user_id}.json.enc")
        try:
            os.makedirs(os.path.dirname(token_file_path), exist_ok=True)
            with open(token_file_path, "wb") as f: f.write(encrypted_tokens)
            log_info("calendar_tool", "oauth2callback", f"Tokens stored successfully for {user_id}.")

            # --- 5. Update Preferences (DIRECTLY in Registry - Phase 1 Only) ---
            prefs_update = {
                "email": email,
                "token_file": token_file_path,
                "Calendar_Enabled": True,
                "status": "active" # Set user active after successful save for Phase 1
            }
            try:
                update_prefs_direct(user_id, prefs_update)
                log_info("calendar_tool", "oauth2callback", f"Preferences updated DIRECTLY for {user_id}: {prefs_update}")
                # Return simple SUCCESS response to user
                return HTMLResponse(content=html_success_template, status_code=200)
            except Exception as pref_e:
                log_error("calendar_tool", "oauth2callback", f"Failed to update preferences registry for {user_id} after token save.", pref_e)
                return HTMLResponse(content=html_error_template.format(details="Credentials saved, but failed to update user profile. Contact support."), status_code=500)

        except IOError as io_e:
             log_error("calendar_tool", "oauth2callback", f"Failed to write token file {token_file_path}", io_e)
             return HTMLResponse(content=html_error_template.format(details="Failed to save credentials locally."), status_code=500)

    except requests.exceptions.HTTPError as http_e:
        response_text = http_e.response.text; status_code = http_e.response.status_code
        error_details = f"Error {status_code} during token exchange.";
        try: error_json = http_e.response.json(); error_details = error_json.get('error_description', error_json.get('error', f"HTTP {status_code}"))
        except ValueError: pass
        log_error("calendar_tool", "oauth2callback", f"HTTP error {status_code} during token exchange for {user_id}. Details: {error_details}", http_e)
        return HTMLResponse(content=html_error_template.format(details=f"Could not get authorization from Google: {error_details}."), status_code=status_code)
    except Exception as e:
        log_error("calendar_tool", "oauth2callback", f"Generic unexpected error during callback for {user_id}", e)
        return HTMLResponse(content=html_error_template.format(details=f"An unexpected server error occurred: {e}."), status_code=500)
# --- END OF FILE tools\calendar_tool.py ---


================================================================================
üìÑ tools\metadata_store.py
================================================================================

# --- START OF FILE tools\metadata_store.py ---
# --- START OF FILE tools/metadata_store.py ---

import csv
import os
from datetime import datetime
from tools.logger import log_info, log_error, log_warning

# --- Configuration ---
METADATA_DIR = "data"
METADATA_FILE = os.path.join(METADATA_DIR, "events_metadata.csv")

# --- UPDATED FIELDNAMES ---
FIELDNAMES = [
    "event_id",                 # Primary Key (Matches Google Calendar Event ID)
    "user_id",                  # User identifier
    "type",                     # 'task', 'reminder'
    "status",                   # 'pending', 'in progress', 'completed', 'cancelled'
    "project",                  # Associated project (or None)
    "progress",                 # Qualitative progress notes? (Potentially deprecated)
    "internal_reminder_minutes",# Reminder setting internal to the app (or None)
    "internal_reminder_sent",   # Flag if internal reminder was sent (True/False or None)
    "series_id",                # ID for recurring events (or None)
    "created_at",               # ISO timestamp when metadata record was created
    "completed_at",             # ISO timestamp when task was completed (or None)
    "original_date",            # Original requested date (e.g., 'tomorrow') (or None)
    "title",                    # Task/Reminder Title
    "description",              # Task/Reminder Description
    "date",                     # Due Date / Reminder Date (YYYY-MM-DD)
    "time",                     # Due Time / Reminder Time (HH:MM or None)
    "duration",                 # GCal event duration (e.g., meeting), NOT work estimate
    # --- NEW/Emphasized Task Fields ---
    "estimated_duration",       # Estimated total work time (e.g., "4h", "90m")
    "sessions_planned",         # Integer: Total work sessions needed/scheduled
    "sessions_completed",       # Integer: Work sessions completed
    "progress_percent",         # Integer: 0-100 calculated progress
    "session_event_ids"         # String: JSON encoded list of GCal IDs for work sessions ["id1", "id2"]
    # ----------------------------------
]
# --- END OF UPDATED FIELDNAMES ---

# --- Initialization ---
def init_metadata_store():
    """Ensures the data directory and metadata CSV file exist with headers."""
    try:
        os.makedirs(METADATA_DIR, exist_ok=True)
        # Check if file exists AND if header needs updating (simple check: read first line)
        header_correct = False
        if os.path.exists(METADATA_FILE):
             try:
                 with open(METADATA_FILE, "r", newline="", encoding="utf-8") as f_check:
                      # Check if file is not empty before reading header
                      first_line = f_check.readline()
                      if first_line:
                           reader = csv.reader([first_line])
                           header = next(reader)
                           header_correct = (header == FIELDNAMES)
                      elif not first_line: # File exists but is empty
                           header_correct = False # Needs header
             except Exception as read_err:
                 log_warning("metadata_store", "init_metadata_store", f"Could not read existing header: {read_err}")
                 header_correct = False # Assume incorrect if read fails

        if not os.path.exists(METADATA_FILE) or not header_correct:
            if os.path.exists(METADATA_FILE) and not header_correct:
                 log_warning("metadata_store", "init_metadata_store", f"Metadata file header mismatch or empty file. Creating/Overwriting {METADATA_FILE} with new headers.")
                 # Consider backing up the old file here if needed: os.rename(METADATA_FILE, METADATA_FILE + ".bak")
            else:
                 log_info("metadata_store", "init_metadata_store", f"Metadata file not found. Creating {METADATA_FILE} with headers.")

            with open(METADATA_FILE, "w", newline="", encoding="utf-8") as f:
                writer = csv.DictWriter(f, fieldnames=FIELDNAMES)
                writer.writeheader()
    except IOError as e:
        log_error("metadata_store", "init_metadata_store", f"Failed to initialize metadata store at {METADATA_FILE}", e)
        raise # Reraise critical error


# --- Core CRUD Functions ---

def save_event_metadata(event_metadata: dict):
    """
    Saves or updates a single event's metadata using atomic write.
    Assumes event_metadata has been prepared with all necessary fields.
    """
    init_metadata_store() # Ensure file exists with correct header

    event_id = event_metadata.get("event_id")
    if not event_id:
        log_error("metadata_store", "save_event_metadata", "Cannot save metadata: 'event_id' is missing.")
        raise KeyError("'event_id' is required in event_metadata dictionary.")

    all_events = []
    updated = False

    # Read existing data safely
    try:
        all_events = load_all_metadata()
    except Exception as load_e:
         log_error("metadata_store", "save_event_metadata", f"Error loading existing metadata before save: {load_e}. Proceeding cautiously.", load_e)
         all_events = []

    # Prepare the row to be saved, ensuring only valid fields are included
    row_to_save = {k: event_metadata.get(k) for k in FIELDNAMES}

    # Update or append
    for i, existing_event in enumerate(all_events):
        if existing_event.get("event_id") == event_id:
            log_info("metadata_store", "save_event_metadata", f"Updating existing metadata for event {event_id}")
            all_events[i] = row_to_save # Replace with the new prepared row
            updated = True
            break

    if not updated:
        log_info("metadata_store", "save_event_metadata", f"Adding new metadata for event {event_id}")
        all_events.append(row_to_save)

    # Write data back safely using atomic write
    temp_file_path = METADATA_FILE + ".tmp"
    try:
        with open(temp_file_path, "w", newline="", encoding="utf-8") as f:
            # Use extrasaction='ignore' just in case row_to_save somehow has extra keys
            writer = csv.DictWriter(f, fieldnames=FIELDNAMES, extrasaction='ignore')
            writer.writeheader()
            writer.writerows(all_events)
        os.replace(temp_file_path, METADATA_FILE)
        log_info("metadata_store", "save_event_metadata", f"Successfully saved metadata. Total records: {len(all_events)}")
    except (IOError, csv.Error, ValueError, TypeError) as e:
         log_error("metadata_store", "save_event_metadata", f"Error writing metadata file {METADATA_FILE}: {e}", e)
         if os.path.exists(temp_file_path):
             try: os.remove(temp_file_path)
             except OSError: pass
         raise # Reraise after logging
    except Exception as e:
         log_error("metadata_store", "save_event_metadata", f"Unexpected error writing metadata file {METADATA_FILE}", e)
         if os.path.exists(temp_file_path):
             try: os.remove(temp_file_path)
             except OSError: pass
         raise

def get_event_metadata(event_id: str) -> dict:
    """
    Retrieves metadata for a specific event ID. Returns empty dict if not found/error.
    """
    init_metadata_store()
    try:
        with open(METADATA_FILE, newline="", encoding="utf-8") as f:
            reader = csv.DictReader(f)
            for row in reader:
                if row.get("event_id") == event_id:
                    return row # Return the found row
        return {} # Not found
    except FileNotFoundError:
        return {} # File doesn't exist yet
    except Exception as e:
        log_error("metadata_store", "get_event_metadata", f"Error reading metadata file for event {event_id}", e)
        return {} # Return empty dict on error


def delete_event_metadata(event_id: str) -> bool:
    """Deletes metadata for a specific event ID. Rewrites the file."""
    init_metadata_store()
    if not event_id: return False

    all_events = []
    found = False
    try:
        all_events = load_all_metadata()
        original_count = len(all_events)
        # Filter out the event to delete
        filtered_events = [e for e in all_events if e.get("event_id") != event_id]
        found = len(filtered_events) < original_count
        if not found:
             log_warning("metadata_store", "delete_event_metadata", f"Event ID {event_id} not found. No deletion.")
             return True # Success if not found

        all_events = filtered_events # Assign filtered list back

    except Exception as load_e:
         log_error("metadata_store", "delete_event_metadata", f"Error loading metadata before delete: {load_e}", load_e)
         return False

    # Write the filtered data back
    temp_file_path = METADATA_FILE + ".tmp"
    try:
        with open(temp_file_path, "w", newline="", encoding="utf-8") as f:
            writer = csv.DictWriter(f, fieldnames=FIELDNAMES, extrasaction='ignore')
            writer.writeheader()
            writer.writerows(all_events)
        os.replace(temp_file_path, METADATA_FILE)
        log_info("metadata_store", "delete_event_metadata", f"Successfully deleted metadata for {event_id}. Remaining: {len(all_events)}")
        return True
    except Exception as e:
        log_error("metadata_store", "delete_event_metadata", f"Error writing metadata after delete for {event_id}", e)
        if os.path.exists(temp_file_path):
            try: os.remove(temp_file_path)
            except OSError: pass
        return False


def list_metadata(user_id: str, start_date_str: str | None = None, end_date_str: str | None = None) -> list[dict]:
    """Lists metadata for a user, optionally filtered by date."""
    init_metadata_store()
    results = []
    try:
        with open(METADATA_FILE, newline="", encoding="utf-8") as f:
            reader = csv.DictReader(f)
            for row in reader:
                if row.get("user_id") == user_id:
                    # Date filtering logic (unchanged)
                    include = True
                    row_date_str = row.get("date")
                    if row_date_str and (start_date_str or end_date_str): # Only parse if filtering
                        try:
                            row_date = datetime.strptime(row_date_str, "%Y-%m-%d").date()
                            if start_date_str:
                                start_date = datetime.strptime(start_date_str, "%Y-%m-%d").date()
                                if row_date < start_date: include = False
                            if include and end_date_str:
                                end_date = datetime.strptime(end_date_str, "%Y-%m-%d").date()
                                if row_date > end_date: include = False
                        except (ValueError, TypeError): include = False # Exclude unparseable dates when filtering
                    if include: results.append(row)
        return results
    except FileNotFoundError: return []
    except Exception as e:
        log_error("metadata_store", "list_metadata", f"Error reading metadata for user {user_id}", e)
        return []


def load_all_metadata() -> list[dict]:
    """Loads all records from the metadata CSV file."""
    init_metadata_store()
    try:
        with open(METADATA_FILE, newline="", encoding="utf-8") as f:
            first_char = f.read(1)
            if not first_char: return []
            f.seek(0)
            reader = csv.DictReader(f)
            # Convert to list to ensure file is read before returning
            data = list(reader)
            return data
    except FileNotFoundError: return []
    except Exception as e:
        log_error("metadata_store", "load_all_metadata", f"Error reading all metadata: {e}", e)
        return []


# --- Ensure store is initialized when module is loaded ---
init_metadata_store()

# --- END OF FILE tools/metadata_store.py ---
# --- END OF FILE tools\metadata_store.py ---


================================================================================
üìÑ tools\token_store.py
================================================================================

# --- START OF FILE tools\token_store.py ---
# tools/token_store.py

import os
import json
# Remove: from cryptography.fernet import Fernet # No longer needed here
from tools.encryption import decrypt_data # <-- IMPORT THIS
from tools.logger import log_info, log_error, log_warning # <-- Added log_error
from tools.encryption import decrypt_data, encrypt_data

TOKEN_DIR = "data" # Keep this if already defined

# Remove or comment out: ENCRYPTION_KEY = os.getenv("ENCRYPTION_KEY") # No longer needed here

def get_user_token(user_id: str) -> dict | None: # <-- Return type changed to dict | None
    """Loads and decrypts the user's token from the encrypted file."""
    path = os.path.join(TOKEN_DIR, f"tokens_{user_id}.json.enc")
    if not os.path.exists(path):
        # This is not necessarily an error, just means no token yet.
        log_info("token_store", "get_user_token", f"No token file found for user {user_id} at {path}")
        return None # <-- Return None if file not found

    try:
        with open(path, "rb") as f:
            encrypted_data = f.read()

        # Use the new decrypt function
        token_data = decrypt_data(encrypted_data) # <-- USE THIS

        if token_data:
            # Log a summary of the token data without exposing full details.
            log_info("token_store", "get_user_token", f"Decrypted token data keys for user {user_id}: {list(token_data.keys())}")
            return token_data
        else:
            # Decryption failed (logged within decrypt_data)
            log_error("token_store", "get_user_token", f"Failed to decrypt token for user {user_id} from {path}.")
            # Consider deleting the corrupted file? os.remove(path)
            return None

    except FileNotFoundError:
         log_info("token_store", "get_user_token", f"Token file not found for user {user_id} at {path} (race condition?).")
         return None
    except Exception as e:
        # Catch any other unexpected errors during file reading etc.
        log_error("token_store", "get_user_token", f"Unexpected error loading token for user {user_id} from {path}: {str(e)}", e)
        return None


def save_user_token_encrypted(user_id: str, token_data: dict) -> bool:
    """
    Encrypts and saves the user's token data to a file using atomic write.

    Args:
        user_id: The user's identifier.
        token_data: The dictionary containing token information. It expects keys like
                    'access_token', 'refresh_token', 'token_uri', etc. It handles if
                    the access token key is 'token' coming from the Credentials object.

    Returns:
        True if saving was successful, False otherwise.
    """
    log_info("token_store", "save_user_token_encrypted", f"Attempting to save token for user {user_id}")
    path = os.path.join(TOKEN_DIR, f"tokens_{user_id}.json.enc")
    temp_path = path + ".tmp" # Temporary file path

    try:
        # Standardize access token key to 'access_token' before saving
        data_to_save = token_data.copy()
        if 'token' in data_to_save and 'access_token' not in data_to_save:
            data_to_save['access_token'] = data_to_save.pop('token')

        # Basic validation: Check if critical tokens are present AFTER standardization
        if not data_to_save.get('access_token'):
             log_error("token_store", "save_user_token_encrypted", f"Attempted to save token data missing access_token for {user_id}.")
             return False
        if not data_to_save.get('refresh_token'):
             # Allow saving even if refresh token is missing, but log warning.
             log_warning("token_store", "save_user_token_encrypted", f"Saving token data potentially missing refresh_token for {user_id}.")

        # Encrypt the standardized data
        encrypted_tokens = encrypt_data(data_to_save)
        if not encrypted_tokens:
            # Error already logged by encrypt_data
            log_error("token_store", "save_user_token_encrypted", f"Encryption failed for {user_id}'s tokens during save.")
            return False

        # Ensure directory exists
        os.makedirs(os.path.dirname(path), exist_ok=True)

        # Write to temporary file first
        with open(temp_path, "wb") as f:
            f.write(encrypted_tokens)

        # Atomically replace original file with temp file
        os.replace(temp_path, path)

        log_info("token_store", "save_user_token_encrypted", f"Token stored successfully for {user_id} at {path}.")
        return True

    except Exception as e:
        log_error("token_store", "save_user_token_encrypted", f"Failed to write token file {path}", e)
        # Clean up temp file if it exists after an error
        if os.path.exists(temp_path):
            try: os.remove(temp_path)
            except OSError as rm_err: log_error("...", f"Failed to remove temp file {temp_path}: {rm_err}")
        return False
# --- END OF FILE tools\token_store.py ---


================================================================================
üìÑ tools\encryption.py
================================================================================

# --- START OF FILE tools\encryption.py ---
# tools/encryption.py

import os
import json
from cryptography.fernet import Fernet, InvalidToken
from tools.logger import log_error, log_info

# --- Key Management ---
# Load the encryption key from environment variables
# CRITICAL: This key MUST be kept secret and secure.
# Generate one using generate_key() below and set it as an environment variable.
ENCRYPTION_KEY_ENV_VAR = "ENCRYPTION_KEY"
_encryption_key = os.getenv(ENCRYPTION_KEY_ENV_VAR)

if not _encryption_key:
    log_error("encryption", "__init__",
              f"CRITICAL ERROR: Environment variable '{ENCRYPTION_KEY_ENV_VAR}' not set. Encryption disabled.")
    # You might want to raise an exception here to halt execution
    # raise ValueError(f"Environment variable '{ENCRYPTION_KEY_ENV_VAR}' is required for encryption.")
    _fernet = None
else:
    try:
        # Ensure the key is bytes
        _key_bytes = _encryption_key.encode('utf-8')
        _fernet = Fernet(_key_bytes)
        log_info("encryption", "__init__", "Fernet encryption service initialized successfully.")
    except Exception as e:
        log_error("encryption", "__init__", f"Failed to initialize Fernet. Invalid key format? Error: {e}", e)
        _fernet = None
        # raise ValueError(f"Invalid encryption key format: {e}") # Optional: Halt execution

# --- Encryption/Decryption Functions ---

def encrypt_data(data: dict) -> bytes | None:
    """
    Encrypts a dictionary using Fernet.

    Args:
        data: The dictionary to encrypt.

    Returns:
        Encrypted bytes if successful, None otherwise.
    """
    if not _fernet:
        log_error("encryption", "encrypt_data", "Encryption service not available (key missing or invalid).")
        return None
    try:
        # Serialize the dictionary to a JSON string, then encode to bytes
        data_bytes = json.dumps(data).encode('utf-8')
        encrypted_data = _fernet.encrypt(data_bytes)
        return encrypted_data
    except Exception as e:
        log_error("encryption", "encrypt_data", f"Encryption failed: {e}", e)
        return None

def decrypt_data(encrypted_data: bytes) -> dict | None:
    """
    Decrypts data encrypted with Fernet back into a dictionary.

    Args:
        encrypted_data: The encrypted bytes to decrypt.

    Returns:
        The original dictionary if successful, None otherwise.
    """
    if not _fernet:
        log_error("encryption", "decrypt_data", "Encryption service not available (key missing or invalid).")
        return None
    try:
        decrypted_bytes = _fernet.decrypt(encrypted_data)
        # Decode bytes back to JSON string, then parse into dictionary
        decrypted_json = decrypted_bytes.decode('utf-8')
        original_data = json.loads(decrypted_json)
        return original_data
    except InvalidToken:
        log_error("encryption", "decrypt_data", "Decryption failed: Invalid token (key mismatch or data corrupted).")
        return None
    except Exception as e:
        log_error("encryption", "decrypt_data", f"Decryption failed: {e}", e)
        return None

# --- Key Generation Utility ---

def generate_key() -> str:
    """Generates a new Fernet key (URL-safe base64 encoded)."""
    return Fernet.generate_key().decode('utf-8')

# Example usage for generating a key (run this file directly: python -m tools.encryption)
if __name__ == "__main__":
    new_key = generate_key()
    print("Generated Fernet Key (set this as your ENCRYPTION_KEY environment variable):")
    print(new_key)
    print("\nWARNING: Keep this key secure and secret!")
# --- END OF FILE tools\encryption.py ---


================================================================================
üìÑ tools\logger.py
================================================================================

# --- START OF FILE tools\logger.py ---
import os
import pytz
from datetime import datetime
import traceback

# === Config ===
DEBUG_MODE = True  # Set to False in production
LOG_DIR = "logs"
LOG_FILE = os.path.join(LOG_DIR, "whats_tasker.log")

# Ensure logs directory exists
os.makedirs(LOG_DIR, exist_ok=True)

# === Helpers ===
def _timestamp():
    tz = pytz.timezone("Asia/Jerusalem")
    return datetime.now(tz).strftime("%Y-%m-%d %H:%M:%S")

def _format_log(level: str, module: str, func: str, message: str):
    return f"[{_timestamp()}] [{level}] [{module}:{func}] {message}"

# === Log functions ===
def log_info(module: str, func: str, message: str):
    entry = _format_log("INFO", module, func, message)
    if DEBUG_MODE:
        print(entry)

def log_error(module: str, func: str, message: str, exception: Exception = None):
    entry = _format_log("ERROR", module, func, message)
    if DEBUG_MODE:
        print(entry)
    else:
        with open(LOG_FILE, "a", encoding="utf-8") as f:
            f.write(entry + "\n")
            if exception:
                f.write(traceback.format_exc())

def log_warning(module: str, func: str, message: str, exception: Exception = None):
    entry = _format_log("WARNING", module, func, message)
    if DEBUG_MODE:
        print(entry)
    else:
        with open(LOG_FILE, "a", encoding="utf-8") as f:
            f.write(entry + "\n")
            if exception:
                f.write(traceback.format_exc())
# --- END OF FILE tools\logger.py ---


================================================================================
üìÑ users\user_manager.py
================================================================================

# --- START OF FILE users\user_manager.py ---
# --- START OF FILE users/user_manager.py ---

import os
import re
from datetime import datetime, timedelta
from typing import Dict, Any, Optional, List

from tools.logger import log_info, log_error, log_warning
from users.user_registry import get_registry, register_user, get_user_preferences

# --- State Manager Import ---
try:
    from services.agent_state_manager import (
        register_agent_instance,
        get_agent_state,
        initialize_state_store
    )
    AGENT_STATE_MANAGER_IMPORTED = True
except ImportError:
     log_error("user_manager", "import", "AgentStateManager not found.")
     AGENT_STATE_MANAGER_IMPORTED = False
     _user_agents_in_memory = {} # Local fallback
     def register_agent_instance(uid, state): _user_agents_in_memory[uid] = state
     def get_agent_state(uid): return _user_agents_in_memory.get(uid)
     def initialize_state_store(ref): global _user_agents_in_memory; _user_agents_in_memory = ref

# --- REMOVED LLM Chain Builders Import (No chains built here anymore) ---
# try:
#     from langchain_chains.chain_builders import build_intent_chain
#     CHAINS_IMPORTED = True
#     log_info("user_manager", "import", "Successfully imported Intent chain builder.")
# except ImportError as e:
#     CHAINS_IMPORTED = False
#     log_warning("user_manager","import", f"Chain builders import failed: {e}. Intent chain unavailable.")
#     build_intent_chain = lambda: None

# --- Service/Tool Imports ---
try:
    from tools.google_calendar_api import GoogleCalendarAPI
    GCAL_API_IMPORTED = True
except ImportError: GCAL_API_IMPORTED = False; log_warning("user_manager","import", "GoogleCalendarAPI not found.")
try:
    from tools.metadata_store import list_metadata
    METADATA_STORE_IMPORTED = True
    log_info("user_manager", "import", "Successfully imported metadata_store.")
except ImportError:
    METADATA_STORE_IMPORTED = False; list_metadata = lambda *a, **k: []
    log_error("user_manager", "import", "metadata_store not found.")


# --- In-Memory State Dictionary Reference ---
_user_agents_in_memory: Dict[str, Dict[str, Any]] = {}
if AGENT_STATE_MANAGER_IMPORTED:
    initialize_state_store(_user_agents_in_memory)

# --- REMOVED LLM Component Creation Function ---
# def _create_agent_llm_components(user_id: str) -> dict:
#     """Initializes LLM chains ONLY for the Intent agent."""
#     # ... (function removed) ...


# --- Preload Context (Unchanged) ---
def _preload_initial_context(user_id: str) -> list:
    """Loads initial context ONLY from the metadata store during startup."""
    log_info("user_manager", "_preload_initial_context", f"Preloading initial context for {user_id} from metadata store.")
    if not METADATA_STORE_IMPORTED:
        log_error("user_manager", "_preload_initial_context", "Metadata store not imported. Cannot preload context.")
        return []
    try:
        metadata_list = list_metadata(user_id=user_id)
        log_info("user_manager", "_preload_initial_context", f"Preloaded {len(metadata_list)} items from metadata store for {user_id}. GCal data requires sync.")
        return metadata_list
    except Exception as e:
        log_error("user_manager", "_preload_initial_context", f"Error preloading context for {user_id} from metadata", e)
        return []


# --- Agent State Creation (MODIFIED - Removed LLM components) ---
def create_and_register_agent_state(user_id: str) -> Optional[Dict]:
    """Creates the full agent state dictionary and registers it."""
    log_info("user_manager", "create_and_register_agent_state", f"Creating FULL agent state for {user_id}")
    norm_user_id = re.sub(r'\D', '', user_id)
    register_user(norm_user_id) # Ensures user exists in registry
    preferences = get_user_preferences(norm_user_id)
    if not preferences:
        log_error("user_manager", "create_and_register_agent_state", f"Failed to get prefs for {norm_user_id} after registration attempt.")
        return None # Critical if prefs can't be loaded/created

    # Init GCal API Instance
    calendar_api_instance = None
    if GCAL_API_IMPORTED and preferences.get("Calendar_Enabled"):
        token_file = preferences.get("token_file")
        if token_file and os.path.exists(token_file):
            log_info("user_manager", "create_and_register_agent_state", f"Attempting GCalAPI init for {norm_user_id}")
            try:
                calendar_api_instance = GoogleCalendarAPI(norm_user_id)
                if not calendar_api_instance.is_active():
                    log_warning("user_manager", "create_and_register_agent_state", f"GCalAPI inactive for {norm_user_id} after init.")
                    calendar_api_instance = None # Treat inactive as None
                else:
                    log_info("user_manager", "create_and_register_agent_state", f"GCalAPI initialized successfully for {norm_user_id}")
            except Exception as cal_e:
                log_error("user_manager", "create_and_register_agent_state", f"Error initializing GCalAPI for {norm_user_id}", cal_e)
                calendar_api_instance = None
        else:
            if preferences.get("Calendar_Enabled"): # Log only if enabled but no token
                 log_warning("user_manager", "create_and_register_agent_state", f"GCal enabled for {norm_user_id} but token file missing/invalid: {token_file}")
    # else: log if GCal not enabled or API lib missing? Maybe too verbose.

    # --- REMOVED LLM Component Initialization Call ---
    # llm_components = _create_agent_llm_components(norm_user_id)

    # Preload initial task context
    initial_context = _preload_initial_context(norm_user_id)

    # Assemble the full agent state dictionary
    agent_state = {
        "user_id": norm_user_id,
        "preferences": preferences,
        # --- REMOVED "llm_components" key ---
        "active_tasks_context": initial_context,
        "calendar": calendar_api_instance,
        "conversation_history": [] # Start with empty history
    }

    # Register state using AgentStateManager
    if AGENT_STATE_MANAGER_IMPORTED:
        try:
            register_agent_instance(norm_user_id, agent_state)
            log_info("user_manager", "create_and_register_agent_state", f"Successfully registered agent state for {norm_user_id}")
            return agent_state
        except Exception as e:
            log_error("user_manager", "create_and_register_agent_state", f"Failed state registration for {norm_user_id}", e)
            return None
    else: # Fallback if state manager didn't import
         _user_agents_in_memory[norm_user_id] = agent_state
         log_warning("user_manager", "create_and_register_agent_state", f"Stored state locally for {norm_user_id} (no state manager).")
         return agent_state


# --- init_all_agents (Unchanged) ---
def init_all_agents():
    log_info("user_manager", "init_all_agents", "Initializing states for all registered users...")
    registry_data = get_registry(); registered_users = list(registry_data.keys())
    initialized_count = 0; failed_count = 0
    if not registered_users:
        log_info("user_manager", "init_all_agents", "No users found in registry.")
        return
    log_info("user_manager", "init_all_agents", f"Found {len(registered_users)} users. Initializing...")
    for user_id in registered_users:
        try:
            norm_user_id = re.sub(r'\D', '', user_id)
            created_state = create_and_register_agent_state(norm_user_id)
            if created_state: initialized_count += 1
            else:
                log_error("user_manager", "init_all_agents", f"Failed to create/register state for user {user_id}")
                failed_count += 1
        except Exception as e:
            log_error("user_manager", "init_all_agents", f"Unexpected error initializing agent for user {user_id}", e)
            failed_count += 1
    log_info("user_manager", "init_all_agents", f"Init complete. Success: {initialized_count}, Failed: {failed_count}")


# --- get_agent (Unchanged) ---
def get_agent(user_id: str) -> Optional[Dict]:
    norm_user_id = re.sub(r'\D', '', user_id)
    agent_state = None
    try:
        agent_state = get_agent_state(norm_user_id)
        if not agent_state:
            log_warning("user_manager", "get_agent", f"State for {norm_user_id} not in memory. Creating now.")
            agent_state = create_and_register_agent_state(norm_user_id)
    except Exception as e:
         log_error("user_manager", "get_agent", f"Error retrieving/creating agent state for {norm_user_id}", e)
         agent_state = None
    # Return the state dictionary (or None if creation failed)
    return agent_state

# --- END OF FILE users/user_manager.py ---
# --- END OF FILE users\user_manager.py ---


================================================================================
üìÑ users\user_registry.py
================================================================================

# --- START OF FILE users\user_registry.py ---
# --- START OF FILE users/user_registry.py ---

import json
import os
from datetime import datetime
from tools.logger import log_info, log_error, log_warning

USER_REGISTRY_PATH = "data/users/registry.json"

# --- UPDATED Default Preferences ---
DEFAULT_PREFERENCES = {
    "status": "new",
    "Work_Days": ["Monday", "Tuesday", "Wednesday", "Thursday", "Friday"], # Default, can be changed later
    "Morning_Summary_Time": None , # Keep field, but don't collect in onboarding
    "Evening_Summary_Time": None , # Keep field, but don't collect in onboarding
    "Enable_Morning": True, # Default enabled
    "Enable_Evening": True, # Default enabled
    "TimeZone": None, # REQUIRED during onboarding
    "Calendar_Enabled": False,
    "Enable_Weekly_Reflection": False,
    "Calendar_Type": "",
    "Last_Sync": "",
    "Holiday_Dates": [],
    "token_file": None,
    "Work_Start_Time": None, # REQUIRED during onboarding
    "Work_End_Time": None,   # REQUIRED during onboarding
    "Preferred_Session_Length": None # REQUIRED during onboarding
}
# --- END OF UPDATED DEFAULT PREFERENCES ---


# Global in-memory registry variable.
_registry = {}

def load_registry():
    """Loads the registry from disk into memory."""
    global _registry
    if os.path.exists(USER_REGISTRY_PATH):
        try:
            with open(USER_REGISTRY_PATH, "r", encoding="utf-8") as f:
                # Handle empty file case
                content = f.read()
                if not content.strip():
                    _registry = {}
                else:
                    f.seek(0) # Go back to start if not empty
                    _registry = json.load(f)
        except (json.JSONDecodeError, IOError) as e:
            log_error("user_registry", "load_registry", f"Failed to load registry file {USER_REGISTRY_PATH}", e)
            _registry = {} # Fallback to empty registry on error
    else:
        _registry = {}
    log_info("user_registry", "load_registry", f"Registry loaded with {len(_registry)} users.")
    return _registry

def get_registry():
    """Returns the in-memory registry. Loads it if not already loaded."""
    global _registry
    # Check if registry is empty dictionary, not just None/False
    if not _registry and not os.path.exists(USER_REGISTRY_PATH):
        load_registry() # Attempt load only if it might exist but isn't loaded
    elif not _registry and os.path.exists(USER_REGISTRY_PATH):
         load_registry() # Load if file exists but memory is empty
    # If still empty after load attempt, it's truly empty or failed load
    return _registry

# Compatibility alias
def load_registered_users():
    return get_registry()

def save_registry():
    """Saves the current in-memory registry to disk."""
    global _registry
    try:
        # Ensure directory exists
        os.makedirs(os.path.dirname(USER_REGISTRY_PATH), exist_ok=True)
        with open(USER_REGISTRY_PATH, "w", encoding="utf-8") as f:
            json.dump(_registry, f, indent=2, ensure_ascii=False) # Use indent and ensure_ascii=False
        # log_info("user_registry", "save_registry", "Registry saved to disk.") # Can be noisy, maybe remove
    except IOError as e:
        log_error("user_registry", "save_registry", f"Failed to write registry file {USER_REGISTRY_PATH}", e)
    except Exception as e:
        log_error("user_registry", "save_registry", f"Unexpected error saving registry", e)


def register_user(user_id):
    """Registers a new user with default preferences if not already present."""
    reg = get_registry() # Ensures registry is loaded
    if user_id not in reg:
        log_info("user_registry", "register_user", f"Registering new user {user_id}...")
        reg[user_id] = {"preferences": DEFAULT_PREFERENCES.copy()}
        # Create user-specific directory (optional, maybe remove if not used)
        # try:
        #     os.makedirs(f"data/users/{user_id}", exist_ok=True)
        # except OSError as e:
        #     log_warning("user_registry", "register_user", f"Could not create user directory for {user_id}: {e}")
        save_registry() # Save after adding the new user
        log_info("user_registry", "register_user", f"Registered new user {user_id} with default preferences.")
    else:
        # No need to log this every time, usually called by get_agent
        # log_info("user_registry", "register_user", f"User {user_id} is already registered.")
        pass

def update_preferences(user_id, new_preferences):
    """Updates preferences for a given user and saves the registry."""
    reg = get_registry()
    if user_id in reg:
        # Ensure the preferences key exists
        if "preferences" not in reg[user_id]:
             reg[user_id]["preferences"] = DEFAULT_PREFERENCES.copy()
        # Update with new values
        reg[user_id]["preferences"].update(new_preferences)
        save_registry() # Save after updating
        log_info("user_registry", "update_preferences", f"Updated preferences for user {user_id}: {list(new_preferences.keys())}")
        return True
    else:
        log_error("user_registry", "update_preferences", f"User {user_id} not registered, cannot update preferences.")
        return False

def get_user_preferences(user_id):
    """Gets preferences for a user, returns None if user not found."""
    reg = get_registry()
    user_data = reg.get(user_id)
    if user_data:
        # Ensure preferences exist, provide defaults if missing (shouldn't happen if register_user is always called first)
        if "preferences" not in user_data:
             log_warning("user_registry", "get_user_preferences", f"User {user_id} found but missing 'preferences' key. Returning defaults.")
             # Optionally update registry with defaults here?
             # user_data["preferences"] = DEFAULT_PREFERENCES.copy()
             # save_registry()
             return DEFAULT_PREFERENCES.copy()
        return user_data.get("preferences") # Return the actual preferences dict
    else:
        # Log only if user genuinely not found
        # log_info("user_registry", "get_user_preferences", f"User {user_id} not found in registry.")
        return None

# Load registry into memory on module import.
load_registry()

if __name__ == "__main__":
    # Simple test cases
    test_user_id = "111222333"
    print(f"--- Testing User Registry ---")
    print(f"Initial Registry Keys: {list(get_registry().keys())}")
    register_user(test_user_id)
    print(f"Registry Keys after register: {list(get_registry().keys())}")
    prefs = get_user_preferences(test_user_id)
    print(f"\nInitial Prefs for {test_user_id}:")
    print(json.dumps(prefs, indent=2))

    print(f"\nUpdating Prefs for {test_user_id}...")
    update_success = update_preferences(test_user_id, {
        "Morning_Summary_Time": "08:45",
        "Work_End_Time": "18:00",
        "Preferred_Session_Length": "45m"
    })
    print(f"Update successful: {update_success}")
    updated_prefs = get_user_preferences(test_user_id)
    print(f"\nUpdated Prefs for {test_user_id}:")
    print(json.dumps(updated_prefs, indent=2))

    print(f"\nGetting Prefs for non-existent user:")
    non_existent_prefs = get_user_preferences("00000")
    print(f"Result: {non_existent_prefs}")
    print(f"--- Test Complete ---")

# --- END OF FILE users/user_registry.py ---
# --- END OF FILE users\user_registry.py ---


================================================================================
üìÑ tests\mock_sender.py
================================================================================

# --- START OF FILE tests\mock_sender.py ---
# --- START OF FILE tests/mock_sender.py ---
import requests
import sys
import os
import json
import time
import threading

# Add project root to sys.path
project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
sys.path.insert(0, project_root)

# Use the project logger, but primarily for errors/warnings in this script
from tools.logger import log_info, log_error, log_warning

DEFAULT_USER = "972547778005" # Example default
BASE_URL = "http://localhost:8000"
INCOMING_URL = f"{BASE_URL}/incoming"
OUTGOING_URL = f"{BASE_URL}/outgoing"
ACK_URL = f"{BASE_URL}/ack"

_stop_polling = threading.Event()

def poll_for_messages(user_id_raw: str):
    """Polls the /outgoing endpoint and prints messages."""
    log_info("mock_sender", "poll_thread", f"Polling thread started for user {user_id_raw}.") # Keep start message
    session = requests.Session()
    connection_lost = False # Flag to track connection state

    while not _stop_polling.is_set():
        try:
            res = session.get(OUTGOING_URL, timeout=10)
            res.raise_for_status() # Raise HTTPError for bad responses (4xx or 5xx)

            # If we reach here, connection is successful
            if connection_lost:
                print("[SYSTEM]: Connection to server restored.")
                log_warning("mock_sender", "poll_thread", "Connection restored.") # Use warning for visibility
                connection_lost = False

            data = res.json()
            messages = data.get("messages", [])

            if messages:
                for msg in messages:
                    print(f"\n[BOT]: {msg.get('message', '[No message content]')}")
                    try:
                        ack_payload = {"message_id": msg.get("message_id"), "user_id": msg.get("user_id")}
                        ack_res = session.post(ACK_URL, json=ack_payload, timeout=5)
                        if ack_res.status_code != 200:
                             log_warning("mock_sender", "poll_thread", f"Failed ACK for {msg.get('message_id')}. Status: {ack_res.status_code}")
                    except Exception as ack_e:
                         log_warning("mock_sender", "poll_thread", f"Error sending ACK for {msg.get('message_id')}: {ack_e}")
                print(f"[YOU]: ", end="", flush=True)

        except requests.exceptions.Timeout:
             if not connection_lost: # Log only the first time timeout happens after connection was okay
                 log_warning("mock_sender", "poll_thread", "Polling request timed out.")
                 print("[SYSTEM]: Polling timed out...")
                 connection_lost = True # Assume connection might be shaky
             time.sleep(2) # Wait longer on timeout
        except requests.exceptions.RequestException as e:
             if not connection_lost: # Only log the first time connection fails
                 error_msg = f"Connection error: {e}"
                 log_error("mock_sender", "poll_thread", error_msg)
                 print(f"[SYSTEM]: {error_msg}")
                 connection_lost = True
             time.sleep(5) # Wait significantly longer if connection refused/error
        except json.JSONDecodeError as e:
             log_error("mock_sender", "poll_thread", f"Failed to decode JSON response: {e}. Response text: {res.text[:100]}")
             print("[SYSTEM]: Received invalid response from server.")
             time.sleep(2)
        except Exception as e:
             # Log unexpected errors within the loop
             log_error("mock_sender", "poll_thread", f"Unexpected error in polling thread: {e}", e)
             if not connection_lost:
                 print(f"[SYSTEM]: Unexpected polling error: {e}")
                 connection_lost = True # Assume connection issue on unexpected error
             time.sleep(5) # Wait longer

        # Wait before polling again only if the loop didn't sleep in except block
        if not connection_lost:
            time.sleep(0.5) # Poll slightly faster when connection is okay

    log_info("mock_sender", "poll_thread", "Polling thread stopped.")


def send_mock_message(user_id_raw: str, message: str):
    """Sends message, expects only ACK back directly."""
    payload = {"user_id": user_id_raw, "message": message}
    # log_info("mock_sender", "send_mock_message", f"Sending to {user_id_raw}: {message}") # REMOVED
    try:
        res = requests.post(INCOMING_URL, json=payload, timeout=15)
        res.raise_for_status()
        response_data = res.json()
        if not response_data.get("ack"):
             # Log if ack is missing, as it indicates a potential server-side issue
             log_warning("mock_sender", "send_mock_message", f"Server response did not contain expected ack: {response_data}")

    except requests.exceptions.RequestException as e:
        # Log actual errors
        log_error("mock_sender", "send_mock_message", f"Failed to send message", e)
        print(f"[SYSTEM]: Error connecting to server to send message: {e}")
    except json.JSONDecodeError:
        # Log actual errors
        log_error("mock_sender", "send_mock_message", f"Received non-JSON ACK response: {res.text}")
        print(f"[SYSTEM ERROR]: Received invalid ACK response from server.")
    except Exception as e:
        # Log actual errors
        log_error("mock_sender", "send_mock_message", f"Unexpected error sending message", e)
        print(f"[SYSTEM]: Unexpected error sending message: {e}")


def main():
    default_display_user = DEFAULT_USER
    user_input_raw = input(f"Enter user ID (default: {default_display_user}): ").strip()
    user_id_to_send = user_input_raw if user_input_raw else default_display_user

    print(f"--- Mock Sender for User: {user_id_to_send} ---")
    print("Polling for messages... Type your message. Use :exit to quit.")

    polling_thread = threading.Thread(target=poll_for_messages, args=(user_id_to_send,), daemon=True)
    polling_thread.start()

    while True:
        try:
            msg = input(f"[YOU]: ")
            if msg.strip().lower() == ":exit":
                break
            if msg.strip() == "": continue
            send_mock_message(user_id_to_send, msg)
        except (EOFError, KeyboardInterrupt):
            print("\nCtrl+C or EOF detected.")
            break

    print("\nStopping polling thread...")
    _stop_polling.set()
    polling_thread.join(timeout=2)
    print("Mock chat ended.")

if __name__ == "__main__":
     try: log_info("mock_sender", "main", "Starting mock sender...") # Keep startup message
     except NameError: print("FATAL ERROR: Logger not loaded."); sys.exit(1)
     except Exception as e: print(f"FATAL ERROR during logging setup: {e}"); sys.exit(1)
     main()

# --- END OF FILE tests/mock_sender.py ---
# --- END OF FILE tests\mock_sender.py ---


